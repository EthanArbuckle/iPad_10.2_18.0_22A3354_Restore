uint64_t vImage.PixelBuffer<>.init(planarBuffers:)@<X0>(_QWORD *a1@<X0>, uint64_t *a2@<X8>)
{
  _QWORD *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  void *v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  _QWORD *v18;
  uint64_t v19;
  __int128 v20;
  uint64_t v21;
  __int128 v22;
  uint64_t result;
  uint64_t v24;
  _OWORD v25[2];
  _OWORD v26[2];
  uint64_t v27;

  v27 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 2)
  {
    __break(1u);
    goto LABEL_18;
  }
  v4 = (_QWORD *)a1[4];
  if (!v4[2])
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v7 = (_QWORD *)a1[5];
  if (!v7[2])
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v9)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (v6 != v9)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v10 = swift_allocObject();
  *(_OWORD *)(v10 + 16) = xmmword_1CAB5E430;
  v11 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v6, 0x40u);
  v13 = v12;
  v15 = v14;
  v17 = v16;
  type metadata accessor for vImage.BufferReference();
  v18 = (_QWORD *)swift_allocObject();
  v18[2] = v11;
  v18[3] = v13;
  v18[4] = v15;
  v18[5] = v17;
  *(_QWORD *)(v10 + 32) = v11;
  *(_QWORD *)(v10 + 40) = v13;
  *(_QWORD *)(v10 + 48) = v15;
  *(_QWORD *)(v10 + 56) = v17;
  *(_QWORD *)(v10 + 64) = v18;
  v24 = v10;
  v19 = a1[4];
  if (!*(_QWORD *)(v19 + 16))
  {
LABEL_30:
    __break(1u);
LABEL_31:
    __break(1u);
  }
  v20 = *(_OWORD *)(v19 + 48);
  v26[0] = *(_OWORD *)(v19 + 32);
  v26[1] = v20;
  v21 = a1[5];
  if (!*(_QWORD *)(v21 + 16))
    goto LABEL_31;
  v22 = *(_OWORD *)(v21 + 48);
  v25[0] = *(_OWORD *)(v21 + 32);
  v25[1] = v22;
  closure #1 in closure #1 in vImage.PixelBuffer<>.init(planarBuffers:)((uint64_t)v25, (uint64_t)v26, (uint64_t)&v24);
  result = swift_bridgeObjectRelease();
  *a2 = v10;
  return result;
}

{
  _QWORD *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  _QWORD *v10;
  uint64_t v11;
  uint64_t v12;
  _QWORD *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  vImagePixelCount v18;
  vImagePixelCount v19;
  vImagePixelCount v20;
  vImagePixelCount v21;
  size_t v22;
  size_t v23;
  _QWORD *v24;
  uint64_t v25;
  __int128 v26;
  uint64_t v27;
  __int128 v28;
  uint64_t v29;
  __int128 v30;
  uint64_t v31;
  __int128 v32;
  uint64_t result;
  vImage_Buffer srcB;
  vImage_Buffer srcG;
  vImage_Buffer srcR;
  vImage_Buffer srcA;
  vImage_Buffer dest;
  uint64_t v39;

  v39 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_34;
  }
  v4 = (_QWORD *)a1[4];
  if (!v4[2])
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v5)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v7 = (_QWORD *)a1[5];
  if (!v7[2])
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v8)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v9)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (v5 != v8)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (v6 != v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v10 = (_QWORD *)a1[6];
  if (!v10[2])
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (!v11)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v12)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (v5 != v11)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v6 != v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  v13 = (_QWORD *)a1[7];
  if (!v13[2])
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  v14 = v13[6];
  if (v14 < 0)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  v15 = v13[5];
  if (v15 < 0)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (!v14)
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  if (!v15)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  if (v5 != v14)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (v6 != v15)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v16 = swift_allocObject();
  *(_OWORD *)(v16 + 16) = xmmword_1CAB5E430;
  v17 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v6, 0x80u);
  v19 = v18;
  v21 = v20;
  v23 = v22;
  type metadata accessor for vImage.BufferReference();
  v24 = (_QWORD *)swift_allocObject();
  v24[2] = v17;
  v24[3] = v19;
  v24[4] = v21;
  v24[5] = v23;
  *(_QWORD *)(v16 + 32) = v17;
  *(_QWORD *)(v16 + 40) = v19;
  *(_QWORD *)(v16 + 48) = v21;
  *(_QWORD *)(v16 + 56) = v23;
  *(_QWORD *)(v16 + 64) = v24;
  dest.data = v17;
  dest.height = v19;
  dest.width = v21;
  dest.rowBytes = v23;
  v25 = a1[4];
  if (!*(_QWORD *)(v25 + 16))
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  v26 = *(_OWORD *)(v25 + 48);
  *(_OWORD *)&srcA.data = *(_OWORD *)(v25 + 32);
  *(_OWORD *)&srcA.width = v26;
  v27 = a1[5];
  if (!*(_QWORD *)(v27 + 16))
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  v28 = *(_OWORD *)(v27 + 48);
  *(_OWORD *)&srcR.data = *(_OWORD *)(v27 + 32);
  *(_OWORD *)&srcR.width = v28;
  v29 = a1[6];
  if (!*(_QWORD *)(v29 + 16))
  {
LABEL_62:
    __break(1u);
LABEL_63:
    __break(1u);
  }
  v30 = *(_OWORD *)(v29 + 48);
  *(_OWORD *)&srcG.data = *(_OWORD *)(v29 + 32);
  *(_OWORD *)&srcG.width = v30;
  v31 = a1[7];
  if (!*(_QWORD *)(v31 + 16))
    goto LABEL_63;
  v32 = *(_OWORD *)(v31 + 48);
  *(_OWORD *)&srcB.data = *(_OWORD *)(v31 + 32);
  *(_OWORD *)&srcB.width = v32;
  vImageConvert_PlanarFtoARGBFFFF(&srcA, &srcR, &srcG, &srcB, &dest, 0);
  result = swift_bridgeObjectRelease();
  *a2 = v16;
  return result;
}

{
  _QWORD *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  void *v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  _QWORD *v18;
  uint64_t v19;
  __int128 v20;
  uint64_t v21;
  __int128 v22;
  uint64_t result;
  uint64_t v24;
  _OWORD v25[2];
  _OWORD v26[2];
  uint64_t v27;

  v27 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 2)
  {
    __break(1u);
    goto LABEL_18;
  }
  v4 = (_QWORD *)a1[4];
  if (!v4[2])
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v7 = (_QWORD *)a1[5];
  if (!v7[2])
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v9)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (v6 != v9)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v10 = swift_allocObject();
  *(_OWORD *)(v10 + 16) = xmmword_1CAB5E430;
  v11 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v6, 0x10u);
  v13 = v12;
  v15 = v14;
  v17 = v16;
  type metadata accessor for vImage.BufferReference();
  v18 = (_QWORD *)swift_allocObject();
  v18[2] = v11;
  v18[3] = v13;
  v18[4] = v15;
  v18[5] = v17;
  *(_QWORD *)(v10 + 32) = v11;
  *(_QWORD *)(v10 + 40) = v13;
  *(_QWORD *)(v10 + 48) = v15;
  *(_QWORD *)(v10 + 56) = v17;
  *(_QWORD *)(v10 + 64) = v18;
  v24 = v10;
  v19 = a1[4];
  if (!*(_QWORD *)(v19 + 16))
  {
LABEL_30:
    __break(1u);
LABEL_31:
    __break(1u);
  }
  v20 = *(_OWORD *)(v19 + 48);
  v26[0] = *(_OWORD *)(v19 + 32);
  v26[1] = v20;
  v21 = a1[5];
  if (!*(_QWORD *)(v21 + 16))
    goto LABEL_31;
  v22 = *(_OWORD *)(v21 + 48);
  v25[0] = *(_OWORD *)(v21 + 32);
  v25[1] = v22;
  closure #1 in closure #1 in vImage.PixelBuffer<>.init(planarBuffers:)((uint64_t)v25, (uint64_t)v26, (uint64_t)&v24);
  result = swift_bridgeObjectRelease();
  *a2 = v10;
  return result;
}

{
  _QWORD *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  _QWORD *v10;
  uint64_t v11;
  uint64_t v12;
  _QWORD *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  vImagePixelCount v18;
  vImagePixelCount v19;
  vImagePixelCount v20;
  vImagePixelCount v21;
  size_t v22;
  size_t v23;
  _QWORD *v24;
  uint64_t v25;
  __int128 v26;
  uint64_t v27;
  __int128 v28;
  uint64_t v29;
  __int128 v30;
  uint64_t v31;
  __int128 v32;
  uint64_t result;
  vImage_Buffer srcB;
  vImage_Buffer srcG;
  vImage_Buffer srcR;
  vImage_Buffer srcA;
  vImage_Buffer dest;
  uint64_t v39;

  v39 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_34;
  }
  v4 = (_QWORD *)a1[4];
  if (!v4[2])
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v5)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v7 = (_QWORD *)a1[5];
  if (!v7[2])
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v8)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v9)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (v5 != v8)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (v6 != v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v10 = (_QWORD *)a1[6];
  if (!v10[2])
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (!v11)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v12)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (v5 != v11)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v6 != v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  v13 = (_QWORD *)a1[7];
  if (!v13[2])
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  v14 = v13[6];
  if (v14 < 0)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  v15 = v13[5];
  if (v15 < 0)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (!v14)
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  if (!v15)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  if (v5 != v14)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (v6 != v15)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v16 = swift_allocObject();
  *(_OWORD *)(v16 + 16) = xmmword_1CAB5E430;
  v17 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v6, 0x20u);
  v19 = v18;
  v21 = v20;
  v23 = v22;
  type metadata accessor for vImage.BufferReference();
  v24 = (_QWORD *)swift_allocObject();
  v24[2] = v17;
  v24[3] = v19;
  v24[4] = v21;
  v24[5] = v23;
  *(_QWORD *)(v16 + 32) = v17;
  *(_QWORD *)(v16 + 40) = v19;
  *(_QWORD *)(v16 + 48) = v21;
  *(_QWORD *)(v16 + 56) = v23;
  *(_QWORD *)(v16 + 64) = v24;
  dest.data = v17;
  dest.height = v19;
  dest.width = v21;
  dest.rowBytes = v23;
  v25 = a1[4];
  if (!*(_QWORD *)(v25 + 16))
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  v26 = *(_OWORD *)(v25 + 48);
  *(_OWORD *)&srcA.data = *(_OWORD *)(v25 + 32);
  *(_OWORD *)&srcA.width = v26;
  v27 = a1[5];
  if (!*(_QWORD *)(v27 + 16))
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  v28 = *(_OWORD *)(v27 + 48);
  *(_OWORD *)&srcR.data = *(_OWORD *)(v27 + 32);
  *(_OWORD *)&srcR.width = v28;
  v29 = a1[6];
  if (!*(_QWORD *)(v29 + 16))
  {
LABEL_62:
    __break(1u);
LABEL_63:
    __break(1u);
  }
  v30 = *(_OWORD *)(v29 + 48);
  *(_OWORD *)&srcG.data = *(_OWORD *)(v29 + 32);
  *(_OWORD *)&srcG.width = v30;
  v31 = a1[7];
  if (!*(_QWORD *)(v31 + 16))
    goto LABEL_63;
  v32 = *(_OWORD *)(v31 + 48);
  *(_OWORD *)&srcB.data = *(_OWORD *)(v31 + 32);
  *(_OWORD *)&srcB.width = v32;
  vImageConvert_Planar8toARGB8888(&srcA, &srcR, &srcG, &srcB, &dest, 0);
  result = swift_bridgeObjectRelease();
  *a2 = v16;
  return result;
}

{
  _QWORD *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  _QWORD *v10;
  uint64_t v11;
  uint64_t v12;
  _QWORD *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  vImagePixelCount v18;
  vImagePixelCount v19;
  vImagePixelCount v20;
  vImagePixelCount v21;
  size_t v22;
  size_t v23;
  _QWORD *v24;
  uint64_t v25;
  __int128 v26;
  uint64_t v27;
  __int128 v28;
  uint64_t v29;
  __int128 v30;
  uint64_t v31;
  __int128 v32;
  uint64_t result;
  vImage_Buffer blue;
  vImage_Buffer green;
  vImage_Buffer red;
  vImage_Buffer alpha;
  vImage_Buffer dest;
  uint64_t v39;

  v39 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_34;
  }
  v4 = (_QWORD *)a1[4];
  if (!v4[2])
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v5)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v7 = (_QWORD *)a1[5];
  if (!v7[2])
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v8)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v9)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (v5 != v8)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (v6 != v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v10 = (_QWORD *)a1[6];
  if (!v10[2])
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (!v11)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v12)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (v5 != v11)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v6 != v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  v13 = (_QWORD *)a1[7];
  if (!v13[2])
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  v14 = v13[6];
  if (v14 < 0)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  v15 = v13[5];
  if (v15 < 0)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (!v14)
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  if (!v15)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  if (v5 != v14)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (v6 != v15)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v16 = swift_allocObject();
  *(_OWORD *)(v16 + 16) = xmmword_1CAB5E430;
  v17 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v6, 0x20u);
  v19 = v18;
  v21 = v20;
  v23 = v22;
  type metadata accessor for vImage.BufferReference();
  v24 = (_QWORD *)swift_allocObject();
  v24[2] = v17;
  v24[3] = v19;
  v24[4] = v21;
  v24[5] = v23;
  *(_QWORD *)(v16 + 32) = v17;
  *(_QWORD *)(v16 + 40) = v19;
  *(_QWORD *)(v16 + 48) = v21;
  *(_QWORD *)(v16 + 56) = v23;
  *(_QWORD *)(v16 + 64) = v24;
  dest.data = v17;
  dest.height = v19;
  dest.width = v21;
  dest.rowBytes = v23;
  v25 = a1[4];
  if (!*(_QWORD *)(v25 + 16))
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  v26 = *(_OWORD *)(v25 + 48);
  *(_OWORD *)&alpha.data = *(_OWORD *)(v25 + 32);
  *(_OWORD *)&alpha.width = v26;
  v27 = a1[5];
  if (!*(_QWORD *)(v27 + 16))
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  v28 = *(_OWORD *)(v27 + 48);
  *(_OWORD *)&red.data = *(_OWORD *)(v27 + 32);
  *(_OWORD *)&red.width = v28;
  v29 = a1[6];
  if (!*(_QWORD *)(v29 + 16))
  {
LABEL_62:
    __break(1u);
LABEL_63:
    __break(1u);
  }
  v30 = *(_OWORD *)(v29 + 48);
  *(_OWORD *)&green.data = *(_OWORD *)(v29 + 32);
  *(_OWORD *)&green.width = v30;
  v31 = a1[7];
  if (!*(_QWORD *)(v31 + 16))
    goto LABEL_63;
  v32 = *(_OWORD *)(v31 + 48);
  *(_OWORD *)&blue.data = *(_OWORD *)(v31 + 32);
  *(_OWORD *)&blue.width = v32;
  vImageConvert_PlanarFToARGB8888(&alpha, &red, &green, &blue, &dest, flt_1E84EDAF8, flt_1E84EDAC8, 0);
  result = swift_bridgeObjectRelease();
  *a2 = v16;
  return result;
}

{
  _QWORD *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  _QWORD *v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  void *v14;
  vImagePixelCount v15;
  vImagePixelCount v16;
  vImagePixelCount v17;
  vImagePixelCount v18;
  size_t v19;
  size_t v20;
  _QWORD *v21;
  uint64_t v22;
  __int128 v23;
  uint64_t v24;
  __int128 v25;
  uint64_t v26;
  __int128 v27;
  uint64_t result;
  vImage_Buffer planarBlue;
  vImage_Buffer planarGreen;
  vImage_Buffer planarRed;
  vImage_Buffer rgbDest;
  uint64_t v33;

  v33 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 3)
  {
    __break(1u);
    goto LABEL_26;
  }
  v4 = (_QWORD *)a1[4];
  if (!v4[2])
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (!v5)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v6)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v7 = (_QWORD *)a1[5];
  if (!v7[2])
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v8)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (!v9)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (v5 != v8)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (v6 != v9)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v10 = (_QWORD *)a1[6];
  if (!v10[2])
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v11)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v12)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (v5 != v11)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (v6 != v12)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v13 = swift_allocObject();
  *(_OWORD *)(v13 + 16) = xmmword_1CAB5E430;
  v14 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v6, 0x18u);
  v16 = v15;
  v18 = v17;
  v20 = v19;
  type metadata accessor for vImage.BufferReference();
  v21 = (_QWORD *)swift_allocObject();
  v21[2] = v14;
  v21[3] = v16;
  v21[4] = v18;
  v21[5] = v20;
  *(_QWORD *)(v13 + 32) = v14;
  *(_QWORD *)(v13 + 40) = v16;
  *(_QWORD *)(v13 + 48) = v18;
  *(_QWORD *)(v13 + 56) = v20;
  *(_QWORD *)(v13 + 64) = v21;
  rgbDest.data = v14;
  rgbDest.height = v16;
  rgbDest.width = v18;
  rgbDest.rowBytes = v20;
  v22 = a1[4];
  if (!*(_QWORD *)(v22 + 16))
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v23 = *(_OWORD *)(v22 + 48);
  *(_OWORD *)&planarRed.data = *(_OWORD *)(v22 + 32);
  *(_OWORD *)&planarRed.width = v23;
  v24 = a1[5];
  if (!*(_QWORD *)(v24 + 16))
  {
LABEL_46:
    __break(1u);
LABEL_47:
    __break(1u);
  }
  v25 = *(_OWORD *)(v24 + 48);
  *(_OWORD *)&planarGreen.data = *(_OWORD *)(v24 + 32);
  *(_OWORD *)&planarGreen.width = v25;
  v26 = a1[6];
  if (!*(_QWORD *)(v26 + 16))
    goto LABEL_47;
  v27 = *(_OWORD *)(v26 + 48);
  *(_OWORD *)&planarBlue.data = *(_OWORD *)(v26 + 32);
  *(_OWORD *)&planarBlue.width = v27;
  vImageConvert_Planar8toRGB888(&planarRed, &planarGreen, &planarBlue, &rgbDest, 0);
  result = swift_bridgeObjectRelease();
  *a2 = v13;
  return result;
}

{
  _QWORD *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  _QWORD *v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  void *v14;
  vImagePixelCount v15;
  vImagePixelCount v16;
  vImagePixelCount v17;
  vImagePixelCount v18;
  size_t v19;
  size_t v20;
  _QWORD *v21;
  uint64_t v22;
  __int128 v23;
  uint64_t v24;
  __int128 v25;
  uint64_t v26;
  __int128 v27;
  uint64_t result;
  vImage_Buffer planarBlue;
  vImage_Buffer planarGreen;
  vImage_Buffer planarRed;
  vImage_Buffer rgbDest;
  uint64_t v33;

  v33 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 3)
  {
    __break(1u);
    goto LABEL_26;
  }
  v4 = (_QWORD *)a1[4];
  if (!v4[2])
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (!v5)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v6)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v7 = (_QWORD *)a1[5];
  if (!v7[2])
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v8)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (!v9)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (v5 != v8)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (v6 != v9)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v10 = (_QWORD *)a1[6];
  if (!v10[2])
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v11)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v12)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (v5 != v11)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (v6 != v12)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v13 = swift_allocObject();
  *(_OWORD *)(v13 + 16) = xmmword_1CAB5E430;
  v14 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v6, 0x60u);
  v16 = v15;
  v18 = v17;
  v20 = v19;
  type metadata accessor for vImage.BufferReference();
  v21 = (_QWORD *)swift_allocObject();
  v21[2] = v14;
  v21[3] = v16;
  v21[4] = v18;
  v21[5] = v20;
  *(_QWORD *)(v13 + 32) = v14;
  *(_QWORD *)(v13 + 40) = v16;
  *(_QWORD *)(v13 + 48) = v18;
  *(_QWORD *)(v13 + 56) = v20;
  *(_QWORD *)(v13 + 64) = v21;
  rgbDest.data = v14;
  rgbDest.height = v16;
  rgbDest.width = v18;
  rgbDest.rowBytes = v20;
  v22 = a1[4];
  if (!*(_QWORD *)(v22 + 16))
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v23 = *(_OWORD *)(v22 + 48);
  *(_OWORD *)&planarRed.data = *(_OWORD *)(v22 + 32);
  *(_OWORD *)&planarRed.width = v23;
  v24 = a1[5];
  if (!*(_QWORD *)(v24 + 16))
  {
LABEL_46:
    __break(1u);
LABEL_47:
    __break(1u);
  }
  v25 = *(_OWORD *)(v24 + 48);
  *(_OWORD *)&planarGreen.data = *(_OWORD *)(v24 + 32);
  *(_OWORD *)&planarGreen.width = v25;
  v26 = a1[6];
  if (!*(_QWORD *)(v26 + 16))
    goto LABEL_47;
  v27 = *(_OWORD *)(v26 + 48);
  *(_OWORD *)&planarBlue.data = *(_OWORD *)(v26 + 32);
  *(_OWORD *)&planarBlue.width = v27;
  vImageConvert_PlanarFtoRGBFFF(&planarRed, &planarGreen, &planarBlue, &rgbDest, 0);
  result = swift_bridgeObjectRelease();
  *a2 = v13;
  return result;
}

{
  _QWORD *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  _QWORD *v10;
  uint64_t v11;
  uint64_t v12;
  _QWORD *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  vImagePixelCount v18;
  vImagePixelCount v19;
  vImagePixelCount v20;
  vImagePixelCount v21;
  size_t v22;
  size_t v23;
  _QWORD *v24;
  uint64_t v25;
  __int128 v26;
  uint64_t v27;
  __int128 v28;
  uint64_t v29;
  __int128 v30;
  uint64_t v31;
  __int128 v32;
  uint64_t result;
  vImage_Buffer bSrc;
  vImage_Buffer gSrc;
  vImage_Buffer rSrc;
  vImage_Buffer aSrc;
  vImage_Buffer argbDest;
  uint64_t v39;

  v39 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_34;
  }
  v4 = (_QWORD *)a1[4];
  if (!v4[2])
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v5)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v7 = (_QWORD *)a1[5];
  if (!v7[2])
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v8)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v9)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (v5 != v8)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (v6 != v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v10 = (_QWORD *)a1[6];
  if (!v10[2])
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (!v11)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v12)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (v5 != v11)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v6 != v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  v13 = (_QWORD *)a1[7];
  if (!v13[2])
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  v14 = v13[6];
  if (v14 < 0)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  v15 = v13[5];
  if (v15 < 0)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (!v14)
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  if (!v15)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  if (v5 != v14)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (v6 != v15)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v16 = swift_allocObject();
  *(_OWORD *)(v16 + 16) = xmmword_1CAB5E430;
  v17 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v6, 0x40u);
  v19 = v18;
  v21 = v20;
  v23 = v22;
  type metadata accessor for vImage.BufferReference();
  v24 = (_QWORD *)swift_allocObject();
  v24[2] = v17;
  v24[3] = v19;
  v24[4] = v21;
  v24[5] = v23;
  *(_QWORD *)(v16 + 32) = v17;
  *(_QWORD *)(v16 + 40) = v19;
  *(_QWORD *)(v16 + 48) = v21;
  *(_QWORD *)(v16 + 56) = v23;
  *(_QWORD *)(v16 + 64) = v24;
  argbDest.data = v17;
  argbDest.height = v19;
  argbDest.width = v21;
  argbDest.rowBytes = v23;
  v25 = a1[4];
  if (!*(_QWORD *)(v25 + 16))
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  v26 = *(_OWORD *)(v25 + 48);
  *(_OWORD *)&aSrc.data = *(_OWORD *)(v25 + 32);
  *(_OWORD *)&aSrc.width = v26;
  v27 = a1[5];
  if (!*(_QWORD *)(v27 + 16))
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  v28 = *(_OWORD *)(v27 + 48);
  *(_OWORD *)&rSrc.data = *(_OWORD *)(v27 + 32);
  *(_OWORD *)&rSrc.width = v28;
  v29 = a1[6];
  if (!*(_QWORD *)(v29 + 16))
  {
LABEL_62:
    __break(1u);
LABEL_63:
    __break(1u);
  }
  v30 = *(_OWORD *)(v29 + 48);
  *(_OWORD *)&gSrc.data = *(_OWORD *)(v29 + 32);
  *(_OWORD *)&gSrc.width = v30;
  v31 = a1[7];
  if (!*(_QWORD *)(v31 + 16))
    goto LABEL_63;
  v32 = *(_OWORD *)(v31 + 48);
  *(_OWORD *)&bSrc.data = *(_OWORD *)(v31 + 32);
  *(_OWORD *)&bSrc.width = v32;
  vImageConvert_Planar16UtoARGB16U(&aSrc, &rSrc, &gSrc, &bSrc, &argbDest, 0);
  result = swift_bridgeObjectRelease();
  *a2 = v16;
  return result;
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>.init(planarBuffers:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t inited;
  const vImage_Buffer **v7;
  uint64_t result;
  _QWORD *v9;
  uint64_t v10;
  vImagePixelCount v11;
  vImagePixelCount v12;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<vImage_Buffer>?>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5E440;
  *(_QWORD *)(inited + 32) = a2;
  v7 = (const vImage_Buffer **)(inited + 32);
  *(_QWORD *)(inited + 40) = a1;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutableRawPointer?>);
  result = swift_initStackObject();
  *(_OWORD *)(result + 16) = xmmword_1CAB5E440;
  v9 = *(_QWORD **)a3;
  if (!*(_QWORD *)(*(_QWORD *)a3 + 16))
  {
    __break(1u);
    goto LABEL_7;
  }
  *(_QWORD *)(result + 32) = v9[4];
  v10 = v9[4];
  if (v10)
  {
    *(_QWORD *)(result + 40) = v10 + 4;
    v11 = v9[6];
    if ((v11 & 0x8000000000000000) == 0)
    {
      v12 = v9[5];
      if ((v12 & 0x8000000000000000) == 0)
      {
        vImageConvert_PlanarToChunkyF(v7, (void **)(result + 32), 2u, 8uLL, v11, v12, v9[7], 0);
        swift_bridgeObjectRelease();
        return swift_bridgeObjectRelease();
      }
      goto LABEL_8;
    }
LABEL_7:
    __break(1u);
LABEL_8:
    __break(1u);
  }
  __break(1u);
  return result;
}

{
  uint64_t inited;
  const vImage_Buffer **v7;
  uint64_t result;
  _QWORD *v9;
  uint64_t v10;
  vImagePixelCount v11;
  vImagePixelCount v12;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<vImage_Buffer>?>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5E440;
  *(_QWORD *)(inited + 32) = a2;
  v7 = (const vImage_Buffer **)(inited + 32);
  *(_QWORD *)(inited + 40) = a1;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutableRawPointer?>);
  result = swift_initStackObject();
  *(_OWORD *)(result + 16) = xmmword_1CAB5E440;
  v9 = *(_QWORD **)a3;
  if (!*(_QWORD *)(*(_QWORD *)a3 + 16))
  {
    __break(1u);
    goto LABEL_7;
  }
  *(_QWORD *)(result + 32) = v9[4];
  v10 = v9[4];
  if (v10)
  {
    *(_QWORD *)(result + 40) = v10 + 1;
    v11 = v9[6];
    if ((v11 & 0x8000000000000000) == 0)
    {
      v12 = v9[5];
      if ((v12 & 0x8000000000000000) == 0)
      {
        vImageConvert_PlanarToChunky8(v7, (void **)(result + 32), 2u, 2uLL, v11, v12, v9[7], 0);
        swift_bridgeObjectRelease();
        return swift_bridgeObjectRelease();
      }
      goto LABEL_8;
    }
LABEL_7:
    __break(1u);
LABEL_8:
    __break(1u);
  }
  __break(1u);
  return result;
}

_QWORD *vImage.PixelBuffer<>.planarBuffers()()
{
  uint64_t v0;
  _QWORD *v1;
  _QWORD *v2;
  unint64_t v3;
  uint64_t v4;
  __int128 v5;
  uint64_t v6;
  __int128 v7;
  uint64_t v9;
  char v10[16];
  _OWORD v11[2];
  _OWORD v12[2];
  uint64_t v13;

  v13 = *MEMORY[0x1E0C80C00];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5Tm(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v10, 0, 2, (uint64_t)&v9, (uint64_t (*)(BOOL, unint64_t, uint64_t))specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:));
  if (!*(_QWORD *)(*(_QWORD *)v0 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  v2 = v1;
  v3 = v1[2];
  if (!v3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v4 = v1[4];
  if (!*(_QWORD *)(v4 + 16))
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  v5 = *(_OWORD *)(v4 + 48);
  v12[0] = *(_OWORD *)(v4 + 32);
  v12[1] = v5;
  if (v3 < 2)
  {
LABEL_10:
    __break(1u);
LABEL_11:
    __break(1u);
  }
  v6 = v1[5];
  if (!*(_QWORD *)(v6 + 16))
    goto LABEL_11;
  v7 = *(_OWORD *)(v6 + 48);
  v11[0] = *(_OWORD *)(v6 + 32);
  v11[1] = v7;
  closure #1 in closure #1 in closure #2 in vImage.PixelBuffer<>.planarBuffers()((uint64_t)v11, v0, (uint64_t)v12);
  return v2;
}

{
  uint64_t *v0;
  _QWORD *v1;
  uint64_t v2;
  _QWORD *v3;
  __int128 v4;
  unint64_t v5;
  uint64_t v6;
  __int128 v7;
  uint64_t v8;
  __int128 v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v12;
  __int128 v13;
  uint64_t v15;
  char v16[16];
  vImage_Buffer blue;
  vImage_Buffer green;
  vImage_Buffer red;
  vImage_Buffer alpha;
  vImage_Buffer src;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7Planar8VGs5NeverOTgq5(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v16, 0, 4, (uint64_t)&v15);
  v2 = *v0;
  if (!*(_QWORD *)(*v0 + 16))
  {
    __break(1u);
    goto LABEL_12;
  }
  v3 = v1;
  v4 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&src.width = v4;
  v5 = v1[2];
  if (!v5)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v6 = v1[4];
  if (!*(_QWORD *)(v6 + 16))
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  v7 = *(_OWORD *)(v6 + 48);
  *(_OWORD *)&alpha.data = *(_OWORD *)(v6 + 32);
  *(_OWORD *)&alpha.width = v7;
  if (v5 < 2)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  v8 = v1[5];
  if (!*(_QWORD *)(v8 + 16))
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v9 = *(_OWORD *)(v8 + 48);
  *(_OWORD *)&red.data = *(_OWORD *)(v8 + 32);
  *(_OWORD *)&red.width = v9;
  if (v5 < 3)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v10 = v1[6];
  if (!*(_QWORD *)(v10 + 16))
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v11 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&green.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&green.width = v11;
  if (v5 < 4)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
  }
  v12 = v1[7];
  if (!*(_QWORD *)(v12 + 16))
    goto LABEL_19;
  v13 = *(_OWORD *)(v12 + 48);
  *(_OWORD *)&blue.data = *(_OWORD *)(v12 + 32);
  *(_OWORD *)&blue.width = v13;
  vImageConvert_ARGBFFFFtoPlanar8(&src, &alpha, &red, &green, &blue, flt_1E84EDCE0, flt_1E84EDCB0, 0);
  return v3;
}

{
  uint64_t *v0;
  _QWORD *v1;
  uint64_t v2;
  _QWORD *v3;
  __int128 v4;
  unint64_t v5;
  uint64_t v6;
  __int128 v7;
  uint64_t v8;
  __int128 v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v12;
  __int128 v13;
  uint64_t v15;
  char v16[16];
  vImage_Buffer destB;
  vImage_Buffer destG;
  vImage_Buffer destR;
  vImage_Buffer destA;
  vImage_Buffer srcARGB;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v16, 0, 4, (uint64_t)&v15);
  v2 = *v0;
  if (!*(_QWORD *)(*v0 + 16))
  {
    __break(1u);
    goto LABEL_12;
  }
  v3 = v1;
  v4 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&srcARGB.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&srcARGB.width = v4;
  v5 = v1[2];
  if (!v5)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v6 = v1[4];
  if (!*(_QWORD *)(v6 + 16))
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  v7 = *(_OWORD *)(v6 + 48);
  *(_OWORD *)&destA.data = *(_OWORD *)(v6 + 32);
  *(_OWORD *)&destA.width = v7;
  if (v5 < 2)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  v8 = v1[5];
  if (!*(_QWORD *)(v8 + 16))
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v9 = *(_OWORD *)(v8 + 48);
  *(_OWORD *)&destR.data = *(_OWORD *)(v8 + 32);
  *(_OWORD *)&destR.width = v9;
  if (v5 < 3)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v10 = v1[6];
  if (!*(_QWORD *)(v10 + 16))
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v11 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&destG.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&destG.width = v11;
  if (v5 < 4)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
  }
  v12 = v1[7];
  if (!*(_QWORD *)(v12 + 16))
    goto LABEL_19;
  v13 = *(_OWORD *)(v12 + 48);
  *(_OWORD *)&destB.data = *(_OWORD *)(v12 + 32);
  *(_OWORD *)&destB.width = v13;
  vImageConvert_ARGBFFFFtoPlanarF(&srcARGB, &destA, &destR, &destG, &destB, 0);
  return v3;
}

{
  uint64_t v0;
  _QWORD *v1;
  _QWORD *v2;
  unint64_t v3;
  uint64_t v4;
  __int128 v5;
  uint64_t v6;
  __int128 v7;
  uint64_t v9;
  char v10[16];
  _OWORD v11[2];
  _OWORD v12[2];
  uint64_t v13;

  v13 = *MEMORY[0x1E0C80C00];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7Planar8VGs5NeverOTgq5(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v10, 0, 2, (uint64_t)&v9);
  if (!*(_QWORD *)(*(_QWORD *)v0 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  v2 = v1;
  v3 = v1[2];
  if (!v3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v4 = v1[4];
  if (!*(_QWORD *)(v4 + 16))
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  v5 = *(_OWORD *)(v4 + 48);
  v12[0] = *(_OWORD *)(v4 + 32);
  v12[1] = v5;
  if (v3 < 2)
  {
LABEL_10:
    __break(1u);
LABEL_11:
    __break(1u);
  }
  v6 = v1[5];
  if (!*(_QWORD *)(v6 + 16))
    goto LABEL_11;
  v7 = *(_OWORD *)(v6 + 48);
  v11[0] = *(_OWORD *)(v6 + 32);
  v11[1] = v7;
  closure #1 in closure #1 in closure #2 in vImage.PixelBuffer<>.planarBuffers()((uint64_t)v11, v0, (uint64_t)v12);
  return v2;
}

{
  uint64_t *v0;
  _QWORD *v1;
  uint64_t v2;
  _QWORD *v3;
  __int128 v4;
  unint64_t v5;
  uint64_t v6;
  __int128 v7;
  uint64_t v8;
  __int128 v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v12;
  __int128 v13;
  uint64_t v15;
  char v16[16];
  vImage_Buffer destB;
  vImage_Buffer destG;
  vImage_Buffer destR;
  vImage_Buffer destA;
  vImage_Buffer srcARGB;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7Planar8VGs5NeverOTgq5(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v16, 0, 4, (uint64_t)&v15);
  v2 = *v0;
  if (!*(_QWORD *)(*v0 + 16))
  {
    __break(1u);
    goto LABEL_12;
  }
  v3 = v1;
  v4 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&srcARGB.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&srcARGB.width = v4;
  v5 = v1[2];
  if (!v5)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v6 = v1[4];
  if (!*(_QWORD *)(v6 + 16))
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  v7 = *(_OWORD *)(v6 + 48);
  *(_OWORD *)&destA.data = *(_OWORD *)(v6 + 32);
  *(_OWORD *)&destA.width = v7;
  if (v5 < 2)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  v8 = v1[5];
  if (!*(_QWORD *)(v8 + 16))
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v9 = *(_OWORD *)(v8 + 48);
  *(_OWORD *)&destR.data = *(_OWORD *)(v8 + 32);
  *(_OWORD *)&destR.width = v9;
  if (v5 < 3)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v10 = v1[6];
  if (!*(_QWORD *)(v10 + 16))
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v11 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&destG.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&destG.width = v11;
  if (v5 < 4)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
  }
  v12 = v1[7];
  if (!*(_QWORD *)(v12 + 16))
    goto LABEL_19;
  v13 = *(_OWORD *)(v12 + 48);
  *(_OWORD *)&destB.data = *(_OWORD *)(v12 + 32);
  *(_OWORD *)&destB.width = v13;
  vImageConvert_ARGB8888toPlanar8(&srcARGB, &destA, &destR, &destG, &destB, 0);
  return v3;
}

{
  uint64_t *v0;
  _QWORD *v1;
  uint64_t v2;
  _QWORD *v3;
  __int128 v4;
  unint64_t v5;
  uint64_t v6;
  __int128 v7;
  uint64_t v8;
  __int128 v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v12;
  __int128 v13;
  uint64_t v15;
  char v16[16];
  vImage_Buffer blue;
  vImage_Buffer green;
  vImage_Buffer red;
  vImage_Buffer alpha;
  vImage_Buffer src;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v16, 0, 4, (uint64_t)&v15);
  v2 = *v0;
  if (!*(_QWORD *)(*v0 + 16))
  {
    __break(1u);
    goto LABEL_12;
  }
  v3 = v1;
  v4 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&src.width = v4;
  v5 = v1[2];
  if (!v5)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v6 = v1[4];
  if (!*(_QWORD *)(v6 + 16))
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  v7 = *(_OWORD *)(v6 + 48);
  *(_OWORD *)&alpha.data = *(_OWORD *)(v6 + 32);
  *(_OWORD *)&alpha.width = v7;
  if (v5 < 2)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  v8 = v1[5];
  if (!*(_QWORD *)(v8 + 16))
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v9 = *(_OWORD *)(v8 + 48);
  *(_OWORD *)&red.data = *(_OWORD *)(v8 + 32);
  *(_OWORD *)&red.width = v9;
  if (v5 < 3)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v10 = v1[6];
  if (!*(_QWORD *)(v10 + 16))
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v11 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&green.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&green.width = v11;
  if (v5 < 4)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
  }
  v12 = v1[7];
  if (!*(_QWORD *)(v12 + 16))
    goto LABEL_19;
  v13 = *(_OWORD *)(v12 + 48);
  *(_OWORD *)&blue.data = *(_OWORD *)(v12 + 32);
  *(_OWORD *)&blue.width = v13;
  vImageConvert_ARGB8888toPlanarF(&src, &alpha, &red, &green, &blue, flt_1E84EDB58, flt_1E84EDB28, 0);
  return v3;
}

{
  uint64_t *v0;
  _QWORD *v1;
  uint64_t v2;
  _QWORD *v3;
  __int128 v4;
  unint64_t v5;
  uint64_t v6;
  __int128 v7;
  uint64_t v8;
  __int128 v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v13;
  char v14[16];
  vImage_Buffer blueDest;
  vImage_Buffer greenDest;
  vImage_Buffer redDest;
  vImage_Buffer rgbSrc;
  uint64_t v19;

  v19 = *MEMORY[0x1E0C80C00];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7Planar8VGs5NeverOTgq5(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v14, 0, 3, (uint64_t)&v13);
  v2 = *v0;
  if (!*(_QWORD *)(*v0 + 16))
  {
    __break(1u);
    goto LABEL_10;
  }
  v3 = v1;
  v4 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&rgbSrc.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&rgbSrc.width = v4;
  v5 = v1[2];
  if (!v5)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  v6 = v1[4];
  if (!*(_QWORD *)(v6 + 16))
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  v7 = *(_OWORD *)(v6 + 48);
  *(_OWORD *)&redDest.data = *(_OWORD *)(v6 + 32);
  *(_OWORD *)&redDest.width = v7;
  if (v5 < 2)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v8 = v1[5];
  if (!*(_QWORD *)(v8 + 16))
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  v9 = *(_OWORD *)(v8 + 48);
  *(_OWORD *)&greenDest.data = *(_OWORD *)(v8 + 32);
  *(_OWORD *)&greenDest.width = v9;
  if (v5 < 3)
  {
LABEL_14:
    __break(1u);
LABEL_15:
    __break(1u);
  }
  v10 = v1[6];
  if (!*(_QWORD *)(v10 + 16))
    goto LABEL_15;
  v11 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&blueDest.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&blueDest.width = v11;
  vImageConvert_RGB888toPlanar8(&rgbSrc, &redDest, &greenDest, &blueDest, 0);
  return v3;
}

{
  uint64_t *v0;
  _QWORD *v1;
  uint64_t v2;
  _QWORD *v3;
  __int128 v4;
  unint64_t v5;
  uint64_t v6;
  __int128 v7;
  uint64_t v8;
  __int128 v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v13;
  char v14[16];
  vImage_Buffer blueDest;
  vImage_Buffer greenDest;
  vImage_Buffer redDest;
  vImage_Buffer rgbSrc;
  uint64_t v19;

  v19 = *MEMORY[0x1E0C80C00];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v14, 0, 3, (uint64_t)&v13);
  v2 = *v0;
  if (!*(_QWORD *)(*v0 + 16))
  {
    __break(1u);
    goto LABEL_10;
  }
  v3 = v1;
  v4 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&rgbSrc.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&rgbSrc.width = v4;
  v5 = v1[2];
  if (!v5)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  v6 = v1[4];
  if (!*(_QWORD *)(v6 + 16))
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  v7 = *(_OWORD *)(v6 + 48);
  *(_OWORD *)&redDest.data = *(_OWORD *)(v6 + 32);
  *(_OWORD *)&redDest.width = v7;
  if (v5 < 2)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v8 = v1[5];
  if (!*(_QWORD *)(v8 + 16))
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  v9 = *(_OWORD *)(v8 + 48);
  *(_OWORD *)&greenDest.data = *(_OWORD *)(v8 + 32);
  *(_OWORD *)&greenDest.width = v9;
  if (v5 < 3)
  {
LABEL_14:
    __break(1u);
LABEL_15:
    __break(1u);
  }
  v10 = v1[6];
  if (!*(_QWORD *)(v10 + 16))
    goto LABEL_15;
  v11 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&blueDest.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&blueDest.width = v11;
  vImageConvert_RGBFFFtoPlanarF(&rgbSrc, &redDest, &greenDest, &blueDest, 0);
  return v3;
}

{
  uint64_t *v0;
  _QWORD *v1;
  uint64_t v2;
  _QWORD *v3;
  __int128 v4;
  unint64_t v5;
  uint64_t v6;
  __int128 v7;
  uint64_t v8;
  __int128 v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v12;
  __int128 v13;
  uint64_t v15;
  char v16[16];
  vImage_Buffer bDest;
  vImage_Buffer gDest;
  vImage_Buffer rDest;
  vImage_Buffer aDest;
  vImage_Buffer argbSrc;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI9Planar16UVGs5NeverOTgq5(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v16, 0, 4, (uint64_t)&v15);
  v2 = *v0;
  if (!*(_QWORD *)(*v0 + 16))
  {
    __break(1u);
    goto LABEL_12;
  }
  v3 = v1;
  v4 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&argbSrc.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&argbSrc.width = v4;
  v5 = v1[2];
  if (!v5)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v6 = v1[4];
  if (!*(_QWORD *)(v6 + 16))
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  v7 = *(_OWORD *)(v6 + 48);
  *(_OWORD *)&aDest.data = *(_OWORD *)(v6 + 32);
  *(_OWORD *)&aDest.width = v7;
  if (v5 < 2)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  v8 = v1[5];
  if (!*(_QWORD *)(v8 + 16))
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v9 = *(_OWORD *)(v8 + 48);
  *(_OWORD *)&rDest.data = *(_OWORD *)(v8 + 32);
  *(_OWORD *)&rDest.width = v9;
  if (v5 < 3)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v10 = v1[6];
  if (!*(_QWORD *)(v10 + 16))
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v11 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&gDest.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&gDest.width = v11;
  if (v5 < 4)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
  }
  v12 = v1[7];
  if (!*(_QWORD *)(v12 + 16))
    goto LABEL_19;
  v13 = *(_OWORD *)(v12 + 48);
  *(_OWORD *)&bDest.data = *(_OWORD *)(v12 + 32);
  *(_OWORD *)&bDest.width = v13;
  vImageConvert_ARGB16UtoPlanar16U(&argbSrc, &aDest, &rDest, &gDest, &bDest, 0);
  return v3;
}

_QWORD *closure #1 in vImage.PixelBuffer<>.planarBuffers()@<X0>(uint64_t *a1@<X1>, uint64_t *a2@<X8>)
{
  void *v2;
  void *v3;
  uint64_t v5;
  vImagePixelCount v6;
  vImagePixelCount v7;
  uint64_t v8;
  void *v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  void *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  _QWORD *result;

  v5 = *a1;
  if (!*(_QWORD *)(*a1 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  v6 = *(_QWORD *)(v5 + 48);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  v7 = *(_QWORD *)(v5 + 40);
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (!v6)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  if (!v7)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v3 = v2;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v8 = swift_allocObject();
  *(_OWORD *)(v8 + 16) = xmmword_1CAB5E430;
  v9 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v6, v7, 0x20u);
  if (!v2)
  {
    v13 = v9;
    v14 = v10;
    v15 = v11;
    v16 = v12;
    type metadata accessor for vImage.BufferReference();
    result = (_QWORD *)swift_allocObject();
    result[2] = v13;
    result[3] = v14;
    result[4] = v15;
    result[5] = v16;
    *(_QWORD *)(v8 + 32) = v13;
    *(_QWORD *)(v8 + 40) = v14;
    *(_QWORD *)(v8 + 48) = v15;
    *(_QWORD *)(v8 + 56) = v16;
    *(_QWORD *)(v8 + 64) = result;
    *a2 = v8;
    return result;
  }
LABEL_13:

  result = (_QWORD *)_assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

{
  void *v2;
  void *v3;
  uint64_t v5;
  vImagePixelCount v6;
  vImagePixelCount v7;
  uint64_t v8;
  void *v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  void *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  _QWORD *result;

  v5 = *a1;
  if (!*(_QWORD *)(*a1 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  v6 = *(_QWORD *)(v5 + 48);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  v7 = *(_QWORD *)(v5 + 40);
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (!v6)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  if (!v7)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v3 = v2;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v8 = swift_allocObject();
  *(_OWORD *)(v8 + 16) = xmmword_1CAB5E430;
  v9 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v6, v7, 8u);
  if (!v2)
  {
    v13 = v9;
    v14 = v10;
    v15 = v11;
    v16 = v12;
    type metadata accessor for vImage.BufferReference();
    result = (_QWORD *)swift_allocObject();
    result[2] = v13;
    result[3] = v14;
    result[4] = v15;
    result[5] = v16;
    *(_QWORD *)(v8 + 32) = v13;
    *(_QWORD *)(v8 + 40) = v14;
    *(_QWORD *)(v8 + 48) = v15;
    *(_QWORD *)(v8 + 56) = v16;
    *(_QWORD *)(v8 + 64) = result;
    *a2 = v8;
    return result;
  }
LABEL_13:

  result = (_QWORD *)_assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

{
  void *v2;
  void *v3;
  uint64_t v5;
  vImagePixelCount v6;
  vImagePixelCount v7;
  uint64_t v8;
  void *v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  void *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  _QWORD *result;

  v5 = *a1;
  if (!*(_QWORD *)(*a1 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  v6 = *(_QWORD *)(v5 + 48);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  v7 = *(_QWORD *)(v5 + 40);
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (!v6)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  if (!v7)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v3 = v2;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v8 = swift_allocObject();
  *(_OWORD *)(v8 + 16) = xmmword_1CAB5E430;
  v9 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v6, v7, 0x10u);
  if (!v2)
  {
    v13 = v9;
    v14 = v10;
    v15 = v11;
    v16 = v12;
    type metadata accessor for vImage.BufferReference();
    result = (_QWORD *)swift_allocObject();
    result[2] = v13;
    result[3] = v14;
    result[4] = v15;
    result[5] = v16;
    *(_QWORD *)(v8 + 32) = v13;
    *(_QWORD *)(v8 + 40) = v14;
    *(_QWORD *)(v8 + 48) = v15;
    *(_QWORD *)(v8 + 56) = v16;
    *(_QWORD *)(v8 + 64) = result;
    *a2 = v8;
    return result;
  }
LABEL_13:

  result = (_QWORD *)_assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

_QWORD *_sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5(_QWORD *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5Tm(a1, a2, a3, a4, a5, (uint64_t (*)(BOOL, unint64_t, uint64_t))specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:));
}

_QWORD *_sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7Planar8VGs5NeverOTgq5(_QWORD *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5Tm(a1, a2, a3, a4, a5, (uint64_t (*)(BOOL, unint64_t, uint64_t))specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:));
}

char *_sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO13BufferWrapperVs5NeverOTg5(char *result, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v10;
  uint64_t v11;
  __int128 v12;
  __int128 v13;
  uint64_t v14;
  unint64_t v15;
  unint64_t v16;
  uint64_t v17;
  __int128 v18;
  __int128 v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  __int128 v23;
  __int128 v24;
  uint64_t v25;
  uint64_t v26;

  v21 = result;
  v5 = a4 - a3;
  if (__OFSUB__(a4, a3))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v6 = v4;
  v7 = MEMORY[0x1E0DEE9D8];
  if (!v5)
    return (char *)v7;
  v26 = MEMORY[0x1E0DEE9D8];
  result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v5 & ~(v5 >> 63), 0);
  if ((v5 & 0x8000000000000000) == 0)
  {
    v7 = v26;
    if (a4 <= a3)
      v10 = a3;
    else
      v10 = a4;
    v20 = v10;
    v11 = a3;
    while (a4 != v11)
    {
      v22 = v11;
      result = (char *)((char *(*)(__int128 *__return_ptr, uint64_t *))v21)(&v23, &v22);
      if (v6)
      {
        swift_release();
        return (char *)v7;
      }
      v6 = 0;
      v12 = v23;
      v13 = v24;
      v14 = v25;
      v26 = v7;
      v16 = *(_QWORD *)(v7 + 16);
      v15 = *(_QWORD *)(v7 + 24);
      if (v16 >= v15 >> 1)
      {
        v18 = v24;
        v19 = v23;
        result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v15 > 1), v16 + 1, 1);
        v13 = v18;
        v12 = v19;
        v7 = v26;
      }
      *(_QWORD *)(v7 + 16) = v16 + 1;
      v17 = v7 + 40 * v16;
      *(_OWORD *)(v17 + 32) = v12;
      *(_OWORD *)(v17 + 48) = v13;
      *(_QWORD *)(v17 + 64) = v14;
      if (a4 < a3)
        goto LABEL_19;
      if (v20 == v11)
        goto LABEL_20;
      if (a4 == ++v11)
        return (char *)v7;
    }
    __break(1u);
LABEL_19:
    __break(1u);
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
LABEL_22:
  __break(1u);
  return result;
}

_QWORD *_sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI9Planar16UVGs5NeverOTgq5(_QWORD *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5Tm(a1, a2, a3, a4, a5, (uint64_t (*)(BOOL, unint64_t, uint64_t))specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:));
}

_QWORD *_sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5Tm(_QWORD *result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(BOOL, unint64_t, uint64_t))
{
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v13;
  uint64_t v14;
  unint64_t v15;
  unint64_t v16;
  uint64_t (*v17)(BOOL, unint64_t, uint64_t);
  _QWORD *(*v18)(uint64_t *__return_ptr, uint64_t *);
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;

  v18 = (_QWORD *(*)(uint64_t *__return_ptr, uint64_t *))result;
  v7 = a4 - a3;
  if (__OFSUB__(a4, a3))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v8 = v6;
  v9 = MEMORY[0x1E0DEE9D8];
  if (!v7)
    return (_QWORD *)v9;
  v21 = MEMORY[0x1E0DEE9D8];
  result = (_QWORD *)a6(0, v7 & ~(v7 >> 63), 0);
  if ((v7 & 0x8000000000000000) == 0)
  {
    v17 = a6;
    v9 = v21;
    if (a4 <= a3)
      v13 = a3;
    else
      v13 = a4;
    v14 = a3;
    while (a4 != v14)
    {
      v19 = v14;
      result = v18(&v20, &v19);
      if (v8)
      {
        swift_release();
        return (_QWORD *)v9;
      }
      v8 = 0;
      v21 = v9;
      v16 = *(_QWORD *)(v9 + 16);
      v15 = *(_QWORD *)(v9 + 24);
      if (v16 >= v15 >> 1)
      {
        result = (_QWORD *)v17(v15 > 1, v16 + 1, 1);
        v9 = v21;
      }
      *(_QWORD *)(v9 + 16) = v16 + 1;
      *(_QWORD *)(v9 + 8 * v16 + 32) = v20;
      if (a4 < a3)
        goto LABEL_19;
      if (v13 == v14)
        goto LABEL_20;
      if (a4 == ++v14)
        return (_QWORD *)v9;
    }
    __break(1u);
LABEL_19:
    __break(1u);
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
LABEL_22:
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #2 in vImage.PixelBuffer<>.planarBuffers()(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t result;
  _QWORD *v7;
  const void **v8;
  uint64_t v9;
  vImagePixelCount v10;
  vImagePixelCount v11;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer?>);
  result = swift_initStackObject();
  *(_OWORD *)(result + 16) = xmmword_1CAB5E440;
  v7 = *(_QWORD **)a2;
  if (!*(_QWORD *)(*(_QWORD *)a2 + 16))
  {
    __break(1u);
    goto LABEL_7;
  }
  v8 = (const void **)(result + 32);
  v9 = v7[4];
  *(_QWORD *)(result + 32) = v9;
  if (v9)
  {
    *(_QWORD *)(result + 40) = v9 + 4;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<vImage_Buffer>?>);
    result = swift_initStackObject();
    *(_OWORD *)(result + 16) = xmmword_1CAB5E440;
    *(_QWORD *)(result + 32) = a3;
    *(_QWORD *)(result + 40) = a1;
    v10 = v7[6];
    if ((v10 & 0x8000000000000000) == 0)
    {
      v11 = v7[5];
      if ((v11 & 0x8000000000000000) == 0)
      {
        vImageConvert_ChunkyToPlanarF(v8, (const vImage_Buffer **)(result + 32), 2u, 8uLL, v10, v11, v7[7], 0);
        swift_bridgeObjectRelease();
        return swift_bridgeObjectRelease();
      }
      goto LABEL_8;
    }
LABEL_7:
    __break(1u);
LABEL_8:
    __break(1u);
  }
  __break(1u);
  return result;
}

{
  uint64_t result;
  _QWORD *v7;
  const void **v8;
  uint64_t v9;
  vImagePixelCount v10;
  vImagePixelCount v11;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer?>);
  result = swift_initStackObject();
  *(_OWORD *)(result + 16) = xmmword_1CAB5E440;
  v7 = *(_QWORD **)a2;
  if (!*(_QWORD *)(*(_QWORD *)a2 + 16))
  {
    __break(1u);
    goto LABEL_7;
  }
  v8 = (const void **)(result + 32);
  v9 = v7[4];
  *(_QWORD *)(result + 32) = v9;
  if (v9)
  {
    *(_QWORD *)(result + 40) = v9 + 1;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<vImage_Buffer>?>);
    result = swift_initStackObject();
    *(_OWORD *)(result + 16) = xmmword_1CAB5E440;
    *(_QWORD *)(result + 32) = a3;
    *(_QWORD *)(result + 40) = a1;
    v10 = v7[6];
    if ((v10 & 0x8000000000000000) == 0)
    {
      v11 = v7[5];
      if ((v11 & 0x8000000000000000) == 0)
      {
        vImageConvert_ChunkyToPlanar8(v8, (const vImage_Buffer **)(result + 32), 2u, 2uLL, v10, v11, v7[7], 0);
        swift_bridgeObjectRelease();
        return swift_bridgeObjectRelease();
      }
      goto LABEL_8;
    }
LABEL_7:
    __break(1u);
LABEL_8:
    __break(1u);
  }
  __break(1u);
  return result;
}

vImage_Error vImage.PixelBuffer<>.convert(to:)(uint64_t a1)
{
  uint64_t v1;
  _QWORD *v2;
  uint64_t v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  void *v9;
  size_t v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v3 = v2[6];
  if (v3 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7)
    goto LABEL_25;
  v8 = (void *)v2[4];
  v9 = (void *)v5[4];
  v10 = v2[7];
  v11 = v5[7];
  src.data = v8;
  src.height = v4;
  src.width = 2 * v3;
  src.rowBytes = v10;
  dest.data = v9;
  dest.height = v4;
  dest.width = 2 * v3;
  dest.rowBytes = v11;
  return vImageConvert_PlanarFtoPlanar16F(&src, &dest, 0);
}

{
  uint64_t *v1;
  uint64_t v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  uint64_t v10;
  vImagePixelCount v11;
  size_t v12;
  uint64_t v13;
  vImagePixelCount v14;
  size_t v15;
  uint64_t v16;
  vImagePixelCount v17;
  size_t v18;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  vImage_Buffer planarBlue;
  vImage_Buffer planarGreen;
  vImage_Buffer planarRed;
  vImage_Buffer rgbDest;
  uint64_t v27;

  v27 = *MEMORY[0x1E0C80C00];
  v2 = *v1;
  if (!*(_QWORD *)(*v1 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  v3 = *(_QWORD *)(v2 + 48);
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v4 = *(_QWORD *)(v2 + 40);
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v3)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v4)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v6)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v7)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v3 != v6)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v4 != v7)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  v8 = (void *)v5[4];
  v9 = v5[7];
  rgbDest.data = v8;
  rgbDest.height = v4;
  rgbDest.width = v3;
  rgbDest.rowBytes = v9;
  v10 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (!*(_QWORD *)(v10 + 16))
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  v20 = *(_OWORD *)(v10 + 32);
  v11 = *(_QWORD *)(v10 + 48);
  v12 = *(_QWORD *)(v10 + 56);
  swift_bridgeObjectRelease();
  *(_OWORD *)&planarRed.data = v20;
  planarRed.width = v11;
  planarRed.rowBytes = v12;
  v13 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(_QWORD *)(v13 + 16) < 2uLL)
  {
LABEL_30:
    __break(1u);
LABEL_31:
    __break(1u);
  }
  v21 = *(_OWORD *)(v13 + 64);
  v14 = *(_QWORD *)(v13 + 80);
  v15 = *(_QWORD *)(v13 + 88);
  swift_bridgeObjectRelease();
  *(_OWORD *)&planarGreen.data = v21;
  planarGreen.width = v14;
  planarGreen.rowBytes = v15;
  v16 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(_QWORD *)(v16 + 16) < 3uLL)
    goto LABEL_31;
  v22 = *(_OWORD *)(v16 + 96);
  v17 = *(_QWORD *)(v16 + 112);
  v18 = *(_QWORD *)(v16 + 120);
  swift_bridgeObjectRelease();
  *(_OWORD *)&planarBlue.data = v22;
  planarBlue.width = v17;
  planarBlue.rowBytes = v18;
  return vImageConvert_PlanarFtoRGBFFF(&planarRed, &planarGreen, &planarBlue, &rgbDest, 0);
}

{
  uint64_t *v1;
  uint64_t v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  uint64_t v10;
  vImagePixelCount v11;
  size_t v12;
  uint64_t v13;
  vImagePixelCount v14;
  size_t v15;
  uint64_t v16;
  vImagePixelCount v17;
  size_t v18;
  uint64_t v19;
  vImagePixelCount v20;
  size_t v21;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  vImage_Buffer srcB;
  vImage_Buffer srcG;
  vImage_Buffer srcR;
  vImage_Buffer srcA;
  vImage_Buffer dest;
  uint64_t v32;

  v32 = *MEMORY[0x1E0C80C00];
  v2 = *v1;
  if (!*(_QWORD *)(*v1 + 16))
  {
    __break(1u);
    goto LABEL_19;
  }
  v3 = *(_QWORD *)(v2 + 48);
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v4 = *(_QWORD *)(v2 + 40);
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v3)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v4)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v6)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v3 != v6)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (v4 != v7)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  v8 = (void *)v5[4];
  v9 = v5[7];
  dest.data = v8;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v9;
  v10 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (!*(_QWORD *)(v10 + 16))
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v23 = *(_OWORD *)(v10 + 32);
  v11 = *(_QWORD *)(v10 + 48);
  v12 = *(_QWORD *)(v10 + 56);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcA.data = v23;
  srcA.width = v11;
  srcA.rowBytes = v12;
  v13 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(_QWORD *)(v13 + 16) < 2uLL)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v24 = *(_OWORD *)(v13 + 64);
  v14 = *(_QWORD *)(v13 + 80);
  v15 = *(_QWORD *)(v13 + 88);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcR.data = v24;
  srcR.width = v14;
  srcR.rowBytes = v15;
  v16 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(_QWORD *)(v16 + 16) < 3uLL)
  {
LABEL_32:
    __break(1u);
LABEL_33:
    __break(1u);
  }
  v25 = *(_OWORD *)(v16 + 96);
  v17 = *(_QWORD *)(v16 + 112);
  v18 = *(_QWORD *)(v16 + 120);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcG.data = v25;
  srcG.width = v17;
  srcG.rowBytes = v18;
  v19 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(_QWORD *)(v19 + 16) < 4uLL)
    goto LABEL_33;
  v26 = *(_OWORD *)(v19 + 128);
  v20 = *(_QWORD *)(v19 + 144);
  v21 = *(_QWORD *)(v19 + 152);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcB.data = v26;
  srcB.width = v20;
  srcB.rowBytes = v21;
  return vImageConvert_Planar8toARGB8888(&srcA, &srcR, &srcG, &srcB, &dest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7)
    goto LABEL_25;
  v8 = (void *)v2[4];
  v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  v10 = (void *)v5[4];
  v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_16Uto16F(&src, &dest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7)
    goto LABEL_25;
  v8 = (void *)v2[4];
  v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  v10 = (void *)v5[4];
  v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_ARGBFFFFtoARGB8888_dithered(&src, &dest, flt_1E84EDD68, flt_1E84EDD10, 0, byte_1E84EDD40, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  unint64_t v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v3)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v4)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v6)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (v3 != v6)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v4 != v7)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v3 >> 62)
    goto LABEL_27;
  v8 = (void *)v2[4];
  v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = 4 * v3;
  src.rowBytes = v9;
  v10 = (void *)v5[4];
  v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = 4 * v3;
  dest.rowBytes = v11;
  return vImageConvert_FTo16U(&src, &dest, 0.0, 0.000015259, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  unint64_t v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  void *v9;
  size_t v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v3)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v4)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v6)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (v3 != v6)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v4 != v7)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v3 >> 62)
    goto LABEL_27;
  v8 = (void *)v2[4];
  v9 = (void *)v5[4];
  v10 = v2[7];
  v11 = v5[7];
  src.data = v8;
  src.height = v4;
  src.width = 4 * v3;
  src.rowBytes = v10;
  dest.data = v9;
  dest.height = v4;
  dest.width = 4 * v3;
  dest.rowBytes = v11;
  return vImageConvert_PlanarFtoPlanar16F(&src, &dest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  uint64_t v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  void *v9;
  size_t v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v3 = v2[6];
  if (v3 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7)
    goto LABEL_25;
  v8 = (void *)v2[4];
  v9 = (void *)v5[4];
  v10 = v2[7];
  v11 = v5[7];
  src.data = v8;
  src.height = v4;
  src.width = 2 * v3;
  src.rowBytes = v10;
  dest.data = v9;
  dest.height = v4;
  dest.width = 2 * v3;
  dest.rowBytes = v11;
  return vImageConvert_Planar8toPlanar16F(&src, &dest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7)
    goto LABEL_25;
  v8 = (void *)v2[4];
  v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  v10 = (void *)v5[4];
  v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_Planar8toPlanarF(&src, &dest, 1.0, 0.0, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7)
    goto LABEL_25;
  v8 = (void *)v2[4];
  v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  v10 = (void *)v5[4];
  v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_Planar8toPlanar16F(&src, &dest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7)
    goto LABEL_25;
  v8 = (void *)v2[4];
  v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  v10 = (void *)v5[4];
  v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_ARGB8888ToARGB16U(&src, &dest, byte_1E84EDC00, 0, word_1E84EDBD8, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  unint64_t v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  void *v9;
  size_t v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v3)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v4)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v6)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (v3 != v6)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v4 != v7)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v3 >> 62)
    goto LABEL_27;
  v8 = (void *)v2[4];
  v9 = (void *)v5[4];
  v10 = v2[7];
  v11 = v5[7];
  src.data = v8;
  src.height = v4;
  src.width = 4 * v3;
  src.rowBytes = v10;
  dest.data = v9;
  dest.height = v4;
  dest.width = 4 * v3;
  dest.rowBytes = v11;
  return vImageConvert_Planar8toPlanar16F(&src, &dest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  uint64_t v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  void *v9;
  size_t v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v3 = v2[6];
  if (v3 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7)
    goto LABEL_25;
  v8 = (void *)v2[4];
  v9 = (void *)v5[4];
  v10 = v2[7];
  v11 = v5[7];
  src.data = v8;
  src.height = v4;
  src.width = 2 * v3;
  src.rowBytes = v10;
  dest.data = v9;
  dest.height = v4;
  dest.width = 2 * v3;
  dest.rowBytes = v11;
  return vImageConvert_16Uto16F(&src, &dest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7)
    goto LABEL_25;
  v8 = (void *)v2[4];
  v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  v10 = (void *)v5[4];
  v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_RGBFFFtoRGB888_dithered(&src, &dest, flt_1E84EDC80, flt_1E84EDC50, 0, 0);
}

{
  uint64_t *v1;
  uint64_t v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  uint64_t v10;
  vImagePixelCount v11;
  size_t v12;
  uint64_t v13;
  vImagePixelCount v14;
  size_t v15;
  uint64_t v16;
  vImagePixelCount v17;
  size_t v18;
  uint64_t v19;
  vImagePixelCount v20;
  size_t v21;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  vImage_Buffer srcB;
  vImage_Buffer srcG;
  vImage_Buffer srcR;
  vImage_Buffer srcA;
  vImage_Buffer dest;
  uint64_t v32;

  v32 = *MEMORY[0x1E0C80C00];
  v2 = *v1;
  if (!*(_QWORD *)(*v1 + 16))
  {
    __break(1u);
    goto LABEL_19;
  }
  v3 = *(_QWORD *)(v2 + 48);
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v4 = *(_QWORD *)(v2 + 40);
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v3)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v4)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v6)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v3 != v6)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (v4 != v7)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  v8 = (void *)v5[4];
  v9 = v5[7];
  dest.data = v8;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v9;
  v10 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (!*(_QWORD *)(v10 + 16))
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v23 = *(_OWORD *)(v10 + 32);
  v11 = *(_QWORD *)(v10 + 48);
  v12 = *(_QWORD *)(v10 + 56);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcA.data = v23;
  srcA.width = v11;
  srcA.rowBytes = v12;
  v13 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(_QWORD *)(v13 + 16) < 2uLL)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v24 = *(_OWORD *)(v13 + 64);
  v14 = *(_QWORD *)(v13 + 80);
  v15 = *(_QWORD *)(v13 + 88);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcR.data = v24;
  srcR.width = v14;
  srcR.rowBytes = v15;
  v16 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(_QWORD *)(v16 + 16) < 3uLL)
  {
LABEL_32:
    __break(1u);
LABEL_33:
    __break(1u);
  }
  v25 = *(_OWORD *)(v16 + 96);
  v17 = *(_QWORD *)(v16 + 112);
  v18 = *(_QWORD *)(v16 + 120);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcG.data = v25;
  srcG.width = v17;
  srcG.rowBytes = v18;
  v19 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(_QWORD *)(v19 + 16) < 4uLL)
    goto LABEL_33;
  v26 = *(_OWORD *)(v19 + 128);
  v20 = *(_QWORD *)(v19 + 144);
  v21 = *(_QWORD *)(v19 + 152);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcB.data = v26;
  srcB.width = v20;
  srcB.rowBytes = v21;
  return vImageConvert_PlanarFtoARGBFFFF(&srcA, &srcR, &srcG, &srcB, &dest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  unint64_t v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v3)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v4)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v6)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (v3 != v6)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v4 != v7)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v3 >> 62)
    goto LABEL_27;
  v8 = (void *)v2[4];
  v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = 4 * v3;
  src.rowBytes = v9;
  v10 = (void *)v5[4];
  v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = 4 * v3;
  dest.rowBytes = v11;
  return vImageConvert_16UToF(&src, &dest, 0.0, 0.000015259, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7)
    goto LABEL_25;
  v8 = (void *)v2[4];
  v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  v10 = (void *)v5[4];
  v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_ARGB16UToARGB8888(&src, &dest, byte_1E84EDF28, 0, byte_1E84EDF00, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  unint64_t v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  void *v9;
  size_t v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v3)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v4)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v6)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (v3 != v6)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v4 != v7)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v3 >> 62)
    goto LABEL_27;
  v8 = (void *)v2[4];
  v9 = (void *)v5[4];
  v10 = v2[7];
  v11 = v5[7];
  src.data = v8;
  src.height = v4;
  src.width = 4 * v3;
  src.rowBytes = v10;
  dest.data = v9;
  dest.height = v4;
  dest.width = 4 * v3;
  dest.rowBytes = v11;
  return vImageConvert_16Uto16F(&src, &dest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7)
    goto LABEL_25;
  v8 = (void *)v2[4];
  v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  v10 = (void *)v5[4];
  v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_PlanarFtoPlanar16F(&src, &dest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7)
    goto LABEL_25;
  v8 = (void *)v2[4];
  v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  v10 = (void *)v5[4];
  v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_PlanarFtoPlanar8(&src, &dest, 1.0, 0.0, 0);
}

{
  uint64_t *v1;
  uint64_t v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  uint64_t v10;
  vImagePixelCount v11;
  size_t v12;
  uint64_t v13;
  vImagePixelCount v14;
  size_t v15;
  uint64_t v16;
  vImagePixelCount v17;
  size_t v18;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  vImage_Buffer planarBlue;
  vImage_Buffer planarGreen;
  vImage_Buffer planarRed;
  vImage_Buffer rgbDest;
  uint64_t v27;

  v27 = *MEMORY[0x1E0C80C00];
  v2 = *v1;
  if (!*(_QWORD *)(*v1 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  v3 = *(_QWORD *)(v2 + 48);
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v4 = *(_QWORD *)(v2 + 40);
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v3)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v4)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v6)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v7)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v3 != v6)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v4 != v7)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  v8 = (void *)v5[4];
  v9 = v5[7];
  rgbDest.data = v8;
  rgbDest.height = v4;
  rgbDest.width = v3;
  rgbDest.rowBytes = v9;
  v10 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (!*(_QWORD *)(v10 + 16))
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  v20 = *(_OWORD *)(v10 + 32);
  v11 = *(_QWORD *)(v10 + 48);
  v12 = *(_QWORD *)(v10 + 56);
  swift_bridgeObjectRelease();
  *(_OWORD *)&planarRed.data = v20;
  planarRed.width = v11;
  planarRed.rowBytes = v12;
  v13 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(_QWORD *)(v13 + 16) < 2uLL)
  {
LABEL_30:
    __break(1u);
LABEL_31:
    __break(1u);
  }
  v21 = *(_OWORD *)(v13 + 64);
  v14 = *(_QWORD *)(v13 + 80);
  v15 = *(_QWORD *)(v13 + 88);
  swift_bridgeObjectRelease();
  *(_OWORD *)&planarGreen.data = v21;
  planarGreen.width = v14;
  planarGreen.rowBytes = v15;
  v16 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(_QWORD *)(v16 + 16) < 3uLL)
    goto LABEL_31;
  v22 = *(_OWORD *)(v16 + 96);
  v17 = *(_QWORD *)(v16 + 112);
  v18 = *(_QWORD *)(v16 + 120);
  swift_bridgeObjectRelease();
  *(_OWORD *)&planarBlue.data = v22;
  planarBlue.width = v17;
  planarBlue.rowBytes = v18;
  return vImageConvert_Planar8toRGB888(&planarRed, &planarGreen, &planarBlue, &rgbDest, 0);
}

uint64_t __swift_instantiateConcreteTypeFromMangledName(uint64_t *a1)
{
  uint64_t result;

  result = *a1;
  if (result < 0)
  {
    result = MEMORY[0x1D1794CF0]((char *)a1 + (int)result, -(result >> 32), 0, 0);
    *a1 = result;
  }
  return result;
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance BNNSDataType(_DWORD *a1, _DWORD *a2)
{
  return *a1 == *a2;
}

_DWORD *protocol witness for RawRepresentable.init(rawValue:) in conformance BNNSDataType@<X0>(_DWORD *result@<X0>, uint64_t a2@<X8>)
{
  *(_DWORD *)a2 = *result;
  *(_BYTE *)(a2 + 4) = 0;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance BNNSDataType(_DWORD *a1@<X8>)
{
  _DWORD *v1;

  *a1 = *v1;
}

uint64_t protocol witness for static Equatable.== infix(_:_:) in conformance CFStringRef(uint64_t a1, uint64_t a2, uint64_t a3)
{
  MEMORY[0x1D1794D08](&protocol conformance descriptor for CFStringRef, a3);
  return static _CFObject.== infix(_:_:)();
}

uint64_t protocol witness for Hashable.hashValue.getter in conformance CFStringRef(uint64_t a1)
{
  MEMORY[0x1D1794D08](&protocol conformance descriptor for CFStringRef, a1);
  return _CFObject.hashValue.getter();
}

uint64_t protocol witness for Hashable.hash(into:) in conformance CFStringRef()
{
  MEMORY[0x1D1794D08](&protocol conformance descriptor for CFStringRef);
  return _CFObject.hash(into:)();
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance CFStringRef(uint64_t a1, uint64_t a2)
{
  Hasher.init(_seed:)();
  MEMORY[0x1D1794D08](&protocol conformance descriptor for CFStringRef, a2);
  _CFObject.hash(into:)();
  return Hasher._finalize()();
}

_QWORD *partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers()@<X0>(uint64_t *a1@<X8>)
{
  uint64_t v1;

  return closure #1 in vImage.PixelBuffer<>.planarBuffers()(*(uint64_t **)(v1 + 16), a1);
}

{
  uint64_t v1;

  return closure #1 in vImage.PixelBuffer<>.planarBuffers()(*(uint64_t **)(v1 + 16), a1);
}

{
  uint64_t v1;

  return closure #1 in vImage.PixelBuffer<>.planarBuffers()(*(uint64_t **)(v1 + 16), a1);
}

void type metadata accessor for vImageCVImageFormatRef(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for vImageCVImageFormatRef);
}

void type metadata accessor for BNNSDataType(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSDataType);
}

void type metadata accessor for BNNSTargetSystem(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSTargetSystem);
}

void type metadata accessor for BNNSRelationalOperator(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSRelationalOperator);
}

__n128 __swift_memcpy40_8(uint64_t a1, uint64_t a2)
{
  __n128 result;
  __int128 v3;

  result = *(__n128 *)a2;
  v3 = *(_OWORD *)(a2 + 16);
  *(_QWORD *)(a1 + 32) = *(_QWORD *)(a2 + 32);
  *(__n128 *)a1 = result;
  *(_OWORD *)(a1 + 16) = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for quadrature_integrate_options(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 40))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for quadrature_integrate_options(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_QWORD *)result = (a2 - 1);
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 40) = v3;
  return result;
}

void type metadata accessor for quadrature_integrate_options(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for quadrature_integrate_options);
}

__n128 __swift_memcpy52_4(uint64_t a1, uint64_t a2)
{
  __n128 result;
  __int128 v3;
  __int128 v4;

  result = *(__n128 *)a2;
  v3 = *(_OWORD *)(a2 + 16);
  v4 = *(_OWORD *)(a2 + 32);
  *(_DWORD *)(a1 + 48) = *(_DWORD *)(a2 + 48);
  *(_OWORD *)(a1 + 16) = v3;
  *(_OWORD *)(a1 + 32) = v4;
  *(__n128 *)a1 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSOptimizerAdamWithClippingFields(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 52))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for BNNSOptimizerAdamWithClippingFields(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_QWORD *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_DWORD *)(result + 48) = 0;
    *(_QWORD *)result = (a2 - 1);
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 52) = v3;
  return result;
}

void type metadata accessor for BNNSOptimizerAdamWithClippingFields(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSOptimizerAdamWithClippingFields);
}

__n128 __swift_memcpy16_8(__n128 *a1, __n128 *a2)
{
  __n128 result;

  result = *a2;
  *a1 = *a2;
  return result;
}

void type metadata accessor for bnns_graph_shape_t(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for bnns_graph_shape_t);
}

void type metadata accessor for bnns_graph_context_t(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for bnns_graph_context_t);
}

uint64_t getEnumTagSinglePayload for bnns_graph_t(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 16))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for bnns_graph_t(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_QWORD *)result = (a2 - 1);
    *(_QWORD *)(result + 8) = 0;
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 16) = v3;
  return result;
}

void type metadata accessor for bnns_graph_t(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for bnns_graph_t);
}

void type metadata accessor for BNNSGraphOptimizationPreference(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSGraphOptimizationPreference);
}

void type metadata accessor for bnns_graph_compile_options_t(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for bnns_graph_compile_options_t);
}

__n128 __swift_memcpy24_4(__n128 *a1, __n128 *a2)
{
  __n128 result;

  result = *a2;
  a1[1].n128_u64[0] = a2[1].n128_u64[0];
  *a1 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSLayerParametersCropResize(unsigned __int8 *a1, unsigned int a2)
{
  unsigned int v3;
  BOOL v4;
  int v5;

  if (!a2)
    return 0;
  if (a2 >= 0xFF && a1[24])
    return (*(_DWORD *)a1 + 255);
  v3 = *a1;
  v4 = v3 >= 2;
  v5 = (v3 + 2147483646) & 0x7FFFFFFF;
  if (!v4)
    v5 = -1;
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNSLayerParametersCropResize(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(_QWORD *)(result + 8) = 0;
    *(_QWORD *)(result + 16) = 0;
    *(_QWORD *)result = a2 - 255;
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 24) = 1;
  }
  else
  {
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 24) = 0;
    if (a2)
      *(_BYTE *)result = a2 + 1;
  }
  return result;
}

void type metadata accessor for BNNSLayerParametersCropResize(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSLayerParametersCropResize);
}

void *__swift_memcpy720_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x2D0uLL);
}

uint64_t getEnumTagSinglePayload for BNNSLayerParametersQuantization(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 720))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for BNNSLayerParametersQuantization(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_QWORD *)(result + 712) = 0;
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 696) = 0u;
    *(_OWORD *)(result + 680) = 0u;
    *(_OWORD *)(result + 664) = 0u;
    *(_OWORD *)(result + 648) = 0u;
    *(_OWORD *)(result + 632) = 0u;
    *(_OWORD *)(result + 616) = 0u;
    *(_OWORD *)(result + 600) = 0u;
    *(_OWORD *)(result + 584) = 0u;
    *(_OWORD *)(result + 568) = 0u;
    *(_OWORD *)(result + 552) = 0u;
    *(_OWORD *)(result + 536) = 0u;
    *(_OWORD *)(result + 520) = 0u;
    *(_OWORD *)(result + 504) = 0u;
    *(_OWORD *)(result + 488) = 0u;
    *(_OWORD *)(result + 472) = 0u;
    *(_OWORD *)(result + 456) = 0u;
    *(_OWORD *)(result + 440) = 0u;
    *(_OWORD *)(result + 424) = 0u;
    *(_OWORD *)(result + 408) = 0u;
    *(_OWORD *)(result + 392) = 0u;
    *(_OWORD *)(result + 376) = 0u;
    *(_OWORD *)(result + 360) = 0u;
    *(_OWORD *)(result + 344) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(_QWORD *)result = (a2 - 1);
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 720) = v3;
  return result;
}

void type metadata accessor for BNNSLayerParametersQuantization(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSLayerParametersQuantization);
}

void *__swift_memcpy1128_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x468uLL);
}

uint64_t getEnumTagSinglePayload for BNNSLayerParametersNormalization(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 1128))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for BNNSLayerParametersNormalization(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 1096) = 0u;
    *(_OWORD *)(result + 1112) = 0u;
    *(_OWORD *)(result + 1080) = 0u;
    *(_OWORD *)(result + 1064) = 0u;
    *(_OWORD *)(result + 1048) = 0u;
    *(_OWORD *)(result + 1032) = 0u;
    *(_OWORD *)(result + 1016) = 0u;
    *(_OWORD *)(result + 1000) = 0u;
    *(_OWORD *)(result + 984) = 0u;
    *(_OWORD *)(result + 968) = 0u;
    *(_OWORD *)(result + 952) = 0u;
    *(_OWORD *)(result + 936) = 0u;
    *(_OWORD *)(result + 920) = 0u;
    *(_OWORD *)(result + 904) = 0u;
    *(_OWORD *)(result + 888) = 0u;
    *(_OWORD *)(result + 872) = 0u;
    *(_OWORD *)(result + 856) = 0u;
    *(_OWORD *)(result + 840) = 0u;
    *(_OWORD *)(result + 824) = 0u;
    *(_OWORD *)(result + 808) = 0u;
    *(_OWORD *)(result + 792) = 0u;
    *(_OWORD *)(result + 776) = 0u;
    *(_OWORD *)(result + 760) = 0u;
    *(_OWORD *)(result + 744) = 0u;
    *(_OWORD *)(result + 728) = 0u;
    *(_OWORD *)(result + 712) = 0u;
    *(_OWORD *)(result + 696) = 0u;
    *(_OWORD *)(result + 680) = 0u;
    *(_OWORD *)(result + 664) = 0u;
    *(_OWORD *)(result + 648) = 0u;
    *(_OWORD *)(result + 632) = 0u;
    *(_OWORD *)(result + 616) = 0u;
    *(_OWORD *)(result + 600) = 0u;
    *(_OWORD *)(result + 584) = 0u;
    *(_OWORD *)(result + 568) = 0u;
    *(_OWORD *)(result + 552) = 0u;
    *(_OWORD *)(result + 536) = 0u;
    *(_OWORD *)(result + 520) = 0u;
    *(_OWORD *)(result + 504) = 0u;
    *(_OWORD *)(result + 488) = 0u;
    *(_OWORD *)(result + 472) = 0u;
    *(_OWORD *)(result + 456) = 0u;
    *(_OWORD *)(result + 440) = 0u;
    *(_OWORD *)(result + 424) = 0u;
    *(_OWORD *)(result + 408) = 0u;
    *(_OWORD *)(result + 392) = 0u;
    *(_OWORD *)(result + 376) = 0u;
    *(_OWORD *)(result + 360) = 0u;
    *(_OWORD *)(result + 344) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(_QWORD *)result = (a2 - 1);
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 1128) = v3;
  return result;
}

void type metadata accessor for BNNSLayerParametersNormalization(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSLayerParametersNormalization);
}

void type metadata accessor for DSPDoubleSplitComplex(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for DSPDoubleSplitComplex);
}

__n128 __swift_memcpy36_4(uint64_t a1, uint64_t a2)
{
  __n128 result;
  __int128 v3;

  result = *(__n128 *)a2;
  v3 = *(_OWORD *)(a2 + 16);
  *(_DWORD *)(a1 + 32) = *(_DWORD *)(a2 + 32);
  *(__n128 *)a1 = result;
  *(_OWORD *)(a1 + 16) = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for vImage_PerpsectiveTransform(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 36))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for vImage_PerpsectiveTransform(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_QWORD *)(result + 16) = 0;
    *(_QWORD *)(result + 24) = 0;
    *(_DWORD *)(result + 32) = 0;
    *(_QWORD *)result = (a2 - 1);
    *(_QWORD *)(result + 8) = 0;
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 36) = v3;
  return result;
}

void type metadata accessor for vImage_PerpsectiveTransform(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for vImage_PerpsectiveTransform);
}

void type metadata accessor for BNNSFlags(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSFlags);
}

void *__swift_memcpy840_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x348uLL);
}

uint64_t getEnumTagSinglePayload for BNNSLayerParametersConvolution(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 840))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for BNNSLayerParametersConvolution(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 808) = 0u;
    *(_OWORD *)(result + 792) = 0u;
    *(_OWORD *)(result + 776) = 0u;
    *(_OWORD *)(result + 824) = 0u;
    *(_OWORD *)(result + 760) = 0u;
    *(_OWORD *)(result + 744) = 0u;
    *(_OWORD *)(result + 728) = 0u;
    *(_OWORD *)(result + 712) = 0u;
    *(_OWORD *)(result + 696) = 0u;
    *(_OWORD *)(result + 680) = 0u;
    *(_OWORD *)(result + 664) = 0u;
    *(_OWORD *)(result + 648) = 0u;
    *(_OWORD *)(result + 632) = 0u;
    *(_OWORD *)(result + 616) = 0u;
    *(_OWORD *)(result + 600) = 0u;
    *(_OWORD *)(result + 584) = 0u;
    *(_OWORD *)(result + 568) = 0u;
    *(_OWORD *)(result + 552) = 0u;
    *(_OWORD *)(result + 536) = 0u;
    *(_OWORD *)(result + 520) = 0u;
    *(_OWORD *)(result + 504) = 0u;
    *(_OWORD *)(result + 488) = 0u;
    *(_OWORD *)(result + 472) = 0u;
    *(_OWORD *)(result + 456) = 0u;
    *(_OWORD *)(result + 440) = 0u;
    *(_OWORD *)(result + 424) = 0u;
    *(_OWORD *)(result + 408) = 0u;
    *(_OWORD *)(result + 392) = 0u;
    *(_OWORD *)(result + 376) = 0u;
    *(_OWORD *)(result + 360) = 0u;
    *(_OWORD *)(result + 344) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(_QWORD *)result = (a2 - 1);
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 840) = v3;
  return result;
}

void type metadata accessor for BNNSLayerParametersConvolution(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSLayerParametersConvolution);
}

void *__swift_memcpy752_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x2F0uLL);
}

uint64_t getEnumTagSinglePayload for BNNSLayerParametersFullyConnected(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 752))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for BNNSLayerParametersFullyConnected(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_QWORD *)(result + 744) = 0;
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 728) = 0u;
    *(_OWORD *)(result + 712) = 0u;
    *(_OWORD *)(result + 696) = 0u;
    *(_OWORD *)(result + 680) = 0u;
    *(_OWORD *)(result + 664) = 0u;
    *(_OWORD *)(result + 648) = 0u;
    *(_OWORD *)(result + 632) = 0u;
    *(_OWORD *)(result + 616) = 0u;
    *(_OWORD *)(result + 600) = 0u;
    *(_OWORD *)(result + 584) = 0u;
    *(_OWORD *)(result + 568) = 0u;
    *(_OWORD *)(result + 552) = 0u;
    *(_OWORD *)(result + 536) = 0u;
    *(_OWORD *)(result + 520) = 0u;
    *(_OWORD *)(result + 504) = 0u;
    *(_OWORD *)(result + 488) = 0u;
    *(_OWORD *)(result + 472) = 0u;
    *(_OWORD *)(result + 456) = 0u;
    *(_OWORD *)(result + 440) = 0u;
    *(_OWORD *)(result + 424) = 0u;
    *(_OWORD *)(result + 408) = 0u;
    *(_OWORD *)(result + 392) = 0u;
    *(_OWORD *)(result + 376) = 0u;
    *(_OWORD *)(result + 360) = 0u;
    *(_OWORD *)(result + 344) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(_QWORD *)result = (a2 - 1);
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 752) = v3;
  return result;
}

void type metadata accessor for BNNSLayerParametersFullyConnected(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSLayerParametersFullyConnected);
}

void type metadata accessor for DSPDoubleComplex(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for DSPDoubleComplex);
}

_QWORD *__swift_memcpy8_4(_QWORD *result, _QWORD *a2)
{
  *result = *a2;
  return result;
}

uint64_t getEnumTagSinglePayload for DSPComplex(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 8))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for DSPComplex(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_QWORD *)result = (a2 - 1);
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 8) = v3;
  return result;
}

void type metadata accessor for DSPComplex(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for DSPComplex);
}

uint64_t getEnumTagSinglePayload for DSPDoubleSplitComplex(uint64_t a1, int a2)
{
  int v3;

  if (!a2)
    return 0;
  if (a2 != 1 && *(_BYTE *)(a1 + 16))
    return (*(_DWORD *)a1 + 2);
  if (*(_QWORD *)a1)
    v3 = -1;
  else
    v3 = 0;
  return (v3 + 1);
}

uint64_t storeEnumTagSinglePayload for DSPDoubleSplitComplex(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 1)
  {
    *(_QWORD *)result = a2 - 2;
    *(_QWORD *)(result + 8) = 0;
    if (a3 >= 2)
      *(_BYTE *)(result + 16) = 1;
  }
  else
  {
    if (a3 >= 2)
      *(_BYTE *)(result + 16) = 0;
    if (a2)
      *(_QWORD *)result = 0;
  }
  return result;
}

void type metadata accessor for DSPSplitComplex(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for DSPSplitComplex);
}

uint64_t getEnumTagSinglePayload for vImage_CGImageFormat(uint64_t a1, unsigned int a2)
{
  unint64_t v3;
  int v4;

  if (!a2)
    return 0;
  if (a2 >= 0x7FFFFFFF && *(_BYTE *)(a1 + 40))
    return (*(_DWORD *)a1 + 0x7FFFFFFF);
  v3 = *(_QWORD *)(a1 + 8);
  if (v3 >= 0xFFFFFFFF)
    LODWORD(v3) = -1;
  v4 = v3 - 1;
  if (v4 < 0)
    v4 = -1;
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for vImage_CGImageFormat(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0x7FFFFFFE)
  {
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_QWORD *)result = a2 - 0x7FFFFFFF;
    if (a3 >= 0x7FFFFFFF)
      *(_BYTE *)(result + 40) = 1;
  }
  else
  {
    if (a3 >= 0x7FFFFFFF)
      *(_BYTE *)(result + 40) = 0;
    if (a2)
      *(_QWORD *)(result + 8) = a2;
  }
  return result;
}

void type metadata accessor for vImage_CGImageFormat(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for vImage_CGImageFormat);
}

__n128 __swift_memcpy64_8(uint64_t a1, uint64_t a2)
{
  __n128 result;
  __int128 v3;
  __int128 v4;

  result = *(__n128 *)a2;
  v3 = *(_OWORD *)(a2 + 16);
  v4 = *(_OWORD *)(a2 + 48);
  *(_OWORD *)(a1 + 32) = *(_OWORD *)(a2 + 32);
  *(_OWORD *)(a1 + 48) = v4;
  *(__n128 *)a1 = result;
  *(_OWORD *)(a1 + 16) = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSLayerParametersArithmetic(uint64_t a1, int a2)
{
  int v3;

  if (!a2)
    return 0;
  if (a2 != 1 && *(_BYTE *)(a1 + 64))
    return (*(_DWORD *)a1 + 2);
  if (*(_QWORD *)(a1 + 8))
    v3 = -1;
  else
    v3 = 0;
  return (v3 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNSLayerParametersArithmetic(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 1)
  {
    *(_QWORD *)(result + 56) = 0;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_QWORD *)result = a2 - 2;
    if (a3 >= 2)
      *(_BYTE *)(result + 64) = 1;
  }
  else
  {
    if (a3 >= 2)
      *(_BYTE *)(result + 64) = 0;
    if (a2)
      *(_QWORD *)(result + 8) = 0;
  }
  return result;
}

void type metadata accessor for BNNSLayerParametersArithmetic(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSLayerParametersArithmetic);
}

uint64_t initializeBufferWithCopyOfBuffer for BNNSTensor(uint64_t *a1, uint64_t *a2)
{
  uint64_t v2;
  uint64_t v3;

  v2 = *a2;
  *a1 = *a2;
  v3 = v2 + 16;
  swift_retain();
  return v3;
}

__n128 __swift_memcpy160_8(uint64_t a1, uint64_t a2)
{
  __int128 v2;
  __int128 v3;
  __int128 v4;
  __int128 v5;
  __n128 result;
  __int128 v7;
  __int128 v8;

  v2 = *(_OWORD *)(a2 + 16);
  *(_OWORD *)a1 = *(_OWORD *)a2;
  *(_OWORD *)(a1 + 16) = v2;
  v3 = *(_OWORD *)(a2 + 32);
  v4 = *(_OWORD *)(a2 + 48);
  v5 = *(_OWORD *)(a2 + 80);
  *(_OWORD *)(a1 + 64) = *(_OWORD *)(a2 + 64);
  *(_OWORD *)(a1 + 80) = v5;
  *(_OWORD *)(a1 + 32) = v3;
  *(_OWORD *)(a1 + 48) = v4;
  result = *(__n128 *)(a2 + 96);
  v7 = *(_OWORD *)(a2 + 112);
  v8 = *(_OWORD *)(a2 + 144);
  *(_OWORD *)(a1 + 128) = *(_OWORD *)(a2 + 128);
  *(_OWORD *)(a1 + 144) = v8;
  *(__n128 *)(a1 + 96) = result;
  *(_OWORD *)(a1 + 112) = v7;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSTensor(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 160))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for BNNSTensor(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_QWORD *)(result + 152) = 0;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_QWORD *)result = (a2 - 1);
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 160) = v3;
  return result;
}

void type metadata accessor for BNNSTensor(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSTensor);
}

void type metadata accessor for bnns_graph_argument_t(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for bnns_graph_argument_t);
}

__n128 __swift_memcpy32_8(_OWORD *a1, uint64_t a2)
{
  __n128 result;
  __int128 v3;

  result = *(__n128 *)a2;
  v3 = *(_OWORD *)(a2 + 16);
  *a1 = *(_OWORD *)a2;
  a1[1] = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for vImage_Buffer(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 32))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for vImage_Buffer(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_QWORD *)(result + 16) = 0;
    *(_QWORD *)(result + 24) = 0;
    *(_QWORD *)result = (a2 - 1);
    *(_QWORD *)(result + 8) = 0;
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 32) = v3;
  return result;
}

void type metadata accessor for vImage_Buffer(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for vImage_Buffer);
}

__n128 __swift_memcpy176_8(uint64_t a1, __int128 *a2)
{
  __int128 v2;
  __int128 v3;
  __int128 v4;
  __int128 v5;
  __int128 v6;
  __n128 result;
  __int128 v8;
  __int128 v9;

  v2 = *a2;
  v3 = a2[2];
  *(_OWORD *)(a1 + 16) = a2[1];
  *(_OWORD *)(a1 + 32) = v3;
  *(_OWORD *)a1 = v2;
  v4 = a2[3];
  v5 = a2[4];
  v6 = a2[6];
  *(_OWORD *)(a1 + 80) = a2[5];
  *(_OWORD *)(a1 + 96) = v6;
  *(_OWORD *)(a1 + 48) = v4;
  *(_OWORD *)(a1 + 64) = v5;
  result = (__n128)a2[7];
  v8 = a2[8];
  v9 = a2[10];
  *(_OWORD *)(a1 + 144) = a2[9];
  *(_OWORD *)(a1 + 160) = v9;
  *(__n128 *)(a1 + 112) = result;
  *(_OWORD *)(a1 + 128) = v8;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSNDArrayDescriptor(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 176))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for BNNSNDArrayDescriptor(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_QWORD *)(result + 168) = 0;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_QWORD *)result = (a2 - 1);
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 176) = v3;
  return result;
}

void type metadata accessor for BNNSNDArrayDescriptor(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSNDArrayDescriptor);
}

void type metadata accessor for BNNSDataLayout(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSDataLayout);
}

void type metadata accessor for BNNSFilterType(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSFilterType);
}

void type metadata accessor for CFStringRef(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for CFStringRef);
}

void type metadata accessor for quadrature_integrator(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for quadrature_integrator);
}

void type metadata accessor for BNNSOptimizerClippingFunction(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSOptimizerClippingFunction);
}

void type metadata accessor for BNNSOptimizerRegularizationFunction(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSOptimizerRegularizationFunction);
}

__n128 __swift_memcpy48_4(uint64_t a1, uint64_t a2)
{
  __n128 result;
  __int128 v3;

  result = *(__n128 *)a2;
  v3 = *(_OWORD *)(a2 + 32);
  *(_OWORD *)(a1 + 16) = *(_OWORD *)(a2 + 16);
  *(_OWORD *)(a1 + 32) = v3;
  *(__n128 *)a1 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSOptimizerSGDMomentumWithClippingFields(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  BOOL v4;
  int v5;

  if (!a2)
    return 0;
  if (a2 >= 0xFF && *(_BYTE *)(a1 + 48))
    return (*(_DWORD *)a1 + 255);
  v3 = *(unsigned __int8 *)(a1 + 16);
  v4 = v3 >= 2;
  v5 = (v3 + 2147483646) & 0x7FFFFFFF;
  if (!v4)
    v5 = -1;
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNSOptimizerSGDMomentumWithClippingFields(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(_QWORD *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_QWORD *)result = a2 - 255;
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 48) = 1;
  }
  else
  {
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 48) = 0;
    if (a2)
      *(_BYTE *)(result + 16) = a2 + 1;
  }
  return result;
}

void type metadata accessor for BNNSOptimizerSGDMomentumWithClippingFields(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSOptimizerSGDMomentumWithClippingFields);
}

__n128 __swift_memcpy40_4(uint64_t a1, uint64_t a2)
{
  __n128 result;
  __int128 v3;

  result = *(__n128 *)a2;
  v3 = *(_OWORD *)(a2 + 16);
  *(_QWORD *)(a1 + 32) = *(_QWORD *)(a2 + 32);
  *(__n128 *)a1 = result;
  *(_OWORD *)(a1 + 16) = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSOptimizerSGDMomentumFields(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  BOOL v4;
  int v5;

  if (!a2)
    return 0;
  if (a2 >= 0xFF && *(_BYTE *)(a1 + 40))
    return (*(_DWORD *)a1 + 255);
  v3 = *(unsigned __int8 *)(a1 + 16);
  v4 = v3 >= 2;
  v5 = (v3 + 2147483646) & 0x7FFFFFFF;
  if (!v4)
    v5 = -1;
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNSOptimizerSGDMomentumFields(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_QWORD *)result = a2 - 255;
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 40) = 1;
  }
  else
  {
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 40) = 0;
    if (a2)
      *(_BYTE *)(result + 16) = a2 + 1;
  }
  return result;
}

void type metadata accessor for BNNSOptimizerSGDMomentumFields(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSOptimizerSGDMomentumFields);
}

uint64_t getEnumTagSinglePayload for BNNSOptimizerRMSPropWithClippingFields(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  BOOL v4;
  int v5;

  if (!a2)
    return 0;
  if (a2 >= 0xFF && *(_BYTE *)(a1 + 52))
    return (*(_DWORD *)a1 + 255);
  v3 = *(unsigned __int8 *)(a1 + 12);
  v4 = v3 >= 2;
  v5 = (v3 + 2147483646) & 0x7FFFFFFF;
  if (!v4)
    v5 = -1;
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNSOptimizerRMSPropWithClippingFields(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(_QWORD *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_DWORD *)(result + 48) = 0;
    *(_QWORD *)result = a2 - 255;
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 52) = 1;
  }
  else
  {
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 52) = 0;
    if (a2)
      *(_BYTE *)(result + 12) = a2 + 1;
  }
  return result;
}

void type metadata accessor for BNNSOptimizerRMSPropWithClippingFields(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSOptimizerRMSPropWithClippingFields);
}

__n128 __swift_memcpy44_4(uint64_t a1, uint64_t a2)
{
  __n128 result;
  __int128 v3;

  result = *(__n128 *)a2;
  v3 = *(_OWORD *)(a2 + 16);
  *(_OWORD *)(a1 + 28) = *(_OWORD *)(a2 + 28);
  *(__n128 *)a1 = result;
  *(_OWORD *)(a1 + 16) = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSOptimizerRMSPropFields(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  BOOL v4;
  int v5;

  if (!a2)
    return 0;
  if (a2 >= 0xFF && *(_BYTE *)(a1 + 44))
    return (*(_DWORD *)a1 + 255);
  v3 = *(unsigned __int8 *)(a1 + 12);
  v4 = v3 >= 2;
  v5 = (v3 + 2147483646) & 0x7FFFFFFF;
  if (!v4)
    v5 = -1;
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNSOptimizerRMSPropFields(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_DWORD *)(result + 40) = 0;
    *(_QWORD *)result = a2 - 255;
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 44) = 1;
  }
  else
  {
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 44) = 0;
    if (a2)
      *(_BYTE *)(result + 12) = a2 + 1;
  }
  return result;
}

void type metadata accessor for BNNSOptimizerRMSPropFields(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSOptimizerRMSPropFields);
}

uint64_t getEnumTagSinglePayload for BNNSOptimizerAdamFields(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  BOOL v4;
  int v5;

  if (!a2)
    return 0;
  if (a2 >= 0xFF && *(_BYTE *)(a1 + 44))
    return (*(_DWORD *)a1 + 255);
  v3 = *(unsigned __int8 *)(a1 + 28);
  v4 = v3 >= 2;
  v5 = (v3 + 2147483646) & 0x7FFFFFFF;
  if (!v4)
    v5 = -1;
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNSOptimizerAdamFields(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_DWORD *)(result + 40) = 0;
    *(_QWORD *)result = a2 - 255;
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 44) = 1;
  }
  else
  {
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 44) = 0;
    if (a2)
      *(_BYTE *)(result + 28) = a2 + 1;
  }
  return result;
}

void type metadata accessor for BNNSOptimizerAdamFields(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSOptimizerAdamFields);
}

void type metadata accessor for BNNSInterpolationMethod(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSInterpolationMethod);
}

void type metadata accessor for BNNSBoxCoordinateMode(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSBoxCoordinateMode);
}

void type metadata accessor for BNNSLinearSamplingMode(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSLinearSamplingMode);
}

void type metadata accessor for BNNSQuantizerFunction(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSQuantizerFunction);
}

void type metadata accessor for CGColorRenderingIntent(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for CGColorRenderingIntent);
}

void type metadata accessor for CGBitmapInfo(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for CGBitmapInfo);
}

void type metadata accessor for CGColorSpaceRef(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for CGColorSpaceRef);
}

__n128 __swift_memcpy48_8(uint64_t a1, uint64_t a2)
{
  __n128 result;
  __int128 v3;

  result = *(__n128 *)a2;
  v3 = *(_OWORD *)(a2 + 32);
  *(_OWORD *)(a1 + 16) = *(_OWORD *)(a2 + 16);
  *(_OWORD *)(a1 + 32) = v3;
  *(__n128 *)a1 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSActivation(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 48))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for BNNSActivation(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_QWORD *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_QWORD *)result = (a2 - 1);
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 48) = v3;
  return result;
}

void type metadata accessor for BNNSActivation(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSActivation);
}

void type metadata accessor for BNNSArithmeticFunction(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSArithmeticFunction);
}

void type metadata accessor for bnns_graph_argument_t.__Unnamed_union___Anonymous_field0(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for bnns_graph_argument_t.__Unnamed_union___Anonymous_field0);
}

void type metadata accessor for BNNSNDArrayFlags(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSNDArrayFlags);
}

uint64_t base witness table accessor for Equatable in CFStringRef()
{
  return lazy protocol witness table accessor for type CFStringRef and conformance CFStringRef(&lazy protocol witness table cache variable for type CFStringRef and conformance CFStringRef, (uint64_t)&protocol conformance descriptor for CFStringRef);
}

uint64_t base witness table accessor for Hashable in CFStringRef()
{
  return lazy protocol witness table accessor for type CFStringRef and conformance CFStringRef(&lazy protocol witness table cache variable for type CFStringRef and conformance CFStringRef, (uint64_t)&protocol conformance descriptor for CFStringRef);
}

uint64_t lazy protocol witness table accessor for type CFStringRef and conformance CFStringRef(unint64_t *a1, uint64_t a2)
{
  uint64_t result;
  uint64_t v5;

  result = *a1;
  if (!result)
  {
    type metadata accessor for CFStringRef(255);
    result = MEMORY[0x1D1794D08](a2, v5);
    atomic_store(result, a1);
  }
  return result;
}

void type metadata accessor for BNNSOptimizerSGDMomentumVariant(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSOptimizerSGDMomentumVariant);
}

void type metadata accessor for BNNSActivationFunction(uint64_t a1)
{
  type metadata accessor for vImageCVImageFormatRef(a1, &lazy cache variable for type metadata for BNNSActivationFunction);
}

void type metadata accessor for vImageCVImageFormatRef(uint64_t a1, unint64_t *a2)
{
  unint64_t ForeignTypeMetadata;
  uint64_t v4;

  if (!*a2)
  {
    ForeignTypeMetadata = swift_getForeignTypeMetadata();
    if (!v4)
      atomic_store(ForeignTypeMetadata, a2);
  }
}

ValueMetadata *type metadata accessor for vImage()
{
  return &type metadata for vImage;
}

ValueMetadata *type metadata accessor for vDSP()
{
  return &type metadata for vDSP;
}

ValueMetadata *type metadata accessor for vForce()
{
  return &type metadata for vForce;
}

ValueMetadata *type metadata accessor for vDSP.VectorizableFloat()
{
  return &type metadata for vDSP.VectorizableFloat;
}

ValueMetadata *type metadata accessor for vDSP.VectorizableDouble()
{
  return &type metadata for vDSP.VectorizableDouble;
}

vImage_Error vImage.PixelBuffer<>.init(interleavedBuffer:)@<X0>(uint64_t *a1@<X0>, _QWORD *a2@<X8>)
{
  uint64_t v3;
  uint64_t v4;
  _QWORD *v5;
  vImagePixelCount v6;
  vImagePixelCount v7;
  void *v8;
  vImagePixelCount v9;
  vImagePixelCount v10;
  vImagePixelCount v11;
  vImagePixelCount v12;
  size_t v13;
  size_t v14;
  _QWORD *v15;
  vImagePixelCount v16;
  vImagePixelCount v17;
  void *v18;
  vImagePixelCount v19;
  vImagePixelCount v20;
  vImagePixelCount v21;
  vImagePixelCount v22;
  size_t v23;
  size_t v24;
  _QWORD *v25;
  vImagePixelCount v26;
  vImagePixelCount v27;
  void *v28;
  vImagePixelCount v29;
  vImagePixelCount v30;
  vImagePixelCount v31;
  vImagePixelCount v32;
  size_t v33;
  size_t v34;
  _QWORD *v35;
  vImagePixelCount v36;
  size_t v37;
  vImage_Error result;
  vImagePixelCount v39;
  vImagePixelCount v40;
  size_t v41;
  vImagePixelCount v42;
  vImagePixelCount v43;
  void *v44;
  _QWORD *v45;
  __int128 v46;
  vImage_Buffer blueDest;
  vImage_Buffer greenDest;
  vImage_Buffer redDest;
  vImage_Buffer rgbSrc;
  uint64_t v51;

  v51 = *MEMORY[0x1E0C80C00];
  v3 = *a1;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v4 = swift_allocObject();
  *(_OWORD *)(v4 + 16) = xmmword_1CAB5EF70;
  if (!*(_QWORD *)(v3 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  v5 = (_QWORD *)v4;
  v6 = *(_QWORD *)(v3 + 48);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v7 = *(_QWORD *)(v3 + 40);
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v6)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v7)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v8 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v6, v7, 0x20u);
  v10 = v9;
  v12 = v11;
  v14 = v13;
  type metadata accessor for vImage.BufferReference();
  v15 = (_QWORD *)swift_allocObject();
  v15[2] = v8;
  v15[3] = v10;
  v15[4] = v12;
  v15[5] = v14;
  v5[4] = v8;
  v5[5] = v10;
  v5[6] = v12;
  v5[7] = v14;
  v5[8] = v15;
  v16 = *(_QWORD *)(v3 + 48);
  if ((v16 & 0x8000000000000000) != 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v17 = *(_QWORD *)(v3 + 40);
  if ((v17 & 0x8000000000000000) != 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v16)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v17)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v44 = v8;
  v45 = a2;
  v18 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v16, v17, 0x20u);
  v20 = v19;
  v22 = v21;
  v24 = v23;
  v25 = (_QWORD *)swift_allocObject();
  v25[2] = v18;
  v25[3] = v20;
  v25[4] = v22;
  v25[5] = v24;
  v5[9] = v18;
  v5[10] = v20;
  v5[11] = v22;
  v5[12] = v24;
  v5[13] = v25;
  v26 = *(_QWORD *)(v3 + 48);
  if ((v26 & 0x8000000000000000) != 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  v27 = *(_QWORD *)(v3 + 40);
  if ((v27 & 0x8000000000000000) != 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v26)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (!v27)
    goto LABEL_27;
  v39 = v22;
  v40 = v20;
  v41 = v14;
  v42 = v12;
  v43 = v10;
  v28 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v26, v27, 0x20u);
  v30 = v29;
  v32 = v31;
  v34 = v33;
  v35 = (_QWORD *)swift_allocObject();
  v35[2] = v28;
  v35[3] = v30;
  v35[4] = v32;
  v35[5] = v34;
  v5[14] = v28;
  v5[15] = v30;
  v5[16] = v32;
  v5[17] = v34;
  v5[18] = v35;
  v46 = *(_OWORD *)(v3 + 32);
  v36 = *(_QWORD *)(v3 + 48);
  v37 = *(_QWORD *)(v3 + 56);
  swift_bridgeObjectRelease();
  *(_OWORD *)&rgbSrc.data = v46;
  rgbSrc.width = v36;
  rgbSrc.rowBytes = v37;
  redDest.data = v44;
  redDest.height = v43;
  redDest.width = v42;
  redDest.rowBytes = v41;
  greenDest.data = v18;
  greenDest.height = v40;
  greenDest.width = v39;
  greenDest.rowBytes = v24;
  blueDest.data = v28;
  blueDest.height = v30;
  blueDest.width = v32;
  blueDest.rowBytes = v34;
  result = vImageConvert_RGBFFFtoPlanarF(&rgbSrc, &redDest, &greenDest, &blueDest, 0);
  *v45 = v5;
  return result;
}

{
  uint64_t v3;
  uint64_t v4;
  _QWORD *v5;
  vImagePixelCount v6;
  vImagePixelCount v7;
  void *v8;
  vImagePixelCount v9;
  vImagePixelCount v10;
  vImagePixelCount v11;
  vImagePixelCount v12;
  size_t v13;
  size_t v14;
  _QWORD *v15;
  vImagePixelCount v16;
  vImagePixelCount v17;
  void *v18;
  vImagePixelCount v19;
  vImagePixelCount v20;
  vImagePixelCount v21;
  vImagePixelCount v22;
  size_t v23;
  size_t v24;
  _QWORD *v25;
  vImagePixelCount v26;
  vImagePixelCount v27;
  _QWORD *v28;
  void *v29;
  vImagePixelCount v30;
  vImagePixelCount v31;
  vImagePixelCount v32;
  vImagePixelCount v33;
  size_t v34;
  size_t v35;
  _QWORD *v36;
  vImagePixelCount v37;
  vImagePixelCount v38;
  void *v39;
  vImagePixelCount v40;
  vImagePixelCount v41;
  vImagePixelCount v42;
  vImagePixelCount v43;
  size_t v44;
  size_t v45;
  _QWORD *v46;
  vImagePixelCount v47;
  size_t v48;
  vImage_Error result;
  __int128 v50;
  size_t v51;
  vImagePixelCount v52;
  size_t v53;
  _QWORD *v54;
  size_t v55;
  vImagePixelCount v56;
  vImagePixelCount v57;
  void *v58;
  vImagePixelCount v59;
  vImagePixelCount v60;
  void *v61;
  vImage_Buffer destB;
  vImage_Buffer destG;
  vImage_Buffer destR;
  vImage_Buffer destA;
  vImage_Buffer srcARGB;
  uint64_t v67;

  v67 = *MEMORY[0x1E0C80C00];
  v3 = *a1;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v4 = swift_allocObject();
  *(_OWORD *)(v4 + 16) = xmmword_1CAB5F170;
  if (!*(_QWORD *)(v3 + 16))
  {
    __break(1u);
    goto LABEL_20;
  }
  v5 = (_QWORD *)v4;
  v6 = *(_QWORD *)(v3 + 48);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = *(_QWORD *)(v3 + 40);
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v8 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v6, v7, 8u);
  v10 = v9;
  v12 = v11;
  v14 = v13;
  type metadata accessor for vImage.BufferReference();
  v15 = (_QWORD *)swift_allocObject();
  v15[2] = v8;
  v15[3] = v10;
  v15[4] = v12;
  v15[5] = v14;
  v5[4] = v8;
  v5[5] = v10;
  v5[6] = v12;
  v5[7] = v14;
  v5[8] = v15;
  v16 = *(_QWORD *)(v3 + 48);
  if ((v16 & 0x8000000000000000) != 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  v17 = *(_QWORD *)(v3 + 40);
  if ((v17 & 0x8000000000000000) != 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v16)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v17)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v59 = v12;
  v60 = v10;
  v61 = v8;
  v18 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v16, v17, 8u);
  v20 = v19;
  v22 = v21;
  v24 = v23;
  v25 = (_QWORD *)swift_allocObject();
  v25[2] = v18;
  v25[3] = v20;
  v25[4] = v22;
  v25[5] = v24;
  v5[9] = v18;
  v5[10] = v20;
  v5[11] = v22;
  v5[12] = v24;
  v5[13] = v25;
  v26 = *(_QWORD *)(v3 + 48);
  if ((v26 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  v27 = *(_QWORD *)(v3 + 40);
  if ((v27 & 0x8000000000000000) != 0)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v26)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (!v27)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v55 = v24;
  v56 = v22;
  v57 = v20;
  v58 = v18;
  v28 = a2;
  v29 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v26, v27, 8u);
  v31 = v30;
  v33 = v32;
  v35 = v34;
  v36 = (_QWORD *)swift_allocObject();
  v36[2] = v29;
  v36[3] = v31;
  v36[4] = v33;
  v36[5] = v35;
  v5[14] = v29;
  v5[15] = v31;
  v5[16] = v33;
  v5[17] = v35;
  v5[18] = v36;
  v37 = *(_QWORD *)(v3 + 48);
  if ((v37 & 0x8000000000000000) != 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v38 = *(_QWORD *)(v3 + 40);
  if ((v38 & 0x8000000000000000) != 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v37)
  {
LABEL_34:
    __break(1u);
LABEL_35:
    __break(1u);
  }
  if (!v38)
    goto LABEL_35;
  v51 = v35;
  v52 = v33;
  v53 = v14;
  v54 = v28;
  v39 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v37, v38, 8u);
  v41 = v40;
  v43 = v42;
  v45 = v44;
  v46 = (_QWORD *)swift_allocObject();
  v46[2] = v39;
  v46[3] = v41;
  v46[4] = v43;
  v46[5] = v45;
  v5[19] = v39;
  v5[20] = v41;
  v5[21] = v43;
  v5[22] = v45;
  v5[23] = v46;
  v50 = *(_OWORD *)(v3 + 32);
  v47 = *(_QWORD *)(v3 + 48);
  v48 = *(_QWORD *)(v3 + 56);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcARGB.data = v50;
  srcARGB.width = v47;
  srcARGB.rowBytes = v48;
  destA.data = v61;
  destA.height = v60;
  destA.width = v59;
  destA.rowBytes = v53;
  destR.data = v58;
  destR.height = v57;
  destR.width = v56;
  destR.rowBytes = v55;
  destG.data = v29;
  destG.height = v31;
  destG.width = v52;
  destG.rowBytes = v51;
  destB.data = v39;
  destB.height = v41;
  destB.width = v43;
  destB.rowBytes = v45;
  result = vImageConvert_ARGB8888toPlanar8(&srcARGB, &destA, &destR, &destG, &destB, 0);
  *v54 = v5;
  return result;
}

{
  uint64_t v3;
  uint64_t v4;
  _QWORD *v5;
  vImagePixelCount v6;
  vImagePixelCount v7;
  void *v8;
  vImagePixelCount v9;
  vImagePixelCount v10;
  vImagePixelCount v11;
  vImagePixelCount v12;
  size_t v13;
  size_t v14;
  _QWORD *v15;
  vImagePixelCount v16;
  vImagePixelCount v17;
  void *v18;
  vImagePixelCount v19;
  vImagePixelCount v20;
  vImagePixelCount v21;
  vImagePixelCount v22;
  size_t v23;
  size_t v24;
  _QWORD *v25;
  vImagePixelCount v26;
  vImagePixelCount v27;
  _QWORD *v28;
  void *v29;
  vImagePixelCount v30;
  vImagePixelCount v31;
  vImagePixelCount v32;
  vImagePixelCount v33;
  size_t v34;
  size_t v35;
  _QWORD *v36;
  vImagePixelCount v37;
  vImagePixelCount v38;
  void *v39;
  vImagePixelCount v40;
  vImagePixelCount v41;
  vImagePixelCount v42;
  vImagePixelCount v43;
  size_t v44;
  size_t v45;
  _QWORD *v46;
  vImagePixelCount v47;
  size_t v48;
  vImage_Error result;
  __int128 v50;
  size_t v51;
  vImagePixelCount v52;
  size_t v53;
  _QWORD *v54;
  size_t v55;
  vImagePixelCount v56;
  vImagePixelCount v57;
  void *v58;
  vImagePixelCount v59;
  vImagePixelCount v60;
  void *v61;
  vImage_Buffer destB;
  vImage_Buffer destG;
  vImage_Buffer destR;
  vImage_Buffer destA;
  vImage_Buffer srcARGB;
  uint64_t v67;

  v67 = *MEMORY[0x1E0C80C00];
  v3 = *a1;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v4 = swift_allocObject();
  *(_OWORD *)(v4 + 16) = xmmword_1CAB5F170;
  if (!*(_QWORD *)(v3 + 16))
  {
    __break(1u);
    goto LABEL_20;
  }
  v5 = (_QWORD *)v4;
  v6 = *(_QWORD *)(v3 + 48);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = *(_QWORD *)(v3 + 40);
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v8 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v6, v7, 0x20u);
  v10 = v9;
  v12 = v11;
  v14 = v13;
  type metadata accessor for vImage.BufferReference();
  v15 = (_QWORD *)swift_allocObject();
  v15[2] = v8;
  v15[3] = v10;
  v15[4] = v12;
  v15[5] = v14;
  v5[4] = v8;
  v5[5] = v10;
  v5[6] = v12;
  v5[7] = v14;
  v5[8] = v15;
  v16 = *(_QWORD *)(v3 + 48);
  if ((v16 & 0x8000000000000000) != 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  v17 = *(_QWORD *)(v3 + 40);
  if ((v17 & 0x8000000000000000) != 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v16)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v17)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v59 = v12;
  v60 = v10;
  v61 = v8;
  v18 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v16, v17, 0x20u);
  v20 = v19;
  v22 = v21;
  v24 = v23;
  v25 = (_QWORD *)swift_allocObject();
  v25[2] = v18;
  v25[3] = v20;
  v25[4] = v22;
  v25[5] = v24;
  v5[9] = v18;
  v5[10] = v20;
  v5[11] = v22;
  v5[12] = v24;
  v5[13] = v25;
  v26 = *(_QWORD *)(v3 + 48);
  if ((v26 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  v27 = *(_QWORD *)(v3 + 40);
  if ((v27 & 0x8000000000000000) != 0)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v26)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (!v27)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v55 = v24;
  v56 = v22;
  v57 = v20;
  v58 = v18;
  v28 = a2;
  v29 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v26, v27, 0x20u);
  v31 = v30;
  v33 = v32;
  v35 = v34;
  v36 = (_QWORD *)swift_allocObject();
  v36[2] = v29;
  v36[3] = v31;
  v36[4] = v33;
  v36[5] = v35;
  v5[14] = v29;
  v5[15] = v31;
  v5[16] = v33;
  v5[17] = v35;
  v5[18] = v36;
  v37 = *(_QWORD *)(v3 + 48);
  if ((v37 & 0x8000000000000000) != 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v38 = *(_QWORD *)(v3 + 40);
  if ((v38 & 0x8000000000000000) != 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v37)
  {
LABEL_34:
    __break(1u);
LABEL_35:
    __break(1u);
  }
  if (!v38)
    goto LABEL_35;
  v51 = v35;
  v52 = v33;
  v53 = v14;
  v54 = v28;
  v39 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v37, v38, 0x20u);
  v41 = v40;
  v43 = v42;
  v45 = v44;
  v46 = (_QWORD *)swift_allocObject();
  v46[2] = v39;
  v46[3] = v41;
  v46[4] = v43;
  v46[5] = v45;
  v5[19] = v39;
  v5[20] = v41;
  v5[21] = v43;
  v5[22] = v45;
  v5[23] = v46;
  v50 = *(_OWORD *)(v3 + 32);
  v47 = *(_QWORD *)(v3 + 48);
  v48 = *(_QWORD *)(v3 + 56);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcARGB.data = v50;
  srcARGB.width = v47;
  srcARGB.rowBytes = v48;
  destA.data = v61;
  destA.height = v60;
  destA.width = v59;
  destA.rowBytes = v53;
  destR.data = v58;
  destR.height = v57;
  destR.width = v56;
  destR.rowBytes = v55;
  destG.data = v29;
  destG.height = v31;
  destG.width = v52;
  destG.rowBytes = v51;
  destB.data = v39;
  destB.height = v41;
  destB.width = v43;
  destB.rowBytes = v45;
  result = vImageConvert_ARGBFFFFtoPlanarF(&srcARGB, &destA, &destR, &destG, &destB, 0);
  *v54 = v5;
  return result;
}

{
  uint64_t v3;
  uint64_t v4;
  _QWORD *v5;
  vImagePixelCount v6;
  vImagePixelCount v7;
  void *v8;
  vImagePixelCount v9;
  vImagePixelCount v10;
  vImagePixelCount v11;
  vImagePixelCount v12;
  size_t v13;
  size_t v14;
  _QWORD *v15;
  vImagePixelCount v16;
  vImagePixelCount v17;
  void *v18;
  vImagePixelCount v19;
  vImagePixelCount v20;
  vImagePixelCount v21;
  vImagePixelCount v22;
  size_t v23;
  size_t v24;
  _QWORD *v25;
  vImagePixelCount v26;
  vImagePixelCount v27;
  void *v28;
  vImagePixelCount v29;
  vImagePixelCount v30;
  vImagePixelCount v31;
  vImagePixelCount v32;
  size_t v33;
  size_t v34;
  _QWORD *v35;
  vImagePixelCount v36;
  size_t v37;
  vImage_Error result;
  vImagePixelCount v39;
  vImagePixelCount v40;
  size_t v41;
  vImagePixelCount v42;
  vImagePixelCount v43;
  void *v44;
  _QWORD *v45;
  __int128 v46;
  vImage_Buffer blueDest;
  vImage_Buffer greenDest;
  vImage_Buffer redDest;
  vImage_Buffer rgbSrc;
  uint64_t v51;

  v51 = *MEMORY[0x1E0C80C00];
  v3 = *a1;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v4 = swift_allocObject();
  *(_OWORD *)(v4 + 16) = xmmword_1CAB5EF70;
  if (!*(_QWORD *)(v3 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  v5 = (_QWORD *)v4;
  v6 = *(_QWORD *)(v3 + 48);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v7 = *(_QWORD *)(v3 + 40);
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v6)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v7)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v8 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v6, v7, 8u);
  v10 = v9;
  v12 = v11;
  v14 = v13;
  type metadata accessor for vImage.BufferReference();
  v15 = (_QWORD *)swift_allocObject();
  v15[2] = v8;
  v15[3] = v10;
  v15[4] = v12;
  v15[5] = v14;
  v5[4] = v8;
  v5[5] = v10;
  v5[6] = v12;
  v5[7] = v14;
  v5[8] = v15;
  v16 = *(_QWORD *)(v3 + 48);
  if ((v16 & 0x8000000000000000) != 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v17 = *(_QWORD *)(v3 + 40);
  if ((v17 & 0x8000000000000000) != 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v16)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v17)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v44 = v8;
  v45 = a2;
  v18 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v16, v17, 8u);
  v20 = v19;
  v22 = v21;
  v24 = v23;
  v25 = (_QWORD *)swift_allocObject();
  v25[2] = v18;
  v25[3] = v20;
  v25[4] = v22;
  v25[5] = v24;
  v5[9] = v18;
  v5[10] = v20;
  v5[11] = v22;
  v5[12] = v24;
  v5[13] = v25;
  v26 = *(_QWORD *)(v3 + 48);
  if ((v26 & 0x8000000000000000) != 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  v27 = *(_QWORD *)(v3 + 40);
  if ((v27 & 0x8000000000000000) != 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v26)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (!v27)
    goto LABEL_27;
  v39 = v22;
  v40 = v20;
  v41 = v14;
  v42 = v12;
  v43 = v10;
  v28 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v26, v27, 8u);
  v30 = v29;
  v32 = v31;
  v34 = v33;
  v35 = (_QWORD *)swift_allocObject();
  v35[2] = v28;
  v35[3] = v30;
  v35[4] = v32;
  v35[5] = v34;
  v5[14] = v28;
  v5[15] = v30;
  v5[16] = v32;
  v5[17] = v34;
  v5[18] = v35;
  v46 = *(_OWORD *)(v3 + 32);
  v36 = *(_QWORD *)(v3 + 48);
  v37 = *(_QWORD *)(v3 + 56);
  swift_bridgeObjectRelease();
  *(_OWORD *)&rgbSrc.data = v46;
  rgbSrc.width = v36;
  rgbSrc.rowBytes = v37;
  redDest.data = v44;
  redDest.height = v43;
  redDest.width = v42;
  redDest.rowBytes = v41;
  greenDest.data = v18;
  greenDest.height = v40;
  greenDest.width = v39;
  greenDest.rowBytes = v24;
  blueDest.data = v28;
  blueDest.height = v30;
  blueDest.width = v32;
  blueDest.rowBytes = v34;
  result = vImageConvert_RGB888toPlanar8(&rgbSrc, &redDest, &greenDest, &blueDest, 0);
  *v45 = v5;
  return result;
}

uint64_t specialized vImage.PixelBuffer<>.vImageBuffers.getter(uint64_t a1)
{
  int64_t v1;
  uint64_t v2;
  unint64_t v4;
  uint64_t v5;
  __int128 *v6;
  __int128 v7;
  __int128 v8;
  unint64_t v9;
  unint64_t v10;
  uint64_t v11;
  __int128 v13;
  __int128 v14;
  uint64_t v15;

  v1 = *(_QWORD *)(a1 + 16);
  v2 = MEMORY[0x1E0DEE9D8];
  if (v1)
  {
    v15 = MEMORY[0x1E0DEE9D8];
    swift_bridgeObjectRetain();
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v1, 0);
    v2 = v15;
    v4 = *(_QWORD *)(v15 + 16);
    v5 = 32 * v4;
    v6 = (__int128 *)(a1 + 48);
    do
    {
      v7 = *(v6 - 1);
      v8 = *v6;
      v9 = *(_QWORD *)(v15 + 24);
      v10 = v4 + 1;
      if (v4 >= v9 >> 1)
      {
        v13 = *v6;
        v14 = *(v6 - 1);
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v9 > 1), v4 + 1, 1);
        v8 = v13;
        v7 = v14;
      }
      *(_QWORD *)(v15 + 16) = v10;
      v11 = v15 + v5;
      *(_OWORD *)(v11 + 32) = v7;
      *(_OWORD *)(v11 + 48) = v8;
      v5 += 32;
      v6 = (__int128 *)((char *)v6 + 40);
      v4 = v10;
      --v1;
    }
    while (v1);
    swift_bridgeObjectRelease();
  }
  return v2;
}

uint64_t vImage.PixelBuffer<>.interleave(destination:)()
{
  uint64_t *v0;
  _QWORD *v1;

  v1 = (_QWORD *)specialized vImage.PixelBuffer<>.pixelBuffers.getter(*v0, (void (*)(BOOL, unint64_t, uint64_t))specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:));
  vImage.PixelBuffer<>.interleave(planarSourceBuffers:)(v1);
  return swift_bridgeObjectRelease();
}

{
  uint64_t *v0;
  _QWORD *v1;

  v1 = (_QWORD *)specialized vImage.PixelBuffer<>.pixelBuffers.getter(*v0);
  vImage.PixelBuffer<>.interleave(planarSourceBuffers:)(v1);
  return swift_bridgeObjectRelease();
}

{
  uint64_t *v0;
  _QWORD *v1;

  v1 = (_QWORD *)specialized vImage.PixelBuffer<>.pixelBuffers.getter(*v0);
  vImage.PixelBuffer<>.interleave(planarSourceBuffers:)(v1);
  return swift_bridgeObjectRelease();
}

{
  uint64_t *v0;
  _QWORD *v1;

  v1 = (_QWORD *)specialized vImage.PixelBuffer<>.pixelBuffers.getter(*v0);
  vImage.PixelBuffer<>.interleave(planarSourceBuffers:)(v1);
  return swift_bridgeObjectRelease();
}

uint64_t specialized vImage.PixelBuffer<>.pixelBuffers.getter(uint64_t a1)
{
  return specialized vImage.PixelBuffer<>.pixelBuffers.getter(a1, (void (*)(BOOL, unint64_t, uint64_t))specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:));
}

{
  return specialized vImage.PixelBuffer<>.pixelBuffers.getter(a1, (void (*)(BOOL, unint64_t, uint64_t))specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:));
}

uint64_t specialized vImage.PixelBuffer<>.pixelBuffers.getter(uint64_t a1, void (*a2)(BOOL, unint64_t, uint64_t))
{
  unint64_t v2;
  uint64_t v3;
  _QWORD *v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  unint64_t v11;
  unint64_t v12;
  void (*v14)(BOOL, unint64_t, uint64_t);
  __int128 v15;
  uint64_t v16;

  v2 = *(_QWORD *)(a1 + 16);
  v3 = MEMORY[0x1E0DEE9D8];
  if (v2)
  {
    v16 = MEMORY[0x1E0DEE9D8];
    swift_bridgeObjectRetain();
    v14 = a2;
    a2(0, v2, 0);
    v3 = v16;
    v6 = (_QWORD *)(a1 + 64);
    do
    {
      v15 = *((_OWORD *)v6 - 2);
      v7 = *(v6 - 2);
      v8 = *(v6 - 1);
      v9 = *v6;
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
      v10 = swift_allocObject();
      *(_OWORD *)(v10 + 16) = xmmword_1CAB5E430;
      *(_OWORD *)(v10 + 32) = v15;
      *(_QWORD *)(v10 + 48) = v7;
      *(_QWORD *)(v10 + 56) = v8;
      *(_QWORD *)(v10 + 64) = v9;
      v12 = *(_QWORD *)(v16 + 16);
      v11 = *(_QWORD *)(v16 + 24);
      swift_retain();
      if (v12 >= v11 >> 1)
        v14(v11 > 1, v12 + 1, 1);
      v6 += 5;
      *(_QWORD *)(v16 + 16) = v12 + 1;
      *(_QWORD *)(v16 + 8 * v12 + 32) = v10;
      --v2;
    }
    while (v2);
    swift_bridgeObjectRelease();
  }
  return v3;
}

uint64_t vImage.PixelBuffer<>.applyPolynomial(coefficientSegments:boundaries:destination:)(uint64_t a1, uint64_t a2, _QWORD **a3)
{
  _QWORD **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 1, *v3, MEMORY[0x1E0C8D580]);
}

{
  _QWORD **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 2, *v3, MEMORY[0x1E0C8D580]);
}

{
  _QWORD **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 3, *v3, MEMORY[0x1E0C8D580]);
}

{
  _QWORD **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 4, *v3, MEMORY[0x1E0C8D580]);
}

{
  _QWORD **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 1, *v3, MEMORY[0x1E0C8D588]);
}

{
  _QWORD **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 1, *v3, MEMORY[0x1E0C8D590]);
}

{
  _QWORD **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 2, *v3, MEMORY[0x1E0C8D588]);
}

{
  _QWORD **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 2, *v3, MEMORY[0x1E0C8D590]);
}

{
  _QWORD **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 3, *v3, MEMORY[0x1E0C8D588]);
}

{
  _QWORD **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 3, *v3, MEMORY[0x1E0C8D590]);
}

{
  _QWORD **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 4, *v3, MEMORY[0x1E0C8D588]);
}

{
  _QWORD **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 4, *v3, MEMORY[0x1E0C8D590]);
}

uint64_t specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(uint64_t a1, uint64_t a2, _QWORD *a3, uint64_t a4, _QWORD *a5, void (*a6)(_QWORD *, _QWORD *, uint64_t, uint64_t, _QWORD, uint64_t, _QWORD))
{
  unint64_t v6;
  uint64_t v8;
  uint64_t v9;
  uint64_t v11;
  uint64_t v12;
  uint64_t v14;
  uint64_t v15;
  unint64_t v16;
  uint64_t v17;
  unint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  float *v22;
  uint64_t v23;
  unint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t inited;
  uint64_t v28;
  int v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  int32x2_t v33;
  uint64_t v34;
  uint64_t v35;
  int v36;
  unint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  unint64_t v41;
  int32x4_t v42;
  int32x4_t *v43;
  unint64_t v44;
  uint64_t v45;
  _DWORD *v46;
  uint64_t v48;
  uint64_t i;
  uint64_t v50;
  int32x2_t v52;
  uint64_t v53;
  uint64_t v54;
  int v55;
  uint64_t v58;
  uint64_t v59;
  uint64_t v60;
  _QWORD v62[4];
  _QWORD v63[24];

  v63[22] = *MEMORY[0x1E0C80C00];
  v6 = *(_QWORD *)(a1 + 16);
  if (!v6)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (!a5[2])
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  v8 = a5[6];
  if (v8 < 0)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  v9 = a5[5];
  if (v9 < 0)
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  if (!v8)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  if (!v9)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!a3[2])
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  v11 = a3[6];
  if (v11 < 0)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  v12 = a3[5];
  if (v12 < 0)
  {
LABEL_68:
    __break(1u);
    goto LABEL_69;
  }
  if (!v11)
  {
LABEL_69:
    __break(1u);
    goto LABEL_70;
  }
  if (!v12)
  {
LABEL_70:
    __break(1u);
    goto LABEL_71;
  }
  if (v8 != v11)
  {
LABEL_71:
    __break(1u);
    goto LABEL_72;
  }
  if (v9 != v12)
  {
LABEL_72:
    __break(1u);
LABEL_73:
    __break(1u);
    goto LABEL_74;
  }
  v60 = a5[5];
  v63[0] = MEMORY[0x1E0DEE9D8];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v6, 0);
  v14 = 0;
  v15 = v63[0];
  v16 = *(_QWORD *)(v63[0] + 16);
  do
  {
    v17 = *(_QWORD *)(*(_QWORD *)(a1 + 8 * v14 + 32) + 16);
    v63[0] = v15;
    v18 = *(_QWORD *)(v15 + 24);
    if (v16 >= v18 >> 1)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v18 > 1), v16 + 1, 1);
      v15 = v63[0];
    }
    ++v14;
    *(_QWORD *)(v15 + 16) = v16 + 1;
    *(_QWORD *)(v15 + 8 * v16++ + 32) = v17;
  }
  while (v6 != v14);
  v19 = specialized Set.init<A>(_:)(v15, MEMORY[0x1E0DEB418], MEMORY[0x1E0DEB428], &demangling cache variable for type metadata for _SetStorage<Int>);
  swift_bridgeObjectRelease();
  v20 = *(_QWORD *)(v19 + 16);
  swift_bridgeObjectRelease();
  if (v20 != 1)
    goto LABEL_73;
  v21 = *(_QWORD *)(a2 + 16);
  if (v21 != v6 + 1)
  {
LABEL_74:
    __break(1u);
    goto LABEL_75;
  }
  if (v21 != 1)
  {
    if (!v21)
    {
LABEL_75:
      __break(1u);
      goto LABEL_76;
    }
    v22 = (float *)(a2 + 36);
    v23 = *(_QWORD *)(a2 + 16);
    while (v23)
    {
      if (*v22 < *(v22 - 1))
        goto LABEL_59;
      --v23;
      ++v22;
      if (v23 == 1)
        goto LABEL_26;
    }
    __break(1u);
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
LABEL_26:
  v24 = *(_QWORD *)(*(_QWORD *)(a1 + 32) + 16);
  if (!v24)
  {
LABEL_76:
    __break(1u);
    goto LABEL_77;
  }
  if (v24 > 0x100000000)
  {
LABEL_77:
    __break(1u);
LABEL_78:
    __break(1u);
    goto LABEL_79;
  }
  v59 = a5[4];
  if (!v59)
  {
LABEL_82:
    __break(1u);
LABEL_83:
    swift_setDeallocating();
    swift_arrayDestroy();
    __break(1u);
  }
  v25 = v8 * a4;
  if ((unsigned __int128)(v8 * (__int128)a4) >> 64 != (v8 * a4) >> 63)
    goto LABEL_78;
  v26 = a5[7];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5E430;
  if (v25 < 0)
  {
LABEL_79:
    __break(1u);
LABEL_80:
    __break(1u);
    goto LABEL_81;
  }
  *(_QWORD *)(inited + 32) = v59;
  *(_QWORD *)(inited + 40) = v60;
  *(_QWORD *)(inited + 48) = v25;
  *(_QWORD *)(inited + 56) = v26;
  v55 = v24;
  *(_QWORD *)(inited + 64) = 0;
  v28 = a3[4];
  if (!v28)
    goto LABEL_83;
  v29 = __clz(v6);
  v30 = a3[7];
  v31 = swift_initStackObject();
  *(_OWORD *)(v31 + 16) = xmmword_1CAB5E430;
  *(_QWORD *)(v31 + 32) = v28;
  *(_QWORD *)(v31 + 40) = v60;
  *(_QWORD *)(v31 + 48) = v25;
  *(_QWORD *)(v31 + 56) = v30;
  *(_QWORD *)(v31 + 64) = 0;
  if (v29 == 1)
    goto LABEL_80;
  v58 = v28;
  v53 = (64 - v29);
  v54 = v30;
  swift_bridgeObjectRetain();
  v32 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_SPySfGSgs5NeverOTg50161_s10Accelerate6vImageO11PixelBufferVA2A06StaticC6FormatRzrlE16_applyPolynomial19coefficientSegments10boundaries11destination14polynomialFunc15widthMultiplierySayw41SfGG_AmEy_qd__GSiSPySo01vb1_D0VG_ARSpySPyx14GSgGASs6UInt32K42WtXESitAA011SinglePlanecF0Rd__lFATSiXEfU0_SaySaySfGGTf1cn_nTf4ng_n(0, 1 << (64 - v29), a1);
  swift_bridgeObjectRelease();
  v34 = *(_QWORD *)(v32 + 16);
  if (v21 == v34 + 1)
  {
    v35 = a2;
    swift_bridgeObjectRetain();
    swift_release();
    v36 = v55;
    goto LABEL_49;
  }
  v37 = v34 - v21;
  if (v34 - v21 < -1)
  {
LABEL_81:
    __break(1u);
    goto LABEL_82;
  }
  if (v37 == -1)
  {
    v39 = MEMORY[0x1E0DEE9D8];
  }
  else
  {
    v33.i32[0] = *(_DWORD *)(a2 + 4 * v21 + 28);
    v52 = v33;
    v38 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    v39 = v38;
    *(_QWORD *)(v38 + 16) = v37 + 1;
    *(_DWORD *)(v38 + 32) = v52.i32[0];
    v40 = v38 + 32;
    if (v34 != v21)
    {
      if (v37 < 8)
      {
        v41 = 0;
LABEL_45:
        v45 = v41 + v21 - v34;
        v46 = (_DWORD *)(v40 + 4);
        v36 = v55;
        do
          *v46++ = v52.i32[0];
        while (!__CFADD__(v45++, 1));
        goto LABEL_48;
      }
      v41 = v37 & 0xFFFFFFFFFFFFFFF8;
      v40 += 4 * (v37 & 0xFFFFFFFFFFFFFFF8);
      v42 = vdupq_lane_s32(v52, 0);
      v43 = (int32x4_t *)(v38 + 52);
      v44 = v37 & 0xFFFFFFFFFFFFFFF8;
      do
      {
        v43[-1] = v42;
        *v43 = v42;
        v43 += 2;
        v44 -= 8;
      }
      while (v44);
      if (v37 != v41)
        goto LABEL_45;
    }
  }
  v36 = v55;
LABEL_48:
  swift_bridgeObjectRetain();
  specialized Array.append<A>(contentsOf:)(v39);
  swift_release();
  v35 = a2;
LABEL_49:
  v63[0] = v59;
  v63[1] = v60;
  v63[2] = v25;
  v63[3] = v26;
  v62[0] = v58;
  v62[1] = v60;
  v62[2] = v25;
  v62[3] = v54;
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    v32 = (uint64_t)specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(_QWORD *)(v32 + 16), 0, (char *)v32, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<Float>?>);
  a6(v63, v62, v32 + 32, v35 + 32, (v36 - 1), v53, 0);
  swift_bridgeObjectRelease();
  swift_release();
  v48 = *(_QWORD *)(v32 + 16);
  if (v48)
  {
    swift_bridgeObjectRetain();
    for (i = 0; i != v48; ++i)
    {
      v50 = *(_QWORD *)(v32 + 8 * i + 32);
      if (v50)
        MEMORY[0x1D1794DA4](v50, -1, -1);
    }
    swift_bridgeObjectRelease();
  }
  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(uint64_t a1, uint64_t a2, uint64_t *a3, void (*a4)(uint64_t *, uint64_t *, uint64_t, uint64_t, _QWORD, uint64_t, _QWORD), uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t *v10;
  unint64_t v11;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  unint64_t v21;
  uint64_t v22;
  unint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  float *v27;
  uint64_t v28;
  unint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t inited;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  int v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  int32x2_t v48;
  uint64_t v49;
  void (*v50)(uint64_t *, uint64_t *, uint64_t, uint64_t, _QWORD, uint64_t, _QWORD);
  unint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  unint64_t v55;
  int32x4_t v56;
  int32x4_t *v57;
  unint64_t v58;
  uint64_t v59;
  _DWORD *v60;
  uint64_t v62;
  uint64_t i;
  uint64_t v64;
  uint64_t v66;
  uint64_t v67;
  uint64_t v68;
  uint64_t v69;
  uint64_t v70;
  void (*v71)(uint64_t *, uint64_t *, uint64_t, uint64_t, _QWORD, uint64_t, _QWORD);
  int32x2_t v72;
  uint64_t v73;
  uint64_t v74;
  uint64_t v77;
  uint64_t v78;
  uint64_t v79;
  uint64_t v80;
  uint64_t v81;
  uint64_t v82;
  uint64_t v83;
  uint64_t v84;
  uint64_t v85;

  v85 = *MEMORY[0x1E0C80C00];
  v11 = *(_QWORD *)(a1 + 16);
  if (!v11)
  {
LABEL_53:
    __break(1u);
LABEL_54:
    __break(1u);
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  v15 = *a3;
  v73 = *v10;
  v77 = *v10;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&v81);
  v17 = v81;
  v16 = v82;
  type metadata accessor for vImage.PixelBuffer(0, a8, *(_QWORD *)(a10 + 8), v18);
  vImage.PixelBuffer.size.getter(&v77);
  swift_bridgeObjectRelease();
  if (v17 != v77 || v16 != v78)
    goto LABEL_54;
  v71 = a4;
  v81 = MEMORY[0x1E0DEE9D8];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v11, 0);
  v19 = 0;
  v20 = v81;
  v21 = *(_QWORD *)(v81 + 16);
  do
  {
    v22 = *(_QWORD *)(*(_QWORD *)(a1 + 8 * v19 + 32) + 16);
    v81 = v20;
    v23 = *(_QWORD *)(v20 + 24);
    if (v21 >= v23 >> 1)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v23 > 1), v21 + 1, 1);
      v20 = v81;
    }
    ++v19;
    *(_QWORD *)(v20 + 16) = v21 + 1;
    *(_QWORD *)(v20 + 8 * v21++ + 32) = v22;
  }
  while (v11 != v19);
  v24 = specialized Set.init<A>(_:)(v20, MEMORY[0x1E0DEB418], MEMORY[0x1E0DEB428], &demangling cache variable for type metadata for _SetStorage<Int>);
  swift_bridgeObjectRelease();
  v25 = *(_QWORD *)(v24 + 16);
  swift_bridgeObjectRelease();
  if (v25 != 1)
    goto LABEL_55;
  v26 = *(_QWORD *)(a2 + 16);
  if (v26 != v11 + 1)
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  if (v26 != 1)
  {
    if (!v26)
    {
LABEL_57:
      __break(1u);
      goto LABEL_58;
    }
    v27 = (float *)(a2 + 36);
    v28 = *(_QWORD *)(a2 + 16);
    while (v28)
    {
      if (*v27 < *(v27 - 1))
        goto LABEL_52;
      --v28;
      ++v27;
      if (v28 == 1)
        goto LABEL_16;
    }
    __break(1u);
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
LABEL_16:
  v29 = *(_QWORD *)(*(_QWORD *)(a1 + 32) + 16);
  if (!v29)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (v29 > 0x100000000)
  {
LABEL_59:
    __break(1u);
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  v70 = v15;
  v81 = v73;
  v30 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v30)
  {
LABEL_66:
    __break(1u);
LABEL_67:
    swift_setDeallocating();
    swift_arrayDestroy();
    __break(1u);
  }
  v31 = v30;
  v81 = v73;
  v32 = vImage.PixelBuffer.width.getter();
  v33 = v32 * a6;
  if ((unsigned __int128)(v32 * (__int128)a6) >> 64 != (v32 * a6) >> 63)
    goto LABEL_60;
  v81 = v73;
  v34 = vImage.PixelBuffer.height.getter();
  v81 = v73;
  vImage.PixelBuffer<>.vImageBuffer.getter();
  v36 = v35;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5E430;
  if ((v34 | v33) < 0)
  {
LABEL_61:
    __break(1u);
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  *(_QWORD *)(inited + 32) = v31;
  *(_QWORD *)(inited + 40) = v34;
  v68 = v34;
  v69 = v33;
  *(_QWORD *)(inited + 48) = v33;
  *(_QWORD *)(inited + 56) = v36;
  v67 = v36;
  *(_QWORD *)(inited + 64) = 0;
  v81 = v70;
  v74 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v74)
    goto LABEL_67;
  v81 = v70;
  v38 = vImage.PixelBuffer.width.getter();
  v39 = v38 * a6;
  if ((unsigned __int128)(v38 * (__int128)a6) >> 64 != (v38 * a6) >> 63)
    goto LABEL_62;
  v66 = v31;
  v81 = v70;
  v40 = vImage.PixelBuffer.height.getter();
  v81 = v70;
  vImage.PixelBuffer<>.vImageBuffer.getter();
  v42 = v41;
  v43 = swift_initStackObject();
  *(_OWORD *)(v43 + 16) = xmmword_1CAB5E430;
  if ((v40 | v39) < 0)
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  v44 = __clz(v11);
  v45 = v74;
  *(_QWORD *)(v43 + 32) = v74;
  *(_QWORD *)(v43 + 40) = v40;
  *(_QWORD *)(v43 + 48) = v39;
  *(_QWORD *)(v43 + 56) = v42;
  *(_QWORD *)(v43 + 64) = 0;
  if (v44 == 1)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  v46 = (64 - v44);
  swift_bridgeObjectRetain();
  v47 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_SPySfGSgs5NeverOTg50161_s10Accelerate6vImageO11PixelBufferVA2A06StaticC6FormatRzrlE16_applyPolynomial19coefficientSegments10boundaries11destination14polynomialFunc15widthMultiplierySayw41SfGG_AmEy_qd__GSiSPySo01vb1_D0VG_ARSpySPyx14GSgGASs6UInt32K42WtXESitAA011SinglePlanecF0Rd__lFATSiXEfU0_SaySaySfGGTf1cn_nTf4ng_n(0, 1 << v46, a1);
  swift_bridgeObjectRelease();
  v49 = *(_QWORD *)(v47 + 16);
  if (v26 == v49 + 1)
  {
    swift_bridgeObjectRetain();
    swift_release();
    v50 = v71;
    goto LABEL_42;
  }
  v51 = v49 - v26;
  if (v49 - v26 < -1)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (v51 == -1)
  {
    v53 = MEMORY[0x1E0DEE9D8];
  }
  else
  {
    v48.i32[0] = *(_DWORD *)(a2 + 4 * v26 + 28);
    v72 = v48;
    v52 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    v53 = v52;
    *(_QWORD *)(v52 + 16) = v51 + 1;
    *(_DWORD *)(v52 + 32) = v72.i32[0];
    v54 = v52 + 32;
    if (v49 != v26)
    {
      if (v51 < 8)
      {
        v55 = 0;
LABEL_37:
        v59 = v55 + v26 - v49;
        v60 = (_DWORD *)(v54 + 4);
        v45 = v74;
        do
          *v60++ = v72.i32[0];
        while (!__CFADD__(v59++, 1));
        v50 = v71;
        goto LABEL_41;
      }
      v55 = v51 & 0xFFFFFFFFFFFFFFF8;
      v54 += 4 * (v51 & 0xFFFFFFFFFFFFFFF8);
      v56 = vdupq_lane_s32(v72, 0);
      v57 = (int32x4_t *)(v52 + 52);
      v58 = v51 & 0xFFFFFFFFFFFFFFF8;
      do
      {
        v57[-1] = v56;
        *v57 = v56;
        v57 += 2;
        v58 -= 8;
      }
      while (v58);
      if (v51 != v55)
        goto LABEL_37;
    }
  }
  v50 = v71;
  v45 = v74;
LABEL_41:
  v81 = a2;
  swift_bridgeObjectRetain();
  specialized Array.append<A>(contentsOf:)(v53);
  swift_release();
  a2 = v81;
LABEL_42:
  v81 = v66;
  v82 = v68;
  v83 = v69;
  v84 = v67;
  v77 = v45;
  v78 = v40;
  v79 = v39;
  v80 = v42;
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    v47 = (uint64_t)specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(_QWORD *)(v47 + 16), 0, (char *)v47, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<Float>?>);
  swift_bridgeObjectRetain();
  v50(&v81, &v77, v47 + 32, a2 + 32, (v29 - 1), v46, 0);
  swift_bridgeObjectRelease();
  swift_release();
  swift_bridgeObjectRelease();
  v62 = *(_QWORD *)(v47 + 16);
  if (v62)
  {
    swift_bridgeObjectRetain();
    for (i = 0; i != v62; ++i)
    {
      v64 = *(_QWORD *)(v47 + 8 * i + 32);
      if (v64)
        MEMORY[0x1D1794DA4](v64, -1, -1);
    }
    swift_bridgeObjectRelease();
  }
  return swift_bridgeObjectRelease();
}

char *specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(char *a1, int64_t a2, char a3, char *a4)
{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<vImage_Buffer>?>);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>?>);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<Int8>?>);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>>);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer?>);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<BNNSNDArrayDescriptor>>);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<UInt>);
}

char *specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(char *result, int64_t a2, char a3, char *a4)
{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    v12 = v11 - 32;
    if (v11 < 32)
      v12 = v11 - 29;
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * (v12 >> 2);
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v13 = v10 + 32;
  v14 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v13 >= &v14[4 * v8])
      memmove(v13, v14, 4 * v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 4 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<DSPComplex>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    v12 = v11 - 32;
    if (v11 < 32)
      v12 = v11 - 25;
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * (v12 >> 3);
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v13 = v10 + 32;
  v14 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v13 >= &v14[8 * v8])
      memmove(v13, v14, 8 * v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 8 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<DSPDoubleComplex>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    v12 = v11 - 32;
    if (v11 < 32)
      v12 = v11 - 17;
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * (v12 >> 4);
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v13 = v10 + 32;
  v14 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v13 >= &v14[16 * v8])
      memmove(v13, v14, 16 * v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 16 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<bnns_graph_shape_t>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    v12 = v11 - 32;
    if (v11 < 32)
      v12 = v11 - 17;
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * (v12 >> 4);
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v13 = v10 + 32;
  v14 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v13 >= &v14[16 * v8])
      memmove(v13, v14, 16 * v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 16 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  size_t v11;
  char *v12;
  char *v13;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<BNNSTensor>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * ((uint64_t)(v11 - 32) / 160);
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v12 = v10 + 32;
  v13 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v12 >= &v13[160 * v8])
      memmove(v12, v13, 160 * v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v12, v13, 160 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<bnns_graph_argument_t>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    v12 = v11 - 32;
    if (v11 < 32)
      v12 = v11 - 17;
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * (v12 >> 4);
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v13 = v10 + 32;
  v14 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v13 >= &v14[16 * v8])
      memmove(v13, v14, 16 * v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 16 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Double>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    v12 = v11 - 32;
    if (v11 < 32)
      v12 = v11 - 25;
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * (v12 >> 3);
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v13 = v10 + 32;
  v14 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v13 >= &v14[8 * v8])
      memmove(v13, v14, 8 * v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 8 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage_Buffer>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    v12 = v11 - 32;
    if (v11 < 32)
      v12 = v11 - 1;
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * (v12 >> 5);
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v13 = v10 + 32;
  v14 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v13 >= &v14[32 * v8])
      memmove(v13, v14, 32 * v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 32 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<(x: Int, y: Int)>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    v12 = v11 - 32;
    if (v11 < 32)
      v12 = v11 - 17;
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * (v12 >> 4);
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v13 = v10 + 32;
  v14 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v13 >= &v14[16 * v8])
      memmove(v13, v14, 16 * v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 16 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  int64_t v8;
  int64_t v9;
  char *v10;
  size_t v11;
  char *v12;
  char *v13;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferType>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * v11 - 64;
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v12 = v10 + 32;
  v13 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v12 >= &v13[v8])
      memmove(v12, v13, v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v12, v13, v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

char *specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(char *result, int64_t a2, char a3, char *a4, uint64_t *a5)
{
  char v6;
  unint64_t v7;
  int64_t v8;
  uint64_t v9;
  uint64_t v10;
  char *v11;
  int64_t v12;
  uint64_t v13;
  char *v14;
  char *v15;

  v6 = (char)result;
  if ((a3 & 1) != 0)
  {
    v7 = *((_QWORD *)a4 + 3);
    v8 = v7 >> 1;
    if ((uint64_t)(v7 >> 1) < a2)
    {
      if (v8 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v8 = v7 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v7 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v8 = a2;
    }
  }
  else
  {
    v8 = a2;
  }
  v9 = *((_QWORD *)a4 + 2);
  if (v8 <= v9)
    v10 = *((_QWORD *)a4 + 2);
  else
    v10 = v8;
  if (v10)
  {
    __swift_instantiateConcreteTypeFromMangledName(a5);
    v11 = (char *)swift_allocObject();
    v12 = _swift_stdlib_malloc_size(v11);
    v13 = v12 - 32;
    if (v12 < 32)
      v13 = v12 - 25;
    *((_QWORD *)v11 + 2) = v9;
    *((_QWORD *)v11 + 3) = 2 * (v13 >> 3);
  }
  else
  {
    v11 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v14 = v11 + 32;
  v15 = a4 + 32;
  if ((v6 & 1) != 0)
  {
    if (v11 != a4 || v14 >= &v15[8 * v9])
      memmove(v14, v15, 8 * v9);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v14, v15, 8 * v9);
  }
  swift_bridgeObjectRelease();
  return v11;
}

_QWORD *specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(_QWORD *result, int64_t a2, char a3, _QWORD *a4)
{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  _QWORD *v10;
  int64_t v11;
  uint64_t v12;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = a4[3];
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = a4[2];
  if (v7 <= v8)
    v9 = a4[2];
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<[UInt]>);
    v10 = (_QWORD *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    v12 = v11 - 32;
    if (v11 < 32)
      v12 = v11 - 25;
    v10[2] = v8;
    v10[3] = 2 * (v12 >> 3);
  }
  else
  {
    v10 = (_QWORD *)MEMORY[0x1E0DEE9D8];
  }
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v10 + 4 >= &a4[v8 + 4])
      memmove(v10 + 4, a4 + 4, 8 * v8);
    a4[2] = 0;
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [UInt]);
    swift_arrayInitWithCopy();
  }
  swift_bridgeObjectRelease();
  return v10;
}

uint64_t specialized Set._Variant.insert(_:)(uint64_t *a1, uint64_t a2, uint64_t *a3)
{
  uint64_t *v3;
  uint64_t *v5;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  unint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t result;
  char isUniquelyReferenced_nonNull_native;
  uint64_t v16;

  v5 = v3;
  v8 = *v3;
  v9 = static Hasher._hash(seed:_:)();
  v10 = -1 << *(_BYTE *)(v8 + 32);
  v11 = v9 & ~v10;
  if (((*(_QWORD *)(v8 + 56 + ((v11 >> 3) & 0xFFFFFFFFFFFFF8)) >> v11) & 1) == 0)
  {
LABEL_7:
    isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
    v16 = *v5;
    *v5 = 0x8000000000000000;
    specialized _NativeSet.insertNew(_:at:isUnique:)(a2, v11, isUniquelyReferenced_nonNull_native, a3);
    *v5 = v16;
    swift_bridgeObjectRelease();
    result = 1;
    goto LABEL_8;
  }
  v12 = *(_QWORD *)(v8 + 48);
  if (*(_QWORD *)(v12 + 8 * v11) != a2)
  {
    v13 = ~v10;
    do
    {
      v11 = (v11 + 1) & v13;
      if (((*(_QWORD *)(v8 + 56 + ((v11 >> 3) & 0xFFFFFFFFFFFFF8)) >> v11) & 1) == 0)
        goto LABEL_7;
    }
    while (*(_QWORD *)(v12 + 8 * v11) != a2);
  }
  result = 0;
LABEL_8:
  *a1 = a2;
  return result;
}

uint64_t specialized Array.append<A>(contentsOf:)(uint64_t result)
{
  uint64_t *v1;
  uint64_t v2;
  uint64_t v3;
  int64_t v4;
  int64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  BOOL v9;
  uint64_t v10;
  int64_t v11;

  v2 = *(_QWORD *)(result + 16);
  v3 = *v1;
  v4 = *(_QWORD *)(*v1 + 16);
  v5 = v4 + v2;
  if (__OFADD__(v4, v2))
  {
    __break(1u);
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v6 = result;
  result = swift_isUniquelyReferenced_nonNull_native();
  if ((_DWORD)result && v5 <= *(_QWORD *)(v3 + 24) >> 1)
  {
    if (*(_QWORD *)(v6 + 16))
      goto LABEL_5;
    goto LABEL_13;
  }
  if (v4 <= v5)
    v11 = v4 + v2;
  else
    v11 = v4;
  result = (uint64_t)specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)result, v11, 1, (char *)v3);
  v3 = result;
  if (!*(_QWORD *)(v6 + 16))
  {
LABEL_13:
    if (!v2)
      goto LABEL_14;
    goto LABEL_16;
  }
LABEL_5:
  v7 = *(_QWORD *)(v3 + 16);
  if ((*(_QWORD *)(v3 + 24) >> 1) - v7 < v2)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  result = (uint64_t)memcpy((void *)(v3 + 4 * v7 + 32), (const void *)(v6 + 32), 4 * v2);
  if (!v2)
  {
LABEL_14:
    result = swift_bridgeObjectRelease();
    *v1 = v3;
    return result;
  }
  v8 = *(_QWORD *)(v3 + 16);
  v9 = __OFADD__(v8, v2);
  v10 = v8 + v2;
  if (!v9)
  {
    *(_QWORD *)(v3 + 16) = v10;
    goto LABEL_14;
  }
LABEL_18:
  __break(1u);
  return result;
}

uint64_t specialized _NativeSet.resize(capacity:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;
  uint64_t *v3;
  uint64_t v4;
  uint64_t result;
  uint64_t v6;
  int64_t v7;
  uint64_t *v8;
  uint64_t v9;
  uint64_t v10;
  unint64_t v11;
  int64_t v12;
  uint64_t v13;
  unint64_t v14;
  unint64_t v15;
  unint64_t v16;
  int64_t v17;
  unint64_t v18;
  int64_t v19;
  uint64_t v20;
  uint64_t v21;
  unint64_t v22;
  unint64_t v23;
  char v24;
  unint64_t v25;
  BOOL v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t *v29;
  uint64_t v30;

  v3 = v2;
  v4 = *v2;
  __swift_instantiateConcreteTypeFromMangledName(a2);
  result = static _SetStorage.resize(original:capacity:move:)();
  v6 = result;
  if (*(_QWORD *)(v4 + 16))
  {
    v7 = 0;
    v8 = (uint64_t *)(v4 + 56);
    v9 = 1 << *(_BYTE *)(v4 + 32);
    v28 = -1 << v9;
    v29 = v3;
    if (v9 < 64)
      v10 = ~(-1 << v9);
    else
      v10 = -1;
    v11 = v10 & *(_QWORD *)(v4 + 56);
    v30 = 1 << *(_BYTE *)(v4 + 32);
    v12 = (unint64_t)(v9 + 63) >> 6;
    v13 = result + 56;
    while (1)
    {
      if (v11)
      {
        v15 = __clz(__rbit64(v11));
        v11 &= v11 - 1;
        v16 = v15 | (v7 << 6);
      }
      else
      {
        v17 = v7 + 1;
        if (__OFADD__(v7, 1))
        {
LABEL_38:
          __break(1u);
LABEL_39:
          __break(1u);
          return result;
        }
        if (v17 >= v12)
          goto LABEL_33;
        v18 = v8[v17];
        ++v7;
        if (!v18)
        {
          v7 = v17 + 1;
          if (v17 + 1 >= v12)
            goto LABEL_33;
          v18 = v8[v7];
          if (!v18)
          {
            v7 = v17 + 2;
            if (v17 + 2 >= v12)
              goto LABEL_33;
            v18 = v8[v7];
            if (!v18)
            {
              v19 = v17 + 3;
              if (v19 >= v12)
              {
LABEL_33:
                if (v30 >= 64)
                  bzero((void *)(v4 + 56), 8 * v12);
                else
                  *v8 = v28;
                v3 = v29;
                *(_QWORD *)(v4 + 16) = 0;
                break;
              }
              v18 = v8[v19];
              if (!v18)
              {
                while (1)
                {
                  v7 = v19 + 1;
                  if (__OFADD__(v19, 1))
                    goto LABEL_39;
                  if (v7 >= v12)
                    goto LABEL_33;
                  v18 = v8[v7];
                  ++v19;
                  if (v18)
                    goto LABEL_23;
                }
              }
              v7 = v19;
            }
          }
        }
LABEL_23:
        v11 = (v18 - 1) & v18;
        v16 = __clz(__rbit64(v18)) + (v7 << 6);
      }
      v20 = *(_QWORD *)(*(_QWORD *)(v4 + 48) + 8 * v16);
      result = static Hasher._hash(seed:_:)();
      v21 = -1 << *(_BYTE *)(v6 + 32);
      v22 = result & ~v21;
      v23 = v22 >> 6;
      if (((-1 << v22) & ~*(_QWORD *)(v13 + 8 * (v22 >> 6))) != 0)
      {
        v14 = __clz(__rbit64((-1 << v22) & ~*(_QWORD *)(v13 + 8 * (v22 >> 6)))) | v22 & 0x7FFFFFFFFFFFFFC0;
      }
      else
      {
        v24 = 0;
        v25 = (unint64_t)(63 - v21) >> 6;
        do
        {
          if (++v23 == v25 && (v24 & 1) != 0)
          {
            __break(1u);
            goto LABEL_38;
          }
          v26 = v23 == v25;
          if (v23 == v25)
            v23 = 0;
          v24 |= v26;
          v27 = *(_QWORD *)(v13 + 8 * v23);
        }
        while (v27 == -1);
        v14 = __clz(__rbit64(~v27)) + (v23 << 6);
      }
      *(_QWORD *)(v13 + ((v14 >> 3) & 0x1FFFFFFFFFFFFFF8)) |= 1 << v14;
      *(_QWORD *)(*(_QWORD *)(v6 + 48) + 8 * v14) = v20;
      ++*(_QWORD *)(v6 + 16);
    }
  }
  result = swift_release();
  *v3 = v6;
  return result;
}

uint64_t specialized _NativeSet.insertNew(_:at:isUnique:)(uint64_t result, unint64_t a2, char a3, uint64_t *a4)
{
  uint64_t *v4;
  uint64_t v6;
  unint64_t v7;
  unint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  BOOL v16;
  uint64_t v17;

  v6 = result;
  v7 = *(_QWORD *)(*v4 + 16);
  v8 = *(_QWORD *)(*v4 + 24);
  if (v8 > v7 && (a3 & 1) != 0)
    goto LABEL_14;
  v9 = v7 + 1;
  if ((a3 & 1) != 0)
  {
    specialized _NativeSet.resize(capacity:)(v9, a4);
  }
  else
  {
    if (v8 > v7)
    {
      result = (uint64_t)specialized _NativeSet.copy()(a4);
      goto LABEL_14;
    }
    specialized _NativeSet.copyAndResize(capacity:)(v9, a4);
  }
  v10 = *v4;
  result = static Hasher._hash(seed:_:)();
  v11 = -1 << *(_BYTE *)(v10 + 32);
  a2 = result & ~v11;
  if (((*(_QWORD *)(v10 + 56 + ((a2 >> 3) & 0xFFFFFFFFFFFFF8)) >> a2) & 1) != 0)
  {
    v12 = *(_QWORD *)(v10 + 48);
    if (*(_QWORD *)(v12 + 8 * a2) == v6)
    {
LABEL_13:
      result = ELEMENT_TYPE_OF_SET_VIOLATES_HASHABLE_REQUIREMENTS(_:)();
      __break(1u);
    }
    else
    {
      v13 = ~v11;
      while (1)
      {
        a2 = (a2 + 1) & v13;
        if (((*(_QWORD *)(v10 + 56 + ((a2 >> 3) & 0xFFFFFFFFFFFFF8)) >> a2) & 1) == 0)
          break;
        if (*(_QWORD *)(v12 + 8 * a2) == v6)
          goto LABEL_13;
      }
    }
  }
LABEL_14:
  v14 = *v4;
  *(_QWORD *)(*v4 + 8 * (a2 >> 6) + 56) |= 1 << a2;
  *(_QWORD *)(*(_QWORD *)(v14 + 48) + 8 * a2) = v6;
  v15 = *(_QWORD *)(v14 + 16);
  v16 = __OFADD__(v15, 1);
  v17 = v15 + 1;
  if (v16)
    __break(1u);
  else
    *(_QWORD *)(v14 + 16) = v17;
  return result;
}

void *specialized _NativeSet.copy()(uint64_t *a1)
{
  uint64_t *v1;
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  void *result;
  uint64_t v7;
  unint64_t v8;
  int64_t v10;
  uint64_t v11;
  uint64_t v12;
  unint64_t v13;
  int64_t v14;
  unint64_t v15;
  unint64_t v16;
  int64_t v17;
  unint64_t v18;
  int64_t v19;

  v2 = v1;
  __swift_instantiateConcreteTypeFromMangledName(a1);
  v3 = *v1;
  v4 = static _SetStorage.copy(original:)();
  v5 = v4;
  if (!*(_QWORD *)(v3 + 16))
  {
LABEL_28:
    result = (void *)swift_release();
    *v2 = v5;
    return result;
  }
  result = (void *)(v4 + 56);
  v7 = v3 + 56;
  v8 = (unint64_t)((1 << *(_BYTE *)(v5 + 32)) + 63) >> 6;
  if (v5 != v3 || (unint64_t)result >= v3 + 56 + 8 * v8)
    result = memmove(result, (const void *)(v3 + 56), 8 * v8);
  v10 = 0;
  *(_QWORD *)(v5 + 16) = *(_QWORD *)(v3 + 16);
  v11 = 1 << *(_BYTE *)(v3 + 32);
  v12 = -1;
  if (v11 < 64)
    v12 = ~(-1 << v11);
  v13 = v12 & *(_QWORD *)(v3 + 56);
  v14 = (unint64_t)(v11 + 63) >> 6;
  while (1)
  {
    if (v13)
    {
      v15 = __clz(__rbit64(v13));
      v13 &= v13 - 1;
      v16 = v15 | (v10 << 6);
      goto LABEL_12;
    }
    v17 = v10 + 1;
    if (__OFADD__(v10, 1))
    {
      __break(1u);
      goto LABEL_30;
    }
    if (v17 >= v14)
      goto LABEL_28;
    v18 = *(_QWORD *)(v7 + 8 * v17);
    ++v10;
    if (!v18)
    {
      v10 = v17 + 1;
      if (v17 + 1 >= v14)
        goto LABEL_28;
      v18 = *(_QWORD *)(v7 + 8 * v10);
      if (!v18)
      {
        v10 = v17 + 2;
        if (v17 + 2 >= v14)
          goto LABEL_28;
        v18 = *(_QWORD *)(v7 + 8 * v10);
        if (!v18)
          break;
      }
    }
LABEL_27:
    v13 = (v18 - 1) & v18;
    v16 = __clz(__rbit64(v18)) + (v10 << 6);
LABEL_12:
    *(_QWORD *)(*(_QWORD *)(v5 + 48) + 8 * v16) = *(_QWORD *)(*(_QWORD *)(v3 + 48) + 8 * v16);
  }
  v19 = v17 + 3;
  if (v19 >= v14)
    goto LABEL_28;
  v18 = *(_QWORD *)(v7 + 8 * v19);
  if (v18)
  {
    v10 = v19;
    goto LABEL_27;
  }
  while (1)
  {
    v10 = v19 + 1;
    if (__OFADD__(v19, 1))
      break;
    if (v10 >= v14)
      goto LABEL_28;
    v18 = *(_QWORD *)(v7 + 8 * v10);
    ++v19;
    if (v18)
      goto LABEL_27;
  }
LABEL_30:
  __break(1u);
  return result;
}

uint64_t specialized _NativeSet.copyAndResize(capacity:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;
  uint64_t *v3;
  uint64_t v4;
  uint64_t result;
  uint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  unint64_t v11;
  int64_t v12;
  uint64_t v13;
  unint64_t v14;
  unint64_t v15;
  unint64_t v16;
  int64_t v17;
  unint64_t v18;
  int64_t v19;
  uint64_t v20;
  uint64_t v21;
  unint64_t v22;
  unint64_t v23;
  char v24;
  unint64_t v25;
  BOOL v26;
  uint64_t v27;
  uint64_t *v28;

  v3 = v2;
  v4 = *v2;
  __swift_instantiateConcreteTypeFromMangledName(a2);
  result = static _SetStorage.resize(original:capacity:move:)();
  v6 = result;
  if (!*(_QWORD *)(v4 + 16))
  {
    result = swift_release();
LABEL_35:
    *v3 = v6;
    return result;
  }
  v28 = v3;
  v7 = 0;
  v8 = v4 + 56;
  v9 = 1 << *(_BYTE *)(v4 + 32);
  if (v9 < 64)
    v10 = ~(-1 << v9);
  else
    v10 = -1;
  v11 = v10 & *(_QWORD *)(v4 + 56);
  v12 = (unint64_t)(v9 + 63) >> 6;
  v13 = result + 56;
  while (1)
  {
    if (v11)
    {
      v15 = __clz(__rbit64(v11));
      v11 &= v11 - 1;
      v16 = v15 | (v7 << 6);
      goto LABEL_24;
    }
    v17 = v7 + 1;
    if (__OFADD__(v7, 1))
    {
LABEL_36:
      __break(1u);
      goto LABEL_37;
    }
    if (v17 >= v12)
      goto LABEL_33;
    v18 = *(_QWORD *)(v8 + 8 * v17);
    ++v7;
    if (!v18)
    {
      v7 = v17 + 1;
      if (v17 + 1 >= v12)
        goto LABEL_33;
      v18 = *(_QWORD *)(v8 + 8 * v7);
      if (!v18)
      {
        v7 = v17 + 2;
        if (v17 + 2 >= v12)
          goto LABEL_33;
        v18 = *(_QWORD *)(v8 + 8 * v7);
        if (!v18)
          break;
      }
    }
LABEL_23:
    v11 = (v18 - 1) & v18;
    v16 = __clz(__rbit64(v18)) + (v7 << 6);
LABEL_24:
    v20 = *(_QWORD *)(*(_QWORD *)(v4 + 48) + 8 * v16);
    result = static Hasher._hash(seed:_:)();
    v21 = -1 << *(_BYTE *)(v6 + 32);
    v22 = result & ~v21;
    v23 = v22 >> 6;
    if (((-1 << v22) & ~*(_QWORD *)(v13 + 8 * (v22 >> 6))) != 0)
    {
      v14 = __clz(__rbit64((-1 << v22) & ~*(_QWORD *)(v13 + 8 * (v22 >> 6)))) | v22 & 0x7FFFFFFFFFFFFFC0;
    }
    else
    {
      v24 = 0;
      v25 = (unint64_t)(63 - v21) >> 6;
      do
      {
        if (++v23 == v25 && (v24 & 1) != 0)
        {
          __break(1u);
          goto LABEL_36;
        }
        v26 = v23 == v25;
        if (v23 == v25)
          v23 = 0;
        v24 |= v26;
        v27 = *(_QWORD *)(v13 + 8 * v23);
      }
      while (v27 == -1);
      v14 = __clz(__rbit64(~v27)) + (v23 << 6);
    }
    *(_QWORD *)(v13 + ((v14 >> 3) & 0x1FFFFFFFFFFFFFF8)) |= 1 << v14;
    *(_QWORD *)(*(_QWORD *)(v6 + 48) + 8 * v14) = v20;
    ++*(_QWORD *)(v6 + 16);
  }
  v19 = v17 + 3;
  if (v19 >= v12)
  {
LABEL_33:
    result = swift_release();
    v3 = v28;
    goto LABEL_35;
  }
  v18 = *(_QWORD *)(v8 + 8 * v19);
  if (v18)
  {
    v7 = v19;
    goto LABEL_23;
  }
  while (1)
  {
    v7 = v19 + 1;
    if (__OFADD__(v19, 1))
      break;
    if (v7 >= v12)
      goto LABEL_33;
    v18 = *(_QWORD *)(v8 + 8 * v7);
    ++v19;
    if (v18)
      goto LABEL_23;
  }
LABEL_37:
  __break(1u);
  return result;
}

uint64_t _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_SPySfGSgs5NeverOTg50161_s10Accelerate6vImageO11PixelBufferVA2A06StaticC6FormatRzrlE16_applyPolynomial19coefficientSegments10boundaries11destination14polynomialFunc15widthMultiplierySayw41SfGG_AmEy_qd__GSiSPySo01vb1_D0VG_ARSpySPyx14GSgGASs6UInt32K42WtXESitAA011SinglePlanecF0Rd__lFATSiXEfU0_SaySaySfGGTf1cn_nTf4ng_n(uint64_t result, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v7;
  uint64_t v8;
  unint64_t v9;
  unint64_t v10;
  uint64_t v11;
  unint64_t v12;
  size_t v13;
  void *v14;
  void *v15;
  unint64_t v16;
  unint64_t v17;
  uint64_t v18;

  v3 = a2 - result;
  if (__OFSUB__(a2, result))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v4 = MEMORY[0x1E0DEE9D8];
  if (!v3)
    return v4;
  v7 = result;
  v18 = MEMORY[0x1E0DEE9D8];
  result = (uint64_t)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v3 & ~(v3 >> 63), 0);
  if (a2 >= v7 && (v3 & 0x8000000000000000) == 0)
  {
    v8 = a3 + 32;
    v4 = v18;
    v9 = *(_QWORD *)(a3 + 16);
    while (a2 != v7)
    {
      if ((uint64_t)(v9 - 1) >= v7)
        v10 = v7;
      else
        v10 = v9 - 1;
      if (v10 >= v9)
        goto LABEL_19;
      v11 = *(_QWORD *)(v8 + 8 * v10);
      v12 = *(_QWORD *)(v11 + 16);
      if (v12 >> 61)
        goto LABEL_20;
      v13 = 4 * v12;
      swift_bridgeObjectRetain();
      v14 = (void *)swift_slowAlloc();
      v15 = v14;
      if (*(_QWORD *)(v11 + 16))
        memcpy(v14, (const void *)(v11 + 32), v13);
      result = swift_bridgeObjectRelease();
      v17 = *(_QWORD *)(v18 + 16);
      v16 = *(_QWORD *)(v18 + 24);
      if (v17 >= v16 >> 1)
        result = (uint64_t)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v16 > 1), v17 + 1, 1);
      *(_QWORD *)(v18 + 16) = v17 + 1;
      *(_QWORD *)(v18 + 8 * v17 + 32) = v15;
      if (a2 == ++v7)
        return v4;
    }
    __break(1u);
LABEL_19:
    __break(1u);
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
LABEL_22:
  __break(1u);
  return result;
}

uint64_t specialized Set.init<A>(_:)(uint64_t a1)
{
  return specialized Set.init<A>(_:)(a1, MEMORY[0x1E0DEB418], MEMORY[0x1E0DEB428], &demangling cache variable for type metadata for _SetStorage<Int>);
}

{
  return specialized Set.init<A>(_:)(a1, MEMORY[0x1E0DEBB98], MEMORY[0x1E0DEBBA8], &demangling cache variable for type metadata for _SetStorage<UInt>);
}

uint64_t specialized Set.init<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t *a4)
{
  uint64_t v6;
  uint64_t result;
  uint64_t *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;

  v6 = *(_QWORD *)(a1 + 16);
  result = Set.init(minimumCapacity:)();
  v11 = result;
  if (v6)
  {
    v8 = (uint64_t *)(a1 + 32);
    do
    {
      v9 = *v8++;
      specialized Set._Variant.insert(_:)(&v10, v9, a4);
      --v6;
    }
    while (v6);
    return v11;
  }
  return result;
}

uint64_t static vDSP.convolve<A, B>(_:withKernel:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:));
}

{
  return static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(uint64_t a1, _QWORD *a2)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

uint64_t static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

{
  return static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

void closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, const float *a3, uint64_t a4, uint64_t a5, float **a6, vDSP_Length a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  uint64_t (*v16)(uint64_t, uint64_t);
  uint64_t v17;
  uint64_t v18;
  float *v19;
  vDSP_Length v20;

  if (!a3)
    goto LABEL_11;
  if (!a1)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v16 = *(uint64_t (**)(uint64_t, uint64_t))(a12 + 16);
  v17 = v16(a9, a12);
  v18 = v17 - 1;
  if (__OFSUB__(v17, 1))
  {
    __break(1u);
    goto LABEL_9;
  }
  v19 = *a6;
  if (v19)
  {
    if ((a7 & 0x8000000000000000) == 0)
    {
      v20 = v16(a9, a12);
      if ((v20 & 0x8000000000000000) == 0)
      {
        vDSP_conv(a3, 1, (const float *)(a1 + 4 * v18), -1, v19, 1, a7, v20);
        return;
      }
      goto LABEL_10;
    }
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
LABEL_13:
  __break(1u);
}

void closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, const double *a3, uint64_t a4, uint64_t a5, double **a6, vDSP_Length a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  uint64_t (*v16)(uint64_t, uint64_t);
  uint64_t v17;
  uint64_t v18;
  double *v19;
  vDSP_Length v20;

  if (!a3)
    goto LABEL_11;
  if (!a1)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v16 = *(uint64_t (**)(uint64_t, uint64_t))(a12 + 16);
  v17 = v16(a9, a12);
  v18 = v17 - 1;
  if (__OFSUB__(v17, 1))
  {
    __break(1u);
    goto LABEL_9;
  }
  v19 = *a6;
  if (v19)
  {
    if ((a7 & 0x8000000000000000) == 0)
    {
      v20 = v16(a9, a12);
      if ((v20 & 0x8000000000000000) == 0)
      {
        vDSP_convD(a3, 1, (const double *)(a1 + 8 * v18), -1, v19, 1, a7, v20);
        return;
      }
      goto LABEL_10;
    }
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
LABEL_13:
  __break(1u);
}

uint64_t static vDSP.correlate<A, B>(_:withKernel:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.correlate<A, B>(_:withKernel:));
}

{
  return static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.correlate<A, B>(_:withKernel:));
}

uint64_t static vDSP.correlate<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:));
}

{
  return static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:));
}

uint64_t static vDSP.convolve<A, B>(_:withKernel:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(_QWORD *, uint64_t *))
{
  uint64_t v10;
  uint64_t v11;
  BOOL v12;
  uint64_t result;
  uint64_t v14;

  v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  v11 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  v12 = __OFSUB__(v10, v11);
  result = v10 - v11;
  if (v12)
  {
    __break(1u);
  }
  else
  {
    v14 = MEMORY[0x1E0C80A78](result);
    return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v14, a7);
  }
  return result;
}

uint64_t static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  uint64_t (*v24)(uint64_t);
  uint64_t v25;
  uint64_t v26;
  void (*v27)(char *, uint64_t, uint64_t);
  uint64_t (*v28)(uint64_t, uint64_t);
  uint64_t v29;
  uint64_t (*v30)(uint64_t, uint64_t);
  uint64_t v31;
  uint64_t result;
  uint64_t v33;
  BOOL v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;

  v49 = a2;
  v15 = *(_QWORD *)(a5 - 8);
  v16 = ((uint64_t (*)(void))MEMORY[0x1E0C80A78])();
  v18 = (char *)&v41 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v20 = *(_QWORD *)(v19 - 8);
  MEMORY[0x1E0C80A78](v16);
  v22 = (char *)&v41 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  v43 = v23;
  v24 = *(uint64_t (**)(uint64_t))(*(_QWORD *)(v23 + 8) + 16);
  v46 = v25;
  v47 = v26;
  v48 = v24(v26);
  v27 = *(void (**)(char *, uint64_t, uint64_t))(v20 + 16);
  v42 = a1;
  v27(v22, a1, a4);
  (*(void (**)(char *, uint64_t, uint64_t))(v15 + 16))(v18, v49, a5);
  v28 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  v44 = a7;
  v29 = v28(a4, a7);
  v30 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  v45 = a8;
  v31 = v30(a5, a8);
  (*(void (**)(char *, uint64_t))(v15 + 8))(v18, a5);
  result = (*(uint64_t (**)(char *, uint64_t))(v20 + 8))(v22, a4);
  v33 = v48 + v31;
  if (__OFADD__(v48, v31))
  {
    __break(1u);
    goto LABEL_6;
  }
  v34 = __OFSUB__(v33, 1);
  v35 = v33 - 1;
  if (v34)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (v29 >= v35)
  {
    v36 = MEMORY[0x1E0C80A78](a10);
    *(&v41 - 10) = a4;
    *(&v41 - 9) = a5;
    v37 = v43;
    v38 = v44;
    *(&v41 - 8) = v47;
    *(&v41 - 7) = v38;
    *(&v41 - 6) = v45;
    *(&v41 - 5) = v37;
    v39 = v49;
    *(&v41 - 4) = v42;
    *(&v41 - 3) = v39;
    *(&v41 - 2) = v40;
    return (*(uint64_t (**)(uint64_t))(v37 + 16))(v36);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t *a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t (*a14)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v14;
  uint64_t v15;

  if (!a3)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  v14 = result;
  if (!result)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  v15 = *a5;
  if (*a5)
  {
    if (a6 < 0)
    {
      __break(1u);
    }
    else
    {
      result = (*(uint64_t (**)(uint64_t))(a12 + 16))(a9);
      if ((result & 0x8000000000000000) == 0)
        return a14(a3, 1, v14, 1, v15, 1, a6, result);
    }
    __break(1u);
    goto LABEL_9;
  }
LABEL_11:
  __break(1u);
  return result;
}

uint64_t static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  return static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, a3, a4, a5, a6, a7, a8, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:));
}

{
  return static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, a3, a4, a5, a6, a7, a8, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:));
}

uint64_t static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  return static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, (uint64_t)partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:));
}

{
  return static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, (uint64_t)partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:));
}

uint64_t static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t (*a9)(_QWORD *, uint64_t *))
{
  uint64_t v9;

  v9 = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v9, a9);
}

uint64_t closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t *a11, unint64_t *a12, void (*a13)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v17;
  uint64_t v18;
  uint64_t result;

  v17 = __swift_instantiateConcreteTypeFromMangledName(a11);
  v18 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a12, a11);
  a13(a3, a4, a5, a6, a1, a7, a8, v17, a9, a10, v18);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a9 + 16))(a7, a9);
  *a2 = result;
  return result;
}

uint64_t static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t result;
  uint64_t v25;
  uint64_t v26;
  char *v27;
  uint64_t v28;
  uint64_t (*v29)(char *, uint64_t, uint64_t);
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;

  v43 = a4;
  v18 = *(_QWORD *)(a7 - 8);
  v19 = MEMORY[0x1E0C80A78](a1);
  v21 = (char *)&v39 - ((v20 + 15) & 0xFFFFFFFFFFFFFFF0);
  v23 = *(_QWORD *)(v22 - 8);
  result = MEMORY[0x1E0C80A78](v19);
  v27 = (char *)&v39 - ((v26 + 15) & 0xFFFFFFFFFFFFFFF0);
  if (v28 < 3 || a3 < 4)
  {
    __break(1u);
    goto LABEL_8;
  }
  v29 = *(uint64_t (**)(char *, uint64_t, uint64_t))(v23 + 16);
  v41 = v25;
  result = v29(v27, v25, a6);
  v30 = a3;
  v31 = a2 * a3;
  v42 = v30;
  if ((unsigned __int128)(a2 * (__int128)v30) >> 64 != v31 >> 63)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v39 = a5;
  v40 = a8;
  v32 = (*(uint64_t (**)(uint64_t, uint64_t))(a9 + 16))(a6, a9);
  result = (*(uint64_t (**)(char *, uint64_t))(v23 + 8))(v27, a6);
  if (v31 != v32)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  v33 = v43;
  (*(void (**)(char *, uint64_t, uint64_t))(v18 + 16))(v21, v43, a7);
  v34 = (*(uint64_t (**)(uint64_t, uint64_t))(a10 + 16))(a7, a10);
  result = (*(uint64_t (**)(char *, uint64_t))(v18 + 8))(v21, a7);
  if (v34 == 9)
  {
    v35 = MEMORY[0x1E0C80A78](a12);
    *(&v39 - 10) = a6;
    *(&v39 - 9) = a7;
    v36 = v41;
    *(&v39 - 8) = v40;
    *(&v39 - 7) = a9;
    *(&v39 - 6) = a10;
    *(&v39 - 5) = v37;
    *(&v39 - 4) = v36;
    *(&v39 - 3) = v33;
    v38 = v42;
    *(&v39 - 2) = a2;
    *(&v39 - 1) = v38;
    return (*(uint64_t (**)(uint64_t))(v37 + 16))(v35);
  }
LABEL_10:
  __break(1u);
  return result;
}

uint64_t static vDSP.convolve<A, B>(_:rowCount:columnCount:with5x5Kernel:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  return static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, a3, a4, a5, a6, a7, a8, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with5x5Kernel:));
}

{
  return static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, a3, a4, a5, a6, a7, a8, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with5x5Kernel:));
}

uint64_t static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  return static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, (uint64_t)partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:));
}

{
  return static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, (uint64_t)partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:));
}

uint64_t static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t result;
  uint64_t v25;
  uint64_t v26;
  char *v27;
  uint64_t v28;
  uint64_t (*v29)(char *, uint64_t, uint64_t);
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;

  v43 = a4;
  v18 = *(_QWORD *)(a7 - 8);
  v19 = MEMORY[0x1E0C80A78](a1);
  v21 = (char *)&v39 - ((v20 + 15) & 0xFFFFFFFFFFFFFFF0);
  v23 = *(_QWORD *)(v22 - 8);
  result = MEMORY[0x1E0C80A78](v19);
  v27 = (char *)&v39 - ((v26 + 15) & 0xFFFFFFFFFFFFFFF0);
  if (v28 < 3 || a3 < 4)
  {
    __break(1u);
    goto LABEL_8;
  }
  v29 = *(uint64_t (**)(char *, uint64_t, uint64_t))(v23 + 16);
  v41 = v25;
  result = v29(v27, v25, a6);
  v30 = a3;
  v31 = a2 * a3;
  v42 = v30;
  if ((unsigned __int128)(a2 * (__int128)v30) >> 64 != v31 >> 63)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v39 = a5;
  v40 = a8;
  v32 = (*(uint64_t (**)(uint64_t, uint64_t))(a9 + 16))(a6, a9);
  result = (*(uint64_t (**)(char *, uint64_t))(v23 + 8))(v27, a6);
  if (v31 != v32)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  v33 = v43;
  (*(void (**)(char *, uint64_t, uint64_t))(v18 + 16))(v21, v43, a7);
  v34 = (*(uint64_t (**)(uint64_t, uint64_t))(a10 + 16))(a7, a10);
  result = (*(uint64_t (**)(char *, uint64_t))(v18 + 8))(v21, a7);
  if (v34 == 25)
  {
    v35 = MEMORY[0x1E0C80A78](a12);
    *(&v39 - 10) = a6;
    *(&v39 - 9) = a7;
    v36 = v41;
    *(&v39 - 8) = v40;
    *(&v39 - 7) = a9;
    *(&v39 - 6) = a10;
    *(&v39 - 5) = v37;
    *(&v39 - 4) = v36;
    *(&v39 - 3) = v33;
    v38 = v42;
    *(&v39 - 2) = a2;
    *(&v39 - 1) = v38;
    return (*(uint64_t (**)(uint64_t))(v37 + 16))(v35);
  }
LABEL_10:
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, _QWORD *a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, _QWORD))
{
  if (!a3)
    goto LABEL_7;
  if ((a6 | a5) < 0)
  {
    __break(1u);
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (!result)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (*a7)
    return a8(a3, a5, a6, result, *a7);
LABEL_9:
  __break(1u);
  return result;
}

uint64_t static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:));
}

{
  return static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:));
}

uint64_t static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13)
{
  return static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, (uint64_t)partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:));
}

{
  return static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, (uint64_t)partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:));
}

uint64_t static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t (*a11)(_QWORD *, uint64_t *))
{
  uint64_t v11;

  v11 = (*(uint64_t (**)(uint64_t, uint64_t))(a9 + 16))(a7, a9);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v11, a11);
}

uint64_t closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t *a13, unint64_t *a14, void (*a15)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v17;
  uint64_t v18;
  uint64_t result;

  v17 = __swift_instantiateConcreteTypeFromMangledName(a13);
  v18 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a14, a13);
  a15(a3, a4, a5, a6, a7, a8, a1, a9, a10, v17, a11, a12, v18);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a11 + 16))(a9, a11);
  *a2 = result;
  return result;
}

uint64_t static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14)
{
  uint64_t v19;
  uint64_t result;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  char *v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t (*v29)(char *, uint64_t, uint64_t);
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  _QWORD v37[2];
  uint64_t v38;

  v19 = *(_QWORD *)(a8 - 8);
  result = MEMORY[0x1E0C80A78](a1);
  v25 = (char *)v37 - ((v24 + 15) & 0xFFFFFFFFFFFFFFF0);
  if (v26 < 3 || a3 < 4)
  {
    __break(1u);
    goto LABEL_9;
  }
  v27 = v21;
  v28 = v22;
  v29 = *(uint64_t (**)(char *, uint64_t, uint64_t))(v19 + 16);
  v38 = v23;
  result = v29(v25, v23, a8);
  if ((unsigned __int128)(a2 * (__int128)a3) >> 64 != (a2 * a3) >> 63)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  v37[0] = a4;
  v37[1] = a7;
  v30 = (*(uint64_t (**)(uint64_t, uint64_t))(a11 + 16))(a8, a11);
  result = (*(uint64_t (**)(char *, uint64_t))(v19 + 8))(v25, a8);
  if ((v28 & 0x8000000000000001) == 1 && (v27 & 0x8000000000000001) == 1 && a2 * a3 == v30)
  {
    v31 = MEMORY[0x1E0C80A78](a14);
    v37[-12] = a8;
    v37[-11] = v32;
    v37[-10] = v33;
    v37[-9] = a11;
    v37[-8] = v35;
    v37[-7] = v34;
    v36 = v37[0];
    v37[-6] = v38;
    v37[-5] = v36;
    v37[-4] = a2;
    v37[-3] = a3;
    v37[-2] = v27;
    v37[-1] = v28;
    return (*(uint64_t (**)(uint64_t))(v34 + 16))(v31);
  }
LABEL_10:
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, _QWORD *a7, uint64_t a8, uint64_t a9, uint64_t (*a10)(uint64_t, uint64_t, uint64_t, uint64_t, _QWORD, uint64_t))
{
  if (!a3)
    goto LABEL_9;
  if ((a6 | a5) < 0)
  {
    __break(1u);
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (!result)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (*a7)
  {
    if (((a9 | a8) & 0x8000000000000000) == 0)
      return a10(a3, a5, a6, result, *a7, a8);
    goto LABEL_8;
  }
LABEL_11:
  __break(1u);
  return result;
}

uint64_t specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(uint64_t a1, uint64_t (*a2)(_QWORD *, uint64_t *))
{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(uint64_t a1, _QWORD *a2, uint64_t *a3, unint64_t *a4, uint64_t (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  _QWORD *v5;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t result;
  uint64_t v17;
  uint64_t v18;

  v9 = v5[2];
  v10 = v5[3];
  v11 = v5[5];
  v18 = v5[4];
  v13 = v5[6];
  v12 = v5[7];
  v17 = v5[8];
  v14 = __swift_instantiateConcreteTypeFromMangledName(a3);
  v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a4, a3);
  result = a5(v13, v12, a1, v9, v10, v14, v18, v11, v15);
  *a2 = v17;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v8;
  __int128 v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;

  v3 = *(_QWORD *)(v2 + 32);
  v4 = *(_QWORD *)(v2 + 56);
  v5 = *(_QWORD *)(v2 + 72);
  v6 = *(_QWORD *)(v2 + 80);
  v9 = *(_OWORD *)(v2 + 16);
  v10 = v3;
  v11 = *(_OWORD *)(v2 + 40);
  v12 = v4;
  v13 = v5;
  v14 = a1;
  v15 = v6;
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t, _QWORD))(v11 + 24))(a2, &v8, MEMORY[0x1E0DEE9C0] + 8, v9);
}

uint64_t partial apply for closure #1 in static vDSP.correlate<A, B>(_:withKernel:)(uint64_t a1, _QWORD *a2)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vDSP.correlate<A, B, C>(_:withKernel:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vDSP.correlate<A, B, C>(_:withKernel:result:));
}

uint64_t partial apply for closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v7;
  __int128 v8;
  uint64_t v9;
  __int128 v10;
  uint64_t v11;
  uint64_t v12;
  __int128 v13;
  uint64_t v14;

  v14 = a1;
  v3 = *(_QWORD *)(v2 + 32);
  v4 = *(_QWORD *)(v2 + 56);
  v5 = *(_QWORD *)(v2 + 72);
  v8 = *(_OWORD *)(v2 + 16);
  v9 = v3;
  v10 = *(_OWORD *)(v2 + 40);
  v11 = v4;
  v12 = v5;
  v13 = *(_OWORD *)(v2 + 80);
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t, _QWORD))(v10 + 24))(a2, &v7, MEMORY[0x1E0DEE9C0] + 8, v8);
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with5x5Kernel:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t *v5;

  return closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, v5[6], v5[7], v5[8], v5[9], v5[2], v5[3], v5[4], v5[5], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t *v5;

  return closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(a1, a2, v5[6], v5[7], v5[8], v5[9], v5[10], v5[11], v5[2], v5[3], v5[4], v5[5], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  __int128 v6;
  uint64_t v8;
  __int128 v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v12;
  uint64_t v13;
  __int128 v14;
  uint64_t v15;
  __int128 v16;

  v3 = *(_QWORD *)(v2 + 32);
  v4 = *(_QWORD *)(v2 + 56);
  v5 = *(_QWORD *)(v2 + 72);
  v9 = *(_OWORD *)(v2 + 16);
  v10 = v3;
  v6 = *(_OWORD *)(v2 + 96);
  v14 = *(_OWORD *)(v2 + 80);
  v11 = *(_OWORD *)(v2 + 40);
  v12 = v4;
  v13 = v5;
  v15 = a1;
  v16 = v6;
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t, _QWORD))(v11 + 24))(a2, &v8, MEMORY[0x1E0DEE9C0] + 8, v9);
}

uint64_t specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(uint64_t result, uint64_t (*a2)(_QWORD *, uint64_t *))
{
  uint64_t v2;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD v7[2];
  uint64_t v8;

  if (result < 0)
  {
    __break(1u);
    goto LABEL_16;
  }
  v4 = result;
  if (result)
  {
    v5 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v5 + 16) = v4;
  }
  else
  {
    v5 = MEMORY[0x1E0DEE9D8];
  }
  v6 = v5 + 32;
  v7[1] = v4;
  v8 = 0;
  v7[0] = v5 + 32;
  result = a2(v7, &v8);
  if (v2)
  {
    if (v8 <= v4)
    {
      if (!v7[0])
      {
LABEL_20:
        __break(1u);
        goto LABEL_21;
      }
      if (v6 == v7[0])
      {
        *(_QWORD *)(v5 + 16) = v8;
        swift_bridgeObjectRelease();
        return v5;
      }
      goto LABEL_17;
    }
LABEL_16:
    __break(1u);
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (v8 > v4)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (v7[0])
  {
    if (v6 == v7[0])
    {
      *(_QWORD *)(v5 + 16) = v8;
      return v5;
    }
    goto LABEL_19;
  }
LABEL_21:
  __break(1u);
  return result;
}

uint64_t specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(uint64_t a1, uint64_t (*a2)(_QWORD *, uint64_t *), uint64_t a3)
{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2, a3, type metadata accessor for DSPComplex);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2, a3, type metadata accessor for DSPDoubleComplex);
}

uint64_t specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(uint64_t result, uint64_t (*a2)(_QWORD *, uint64_t *), uint64_t a3, void (*a4)(_QWORD))
{
  uint64_t v4;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  _QWORD v9[2];
  uint64_t v10;

  if (result < 0)
  {
    __break(1u);
    goto LABEL_16;
  }
  v6 = result;
  if (result)
  {
    a4(0);
    v7 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v7 + 16) = v6;
  }
  else
  {
    v7 = MEMORY[0x1E0DEE9D8];
  }
  v8 = v7 + 32;
  v9[1] = v6;
  v10 = 0;
  v9[0] = v7 + 32;
  result = a2(v9, &v10);
  if (v4)
  {
    if (v10 <= v6)
    {
      if (!v9[0])
      {
LABEL_20:
        __break(1u);
        goto LABEL_21;
      }
      if (v8 == v9[0])
      {
        *(_QWORD *)(v7 + 16) = v10;
        swift_bridgeObjectRelease();
        return v7;
      }
      goto LABEL_17;
    }
LABEL_16:
    __break(1u);
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (v10 > v6)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (v9[0])
  {
    if (v8 == v9[0])
    {
      *(_QWORD *)(v7 + 16) = v10;
      return v7;
    }
    goto LABEL_19;
  }
LABEL_21:
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, a2, MEMORY[0x1E0C8C178]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, a2, MEMORY[0x1E0C8C170]);
}

uint64_t __swift_instantiateConcreteTypeFromMangledNameAbstract(uint64_t *a1)
{
  uint64_t result;

  result = *a1;
  if (result < 0)
  {
    result = MEMORY[0x1D1794CFC](255, (char *)a1 + (int)result, -(result >> 32), 0, 0);
    *a1 = result;
  }
  return result;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD v8[4];
  __int128 v9;
  uint64_t v10;
  __int128 v11;

  v4 = *(_QWORD *)(v3 + 24);
  v5 = *(_QWORD *)(v3 + 48);
  v6 = *(_QWORD *)(v3 + 88);
  v8[2] = a1;
  v8[3] = a2;
  v9 = *(_OWORD *)(v3 + 72);
  v10 = v6;
  v11 = *(_OWORD *)(v3 + 96);
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E0DEE9C0] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, _QWORD, uint64_t))
{
  uint64_t v3;

  return closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, a2, *(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24), *(_QWORD *)(v3 + 32), *(_QWORD *)(v3 + 40), *(_QWORD **)(v3 + 48), *(_QWORD *)(v3 + 56), *(_QWORD *)(v3 + 64), a3);
}

uint64_t lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(unint64_t *a1, uint64_t *a2)
{
  uint64_t result;
  uint64_t v4;

  result = *a1;
  if (!result)
  {
    v4 = __swift_instantiateConcreteTypeFromMangledNameAbstract(a2);
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for UnsafeMutableBufferPointer<A>, v4);
    atomic_store(result, a1);
  }
  return result;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, MEMORY[0x1E0C8C100]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, MEMORY[0x1E0C8C0F8]);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, _QWORD))
{
  uint64_t v3;

  return closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(a1, a2, *(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24), *(_QWORD *)(v3 + 32), *(_QWORD *)(v3 + 40), *(_QWORD **)(v3 + 48), a3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, MEMORY[0x1E0C8C0F0]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, MEMORY[0x1E0C8C0E8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD v8[4];
  __int128 v9;
  uint64_t v10;

  v4 = *(_QWORD *)(v3 + 24);
  v5 = *(_QWORD *)(v3 + 48);
  v6 = *(_QWORD *)(v3 + 88);
  v8[2] = a1;
  v8[3] = a2;
  v9 = *(_OWORD *)(v3 + 72);
  v10 = v6;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E0DEE9C0] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(a1, a2, MEMORY[0x1E0C8C058]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(a1, a2, MEMORY[0x1E0C8C050]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  _QWORD v9[3];
  __int128 v10;
  uint64_t v11;
  __int128 v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;

  v4 = *(_QWORD *)(v3 + 40);
  v5 = *(_QWORD *)(v3 + 64);
  v6 = *(_QWORD *)(v3 + 72);
  v7 = *(_QWORD *)(v3 + 80);
  v9[2] = *(_QWORD *)(v3 + 16);
  v10 = *(_OWORD *)(v3 + 24);
  v11 = v4;
  v12 = *(_OWORD *)(v3 + 48);
  v13 = a1;
  v14 = a2;
  v15 = v6;
  v16 = v7;
  v17 = v5;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, _QWORD))(v12 + 24))(a3, v9, MEMORY[0x1E0DEE9C0] + 8, v10);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v3;

  return closure #1 in closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(a1, a2, *(_QWORD *)(v3 + 64), *(_QWORD *)(v3 + 72), *(uint64_t **)(v3 + 80), *(_QWORD *)(v3 + 88), *(_QWORD *)(v3 + 96), *(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24), *(_QWORD *)(v3 + 32), *(_QWORD *)(v3 + 40), *(_QWORD *)(v3 + 48), *(_QWORD *)(v3 + 56), a3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  __int128 v6;
  _QWORD v8[3];
  __int128 v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v12;
  uint64_t v13;
  __int128 v14;
  uint64_t v15;

  v4 = *(_QWORD *)(v3 + 40);
  v5 = *(_QWORD *)(v3 + 80);
  v8[2] = *(_QWORD *)(v3 + 16);
  v9 = *(_OWORD *)(v3 + 24);
  v10 = v4;
  v6 = *(_OWORD *)(v3 + 64);
  v11 = *(_OWORD *)(v3 + 48);
  v12 = a1;
  v13 = a2;
  v14 = v6;
  v15 = v5;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, _QWORD))(v11 + 24))(a3, v8, MEMORY[0x1E0DEE9C0] + 8, v9);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))
{
  _QWORD *v3;

  return a3(a1, a2, v3[8], v3[9], v3[10], v3[11], v3[12], v3[2], v3[3], v3[4], v3[5], v3[6], v3[7]);
}

BOOL static BNNS.Norm.== infix(_:_:)(float a1, float a2)
{
  return a1 == a2;
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance BNNS.Norm(float *a1, float *a2)
{
  return *a1 == *a2;
}

uint64_t BNNS.EmbeddingLayer.__allocating_init(input:output:dictionary:paddingIndex:maximumNorm:normType:scalesGradientByFrequency:filterParameters:)(_OWORD *a1, __int128 *a2, __int128 *a3, uint64_t a4, char a5, int a6, uint64_t a7, uint64_t a8, float a9, float a10, uint64_t a11)
{
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  int *v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  int v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  int v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  int v49;
  __int128 v50;
  __int128 v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  __int128 v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  __int128 v65;
  __int128 v66;
  __int128 v67;
  __int128 v68;
  __int128 v69;
  __int128 v70;
  __int128 v71;
  uint64_t v72;
  float v73;
  float v74;
  _BYTE v75[180];
  uint64_t v76;

  v76 = *MEMORY[0x1E0C80C00];
  v11 = a3[9];
  v69 = a3[8];
  v70 = v11;
  v71 = a3[10];
  v12 = a3[5];
  v65 = a3[4];
  v66 = v12;
  v13 = a3[7];
  v67 = a3[6];
  v68 = v13;
  v14 = a3[1];
  v61 = *a3;
  v62 = v14;
  v15 = a3[3];
  v63 = a3[2];
  v64 = v15;
  v16 = a2[8];
  v17 = a2[9];
  v18 = a2[6];
  v57 = a2[7];
  v58 = v16;
  v19 = a2[10];
  v59 = v17;
  v60 = v19;
  v20 = a2[4];
  v55 = a2[5];
  v56 = v18;
  v21 = a2[2];
  v53 = a2[3];
  v54 = v20;
  v22 = a2[1];
  v50 = *a2;
  v51 = v22;
  v52 = v21;
  v23 = a1[5];
  *(_OWORD *)&v75[68] = a1[4];
  v24 = a1[2];
  *(_OWORD *)&v75[52] = a1[3];
  v25 = a1[6];
  *(_OWORD *)&v75[116] = a1[7];
  v26 = a1[9];
  *(_OWORD *)&v75[132] = a1[8];
  *(_OWORD *)&v75[148] = v26;
  *(_OWORD *)&v75[164] = a1[10];
  *(_OWORD *)&v75[84] = v23;
  *(_OWORD *)&v75[100] = v25;
  v27 = a1[1];
  *(_OWORD *)&v75[4] = *a1;
  *(_OWORD *)&v75[20] = v27;
  *(_OWORD *)&v75[36] = v24;
  v46 = *(_OWORD *)&v75[128];
  v47 = *(_OWORD *)&v75[144];
  v48 = *(_OWORD *)&v75[160];
  v42 = *(_OWORD *)&v75[64];
  v43 = *(_OWORD *)&v75[80];
  v44 = *(_OWORD *)&v75[96];
  v45 = *(_OWORD *)&v75[112];
  v38 = *(_OWORD *)v75;
  v39 = *(_OWORD *)&v75[16];
  v40 = *(_OWORD *)&v75[32];
  v37 = a5 & 1;
  v49 = *(_DWORD *)&v75[176];
  v41 = *(_OWORD *)&v75[48];
  v72 = a4;
  v73 = a9;
  v74 = a10;
  if (a8 == 1)
  {
    v28 = 0;
  }
  else
  {
    v33 = a6;
    v34 = a7;
    v35 = a8;
    v36 = a11;
    v28 = &v33;
  }
  v29 = MEMORY[0x1D17945C4](&v37, v28);
  type metadata accessor for BNNS.EmbeddingLayer();
  v30 = swift_allocObject();
  v31 = v30;
  if (v29)
  {
    *(_QWORD *)(v30 + 16) = v29;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v31;
}

uint64_t BNNS.EmbeddingLayer.apply(batchSize:input:output:)(size_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;

  return specialized static BNNS.layerApply(_:batchSize:input:output:)(v3, a1, a2, a3);
}

uint64_t BNNS.EmbeddingLayer.applyBackward(batchSize:input:output:outputGradient:generatingWeightsGradient:)(size_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, _OWORD *a5)
{
  uint64_t v5;
  __int128 v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  uint64_t result;
  _BYTE *v17;
  BNNSNDArrayDescriptor v18;
  BNNSNDArrayDescriptor v19;
  uint64_t v20;

  v20 = *MEMORY[0x1E0C80C00];
  v6 = a4[9];
  *(_OWORD *)&v19.stride[7] = a4[8];
  *(_OWORD *)&v19.data_type = v6;
  *(_OWORD *)&v19.table_data_type = a4[10];
  v7 = a4[5];
  *(_OWORD *)&v19.size[7] = a4[4];
  *(_OWORD *)&v19.stride[1] = v7;
  v8 = a4[7];
  *(_OWORD *)&v19.stride[3] = a4[6];
  *(_OWORD *)&v19.stride[5] = v8;
  v9 = a4[1];
  *(_OWORD *)&v19.flags = *a4;
  *(_OWORD *)&v19.size[1] = v9;
  v10 = a4[3];
  *(_OWORD *)&v19.size[3] = a4[2];
  *(_OWORD *)&v19.size[5] = v10;
  v11 = a5[9];
  *(_OWORD *)&v18.stride[7] = a5[8];
  *(_OWORD *)&v18.data_type = v11;
  *(_OWORD *)&v18.table_data_type = a5[10];
  v12 = a5[5];
  *(_OWORD *)&v18.size[7] = a5[4];
  *(_OWORD *)&v18.stride[1] = v12;
  v13 = a5[7];
  *(_OWORD *)&v18.stride[3] = a5[6];
  *(_OWORD *)&v18.stride[5] = v13;
  v14 = a5[1];
  *(_OWORD *)&v18.flags = *a5;
  *(_OWORD *)&v18.size[1] = v14;
  v15 = a5[3];
  *(_OWORD *)&v18.size[3] = a5[2];
  *(_OWORD *)&v18.size[5] = v15;
  result = closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingWeightsGradient:)(&v18, v5, a1, a2, a3, &v19);
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v17 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t BNNS.EmbeddingLayer.__allocating_init(bnnsFilter:)(uint64_t a1)
{
  uint64_t v2;
  uint64_t v3;

  v2 = swift_allocObject();
  v3 = v2;
  if (a1)
  {
    *(_QWORD *)(v2 + 16) = a1;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v3;
}

uint64_t type metadata accessor for BNNS.EmbeddingLayer()
{
  return objc_opt_self();
}

uint64_t BNNS.EmbeddingLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.EmbeddingLayer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

unint64_t lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNS.Error and conformance BNNS.Error;
  if (!lazy protocol witness table cache variable for type BNNS.Error and conformance BNNS.Error)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNS.Error, &type metadata for BNNS.Error);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.Error and conformance BNNS.Error);
  }
  return result;
}

{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNS.Error and conformance BNNS.Error;
  if (!lazy protocol witness table cache variable for type BNNS.Error and conformance BNNS.Error)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNS.Error, &type metadata for BNNS.Error);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.Error and conformance BNNS.Error);
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.Norm()
{
  return &type metadata for BNNS.Norm;
}

uint64_t method lookup function for BNNS.EmbeddingLayer()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of BNNS.EmbeddingLayer.apply(batchSize:input:output:)(uint64_t a1, uint64_t *a2, uint64_t *a3)
{
  uint64_t v3;
  uint64_t v4;
  int v5;
  uint64_t v6;
  int v7;
  uint64_t v8;
  int v9;
  uint64_t v10;
  int v11;
  uint64_t (*v12)(uint64_t, uint64_t *, uint64_t *);
  uint64_t v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  uint64_t v23;
  int v24;
  uint64_t v25;
  int v26;
  uint64_t v27;
  uint64_t v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  uint64_t v37;
  int v38;
  uint64_t v39;
  int v40;
  uint64_t v41;

  v4 = a2[17];
  v5 = *((_DWORD *)a2 + 36);
  v6 = a2[19];
  v7 = *((_DWORD *)a2 + 40);
  v8 = a3[17];
  v9 = *((_DWORD *)a3 + 36);
  v10 = a3[19];
  v11 = *((_DWORD *)a3 + 40);
  v12 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *))(*(_QWORD *)v3 + 96);
  v28 = *a2;
  v29 = *(_OWORD *)(a2 + 1);
  v30 = *(_OWORD *)(a2 + 3);
  v31 = *(_OWORD *)(a2 + 5);
  v32 = *(_OWORD *)(a2 + 7);
  v33 = *(_OWORD *)(a2 + 9);
  v34 = *(_OWORD *)(a2 + 11);
  v35 = *(_OWORD *)(a2 + 13);
  v36 = *(_OWORD *)(a2 + 15);
  v37 = v4;
  v38 = v5;
  v39 = v6;
  v40 = v7;
  v41 = *(uint64_t *)((char *)a2 + 164);
  v14 = *a3;
  v15 = *(_OWORD *)(a3 + 1);
  v16 = *(_OWORD *)(a3 + 3);
  v17 = *(_OWORD *)(a3 + 5);
  v18 = *(_OWORD *)(a3 + 7);
  v19 = *(_OWORD *)(a3 + 9);
  v20 = *(_OWORD *)(a3 + 11);
  v21 = *(_OWORD *)(a3 + 13);
  v22 = *(_OWORD *)(a3 + 15);
  v23 = v8;
  v24 = v9;
  v25 = v10;
  v26 = v11;
  v27 = *(uint64_t *)((char *)a3 + 164);
  return v12(a1, &v28, &v14);
}

uint64_t dispatch thunk of BNNS.EmbeddingLayer.applyBackward(batchSize:input:output:outputGradient:generatingWeightsGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5)
{
  uint64_t v5;
  uint64_t v6;
  int v7;
  uint64_t v8;
  int v9;
  uint64_t v10;
  int v11;
  uint64_t v12;
  int v13;
  uint64_t v14;
  int v15;
  uint64_t v16;
  int v17;
  uint64_t v18;
  int v19;
  uint64_t v20;
  int v21;
  uint64_t (*v22)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *);
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  uint64_t v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  uint64_t v40;
  __int128 v41;
  uint64_t v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  uint64_t v52;
  int v53;
  uint64_t v54;
  int v55;
  uint64_t v56;
  uint64_t v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  __int128 v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  __int128 v65;
  uint64_t v66;
  int v67;
  uint64_t v68;
  int v69;
  uint64_t v70;
  uint64_t v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  __int128 v76;
  __int128 v77;
  __int128 v78;
  __int128 v79;
  uint64_t v80;
  int v81;
  uint64_t v82;
  int v83;
  uint64_t v84;
  uint64_t v85;
  __int128 v86;
  __int128 v87;
  __int128 v88;
  __int128 v89;
  __int128 v90;
  __int128 v91;
  __int128 v92;
  __int128 v93;
  uint64_t v94;
  int v95;
  uint64_t v96;
  int v97;
  uint64_t v98;

  v6 = a2[17];
  v7 = *((_DWORD *)a2 + 36);
  v8 = a2[19];
  v9 = *((_DWORD *)a2 + 40);
  v10 = a3[17];
  v11 = *((_DWORD *)a3 + 36);
  v12 = a3[19];
  v13 = *((_DWORD *)a3 + 40);
  v14 = a4[17];
  v15 = *((_DWORD *)a4 + 36);
  v16 = a4[19];
  v17 = *((_DWORD *)a4 + 40);
  v18 = a5[17];
  v19 = *((_DWORD *)a5 + 36);
  v20 = a5[19];
  v21 = *((_DWORD *)a5 + 40);
  v22 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(_QWORD *)v5 + 104);
  v85 = *a2;
  v86 = *(_OWORD *)(a2 + 1);
  v87 = *(_OWORD *)(a2 + 3);
  v88 = *(_OWORD *)(a2 + 5);
  v89 = *(_OWORD *)(a2 + 7);
  v90 = *(_OWORD *)(a2 + 9);
  v91 = *(_OWORD *)(a2 + 11);
  v92 = *(_OWORD *)(a2 + 13);
  v93 = *(_OWORD *)(a2 + 15);
  v94 = v6;
  v95 = v7;
  v96 = v8;
  v97 = v9;
  v98 = *(uint64_t *)((char *)a2 + 164);
  v71 = *a3;
  v72 = *(_OWORD *)(a3 + 1);
  v73 = *(_OWORD *)(a3 + 3);
  v74 = *(_OWORD *)(a3 + 5);
  v75 = *(_OWORD *)(a3 + 7);
  v76 = *(_OWORD *)(a3 + 9);
  v77 = *(_OWORD *)(a3 + 11);
  v23 = *(_OWORD *)(a3 + 15);
  v78 = *(_OWORD *)(a3 + 13);
  v24 = *(_OWORD *)(a4 + 1);
  v25 = *(_OWORD *)(a4 + 3);
  v26 = *(_OWORD *)(a4 + 5);
  v27 = *(_OWORD *)(a4 + 7);
  v28 = *(_OWORD *)(a4 + 9);
  v29 = *(_OWORD *)(a4 + 11);
  v30 = *(_OWORD *)(a4 + 13);
  v31 = *a4;
  v32 = *(_OWORD *)(a4 + 15);
  v33 = *(_OWORD *)(a5 + 1);
  v34 = *(_OWORD *)(a5 + 3);
  v35 = *(_OWORD *)(a5 + 5);
  v36 = *(_OWORD *)(a5 + 7);
  v37 = *(_OWORD *)(a5 + 9);
  v38 = *(_OWORD *)(a5 + 11);
  v39 = *(_OWORD *)(a5 + 13);
  v40 = *a5;
  v41 = *(_OWORD *)(a5 + 15);
  v79 = v23;
  v80 = v10;
  v81 = v11;
  v82 = v12;
  v83 = v13;
  v84 = *(uint64_t *)((char *)a3 + 164);
  *(_QWORD *)&v23 = *(uint64_t *)((char *)a5 + 164);
  v57 = v31;
  v58 = v24;
  *(_QWORD *)&v24 = *(uint64_t *)((char *)a4 + 164);
  v59 = v25;
  v60 = v26;
  v61 = v27;
  v62 = v28;
  v63 = v29;
  v64 = v30;
  v65 = v32;
  v66 = v14;
  v67 = v15;
  v68 = v16;
  v69 = v17;
  v70 = v24;
  v43 = v40;
  v44 = v33;
  v45 = v34;
  v46 = v35;
  v47 = v36;
  v48 = v37;
  v49 = v38;
  v50 = v39;
  v51 = v41;
  v52 = v18;
  v53 = v19;
  v54 = v20;
  v55 = v21;
  v56 = v23;
  return v22(a1, &v85, &v71, &v57, &v43);
}

uint64_t static BNNS.transpose(input:output:firstTransposeAxis:secondTransposeAxis:filterParameters:)(__int128 *a1, __int128 *a2, uint64_t a3, uint64_t a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  uint64_t result;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  _BYTE *v29;
  int v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  uint64_t v56;

  v56 = *MEMORY[0x1E0C80C00];
  if (a7 != 1)
  {
    v30 = a5;
    v31 = a6;
    v32 = a7;
    v33 = a8;
    v19 = a1[9];
    v53 = a1[8];
    v54 = v19;
    v55 = a1[10];
    v20 = a1[5];
    v49 = a1[4];
    v50 = v20;
    v21 = a1[7];
    v51 = a1[6];
    v52 = v21;
    v22 = a1[1];
    v45 = *a1;
    v46 = v22;
    v23 = a1[3];
    v47 = a1[2];
    v48 = v23;
    v24 = a2[9];
    v42 = a2[8];
    v43 = v24;
    v44 = a2[10];
    v25 = a2[5];
    v38 = a2[4];
    v39 = v25;
    v26 = a2[7];
    v40 = a2[6];
    v41 = v26;
    v27 = a2[1];
    v34 = *a2;
    v35 = v27;
    v28 = a2[3];
    v36 = a2[2];
    v37 = v28;
    result = MEMORY[0x1D17948DC](&v34, &v45, a3, a4, &v30);
    if (!(_DWORD)result)
      return result;
    goto LABEL_5;
  }
  v8 = a1[9];
  v53 = a1[8];
  v54 = v8;
  v55 = a1[10];
  v9 = a1[5];
  v49 = a1[4];
  v50 = v9;
  v10 = a1[7];
  v51 = a1[6];
  v52 = v10;
  v11 = a1[1];
  v45 = *a1;
  v46 = v11;
  v12 = a1[3];
  v47 = a1[2];
  v48 = v12;
  v13 = a2[9];
  v42 = a2[8];
  v43 = v13;
  v44 = a2[10];
  v14 = a2[5];
  v38 = a2[4];
  v39 = v14;
  v15 = a2[7];
  v40 = a2[6];
  v41 = v15;
  v16 = a2[1];
  v34 = *a2;
  v35 = v16;
  v17 = a2[3];
  v36 = a2[2];
  v37 = v17;
  result = MEMORY[0x1D17948DC](&v34, &v45, a3, a4, 0);
  if ((_DWORD)result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v29 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static vDSP.downsample<A, B>(_:decimationFactor:filter:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  return static vDSP.downsample<A, B>(_:decimationFactor:filter:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.downsample<A, B>(_:decimationFactor:filter:));
}

{
  return static vDSP.downsample<A, B>(_:decimationFactor:filter:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.downsample<A, B>(_:decimationFactor:filter:));
}

uint64_t partial apply for closure #1 in static vDSP.downsample<A, B>(_:decimationFactor:filter:)(uint64_t a1, _QWORD *a2)
{
  return partial apply for closure #1 in static vDSP.downsample<A, B>(_:decimationFactor:filter:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:));
}

{
  return partial apply for closure #1 in static vDSP.downsample<A, B>(_:decimationFactor:filter:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:));
}

uint64_t static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:));
}

{
  return static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:));
}

uint64_t static vDSP.downsample<A, B>(_:decimationFactor:filter:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v16;
  uint64_t result;
  uint64_t v18;
  uint64_t v20;
  uint64_t (*v21)(uint64_t, uint64_t);
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;

  v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  v18 = v16 - result;
  if (__OFSUB__(v16, result))
  {
    __break(1u);
    goto LABEL_10;
  }
  if (!a2)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (v18 == 0x8000000000000000 && a2 == -1)
    goto LABEL_12;
  v20 = v18 / a2;
  result = v20 + 1;
  if (!__OFADD__(v20, 1))
  {
    v22 = a4;
    v23 = a5;
    v24 = a6;
    v25 = a7;
    v26 = a1;
    v27 = a2;
    v28 = a3;
    v29 = MEMORY[0x1E0C80A78](result);
    return v21(v29, a8);
  }
LABEL_11:
  __break(1u);
LABEL_12:
  __break(1u);
  return result;
}

uint64_t static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t (*v20)(uint64_t, uint64_t);
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t (*v24)(uint64_t);
  uint64_t v25;
  uint64_t v26;
  uint64_t result;
  uint64_t v28;
  uint64_t v29;
  BOOL v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;

  v16 = *(_QWORD *)(a6 - 8);
  MEMORY[0x1E0C80A78](a1);
  v18 = (char *)&v36 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v20 = *(uint64_t (**)(uint64_t, uint64_t))(v19 + 16);
  v38 = v19;
  v39 = v21;
  v37 = v22;
  v23 = v20(v22, v19);
  v41 = a7;
  v42 = v23;
  v24 = *(uint64_t (**)(uint64_t))(*(_QWORD *)(a10 + 8) + 16);
  v40 = a4;
  v25 = v24(a7);
  (*(void (**)(char *, uint64_t, uint64_t))(v16 + 16))(v18, a1, a6);
  v26 = (*(uint64_t (**)(uint64_t, uint64_t))(a9 + 16))(a6, a9);
  result = (*(uint64_t (**)(char *, uint64_t))(v16 + 8))(v18, a6);
  v28 = v25 - 1;
  if (__OFSUB__(v25, 1))
  {
    __break(1u);
    goto LABEL_7;
  }
  v29 = a2 * v28;
  if ((unsigned __int128)(a2 * (__int128)v28) >> 64 != (a2 * v28) >> 63)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  v30 = __OFADD__(v29, v42);
  v31 = v29 + v42;
  if (v30)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (v26 >= v31)
  {
    v32 = MEMORY[0x1E0C80A78](a11);
    *(&v36 - 12) = v37;
    *(&v36 - 11) = a6;
    v33 = v38;
    *(&v36 - 10) = v41;
    *(&v36 - 9) = v33;
    *(&v36 - 8) = a9;
    *(&v36 - 7) = a10;
    v34 = v39;
    *(&v36 - 6) = a1;
    *(&v36 - 5) = v34;
    *(&v36 - 4) = a2;
    *(&v36 - 3) = v25;
    *(&v36 - 2) = v35;
    return (*(uint64_t (**)(uint64_t))(a10 + 16))(v32);
  }
LABEL_9:
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, _QWORD *a6, uint64_t a7, uint64_t a8, uint64_t (*a9)(uint64_t, uint64_t, uint64_t))
{
  if (!a3)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (!result)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (*a6)
  {
    if (((a8 | a7) & 0x8000000000000000) == 0)
      return a9(a3, a5, result);
    __break(1u);
    goto LABEL_7;
  }
LABEL_9:
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:));
}

{
  return partial apply for closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:));
}

uint64_t partial apply for closure #1 in static vDSP.downsample<A, B>(_:decimationFactor:filter:)(uint64_t a1, _QWORD *a2, uint64_t *a3, unint64_t *a4, uint64_t (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  _QWORD *v5;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t result;
  uint64_t v17;
  uint64_t v19;

  v8 = v5[3];
  v17 = v5[2];
  v9 = v5[5];
  v19 = v5[4];
  v10 = v5[6];
  v11 = v5[7];
  v12 = v5[8];
  v13 = v5[9];
  v14 = __swift_instantiateConcreteTypeFromMangledName(a3);
  v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a4, a3);
  result = a5(v10, v11, v12, a1, v17, v8, v14, v19, v9, v15);
  *a2 = v13;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  _QWORD v7[3];
  __int128 v8;
  uint64_t v9;
  __int128 v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  __int128 v14;

  v3 = *(_QWORD *)(v2 + 40);
  v4 = *(_QWORD *)(v2 + 72);
  v5 = *(_QWORD *)(v2 + 80);
  v7[2] = *(_QWORD *)(v2 + 16);
  v8 = *(_OWORD *)(v2 + 24);
  v9 = v3;
  v10 = *(_OWORD *)(v2 + 48);
  v11 = v4;
  v12 = v5;
  v13 = a1;
  v14 = *(_OWORD *)(v2 + 88);
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, _QWORD))(v10 + 24))(a2, v7, MEMORY[0x1E0DEE9C0] + 8, v8);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, a2, MEMORY[0x1E0C8C098]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, a2, MEMORY[0x1E0C8C090]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  _QWORD v9[6];
  __int128 v10;

  v4 = *(_QWORD *)(v3 + 16);
  v5 = *(_QWORD *)(v3 + 40);
  v6 = *(_QWORD *)(v3 + 72);
  v7 = *(_QWORD *)(v3 + 80);
  v9[2] = a1;
  v9[3] = a2;
  v9[4] = v6;
  v9[5] = v7;
  v10 = *(_OWORD *)(v3 + 88);
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v5 + 24))(a3, v9, MEMORY[0x1E0DEE9C0] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t))
{
  uint64_t v3;

  return closure #1 in closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, a2, *(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24), *(_QWORD *)(v3 + 32), *(_QWORD **)(v3 + 40), *(_QWORD *)(v3 + 48), *(_QWORD *)(v3 + 56), a3);
}

uint64_t vImage.Options.init(rawValue:)@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = result;
  return result;
}

uint64_t vImage.Options.rawValue.getter()
{
  unsigned int *v0;

  return *v0;
}

void static vImage.Options.noFlags.getter(_DWORD *a1@<X8>)
{
  *a1 = 0;
}

void static vImage.Options.leaveAlphaUnchanged.getter(_DWORD *a1@<X8>)
{
  *a1 = 1;
}

void static vImage.Options.copyInPlace.getter(_DWORD *a1@<X8>)
{
  *a1 = 2;
}

void static vImage.Options.backgroundColorFill.getter(_DWORD *a1@<X8>)
{
  *a1 = 4;
}

void static vImage.Options.imageExtend.getter(_DWORD *a1@<X8>)
{
  *a1 = 8;
}

void static vImage.Options.doNotTile.getter(_DWORD *a1@<X8>)
{
  *a1 = 16;
}

void static vImage.Options.highQualityResampling.getter(_DWORD *a1@<X8>)
{
  *a1 = 32;
}

void static vImage.Options.truncateKernel.getter(_DWORD *a1@<X8>)
{
  *a1 = 64;
}

void static vImage.Options.getTempBufferSize.getter(_DWORD *a1@<X8>)
{
  *a1 = 128;
}

void static vImage.Options.printDiagnosticsToConsole.getter(_DWORD *a1@<X8>)
{
  *a1 = 256;
}

void static vImage.Options.noAllocate.getter(_DWORD *a1@<X8>)
{
  *a1 = 512;
}

void static vImage.Options.hdrContent.getter(_DWORD *a1@<X8>)
{
  *a1 = 1024;
}

void static vImage.Options.doNotClamp.getter(_DWORD *a1@<X8>)
{
  *a1 = 2048;
}

uint64_t vImage.Options.flags.getter()
{
  unsigned int *v0;

  return *v0;
}

unint64_t lazy protocol witness table accessor for type vImage.Options and conformance vImage.Options()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options;
  if (!lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImage.Options, &type metadata for vImage.Options);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options);
  }
  return result;
}

{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options;
  if (!lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImage.Options, &type metadata for vImage.Options);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options);
  }
  return result;
}

{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options;
  if (!lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImage.Options, &type metadata for vImage.Options);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options);
  }
  return result;
}

{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options;
  if (!lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImage.Options, &type metadata for vImage.Options);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options);
  }
  return result;
}

_DWORD *protocol witness for OptionSet.init(rawValue:) in conformance vImage.Options@<X0>(_DWORD *result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *result;
  return result;
}

void protocol witness for SetAlgebra.init() in conformance vImage.Options(_DWORD *a1@<X8>)
{
  *a1 = 0;
}

_DWORD *protocol witness for SetAlgebra.union(_:) in conformance vImage.Options@<X0>(_DWORD *result@<X0>, int *a2@<X8>)
{
  _DWORD *v2;

  *a2 = *v2 | *result;
  return result;
}

_DWORD *protocol witness for SetAlgebra.intersection(_:) in conformance vImage.Options@<X0>(_DWORD *result@<X0>, int *a2@<X8>)
{
  _DWORD *v2;

  *a2 = *v2 & *result;
  return result;
}

_DWORD *protocol witness for SetAlgebra.symmetricDifference(_:) in conformance vImage.Options@<X0>(_DWORD *result@<X0>, int *a2@<X8>)
{
  _DWORD *v2;

  *a2 = *v2 ^ *result;
  return result;
}

BOOL protocol witness for SetAlgebra.insert(_:) in conformance vImage.Options(_DWORD *a1, int *a2)
{
  _DWORD *v2;
  int v3;
  int v4;

  v3 = *a2;
  v4 = *v2 & *a2;
  if (v4 != *a2)
    *v2 |= v3;
  *a1 = v3;
  return v4 != v3;
}

_DWORD *protocol witness for SetAlgebra.remove(_:) in conformance vImage.Options@<X0>(_DWORD *result@<X0>, uint64_t a2@<X8>)
{
  _DWORD *v2;
  int v3;

  v3 = *v2 & *result;
  if (v3)
    *v2 &= ~*result;
  *(_DWORD *)a2 = v3;
  *(_BYTE *)(a2 + 4) = v3 == 0;
  return result;
}

int *protocol witness for SetAlgebra.update(with:) in conformance vImage.Options@<X0>(int *result@<X0>, uint64_t a2@<X8>)
{
  int *v2;
  int v3;
  int v4;
  int v5;

  v3 = *result;
  v4 = *v2;
  *v2 |= *result;
  v5 = v4 & v3;
  *(_DWORD *)a2 = v5;
  *(_BYTE *)(a2 + 4) = v5 == 0;
  return result;
}

_DWORD *protocol witness for SetAlgebra.formUnion(_:) in conformance vImage.Options(_DWORD *result)
{
  _DWORD *v1;

  *v1 |= *result;
  return result;
}

_DWORD *protocol witness for SetAlgebra.formIntersection(_:) in conformance vImage.Options(_DWORD *result)
{
  _DWORD *v1;

  *v1 &= *result;
  return result;
}

_DWORD *protocol witness for SetAlgebra.formSymmetricDifference(_:) in conformance vImage.Options(_DWORD *result)
{
  _DWORD *v1;

  *v1 ^= *result;
  return result;
}

_DWORD *protocol witness for SetAlgebra.subtracting(_:) in conformance vImage.Options@<X0>(_DWORD *result@<X0>, int *a2@<X8>)
{
  _DWORD *v2;

  *a2 = *v2 & ~*result;
  return result;
}

BOOL protocol witness for SetAlgebra.isSubset(of:) in conformance vImage.Options(_DWORD *a1)
{
  _DWORD *v1;

  return (*v1 & ~*a1) == 0;
}

BOOL protocol witness for SetAlgebra.isDisjoint(with:) in conformance vImage.Options(_DWORD *a1)
{
  _DWORD *v1;

  return (*v1 & *a1) == 0;
}

BOOL protocol witness for SetAlgebra.isSuperset(of:) in conformance vImage.Options(_DWORD *a1)
{
  _DWORD *v1;

  return (*a1 & ~*v1) == 0;
}

BOOL protocol witness for SetAlgebra.isEmpty.getter in conformance vImage.Options()
{
  _DWORD *v0;

  return *v0 == 0;
}

uint64_t protocol witness for SetAlgebra.init<A>(_:) in conformance vImage.Options()
{
  return SetAlgebra.init<A>(_:)();
}

_DWORD *protocol witness for SetAlgebra.subtract(_:) in conformance vImage.Options(_DWORD *result)
{
  _DWORD *v1;

  *v1 &= ~*result;
  return result;
}

ValueMetadata *type metadata accessor for vImage.Options()
{
  return &type metadata for vImage.Options;
}

uint64_t vImage.PixelBuffer<>.multiply<A, B>(by:divisor:preBias:postBias:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _QWORD *a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  uint64_t *v11;
  uint64_t v13;
  uint64_t v14;
  uint64_t result;
  uint64_t v16;
  _QWORD v17[2];
  _QWORD v18[4];

  v13 = *v11;
  v18[2] = *a5;
  v18[3] = v13;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(v18);
  type metadata accessor for vImage.PixelBuffer(0, a8, *(_QWORD *)(a11 + 8), v14);
  vImage.PixelBuffer.size.getter(v17);
  swift_bridgeObjectRelease();
  result = swift_bridgeObjectRelease();
  if (v18[0] == v17[0] && v18[1] == v17[1])
  {
    MEMORY[0x1E0C80A78](result);
    return (*(uint64_t (**)(unint64_t (*)(uint64_t, uint64_t)))(v16 + 24))(partial apply for closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:divisor:preBias:postBias:destination:));
  }
  else
  {
    __break(1u);
  }
  return result;
}

unint64_t closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:divisor:preBias:postBias:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, unint64_t a12, uint64_t a13, uint64_t a14)
{
  uint64_t v15;
  const void *v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t (*v20)(void);
  uint64_t (*v21)(void);
  uint64_t v22;
  uint64_t (*v23)(void);
  uint64_t (*v24)(void);
  uint64_t v25;
  unint64_t result;
  int64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  unint64_t v31;
  int64_t v32;
  unint64_t v33;
  unint64_t v34;
  int64x2_t v35;
  int64x2_t *v36;
  unint64_t v37;
  int64x2_t v38;
  int8x16_t v39;
  uint64_t v40;
  unint64_t v41;
  int64_t *v42;
  int64_t v43;
  int64_t v44;
  BOOL v45;
  uint64_t v46;
  unint64_t v47;
  int64_t v48;
  unint64_t v49;
  unint64_t v50;
  int64x2_t v51;
  int64x2_t *v52;
  unint64_t v53;
  int64x2_t v54;
  int8x16_t v55;
  uint64_t v56;
  unint64_t v57;
  int64_t *v58;
  int64_t v59;
  int64_t v60;
  unint64_t v61;
  int64_t v62;
  unint64_t v63;
  unint64_t v64;
  int64x2_t v65;
  int64x2_t *v66;
  unint64_t v67;
  int64x2_t v68;
  int8x16_t v69;
  uint64_t v70;
  unint64_t v71;
  int64_t *v72;
  int64_t v73;
  int64_t v74;
  uint64_t v75;
  int64_t v76;
  uint64_t v77;
  char *v78;
  uint64_t *v79;
  uint64_t v80;
  uint64_t v81;
  _OWORD *v82;
  uint64_t inited;
  __int128 v84;
  unint64_t v85;
  unint64_t v86;
  uint64_t v87;
  uint64_t v88;
  int64_t v89;
  char *v90;
  uint64_t *v91;
  uint64_t v92;
  uint64_t v93;
  _OWORD *v94;
  uint64_t v95;
  __int128 v96;
  unint64_t v97;
  unint64_t v98;
  unint64_t v99;
  int64_t v100;
  uint64_t v101;
  uint64_t v102;
  uint64_t v103;
  unint64_t v104;
  unint64_t v105;
  uint64_t v106;
  uint64_t v107;
  uint64_t *v108;
  uint64_t v109;
  uint64_t v110;
  uint64_t v111;
  unint64_t v112;
  unint64_t v113;
  uint64_t v114;
  uint64_t v115;
  int64_t *v116;
  uint64_t v117;
  int64_t v118;
  unint64_t v119;
  unint64_t v120;
  char *v121;
  uint64_t v122;
  uint64_t (*v123)(void);
  uint64_t (*v124)(void);
  uint64_t v125;
  uint64_t v126;
  uint64_t v127;
  uint64_t v128;
  _QWORD *v129;
  uint64_t v130;
  uint64_t v131;
  uint64_t v132;
  uint64_t v133;
  unint64_t v134;
  uint64_t v135;
  __int128 v136;
  char *v137;
  char *v138;

  v125 = a8;
  v126 = a7;
  v130 = a6;
  v128 = a5;
  v135 = a4;
  v15 = *(_QWORD *)(a10 - 8);
  v16 = (const void *)MEMORY[0x1E0C80A78](a1);
  v18 = (char *)&v122 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v129 = specialized _copyCollectionToContiguousArray<A>(_:)(v16, v19);
  (*(void (**)(char *, uint64_t, uint64_t))(v15 + 16))(v18, a3, a10);
  *(_QWORD *)&v136 = (*(uint64_t (**)(uint64_t, uint64_t))(a13 + 16))(a10, a13);
  v20 = *(uint64_t (**)(void))(a12 + 32);
  v133 = a9;
  v21 = v20;
  v134 = a12;
  v22 = ((uint64_t (*)(uint64_t, unint64_t))v20)(a9, a12);
  v23 = *(uint64_t (**)(void))(a14 + 32);
  v131 = a11;
  v132 = a14;
  v24 = v23;
  v25 = ((uint64_t (*)(uint64_t, uint64_t))v23)(a11, a14);
  result = (*(uint64_t (**)(char *, uint64_t))(v15 + 8))(v18, a10);
  if ((unsigned __int128)(v22 * (__int128)v25) >> 64 != (v22 * v25) >> 63)
    goto LABEL_113;
  if ((_QWORD)v136 != v22 * v25)
  {
LABEL_114:
    __break(1u);
    goto LABEL_115;
  }
  v27 = *(_QWORD *)(v135 + 16);
  result = v21();
  if (v27 != result)
  {
LABEL_115:
    __break(1u);
    goto LABEL_116;
  }
  v124 = v21;
  v28 = v128;
  v29 = *(_QWORD *)(v128 + 16);
  result = v24();
  if (v29 != result)
  {
LABEL_116:
    __break(1u);
LABEL_117:
    __break(1u);
LABEL_118:
    __break(1u);
    goto LABEL_119;
  }
  v127 = v29;
  v31 = v129[2];
  if (!v31)
    goto LABEL_19;
  v32 = v129[4];
  v33 = v31 - 1;
  if (v31 != 1)
  {
    if (v31 >= 5)
    {
      v34 = v33 & 0xFFFFFFFFFFFFFFFCLL | 1;
      v35 = vdupq_n_s64(v32);
      v36 = (int64x2_t *)(v129 + 7);
      v37 = v33 & 0xFFFFFFFFFFFFFFFCLL;
      v38 = v35;
      do
      {
        v35 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v35, v36[-1]), (int8x16_t)v35, (int8x16_t)v36[-1]);
        v38 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v38, *v36), (int8x16_t)v38, *(int8x16_t *)v36);
        v36 += 2;
        v37 -= 4;
      }
      while (v37);
      v39 = vbslq_s8((int8x16_t)vcgtq_s64(v35, v38), (int8x16_t)v35, (int8x16_t)v38);
      v40 = vextq_s8(v39, v39, 8uLL).u64[0];
      v32 = (int64_t)vbsl_s8((int8x8_t)vcgtd_s64(v39.i64[0], v40), *(int8x8_t *)v39.i8, (int8x8_t)v40);
      if (v33 == (v33 & 0xFFFFFFFFFFFFFFFCLL))
        goto LABEL_16;
    }
    else
    {
      v34 = 1;
    }
    v41 = v31 - v34;
    v42 = &v129[v34 + 4];
    do
    {
      v44 = *v42++;
      v43 = v44;
      if (v32 <= v44)
        v32 = v43;
      --v41;
    }
    while (v41);
  }
LABEL_16:
  if (v32 < 0)
  {
    v45 = __OFSUB__(0, v32);
    v32 = -v32;
    if (v45)
      goto LABEL_125;
  }
  if (v32 >= 0x8000)
    goto LABEL_110;
LABEL_19:
  v46 = v130;
  if (v130 < 0)
  {
    v46 = -v130;
    if (__OFSUB__(0, v130))
      goto LABEL_124;
  }
  if (v46 > 0x7FFFFFFF)
    goto LABEL_117;
  v47 = *(_QWORD *)(v135 + 16);
  if (!v47)
    goto LABEL_36;
  v48 = *(_QWORD *)(v135 + 32);
  v49 = v47 - 1;
  if (v47 != 1)
  {
    if (v47 >= 5)
    {
      v50 = v49 & 0xFFFFFFFFFFFFFFFCLL | 1;
      v51 = vdupq_n_s64(v48);
      v52 = (int64x2_t *)(v135 + 56);
      v53 = v49 & 0xFFFFFFFFFFFFFFFCLL;
      v54 = v51;
      do
      {
        v51 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v51, v52[-1]), (int8x16_t)v51, (int8x16_t)v52[-1]);
        v54 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v54, *v52), (int8x16_t)v54, *(int8x16_t *)v52);
        v52 += 2;
        v53 -= 4;
      }
      while (v53);
      v55 = vbslq_s8((int8x16_t)vcgtq_s64(v51, v54), (int8x16_t)v51, (int8x16_t)v54);
      v56 = vextq_s8(v55, v55, 8uLL).u64[0];
      v48 = (int64_t)vbsl_s8((int8x8_t)vcgtd_s64(v55.i64[0], v56), *(int8x8_t *)v55.i8, (int8x8_t)v56);
      if (v49 == (v49 & 0xFFFFFFFFFFFFFFFCLL))
        goto LABEL_33;
    }
    else
    {
      v50 = 1;
    }
    v57 = v47 - v50;
    v58 = (int64_t *)(v135 + 8 * v50 + 32);
    do
    {
      v60 = *v58++;
      v59 = v60;
      if (v48 <= v60)
        v48 = v59;
      --v57;
    }
    while (v57);
  }
LABEL_33:
  if (v48 < 0)
  {
    v45 = __OFSUB__(0, v48);
    v48 = -v48;
    if (v45)
      goto LABEL_126;
  }
  if (v48 >= 0x8000)
    goto LABEL_111;
LABEL_36:
  v61 = *(_QWORD *)(v28 + 16);
  if (!v61)
    goto LABEL_50;
  v62 = *(_QWORD *)(v28 + 32);
  v63 = v61 - 1;
  if (v61 != 1)
  {
    if (v61 >= 5)
    {
      v64 = v63 & 0xFFFFFFFFFFFFFFFCLL | 1;
      v65 = vdupq_n_s64(v62);
      v66 = (int64x2_t *)(v28 + 56);
      v67 = v63 & 0xFFFFFFFFFFFFFFFCLL;
      v68 = v65;
      do
      {
        v65 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v65, v66[-1]), (int8x16_t)v65, (int8x16_t)v66[-1]);
        v68 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v68, *v66), (int8x16_t)v68, *(int8x16_t *)v66);
        v66 += 2;
        v67 -= 4;
      }
      while (v67);
      v69 = vbslq_s8((int8x16_t)vcgtq_s64(v65, v68), (int8x16_t)v65, (int8x16_t)v68);
      v70 = vextq_s8(v69, v69, 8uLL).u64[0];
      v62 = (int64_t)vbsl_s8((int8x8_t)vcgtd_s64(v69.i64[0], v70), *(int8x8_t *)v69.i8, (int8x8_t)v70);
      if (v63 == (v63 & 0xFFFFFFFFFFFFFFFCLL))
        goto LABEL_47;
    }
    else
    {
      v64 = 1;
    }
    v71 = v61 - v64;
    v72 = (int64_t *)(v28 + 8 * v64 + 32);
    do
    {
      v74 = *v72++;
      v73 = v74;
      if (v62 <= v74)
        v62 = v73;
      --v71;
    }
    while (v71);
  }
LABEL_47:
  if (v62 < 0)
  {
    v45 = __OFSUB__(0, v62);
    v62 = -v62;
    if (v45)
      goto LABEL_127;
  }
  if (v62 > 0x7FFFFFFF)
    goto LABEL_112;
LABEL_50:
  v123 = v24;
  type metadata accessor for vImage.PixelBuffer(0, v133, *(_QWORD *)(v134 + 8), v30);
  v75 = vImage.PixelBuffer<>.vImageBuffers.getter();
  v76 = *(_QWORD *)(v75 + 16);
  v77 = MEMORY[0x1E0DEE9D8];
  if (v76)
  {
    v138 = (char *)MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v76, 0);
    v78 = v138;
    v126 = v75;
    v79 = (uint64_t *)(v75 + 56);
    do
    {
      v136 = *(_OWORD *)(v79 - 3);
      v80 = *(v79 - 1);
      v81 = *v79;
      v82 = (_OWORD *)swift_slowAlloc();
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage_Buffer>);
      inited = swift_initStackObject();
      *(_OWORD *)(inited + 32) = v136;
      *(_QWORD *)(inited + 48) = v80;
      *(_QWORD *)(inited + 56) = v81;
      v84 = *(_OWORD *)(inited + 48);
      *v82 = *(_OWORD *)(inited + 32);
      v82[1] = v84;
      swift_setDeallocating();
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v78 + 2) + 1, 1);
        v78 = v138;
      }
      v86 = *((_QWORD *)v78 + 2);
      v85 = *((_QWORD *)v78 + 3);
      if (v86 >= v85 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v85 > 1), v86 + 1, 1);
        v78 = v138;
      }
      v79 += 4;
      *((_QWORD *)v78 + 2) = v86 + 1;
      *(_QWORD *)&v78[8 * v86 + 32] = v82;
      --v76;
    }
    while (v76);
    swift_bridgeObjectRelease();
    v77 = MEMORY[0x1E0DEE9D8];
  }
  else
  {
    swift_bridgeObjectRelease();
    v78 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v138 = v78;
  type metadata accessor for vImage.PixelBuffer(0, v131, *(_QWORD *)(v132 + 8), v87);
  v88 = vImage.PixelBuffer<>.vImageBuffers.getter();
  v89 = *(_QWORD *)(v88 + 16);
  if (v89)
  {
    v137 = (char *)v77;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v89, 0);
    v90 = v137;
    v126 = v88;
    v91 = (uint64_t *)(v88 + 56);
    do
    {
      v136 = *(_OWORD *)(v91 - 3);
      v92 = *(v91 - 1);
      v93 = *v91;
      v94 = (_OWORD *)swift_slowAlloc();
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage_Buffer>);
      v95 = swift_initStackObject();
      *(_OWORD *)(v95 + 32) = v136;
      *(_QWORD *)(v95 + 48) = v92;
      *(_QWORD *)(v95 + 56) = v93;
      v96 = *(_OWORD *)(v95 + 48);
      *v94 = *(_OWORD *)(v95 + 32);
      v94[1] = v96;
      swift_setDeallocating();
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v90 + 2) + 1, 1);
        v90 = v137;
      }
      v98 = *((_QWORD *)v90 + 2);
      v97 = *((_QWORD *)v90 + 3);
      if (v98 >= v97 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v97 > 1), v98 + 1, 1);
        v90 = v137;
      }
      v91 += 4;
      *((_QWORD *)v90 + 2) = v98 + 1;
      *(_QWORD *)&v90[8 * v98 + 32] = v94;
      --v89;
    }
    while (v89);
    swift_bridgeObjectRelease();
    v77 = MEMORY[0x1E0DEE9D8];
  }
  else
  {
    swift_bridgeObjectRelease();
    v90 = (char *)MEMORY[0x1E0DEE9D8];
  }
  result = v124();
  if ((result & 0x8000000000000000) != 0)
    goto LABEL_118;
  v99 = result;
  if (HIDWORD(result))
  {
LABEL_119:
    __break(1u);
    goto LABEL_120;
  }
  result = v123();
  if ((result & 0x8000000000000000) != 0)
  {
LABEL_120:
    __break(1u);
    goto LABEL_121;
  }
  if (HIDWORD(result))
  {
LABEL_121:
    __break(1u);
LABEL_122:
    __break(1u);
    goto LABEL_123;
  }
  v134 = result;
  *(_QWORD *)&v136 = v99;
  v100 = v129[2];
  if (v100)
  {
    v137 = (char *)v77;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v100, 0);
    result = (unint64_t)v129;
    v101 = 0;
    v102 = (uint64_t)v137;
    while (1)
    {
      v103 = *(_QWORD *)(result + 8 * v101 + 32);
      if (v103 < -32768)
        break;
      if (v103 >= 0x8000)
        goto LABEL_105;
      v137 = (char *)v102;
      v105 = *(_QWORD *)(v102 + 16);
      v104 = *(_QWORD *)(v102 + 24);
      if (v105 >= v104 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v104 > 1), v105 + 1, 1);
        result = (unint64_t)v129;
        v102 = (uint64_t)v137;
      }
      ++v101;
      *(_QWORD *)(v102 + 16) = v105 + 1;
      *(_WORD *)(v102 + 2 * v105 + 32) = v103;
      if (v100 == v101)
      {
        result = swift_release();
        goto LABEL_81;
      }
    }
    __break(1u);
LABEL_105:
    __break(1u);
    goto LABEL_106;
  }
  result = swift_release();
  v102 = MEMORY[0x1E0DEE9D8];
LABEL_81:
  v106 = v128;
  if (v130 < (uint64_t)0xFFFFFFFF80000000)
    goto LABEL_122;
  if (v130 > 0x7FFFFFFF)
  {
LABEL_123:
    __break(1u);
LABEL_124:
    __break(1u);
LABEL_125:
    __break(1u);
LABEL_126:
    __break(1u);
LABEL_127:
    __break(1u);
    return result;
  }
  v107 = MEMORY[0x1E0DEE9D8];
  if (v27)
  {
    v137 = (char *)MEMORY[0x1E0DEE9D8];
    result = (unint64_t)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v27, 0);
    v107 = (uint64_t)v137;
    v108 = (uint64_t *)(v135 + 32);
    v109 = v127;
    while (1)
    {
      v111 = *v108++;
      v110 = v111;
      if (v111 < -32768)
        break;
      if (v110 >= 0x8000)
        goto LABEL_107;
      v137 = (char *)v107;
      v113 = *(_QWORD *)(v107 + 16);
      v112 = *(_QWORD *)(v107 + 24);
      if (v113 >= v112 >> 1)
      {
        result = (unint64_t)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v112 > 1), v113 + 1, 1);
        v109 = v127;
        v107 = (uint64_t)v137;
      }
      *(_QWORD *)(v107 + 16) = v113 + 1;
      *(_WORD *)(v107 + 2 * v113 + 32) = v110;
      if (!--v27)
      {
        v106 = v128;
        goto LABEL_92;
      }
    }
LABEL_106:
    __break(1u);
LABEL_107:
    __break(1u);
    goto LABEL_108;
  }
  v109 = v127;
LABEL_92:
  v114 = MEMORY[0x1E0DEE9D8];
  if (v109)
  {
    v137 = (char *)MEMORY[0x1E0DEE9D8];
    result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v109, 0);
    v115 = v127;
    v114 = (uint64_t)v137;
    v116 = (int64_t *)(v106 + 32);
    while (1)
    {
      v118 = *v116++;
      v117 = v118;
      if (v118 < (uint64_t)0xFFFFFFFF80000000)
        break;
      if (v117 > 0x7FFFFFFF)
        goto LABEL_109;
      v137 = (char *)v114;
      v120 = *(_QWORD *)(v114 + 16);
      v119 = *(_QWORD *)(v114 + 24);
      if (v120 >= v119 >> 1)
      {
        v127 = v115;
        result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v119 > 1, v120 + 1, 1);
        v115 = v127;
        v114 = (uint64_t)v137;
      }
      *(_QWORD *)(v114 + 16) = v120 + 1;
      *(_DWORD *)(v114 + 4 * v120 + 32) = v117;
      if (!--v115)
        goto LABEL_99;
    }
LABEL_108:
    __break(1u);
LABEL_109:
    __break(1u);
LABEL_110:
    __break(1u);
LABEL_111:
    __break(1u);
LABEL_112:
    __break(1u);
LABEL_113:
    __break(1u);
    goto LABEL_114;
  }
LABEL_99:
  v121 = v138;
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    v121 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v121 + 2), 0, v121);
  v138 = v121;
  swift_bridgeObjectRetain();
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    v90 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v90 + 2), 0, v90);
  v137 = v90;
  swift_bridgeObjectRetain();
  vImageMatrixMultiply_Planar8((const vImage_Buffer **)v121 + 4, (const vImage_Buffer **)v90 + 4, v136, v134, (const int16_t *)(v102 + 32), v130, (const int16_t *)(v107 + 32), (const int32_t *)(v114 + 32), 0);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  $defer #1 <A><A1, B1>() in closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:divisor:preBias:postBias:destination:)((uint64_t)&v138, (uint64_t *)&v137);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

unint64_t partial apply for closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:divisor:preBias:postBias:destination:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:divisor:preBias:postBias:destination:)(a1, a2, *(_QWORD *)(v2 + 64), *(_QWORD *)(v2 + 72), *(_QWORD *)(v2 + 80), *(_QWORD *)(v2 + 88), *(_QWORD *)(v2 + 96), *(_QWORD *)(v2 + 104), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), *(_QWORD *)(v2 + 48), *(_QWORD *)(v2 + 56));
}

uint64_t specialized Sequence<>.max()(uint64_t a1)
{
  unint64_t v1;
  unsigned int v2;
  unint64_t v3;
  unint64_t v4;
  unint64_t v5;
  uint8x16_t v6;
  uint8x16_t *v7;
  unint64_t v8;
  uint8x16_t v9;
  uint8x16_t v10;
  uint8x8_t v11;
  uint8x8_t *v12;
  unint64_t v13;
  uint8x8_t v14;
  unint64_t v15;
  unsigned __int8 *v16;
  char v17;
  unsigned int v18;

  v1 = *(_QWORD *)(a1 + 16);
  if (!v1)
  {
    LOBYTE(v2) = 0;
    return v2 | ((v1 == 0) << 8);
  }
  v2 = *(unsigned __int8 *)(a1 + 32);
  v3 = v1 - 1;
  if (v1 != 1)
  {
    if (v1 < 9)
    {
      v4 = 1;
      goto LABEL_17;
    }
    if (v1 >= 0x21)
    {
      v5 = v3 & 0xFFFFFFFFFFFFFFE0;
      v6 = (uint8x16_t)vdupq_n_s8(v2);
      v7 = (uint8x16_t *)(a1 + 49);
      v8 = v3 & 0xFFFFFFFFFFFFFFE0;
      v9 = v6;
      do
      {
        v6 = vmaxq_u8(v6, v7[-1]);
        v9 = vmaxq_u8(v9, *v7);
        v7 += 2;
        v8 -= 32;
      }
      while (v8);
      v10 = vmaxq_u8(v6, v9);
      v10.i8[0] = vmaxvq_u8(v10);
      v2 = v10.i32[0];
      if (v3 == v5)
        return v2 | ((v1 == 0) << 8);
      if ((v3 & 0x18) == 0)
      {
        v4 = v5 | 1;
LABEL_17:
        v15 = v1 - v4;
        v16 = (unsigned __int8 *)(v4 + a1 + 32);
        do
        {
          v18 = *v16++;
          v17 = v18;
          if (v2 <= v18)
            LOBYTE(v2) = v17;
          --v15;
        }
        while (v15);
        return v2 | ((v1 == 0) << 8);
      }
    }
    else
    {
      v5 = 0;
    }
    v4 = v3 & 0xFFFFFFFFFFFFFFF8 | 1;
    v11 = (uint8x8_t)vdup_n_s8(v2);
    v12 = (uint8x8_t *)(v5 + a1 + 33);
    v13 = v5 - (v3 & 0xFFFFFFFFFFFFFFF8);
    do
    {
      v14 = *v12++;
      v11 = vmax_u8(v11, v14);
      v13 += 8;
    }
    while (v13);
    LOBYTE(v2) = vmaxv_u8(v11);
    if (v3 == (v3 & 0xFFFFFFFFFFFFFFF8))
      return v2 | ((v1 == 0) << 8);
    goto LABEL_17;
  }
  return v2 | ((v1 == 0) << 8);
}

uint64_t vImage.PixelBuffer<>.multiply<A, B>(by:preBias:postBias:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, _QWORD *a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t *v10;
  uint64_t v12;
  uint64_t v13;
  uint64_t result;
  uint64_t v15;
  _QWORD v16[2];
  _QWORD v17[4];

  v12 = *v10;
  v17[2] = *a4;
  v17[3] = v12;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(v17);
  type metadata accessor for vImage.PixelBuffer(0, a7, *(_QWORD *)(a10 + 8), v13);
  vImage.PixelBuffer.size.getter(v16);
  swift_bridgeObjectRelease();
  result = swift_bridgeObjectRelease();
  if (v17[0] == v16[0] && v17[1] == v16[1])
  {
    MEMORY[0x1E0C80A78](result);
    return (*(uint64_t (**)(unint64_t (*)(uint64_t, uint64_t)))(v15 + 24))(partial apply for closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:preBias:postBias:destination:));
  }
  else
  {
    __break(1u);
  }
  return result;
}

unint64_t closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:preBias:postBias:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13)
{
  uint64_t v15;
  const void *v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t (*v20)(uint64_t, uint64_t);
  uint64_t v21;
  uint64_t (*v22)(void);
  uint64_t v23;
  unint64_t result;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  int64_t v30;
  uint64_t v31;
  char *v32;
  uint64_t *v33;
  uint64_t v34;
  uint64_t v35;
  _OWORD *v36;
  uint64_t inited;
  __int128 v38;
  unint64_t v39;
  unint64_t v40;
  uint64_t v41;
  uint64_t v42;
  int64_t v43;
  char *v44;
  uint64_t *v45;
  uint64_t v46;
  uint64_t v47;
  _OWORD *v48;
  uint64_t v49;
  __int128 v50;
  unint64_t v51;
  unint64_t v52;
  uint32_t v53;
  uint32_t v54;
  char *v55;
  const float *v56;
  const float *v57;
  const float *v58;
  uint64_t v59;
  const float *v60;
  uint64_t v61;
  uint64_t v62;
  uint64_t v63;
  uint64_t v64;
  uint64_t v65;
  uint64_t (*v66)(void);
  uint64_t (*v67)(uint64_t, uint64_t);
  uint64_t v68;
  uint64_t v69;
  __int128 v70;
  char *v71;
  char *v72;

  v62 = a7;
  v61 = a6;
  v64 = a5;
  v65 = a4;
  v15 = *(_QWORD *)(a9 - 8);
  v16 = (const void *)MEMORY[0x1E0C80A78](a1);
  v18 = (char *)&v59 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v60 = (const float *)specialized _copyCollectionToContiguousArray<A>(_:)(v16, v19);
  (*(void (**)(char *, uint64_t, uint64_t))(v15 + 16))(v18, a3, a9);
  *(_QWORD *)&v70 = (*(uint64_t (**)(uint64_t, uint64_t))(a12 + 16))(a9, a12);
  v20 = *(uint64_t (**)(uint64_t, uint64_t))(a11 + 32);
  v63 = a8;
  v67 = v20;
  v21 = v20(a8, a11);
  v22 = *(uint64_t (**)(void))(a13 + 32);
  v68 = a10;
  v69 = a13;
  v66 = v22;
  v23 = ((uint64_t (*)(uint64_t, uint64_t))v22)(a10, a13);
  result = (*(uint64_t (**)(char *, uint64_t))(v15 + 8))(v18, a9);
  if ((unsigned __int128)(v21 * (__int128)v23) >> 64 != (v21 * v23) >> 63)
  {
    __break(1u);
    goto LABEL_33;
  }
  if ((_QWORD)v70 != v21 * v23)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  v25 = *(_QWORD *)(v65 + 16);
  v26 = v63;
  result = v67(v63, a11);
  if (v25 != result)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v27 = *(_QWORD *)(v64 + 16);
  result = v66();
  if (v27 != result)
  {
LABEL_35:
    __break(1u);
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  type metadata accessor for vImage.PixelBuffer(0, v26, *(_QWORD *)(a11 + 8), v28);
  v59 = a11;
  v29 = vImage.PixelBuffer<>.vImageBuffers.getter();
  v30 = *(_QWORD *)(v29 + 16);
  v31 = MEMORY[0x1E0DEE9D8];
  if (v30)
  {
    v72 = (char *)MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v30, 0);
    v32 = v72;
    v33 = (uint64_t *)(v29 + 56);
    do
    {
      v70 = *(_OWORD *)(v33 - 3);
      v34 = *(v33 - 1);
      v35 = *v33;
      v36 = (_OWORD *)swift_slowAlloc();
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage_Buffer>);
      inited = swift_initStackObject();
      *(_OWORD *)(inited + 32) = v70;
      *(_QWORD *)(inited + 48) = v34;
      *(_QWORD *)(inited + 56) = v35;
      v38 = *(_OWORD *)(inited + 48);
      *v36 = *(_OWORD *)(inited + 32);
      v36[1] = v38;
      swift_setDeallocating();
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v32 + 2) + 1, 1);
        v32 = v72;
      }
      v40 = *((_QWORD *)v32 + 2);
      v39 = *((_QWORD *)v32 + 3);
      if (v40 >= v39 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v39 > 1), v40 + 1, 1);
        v32 = v72;
      }
      v33 += 4;
      *((_QWORD *)v32 + 2) = v40 + 1;
      *(_QWORD *)&v32[8 * v40 + 32] = v36;
      --v30;
    }
    while (v30);
    swift_bridgeObjectRelease();
    v31 = MEMORY[0x1E0DEE9D8];
  }
  else
  {
    swift_bridgeObjectRelease();
    v32 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v72 = v32;
  type metadata accessor for vImage.PixelBuffer(0, v68, *(_QWORD *)(v69 + 8), v41);
  v42 = vImage.PixelBuffer<>.vImageBuffers.getter();
  v43 = *(_QWORD *)(v42 + 16);
  if (v43)
  {
    v71 = (char *)v31;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v43, 0);
    v44 = v71;
    v45 = (uint64_t *)(v42 + 56);
    do
    {
      v70 = *(_OWORD *)(v45 - 3);
      v46 = *(v45 - 1);
      v47 = *v45;
      v48 = (_OWORD *)swift_slowAlloc();
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage_Buffer>);
      v49 = swift_initStackObject();
      *(_OWORD *)(v49 + 32) = v70;
      *(_QWORD *)(v49 + 48) = v46;
      *(_QWORD *)(v49 + 56) = v47;
      v50 = *(_OWORD *)(v49 + 48);
      *v48 = *(_OWORD *)(v49 + 32);
      v48[1] = v50;
      swift_setDeallocating();
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v44 + 2) + 1, 1);
        v44 = v71;
      }
      v52 = *((_QWORD *)v44 + 2);
      v51 = *((_QWORD *)v44 + 3);
      if (v52 >= v51 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v51 > 1), v52 + 1, 1);
        v44 = v71;
      }
      v45 += 4;
      *((_QWORD *)v44 + 2) = v52 + 1;
      *(_QWORD *)&v44[8 * v52 + 32] = v48;
      --v43;
    }
    while (v43);
    swift_bridgeObjectRelease();
    v26 = v63;
  }
  else
  {
    swift_bridgeObjectRelease();
    v44 = (char *)MEMORY[0x1E0DEE9D8];
  }
  result = v67(v26, v59);
  if ((result & 0x8000000000000000) != 0)
    goto LABEL_36;
  v53 = result;
  if (HIDWORD(result))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  result = v66();
  if ((result & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v54 = result;
  if (HIDWORD(result))
  {
LABEL_39:
    __break(1u);
    return result;
  }
  v55 = v72;
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    v55 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v55 + 2), 0, v55);
  v72 = v55;
  swift_bridgeObjectRetain();
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    v44 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v44 + 2), 0, v44);
  v71 = v44;
  v56 = v60;
  v57 = (const float *)(v65 + 32);
  v58 = (const float *)(v64 + 32);
  swift_bridgeObjectRetain();
  vImageMatrixMultiply_PlanarF((const vImage_Buffer **)v55 + 4, (const vImage_Buffer **)v44 + 4, v53, v54, v56 + 8, v57, v58, 0);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  swift_release();
  $defer #1 <A><A1, B1>() in closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:divisor:preBias:postBias:destination:)((uint64_t)&v72, (uint64_t *)&v71);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

unint64_t partial apply for closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:preBias:postBias:destination:)(uint64_t a1, uint64_t a2)
{
  uint64_t *v2;

  return closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:preBias:postBias:destination:)(a1, a2, v2[8], v2[9], v2[10], v2[11], v2[12], v2[2], v2[3], v2[4], v2[5], v2[6], v2[7]);
}

uint64_t $defer #1 <A><A1, B1>() in closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:divisor:preBias:postBias:destination:)(uint64_t result, uint64_t *a2)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t i;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t j;
  uint64_t v10;

  v3 = *(_QWORD *)result;
  v4 = *(_QWORD *)(*(_QWORD *)result + 16);
  if (v4)
  {
    swift_bridgeObjectRetain();
    for (i = 0; i != v4; ++i)
    {
      v6 = *(_QWORD *)(v3 + 8 * i + 32);
      if (v6)
        MEMORY[0x1D1794DA4](v6, -1, -1);
    }
    result = swift_bridgeObjectRelease();
  }
  v7 = *a2;
  v8 = *(_QWORD *)(v7 + 16);
  if (v8)
  {
    swift_bridgeObjectRetain();
    for (j = 0; j != v8; ++j)
    {
      v10 = *(_QWORD *)(v7 + 8 * j + 32);
      if (v10)
        MEMORY[0x1D1794DA4](v10, -1, -1);
    }
    return swift_bridgeObjectRelease();
  }
  return result;
}

vImage_Error vImage.PixelBuffer<>.multiply(by:divisor:preBias:postBias:destination:)(uint64_t a1, uint64_t divisor, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v5;
  _QWORD *v6;
  vImagePixelCount v7;
  vImagePixelCount v8;
  _QWORD *v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  void *v16;
  size_t v17;
  void *v18;
  size_t v19;
  int32_t post_bias;
  int16_t pre_bias;
  vImage_Buffer v23;
  vImage_Buffer v24;
  uint64_t v25;

  v25 = *MEMORY[0x1E0C80C00];
  v6 = *(_QWORD **)v5;
  if (!*(_QWORD *)(*(_QWORD *)v5 + 16))
  {
    __break(1u);
    goto LABEL_31;
  }
  v7 = v6[6];
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v8 = v6[5];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v7)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v8)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v9 = *(_QWORD **)a5;
  if (!*(_QWORD *)(*(_QWORD *)a5 + 16))
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v10 = v9[6];
  if (v10 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  v11 = v9[5];
  if (v11 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v10)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v11)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v7 != v10)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (v8 != v11)
  {
LABEL_41:
    __break(1u);
LABEL_42:
    __break(1u);
LABEL_43:
    __break(1u);
LABEL_44:
    __break(1u);
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v12 = a1;
  if (a1 < 0)
  {
    v12 = -a1;
    if (__OFSUB__(0, a1))
      goto LABEL_50;
  }
  if (v12 >= 0x8000)
    goto LABEL_42;
  v13 = divisor;
  if (divisor < 0)
  {
    v13 = -divisor;
    if (__OFSUB__(0, divisor))
      goto LABEL_51;
  }
  if (v13 > 0x7FFFFFFF)
    goto LABEL_43;
  v14 = a3;
  if (a3 < 0)
  {
    v14 = -a3;
    if (__OFSUB__(0, a3))
      goto LABEL_52;
  }
  if (v14 >= 0x8000)
    goto LABEL_44;
  v15 = a4;
  if (a4 < 0)
  {
    v15 = -a4;
    if (__OFSUB__(0, a4))
LABEL_53:
      __break(1u);
  }
  if (v15 > 0x7FFFFFFF)
    goto LABEL_45;
  if (a3 < -32768)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (a3 >= 0x8000)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  pre_bias = a3;
  if (a4 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (a4 > 0x7FFFFFFF)
  {
LABEL_49:
    __break(1u);
LABEL_50:
    __break(1u);
LABEL_51:
    __break(1u);
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  post_bias = a4;
  v16 = (void *)v6[4];
  v17 = v6[7];
  v24.data = v16;
  v24.height = v8;
  v24.width = v7;
  v24.rowBytes = v17;
  v18 = (void *)v9[4];
  v19 = v9[7];
  v23.data = v18;
  v23.height = v8;
  v23.width = v7;
  v23.rowBytes = v19;
  return closure #1 in closure #1 in vImage.PixelBuffer<>.multiply(by:divisor:preBias:postBias:destination:)(&v23, &v24, a1, divisor, &pre_bias, &post_bias);
}

vImage_Error closure #1 in closure #1 in vImage.PixelBuffer<>.multiply(by:divisor:preBias:postBias:destination:)(vImage_Buffer *a1, vImage_Buffer *a2, uint64_t a3, uint64_t divisor, int16_t *pre_bias, int32_t *post_bias)
{
  int16_t v7;
  vImage_Buffer *dests;
  vImage_Buffer *srcs[2];

  srcs[1] = *(vImage_Buffer **)MEMORY[0x1E0C80C00];
  dests = a1;
  srcs[0] = a2;
  if (a3 < -32768)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (a3 >= 0x8000)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  v7 = a3;
  if (divisor < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (divisor > 0x7FFFFFFF)
    goto LABEL_9;
  return vImageMatrixMultiply_Planar8((const vImage_Buffer **)srcs, (const vImage_Buffer **)&dests, 1u, 1u, &v7, divisor, pre_bias, post_bias, 0);
}

uint64_t vImage.PixelBuffer<>.multiply(by:divisor:preBias:postBias:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t *a11)
{
  uint64_t *v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t inited;
  uint64_t v27;
  unint64_t v28;
  int64_t v29;
  unint64_t v30;
  unint64_t v31;
  int64x2_t v32;
  int64x2_t *v33;
  unint64_t v34;
  int64x2_t v35;
  int8x16_t v36;
  uint64_t v37;
  unint64_t v38;
  int64_t *v39;
  int64_t v40;
  int64_t v41;
  BOOL v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  __int128 v46;
  __int128 v47;
  uint64_t v49;
  uint64_t v50;
  _OWORD v51[2];
  vImage_Buffer v52;
  uint64_t v53;

  v53 = *MEMORY[0x1E0C80C00];
  v12 = *v11;
  if (!*(_QWORD *)(*v11 + 16))
    goto LABEL_47;
  v13 = *(_QWORD *)(v12 + 48);
  if (v13 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  v14 = *(_QWORD *)(v12 + 40);
  if (v14 < 0)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v13)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (!v14)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  v15 = *a11;
  if (!*(_QWORD *)(*a11 + 16))
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  v16 = *(_QWORD *)(v15 + 48);
  if (v16 < 0)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  v17 = *(_QWORD *)(v15 + 40);
  if (v17 < 0)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (!v16)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (!v17)
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  if (v13 != v16)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  if (v14 != v17)
  {
LABEL_58:
    __break(1u);
LABEL_59:
    __break(1u);
LABEL_60:
    __break(1u);
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  v49 = *a11;
  v50 = *v11;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5F170;
  *(_QWORD *)(inited + 32) = a1;
  *(_QWORD *)(inited + 40) = a2;
  *(_QWORD *)(inited + 48) = a3;
  *(_QWORD *)(inited + 56) = a4;
  v27 = swift_initStackObject();
  *(_OWORD *)(v27 + 16) = xmmword_1CAB5F170;
  *(_QWORD *)(v27 + 32) = a6;
  *(_QWORD *)(v27 + 40) = a7;
  *(_QWORD *)(v27 + 48) = a8;
  *(_QWORD *)(v27 + 56) = a9;
  v28 = *(_QWORD *)(inited + 16);
  if (!v28)
    goto LABEL_27;
  v29 = *(_QWORD *)(inited + 32);
  v30 = v28 - 1;
  if (v28 == 1)
    goto LABEL_24;
  if (v28 >= 5)
  {
    v31 = v30 & 0xFFFFFFFFFFFFFFFCLL | 1;
    v32 = vdupq_n_s64(v29);
    v33 = (int64x2_t *)(inited + 56);
    v34 = v30 & 0xFFFFFFFFFFFFFFFCLL;
    v35 = v32;
    do
    {
      v32 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v32, v33[-1]), (int8x16_t)v32, (int8x16_t)v33[-1]);
      v35 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v35, *v33), (int8x16_t)v35, *(int8x16_t *)v33);
      v33 += 2;
      v34 -= 4;
    }
    while (v34);
    v36 = vbslq_s8((int8x16_t)vcgtq_s64(v32, v35), (int8x16_t)v32, (int8x16_t)v35);
    v37 = vextq_s8(v36, v36, 8uLL).u64[0];
    v29 = (int64_t)vbsl_s8((int8x8_t)vcgtd_s64(v36.i64[0], v37), *(int8x8_t *)v36.i8, (int8x8_t)v37);
    if (v30 == (v30 & 0xFFFFFFFFFFFFFFFCLL))
      goto LABEL_24;
  }
  else
  {
    v31 = 1;
  }
  v38 = v28 - v31;
  v39 = (int64_t *)(inited + 8 * v31 + 32);
  do
  {
    v41 = *v39++;
    v40 = v41;
    if (v29 <= v41)
      v29 = v40;
    --v38;
  }
  while (v38);
LABEL_24:
  if (v29 < 0)
  {
    v42 = __OFSUB__(0, v29);
    v29 = -v29;
    if (v42)
LABEL_67:
      __break(1u);
  }
  if (v29 >= 0x8000)
  {
    __break(1u);
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
LABEL_27:
  v43 = a5;
  if (a5 < 0)
  {
    v43 = -a5;
    if (__OFSUB__(0, a5))
      goto LABEL_64;
  }
  if (v43 > 0x7FFFFFFF)
    goto LABEL_59;
  if (a6 <= a7)
    v44 = a7;
  else
    v44 = a6;
  if (v44 <= a8)
    v44 = a8;
  if (v44 <= a9)
    v44 = a9;
  if (v44 < 0)
  {
    v42 = __OFSUB__(0, v44);
    v44 = -v44;
    if (v42)
      goto LABEL_65;
  }
  if (v44 >= 0x8000)
    goto LABEL_60;
  v45 = a10;
  if (a10 < 0)
  {
    v45 = -a10;
    if (__OFSUB__(0, a10))
      goto LABEL_66;
  }
  if (v45 > 0x7FFFFFFF)
    goto LABEL_61;
  if (!*(_QWORD *)(v50 + 16))
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  v46 = *(_OWORD *)(v50 + 48);
  *(_OWORD *)&v52.data = *(_OWORD *)(v50 + 32);
  *(_OWORD *)&v52.width = v46;
  if (!*(_QWORD *)(v49 + 16))
  {
LABEL_63:
    __break(1u);
LABEL_64:
    __break(1u);
LABEL_65:
    __break(1u);
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  v47 = *(_OWORD *)(v49 + 48);
  v51[0] = *(_OWORD *)(v49 + 32);
  v51[1] = v47;
  closure #1 in closure #1 in vImage.PixelBuffer<>.multiply(by:divisor:preBias:postBias:destination:)((char *)v51, &v52, inited, a5, v27, a10);
  swift_bridgeObjectRelease();
  return swift_setDeallocating();
}

char *closure #1 in closure #1 in vImage.PixelBuffer<>.multiply(by:divisor:preBias:postBias:destination:)(char *result, const vImage_Buffer *a2, uint64_t a3, uint64_t divisor, uint64_t a5, uint64_t a6)
{
  int64_t v8;
  uint64_t v9;
  uint64_t *v11;
  uint64_t v12;
  uint64_t v13;
  unint64_t v14;
  unint64_t v15;
  int64_t v16;
  uint64_t v17;
  uint64_t *v18;
  uint64_t v19;
  uint64_t v20;
  unint64_t v21;
  unint64_t v22;
  const vImage_Buffer *v24;
  uint64_t v26;
  uint64_t v27;

  v24 = (const vImage_Buffer *)result;
  v8 = *(_QWORD *)(a3 + 16);
  v9 = MEMORY[0x1E0DEE9D8];
  if (v8)
  {
    v26 = MEMORY[0x1E0DEE9D8];
    result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v8, 0);
    v9 = v26;
    v11 = (uint64_t *)(a3 + 32);
    while (1)
    {
      v13 = *v11++;
      v12 = v13;
      if (v13 < -32768)
        break;
      if (v12 >= 0x8000)
        goto LABEL_21;
      v15 = *(_QWORD *)(v26 + 16);
      v14 = *(_QWORD *)(v26 + 24);
      if (v15 >= v14 >> 1)
        result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v14 > 1), v15 + 1, 1);
      *(_QWORD *)(v26 + 16) = v15 + 1;
      *(_WORD *)(v26 + 2 * v15 + 32) = v12;
      if (!--v8)
        goto LABEL_8;
    }
    __break(1u);
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
LABEL_8:
  if (divisor < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (divisor > 0x7FFFFFFF)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  v16 = *(_QWORD *)(a5 + 16);
  v17 = MEMORY[0x1E0DEE9D8];
  if (v16)
  {
    v27 = MEMORY[0x1E0DEE9D8];
    result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v16, 0);
    v17 = v27;
    v18 = (uint64_t *)(a5 + 32);
    while (1)
    {
      v20 = *v18++;
      v19 = v20;
      if (v20 < -32768)
        break;
      if (v19 >= 0x8000)
        goto LABEL_23;
      v22 = *(_QWORD *)(v27 + 16);
      v21 = *(_QWORD *)(v27 + 24);
      if (v22 >= v21 >> 1)
        result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v21 > 1), v22 + 1, 1);
      *(_QWORD *)(v27 + 16) = v22 + 1;
      *(_WORD *)(v27 + 2 * v22 + 32) = v19;
      if (!--v16)
        goto LABEL_17;
    }
LABEL_22:
    __break(1u);
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
LABEL_17:
  if (a6 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (a6 <= 0x7FFFFFFF)
  {
    vImageMatrixMultiply_ARGB8888ToPlanar8(a2, v24, (const int16_t *)(v9 + 32), divisor, (const int16_t *)(v17 + 32), a6, 0);
    swift_bridgeObjectRelease();
    return (char *)swift_bridgeObjectRelease();
  }
LABEL_27:
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.multiply<A>(by:divisor:preBias:postBias:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t *a11, uint64_t a12, uint64_t a13)
{
  uint64_t *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t inited;
  uint64_t v27;
  uint64_t result;

  v14 = *v13;
  if (!*(_QWORD *)(*v13 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v15 = *(_QWORD *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v16 = *(_QWORD *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v15)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v16)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v17 = *a11;
  if (!*(_QWORD *)(*a11 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v18 = *(_QWORD *)(v17 + 48);
  if (v18 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v19 = *(_QWORD *)(v17 + 40);
  if (v19 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v18)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v19)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v15 != v18)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (v16 == v19)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
    inited = swift_initStackObject();
    *(_OWORD *)(inited + 16) = xmmword_1CAB5F170;
    *(_QWORD *)(inited + 32) = a3;
    *(_QWORD *)(inited + 40) = a4;
    *(_QWORD *)(inited + 48) = a5;
    *(_QWORD *)(inited + 56) = a6;
    v27 = swift_initStackObject();
    *(_OWORD *)(v27 + 16) = xmmword_1CAB5F170;
    *(_QWORD *)(v27 + 32) = a7;
    *(_QWORD *)(v27 + 40) = a8;
    *(_QWORD *)(v27 + 48) = a9;
    *(_QWORD *)(v27 + 56) = a10;
    MEMORY[0x1E0C80A78](v27);
    (*(void (**)(uint64_t (*)(uint64_t, uint64_t)))(a13 + 24))(partial apply for closure #1 in vImage.PixelBuffer<>.multiply<A>(by:divisor:preBias:postBias:destination:));
    swift_bridgeObjectRelease();
    return swift_setDeallocating();
  }
LABEL_25:
  __break(1u);
  return result;
}

uint64_t closure #1 in vImage.PixelBuffer<>.multiply<A>(by:divisor:preBias:postBias:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t *a7, uint64_t *a8, uint64_t a9, uint64_t a10)
{
  uint64_t v14;
  const void *v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  _QWORD *v19;
  uint64_t v20;
  unint64_t v21;
  int64_t v22;
  unint64_t v23;
  unint64_t v24;
  int64x2_t v25;
  int64x2_t *v26;
  unint64_t v27;
  int64x2_t v28;
  int8x16_t v29;
  uint64_t v30;
  unint64_t v31;
  int64_t *v32;
  int64_t v33;
  int64_t v34;
  BOOL v35;
  uint64_t v36;
  unint64_t v37;
  int64_t v38;
  unint64_t v39;
  unint64_t v40;
  int64x2_t v41;
  int64x2_t *v42;
  unint64_t v43;
  int64x2_t v44;
  int8x16_t v45;
  uint64_t v46;
  unint64_t v47;
  int64_t *v48;
  int64_t v49;
  int64_t v50;
  unint64_t v51;
  int64_t v52;
  unint64_t v53;
  unint64_t v54;
  int64x2_t v55;
  int64x2_t *v56;
  unint64_t v57;
  int64x2_t v58;
  int8x16_t v59;
  uint64_t v60;
  unint64_t v61;
  int64_t *v62;
  int64_t v63;
  int64_t v64;
  uint64_t v65;
  __int128 v66;
  uint64_t v67;
  __int128 v68;
  uint64_t v70;
  uint64_t *v71;
  uint64_t *v72;
  _OWORD v73[2];
  vImage_Buffer v74;
  uint64_t v75;

  v71 = a8;
  v72 = a7;
  v75 = *MEMORY[0x1E0C80C00];
  v14 = *(_QWORD *)(a9 - 8);
  v15 = (const void *)MEMORY[0x1E0C80A78](a1);
  v17 = (char *)&v70 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = specialized _copyCollectionToContiguousArray<A>(_:)(v15, v18);
  (*(void (**)(char *, uint64_t, uint64_t))(v14 + 16))(v17, a3, a9);
  v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a10 + 16))(a9, a10);
  (*(void (**)(char *, uint64_t))(v14 + 8))(v17, a9);
  if (v20 != 16)
    goto LABEL_53;
  v21 = v19[2];
  if (!v21)
    goto LABEL_16;
  v22 = v19[4];
  v23 = v21 - 1;
  if (v21 != 1)
  {
    if (v21 >= 5)
    {
      v24 = v23 & 0xFFFFFFFFFFFFFFFCLL | 1;
      v25 = vdupq_n_s64(v22);
      v26 = (int64x2_t *)(v19 + 7);
      v27 = v23 & 0xFFFFFFFFFFFFFFFCLL;
      v28 = v25;
      do
      {
        v25 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v25, v26[-1]), (int8x16_t)v25, (int8x16_t)v26[-1]);
        v28 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v28, *v26), (int8x16_t)v28, *(int8x16_t *)v26);
        v26 += 2;
        v27 -= 4;
      }
      while (v27);
      v29 = vbslq_s8((int8x16_t)vcgtq_s64(v25, v28), (int8x16_t)v25, (int8x16_t)v28);
      v30 = vextq_s8(v29, v29, 8uLL).u64[0];
      v22 = (int64_t)vbsl_s8((int8x8_t)vcgtd_s64(v29.i64[0], v30), *(int8x8_t *)v29.i8, (int8x8_t)v30);
      if (v23 == (v23 & 0xFFFFFFFFFFFFFFFCLL))
        goto LABEL_13;
    }
    else
    {
      v24 = 1;
    }
    v31 = v21 - v24;
    v32 = &v19[v24 + 4];
    do
    {
      v34 = *v32++;
      v33 = v34;
      if (v22 <= v34)
        v22 = v33;
      --v31;
    }
    while (v31);
  }
LABEL_13:
  if (v22 < 0)
  {
    v35 = __OFSUB__(0, v22);
    v22 = -v22;
    if (v35)
      goto LABEL_58;
  }
  if (v22 >= 0x8000)
  {
    __break(1u);
    goto LABEL_51;
  }
LABEL_16:
  v36 = a4;
  if (a4 < 0)
  {
    v36 = -a4;
    if (__OFSUB__(0, a4))
      goto LABEL_57;
  }
  if (v36 > 0x7FFFFFFF)
    goto LABEL_54;
  v37 = *(_QWORD *)(a5 + 16);
  if (v37)
  {
    v38 = *(_QWORD *)(a5 + 32);
    v39 = v37 - 1;
    if (v37 == 1)
      goto LABEL_30;
    if (v37 >= 5)
    {
      v40 = v39 & 0xFFFFFFFFFFFFFFFCLL | 1;
      v41 = vdupq_n_s64(v38);
      v42 = (int64x2_t *)(a5 + 56);
      v43 = v39 & 0xFFFFFFFFFFFFFFFCLL;
      v44 = v41;
      do
      {
        v41 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v41, v42[-1]), (int8x16_t)v41, (int8x16_t)v42[-1]);
        v44 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v44, *v42), (int8x16_t)v44, *(int8x16_t *)v42);
        v42 += 2;
        v43 -= 4;
      }
      while (v43);
      v45 = vbslq_s8((int8x16_t)vcgtq_s64(v41, v44), (int8x16_t)v41, (int8x16_t)v44);
      v46 = vextq_s8(v45, v45, 8uLL).u64[0];
      v38 = (int64_t)vbsl_s8((int8x8_t)vcgtd_s64(v45.i64[0], v46), *(int8x8_t *)v45.i8, (int8x8_t)v46);
      if (v39 == (v39 & 0xFFFFFFFFFFFFFFFCLL))
        goto LABEL_30;
    }
    else
    {
      v40 = 1;
    }
    v47 = v37 - v40;
    v48 = (int64_t *)(a5 + 8 * v40 + 32);
    do
    {
      v50 = *v48++;
      v49 = v50;
      if (v38 <= v50)
        v38 = v49;
      --v47;
    }
    while (v47);
LABEL_30:
    if (v38 < 0)
    {
      v35 = __OFSUB__(0, v38);
      v38 = -v38;
      if (v35)
        goto LABEL_59;
    }
    if (v38 < 0x8000)
      goto LABEL_33;
LABEL_51:
    __break(1u);
LABEL_52:
    __break(1u);
LABEL_53:
    __break(1u);
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
LABEL_33:
  v51 = *(_QWORD *)(a6 + 16);
  if (!v51)
    goto LABEL_47;
  v52 = *(_QWORD *)(a6 + 32);
  v53 = v51 - 1;
  if (v51 != 1)
  {
    if (v51 >= 5)
    {
      v54 = v53 & 0xFFFFFFFFFFFFFFFCLL | 1;
      v55 = vdupq_n_s64(v52);
      v56 = (int64x2_t *)(a6 + 56);
      v57 = v53 & 0xFFFFFFFFFFFFFFFCLL;
      v58 = v55;
      do
      {
        v55 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v55, v56[-1]), (int8x16_t)v55, (int8x16_t)v56[-1]);
        v58 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v58, *v56), (int8x16_t)v58, *(int8x16_t *)v56);
        v56 += 2;
        v57 -= 4;
      }
      while (v57);
      v59 = vbslq_s8((int8x16_t)vcgtq_s64(v55, v58), (int8x16_t)v55, (int8x16_t)v58);
      v60 = vextq_s8(v59, v59, 8uLL).u64[0];
      v52 = (int64_t)vbsl_s8((int8x8_t)vcgtd_s64(v59.i64[0], v60), *(int8x8_t *)v59.i8, (int8x8_t)v60);
      if (v53 == (v53 & 0xFFFFFFFFFFFFFFFCLL))
        goto LABEL_44;
    }
    else
    {
      v54 = 1;
    }
    v61 = v51 - v54;
    v62 = (int64_t *)(a6 + 8 * v54 + 32);
    do
    {
      v64 = *v62++;
      v63 = v64;
      if (v52 <= v64)
        v52 = v63;
      --v61;
    }
    while (v61);
  }
LABEL_44:
  if (v52 < 0)
  {
    v35 = __OFSUB__(0, v52);
    v52 = -v52;
    if (v35)
LABEL_60:
      __break(1u);
  }
  if (v52 > 0x7FFFFFFF)
    goto LABEL_52;
LABEL_47:
  v65 = *v72;
  if (!*(_QWORD *)(*v72 + 16))
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  v66 = *(_OWORD *)(v65 + 48);
  *(_OWORD *)&v74.data = *(_OWORD *)(v65 + 32);
  *(_OWORD *)&v74.width = v66;
  v67 = *v71;
  if (!*(_QWORD *)(*v71 + 16))
  {
LABEL_56:
    __break(1u);
LABEL_57:
    __break(1u);
LABEL_58:
    __break(1u);
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  v68 = *(_OWORD *)(v67 + 48);
  v73[0] = *(_OWORD *)(v67 + 32);
  v73[1] = v68;
  closure #1 in closure #1 in closure #1 in vImage.PixelBuffer<>.multiply<A>(by:divisor:preBias:postBias:destination:)((char *)v73, &v74, (uint64_t)v19, a4, a5, a6);
  return swift_release();
}

char *closure #1 in closure #1 in closure #1 in vImage.PixelBuffer<>.multiply<A>(by:divisor:preBias:postBias:destination:)(char *result, const vImage_Buffer *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v6;
  int64_t v8;
  uint64_t v9;
  uint64_t v11;
  uint64_t *v12;
  uint64_t v13;
  uint64_t v14;
  unint64_t v15;
  unint64_t v16;
  int64_t v17;
  uint64_t v18;
  uint64_t *v19;
  uint64_t v20;
  uint64_t v21;
  unint64_t v22;
  unint64_t v23;
  uint64_t v24;
  uint64_t v25;
  int64_t *v26;
  uint64_t v27;
  int64_t v28;
  unint64_t v29;
  unint64_t v30;
  int32_t divisora;
  const vImage_Buffer *v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;

  v6 = a6;
  v34 = (const vImage_Buffer *)result;
  v8 = *(_QWORD *)(a3 + 16);
  v9 = MEMORY[0x1E0DEE9D8];
  if (v8)
  {
    v11 = a4;
    v35 = MEMORY[0x1E0DEE9D8];
    result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v8, 0);
    v9 = v35;
    v12 = (uint64_t *)(a3 + 32);
    while (1)
    {
      v14 = *v12++;
      v13 = v14;
      if (v14 < -32768)
        break;
      if (v13 >= 0x8000)
        goto LABEL_27;
      v16 = *(_QWORD *)(v35 + 16);
      v15 = *(_QWORD *)(v35 + 24);
      if (v16 >= v15 >> 1)
        result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v15 > 1), v16 + 1, 1);
      *(_QWORD *)(v35 + 16) = v16 + 1;
      *(_WORD *)(v35 + 2 * v16 + 32) = v13;
      if (!--v8)
      {
        a4 = v11;
        v6 = a6;
        goto LABEL_9;
      }
    }
    __break(1u);
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
LABEL_9:
  if (a4 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (a4 <= 0x7FFFFFFF)
  {
    divisora = a4;
    v17 = *(_QWORD *)(a5 + 16);
    v18 = MEMORY[0x1E0DEE9D8];
    if (!v17)
    {
LABEL_18:
      v24 = *(_QWORD *)(v6 + 16);
      v25 = MEMORY[0x1E0DEE9D8];
      if (!v24)
      {
LABEL_25:
        vImageMatrixMultiply_ARGB8888(a2, v34, (const int16_t *)(v9 + 32), divisora, (const int16_t *)(v18 + 32), (const int32_t *)(v25 + 32), 0);
        swift_bridgeObjectRelease();
        swift_bridgeObjectRelease();
        return (char *)swift_bridgeObjectRelease();
      }
      v37 = MEMORY[0x1E0DEE9D8];
      result = (char *)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v24, 0);
      v25 = v37;
      v26 = (int64_t *)(v6 + 32);
      while (1)
      {
        v28 = *v26++;
        v27 = v28;
        if (v28 < (uint64_t)0xFFFFFFFF80000000)
          goto LABEL_30;
        if (v27 > 0x7FFFFFFF)
          goto LABEL_31;
        v30 = *(_QWORD *)(v37 + 16);
        v29 = *(_QWORD *)(v37 + 24);
        if (v30 >= v29 >> 1)
          result = (char *)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v29 > 1, v30 + 1, 1);
        *(_QWORD *)(v37 + 16) = v30 + 1;
        *(_DWORD *)(v37 + 4 * v30 + 32) = v27;
        if (!--v24)
          goto LABEL_25;
      }
    }
    v36 = MEMORY[0x1E0DEE9D8];
    result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v17, 0);
    v18 = v36;
    v19 = (uint64_t *)(a5 + 32);
    while (1)
    {
      v21 = *v19++;
      v20 = v21;
      if (v21 < -32768)
        break;
      if (v20 >= 0x8000)
        goto LABEL_29;
      v23 = *(_QWORD *)(v36 + 16);
      v22 = *(_QWORD *)(v36 + 24);
      if (v23 >= v22 >> 1)
        result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v22 > 1), v23 + 1, 1);
      *(_QWORD *)(v36 + 16) = v23 + 1;
      *(_WORD *)(v36 + 2 * v23 + 32) = v20;
      if (!--v17)
        goto LABEL_18;
    }
LABEL_28:
    __break(1u);
LABEL_29:
    __break(1u);
LABEL_30:
    __break(1u);
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
LABEL_33:
  __break(1u);
  return result;
}

vImage_Error vImage.PixelBuffer<>.multiply(by:preBias:postBias:destination:)(uint64_t a1, float a2, float a3, float a4)
{
  uint64_t v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  float post_bias;
  float pre_bias;
  float v18;
  vImage_Buffer *dests;
  vImage_Buffer *srcs;
  _QWORD v21[4];
  _QWORD v22[5];

  v22[4] = *MEMORY[0x1E0C80C00];
  v5 = *(_QWORD **)v4;
  if (!*(_QWORD *)(*(_QWORD *)v4 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v6)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v7)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v8 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v9 = v8[6];
  if (v9 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v10 = v8[5];
  if (v10 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v9)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v10)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v6 != v9)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v7 != v10)
    goto LABEL_25;
  post_bias = a4;
  pre_bias = a3;
  v11 = v5[4];
  v12 = v5[7];
  v22[0] = v11;
  v22[1] = v7;
  v22[2] = v6;
  v22[3] = v12;
  v13 = v8[4];
  v14 = v8[7];
  v21[0] = v13;
  v21[1] = v7;
  v21[2] = v6;
  v21[3] = v14;
  srcs = (vImage_Buffer *)v22;
  dests = (vImage_Buffer *)v21;
  v18 = a2;
  return vImageMatrixMultiply_PlanarF((const vImage_Buffer **)&srcs, (const vImage_Buffer **)&dests, 1u, 1u, &v18, &pre_bias, &post_bias, 0);
}

uint64_t vImage.PixelBuffer<>.multiply(by:preBias:postBias:destination:)(uint64_t *a1, float a2, float a3, float a4, float a5, float a6, float a7, float a8, float a9, float a10)
{
  uint64_t *v17;
  uint64_t v18;
  vImagePixelCount v19;
  vImagePixelCount v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t inited;
  size_t v33;
  size_t v34;
  vImage_Buffer dest;
  vImage_Buffer src;
  float v38[4];
  uint64_t v39;

  v39 = *MEMORY[0x1E0C80C00];
  v18 = *v17;
  if (!*(_QWORD *)(*v17 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v19 = *(_QWORD *)(v18 + 48);
  if ((v19 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v20 = *(_QWORD *)(v18 + 40);
  if ((v20 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v19)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v20)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v21 = *a1;
  if (!*(_QWORD *)(*a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v22 = *(_QWORD *)(v21 + 48);
  if (v22 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v23 = *(_QWORD *)(v21 + 40);
  if (v23 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v22)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v23)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v19 != v22)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v20 != v23)
    goto LABEL_25;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5F170;
  *(float *)(inited + 32) = a2;
  *(float *)(inited + 36) = a3;
  *(float *)(inited + 40) = a4;
  *(float *)(inited + 44) = a5;
  v38[0] = a6;
  v38[1] = a7;
  v38[2] = a8;
  v38[3] = a9;
  v33 = *(_QWORD *)(v18 + 56);
  src.data = *(void **)(v18 + 32);
  src.height = v20;
  src.width = v19;
  src.rowBytes = v33;
  v34 = *(_QWORD *)(v21 + 56);
  dest.data = *(void **)(v21 + 32);
  dest.height = v20;
  dest.width = v19;
  dest.rowBytes = v34;
  vImageMatrixMultiply_ARGBFFFFToPlanarF(&src, &dest, (const float *)(inited + 32), v38, a10, 0);
  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.multiply<A>(by:preBias:postBias:destination:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, float a5, float a6, float a7, float a8, float a9, float a10, float a11, float a12)
{
  uint64_t v12;
  uint64_t v13;
  uint64_t v26;
  uint64_t v27;
  char *v28;
  uint64_t v29;
  _QWORD *v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t inited;
  uint64_t v37;
  uint64_t v38;
  __int128 v40;
  _QWORD v41[17];

  v13 = v12;
  v41[16] = *MEMORY[0x1E0C80C00];
  v26 = *(_QWORD *)(a3 - 8);
  MEMORY[0x1E0C80A78](a1);
  v28 = (char *)&v41[-1] - ((v27 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(char *, uint64_t))(v26 + 16))(v28, a1);
  v29 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  (*(void (**)(char *, uint64_t))(v26 + 8))(v28, a3);
  if (v29 != 16)
  {
    __break(1u);
    goto LABEL_16;
  }
  v30 = *(_QWORD **)v13;
  if (!*(_QWORD *)(*(_QWORD *)v13 + 16))
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v31 = v30[6];
  if (v31 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v32 = v30[5];
  if (v32 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v31)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v32)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v33 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v34 = *(_QWORD *)(v33 + 48);
  if (v34 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v35 = *(_QWORD *)(v33 + 40);
  if (v35 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v34)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v35)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v31 != v34)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v32 != v35)
    goto LABEL_27;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
  inited = swift_initStackObject();
  v40 = xmmword_1CAB5F170;
  *(_OWORD *)(inited + 16) = xmmword_1CAB5F170;
  *(float *)(inited + 32) = a5;
  *(float *)(inited + 36) = a6;
  *(float *)(inited + 40) = a7;
  *(float *)(inited + 44) = a8;
  v37 = swift_initStackObject();
  *(_OWORD *)(v37 + 16) = v40;
  *(float *)(v37 + 32) = a9;
  *(float *)(v37 + 36) = a10;
  *(float *)(v37 + 40) = a11;
  *(float *)(v37 + 44) = a12;
  v38 = v30[7];
  v41[0] = v30[4];
  v41[1] = v32;
  v41[2] = v31;
  v41[3] = v38;
  closure #1 in vImage.PixelBuffer<>.multiply<A>(by:preBias:postBias:destination:)((uint64_t)v41, a2, a1, inited, v37, a3, a4);
  swift_bridgeObjectRelease();
  return swift_setDeallocating();
}

uint64_t closure #1 in vImage.PixelBuffer<>.multiply<A>(by:preBias:postBias:destination:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v7;
  __int128 v8;
  _OWORD v10[2];
  _BYTE v11[16];
  uint64_t v12;
  _OWORD *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;

  v16 = *MEMORY[0x1E0C80C00];
  v7 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
    __break(1u);
  v8 = *(_OWORD *)(v7 + 48);
  v10[0] = *(_OWORD *)(v7 + 32);
  v10[1] = v8;
  v12 = a1;
  v13 = v10;
  v14 = a4;
  v15 = a5;
  return (*(uint64_t (**)(const float *(*)(const float *), _BYTE *, uint64_t, uint64_t, uint64_t))(a7 + 24))(partial apply for closure #1 in closure #1 in closure #1 in vImage.PixelBuffer<>.multiply<A>(by:preBias:postBias:destination:), v11, MEMORY[0x1E0DEE9C0] + 8, a6, a7);
}

uint64_t vImage.PixelBuffer<>.multiply(by:preBias:postBias:destination:)(uint64_t *a1, int8x16_t a2, int8x16_t a3, int8x16_t a4, int8x16_t a5, float a6, float a7, float a8, float a9, float a10, float a11, float a12, float a13)
{
  uint64_t *v20;
  uint64_t v21;
  vImagePixelCount v22;
  vImagePixelCount v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t inited;
  const float *v32;
  uint64_t v33;
  int8x8_t v34;
  int32x2_t v35;
  int32x2_t v36;
  uint64_t v37;
  size_t v38;
  size_t v39;
  vImage_Buffer dest;
  vImage_Buffer src;
  float v47[4];
  uint64_t v48;

  v48 = *MEMORY[0x1E0C80C00];
  v21 = *v20;
  if (!*(_QWORD *)(*v20 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v22 = *(_QWORD *)(v21 + 48);
  if ((v22 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v23 = *(_QWORD *)(v21 + 40);
  if ((v23 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v22)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v23)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v24 = *a1;
  if (!*(_QWORD *)(*a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v25 = *(_QWORD *)(v24 + 48);
  if (v25 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v26 = *(_QWORD *)(v24 + 40);
  if (v26 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v25)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v26)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v22 != v25)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v23 != v26)
    goto LABEL_25;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5F170;
  *(float *)(inited + 32) = a6;
  v32 = (const float *)(inited + 32);
  *(float *)(inited + 36) = a7;
  *(float *)(inited + 40) = a8;
  *(float *)(inited + 44) = a9;
  v47[0] = a10;
  v47[1] = a11;
  v47[2] = a12;
  v47[3] = a13;
  v33 = swift_initStackObject();
  *(int32x2_t *)(v33 + 32) = vzip1_s32(*(int32x2_t *)a2.i8, *(int32x2_t *)a3.i8);
  *(_OWORD *)(v33 + 16) = xmmword_1CAB5F180;
  *(_DWORD *)(v33 + 40) = a4.i32[0];
  *(_QWORD *)(v33 + 44) = __PAIR64__(a2.u32[1], a5.u32[0]);
  *(int32x2_t *)(v33 + 52) = vzip2_s32(*(int32x2_t *)a3.i8, *(int32x2_t *)a4.i8);
  v34 = (int8x8_t)vextq_s8(a2, a2, 8uLL).u64[0];
  *(int8x8_t *)(v33 + 60) = vext_s8(*(int8x8_t *)a5.i8, v34, 4uLL);
  v35 = (int32x2_t)vextq_s8(a4, a4, 8uLL).u64[0];
  v36 = (int32x2_t)vextq_s8(a3, a3, 8uLL).u64[0];
  *(int32x2_t *)(v33 + 68) = vzip1_s32(v36, v35);
  LODWORD(v37) = vextq_s8(a5, a5, 8uLL).u32[0];
  HIDWORD(v37) = v34.i32[1];
  *(_QWORD *)(v33 + 76) = v37;
  *(int32x2_t *)(v33 + 84) = vzip2_s32(v36, v35);
  *(_DWORD *)(v33 + 92) = a5.i32[3];
  v38 = *(_QWORD *)(v21 + 56);
  src.data = *(void **)(v21 + 32);
  src.height = v23;
  src.width = v22;
  src.rowBytes = v38;
  v39 = *(_QWORD *)(v24 + 56);
  dest.data = *(void **)(v24 + 32);
  dest.height = v23;
  dest.width = v22;
  dest.rowBytes = v39;
  vImageMatrixMultiply_ARGBFFFF(&src, &dest, (const float *)(v33 + 32), v32, v47, 0);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

uint64_t partial apply for closure #1 in vImage.PixelBuffer<>.multiply<A>(by:divisor:preBias:postBias:destination:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in vImage.PixelBuffer<>.multiply<A>(by:divisor:preBias:postBias:destination:)(a1, a2, *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), *(_QWORD *)(v2 + 48), *(_QWORD *)(v2 + 56), *(uint64_t **)(v2 + 64), *(uint64_t **)(v2 + 72), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24));
}

const float *partial apply for closure #1 in closure #1 in closure #1 in vImage.PixelBuffer<>.multiply<A>(by:preBias:postBias:destination:)(const float *matrix)
{
  uint64_t v1;

  if (matrix)
    return (const float *)vImageMatrixMultiply_ARGBFFFF(*(const vImage_Buffer **)(v1 + 16), *(const vImage_Buffer **)(v1 + 24), matrix, (const float *)(*(_QWORD *)(v1 + 32) + 32), (const float *)(*(_QWORD *)(v1 + 40) + 32), 0);
  __break(1u);
  return matrix;
}

uint64_t vImage.PixelBuffer<>.extractChannel(at:destination:)(uint64_t a1, uint64_t a2)
{
  return vImage.PixelBuffer<>.extractChannel(at:destination:)(a1, a2, MEMORY[0x1E0C8D288]);
}

{
  return vImage.PixelBuffer<>.extractChannel(at:destination:)(a1, a2, MEMORY[0x1E0C8D290]);
}

{
  return vImage.PixelBuffer<>.extractChannel(at:destination:)(a1, a2, MEMORY[0x1E0C8D280]);
}

uint64_t vImage.PixelBuffer<>._extractChannel<A>(channelIndex:destination:extractFunc:)(uint64_t a1, uint64_t *a2, uint64_t (*a3)(uint64_t *, uint64_t *, uint64_t, _QWORD), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t *v8;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;

  v33 = *MEMORY[0x1E0C80C00];
  if (a1 > 3)
  {
    __break(1u);
    goto LABEL_6;
  }
  v12 = *a2;
  v13 = *v8;
  v25 = *v8;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&v29);
  v15 = v29;
  v14 = v30;
  type metadata accessor for vImage.PixelBuffer(0, a6, *(_QWORD *)(*(_QWORD *)(a8 + 8) + 8), v16);
  vImage.PixelBuffer.size.getter(&v25);
  swift_bridgeObjectRelease();
  if (v15 != v25 || v14 != v26)
LABEL_6:
    __break(1u);
  v29 = v13;
  v25 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v26 = v17;
  v27 = v18;
  v28 = v19;
  v29 = v12;
  v29 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v30 = v20;
  v31 = v21;
  v32 = v22;
  return a3(&v25, &v29, a1, 0);
}

uint64_t vImage.PixelBuffer<>.extractChannel(at:destination:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(_QWORD *, _QWORD *, uint64_t, _QWORD))
{
  uint64_t v3;
  _QWORD *v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  _QWORD v15[4];
  _QWORD v16[5];

  v16[4] = *MEMORY[0x1E0C80C00];
  if (a1 > 3)
  {
    __break(1u);
    goto LABEL_16;
  }
  v4 = *(_QWORD **)v3;
  if (!*(_QWORD *)(*(_QWORD *)v3 + 16))
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v5 = v4[6];
  if (v5 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v6 = v4[5];
  if (v6 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v5)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v6)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = *(_QWORD **)a2;
  if (!*(_QWORD *)(*(_QWORD *)a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v8)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v9)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v5 != v8)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v6 != v9)
    goto LABEL_27;
  v10 = v4[4];
  v11 = v4[7];
  v16[0] = v10;
  v16[1] = v6;
  v16[2] = v5;
  v16[3] = v11;
  v12 = v7[4];
  v13 = v7[7];
  v15[0] = v12;
  v15[1] = v6;
  v15[2] = v5;
  v15[3] = v13;
  return a3(v16, v15, a1, 0);
}

uint64_t BNNS.ActivationFunction.bnnsActivation.getter()
{
  uint64_t v0;

  return ((uint64_t (*)(void))((char *)sub_1CAAD8D18 + 4 * byte_1CAB5F190[*(unsigned __int8 *)(v0 + 8)]))();
}

double sub_1CAAD8D18@<D0>(uint64_t a1@<X8>)
{
  uint64_t v1;
  double result;

  *(_DWORD *)a1 = 30;
  *(_QWORD *)(a1 + 4) = v1;
  *(_DWORD *)(a1 + 12) = 1;
  result = 0.0;
  *(_OWORD *)(a1 + 16) = 0u;
  *(_OWORD *)(a1 + 32) = 0u;
  return result;
}

uint64_t BNNS.ActivationLayer.__allocating_init(function:input:output:filterParameters:)(uint64_t *a1, _OWORD *a2, __int128 *a3, int a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  char v24;
  uint64_t *v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  _BYTE __dst[352];
  int v35;
  uint64_t v36;
  uint64_t v37;
  int v38;
  __int128 v39;
  uint64_t v40;
  int v41;
  int v42;
  uint64_t v43;
  uint64_t v44;
  int v45;
  __int128 v46;
  uint64_t v47;
  _OWORD __src[22];
  uint64_t v49;

  v49 = *MEMORY[0x1E0C80C00];
  v11 = a3[8];
  v12 = a3[9];
  v13 = a3[6];
  __src[18] = a3[7];
  __src[19] = v11;
  v14 = a3[10];
  __src[20] = v12;
  __src[21] = v14;
  v15 = a3[4];
  v16 = a3[5];
  v17 = a3[2];
  __src[14] = a3[3];
  __src[15] = v15;
  __src[16] = v16;
  __src[17] = v13;
  v18 = *a3;
  __src[12] = a3[1];
  __src[13] = v17;
  v19 = a2[9];
  __src[8] = a2[8];
  __src[9] = v19;
  __src[10] = a2[10];
  __src[11] = v18;
  v20 = a2[5];
  __src[4] = a2[4];
  __src[5] = v20;
  v21 = a2[7];
  __src[6] = a2[6];
  __src[7] = v21;
  v22 = a2[1];
  __src[0] = *a2;
  __src[1] = v22;
  v23 = a2[3];
  __src[2] = a2[2];
  __src[3] = v23;
  v24 = *((_BYTE *)a1 + 8);
  v30 = *a1;
  LOBYTE(v31) = v24;
  BNNS.ActivationFunction.bnnsActivation.getter();
  memcpy(__dst, __src, sizeof(__dst));
  v35 = v42;
  v36 = v43;
  v37 = v44;
  v38 = v45;
  v39 = v46;
  v40 = v47;
  v41 = 0;
  if (a6 == 1)
  {
    v25 = 0;
  }
  else
  {
    LODWORD(v30) = a4;
    v31 = a5;
    v32 = a6;
    v33 = a7;
    v25 = &v30;
  }
  v26 = MEMORY[0x1D1794588](__dst, v25);
  type metadata accessor for BNNS.ActivationLayer();
  v27 = swift_allocObject();
  v28 = v27;
  if (v26)
  {
    *(_QWORD *)(v27 + 16) = v26;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v28;
}

uint64_t BNNS.ActivationLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.ActivationLayer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

uint64_t static BNNS.applyActivation(activation:input:output:batchSize:filterParameters:)(uint64_t *a1, _OWORD *a2, __int128 *a3, uint64_t a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  char v24;
  unint64_t v25;
  int *v26;
  uint64_t result;
  _BYTE *v28;
  unint64_t v31;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  int v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  _BYTE v45[136];
  unint64_t v46;
  unint64_t v47;
  unint64_t v48;
  unint64_t v49;
  unint64_t v50;
  unint64_t v51;
  unint64_t v52;
  unint64_t v53;
  _BYTE v54[136];
  _BYTE v55[136];
  uint64_t v56;
  char v57;
  _BYTE __dst[352];
  int v59;
  uint64_t v60;
  uint64_t v61;
  int v62;
  __int128 v63;
  uint64_t v64;
  int v65;
  int v66;
  uint64_t v67;
  uint64_t v68;
  int v69;
  __int128 v70;
  uint64_t v71;
  _OWORD __src[22];
  uint64_t v73;

  v73 = *MEMORY[0x1E0C80C00];
  v10 = a3[8];
  v11 = a3[9];
  v12 = a3[6];
  __src[18] = a3[7];
  __src[19] = v10;
  v13 = a3[10];
  __src[20] = v11;
  __src[21] = v13;
  v14 = a3[4];
  v15 = a3[5];
  v16 = a3[2];
  __src[14] = a3[3];
  __src[15] = v14;
  __src[16] = v15;
  __src[17] = v12;
  v17 = *a3;
  __src[12] = a3[1];
  __src[13] = v16;
  v18 = a2[9];
  __src[8] = a2[8];
  __src[9] = v18;
  v19 = a2[10];
  __src[11] = v17;
  __src[10] = v19;
  v20 = a2[5];
  __src[4] = a2[4];
  __src[5] = v20;
  v21 = a2[6];
  __src[7] = a2[7];
  __src[6] = v21;
  v22 = a2[1];
  __src[0] = *a2;
  __src[1] = v22;
  v23 = a2[2];
  __src[3] = a2[3];
  __src[2] = v23;
  v24 = *((_BYTE *)a1 + 8);
  v56 = *a1;
  v57 = v24;
  BNNS.ActivationFunction.bnnsActivation.getter();
  memcpy(__dst, __src, sizeof(__dst));
  v59 = v66;
  v60 = v67;
  v61 = v68;
  v62 = v69;
  v63 = v70;
  v64 = v71;
  v65 = 0;
  if (a7 == 1)
  {
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v54);
    outlined init with take of BNNS.Shape((uint64_t)v54, (uint64_t)v55);
    outlined init with take of BNNS.Shape((uint64_t)v55, (uint64_t)&v56);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v55, (uint64_t)&v56);
    BNNS.Shape.stride.getter();
    v31 = specialized static BNNS.calculateBatchStride(size:stride:)(v46, v47, v48, v49, v50, v51, v52, v53, v46, v47, v48, v49, v50, v51, v52, v53);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v46);
    outlined init with take of BNNS.Shape((uint64_t)&v46, (uint64_t)&v56);
    outlined init with take of BNNS.Shape((uint64_t)&v56, (uint64_t)v45);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)&v56, (uint64_t)v45);
    BNNS.Shape.stride.getter();
    v25 = specialized static BNNS.calculateBatchStride(size:stride:)(v33, v34, v35, v36, v37, v38, v39, v40, v33, v34, v35, v36, v37, v38, v39, v40);
    v26 = 0;
  }
  else
  {
    v41 = a5;
    v42 = a6;
    v43 = a7;
    v44 = a8;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v54);
    outlined init with take of BNNS.Shape((uint64_t)v54, (uint64_t)v55);
    outlined init with take of BNNS.Shape((uint64_t)v55, (uint64_t)&v56);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v55, (uint64_t)&v56);
    BNNS.Shape.stride.getter();
    v31 = specialized static BNNS.calculateBatchStride(size:stride:)(v46, v47, v48, v49, v50, v51, v52, v53, v46, v47, v48, v49, v50, v51, v52, v53);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v46);
    outlined init with take of BNNS.Shape((uint64_t)&v46, (uint64_t)&v56);
    outlined init with take of BNNS.Shape((uint64_t)&v56, (uint64_t)v45);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)&v56, (uint64_t)v45);
    BNNS.Shape.stride.getter();
    v25 = specialized static BNNS.calculateBatchStride(size:stride:)(v33, v34, v35, v36, v37, v38, v39, v40, v33, v34, v35, v36, v37, v38, v39, v40);
    v26 = &v41;
  }
  result = MEMORY[0x1D1794528](__dst, v26, a4, v31, v25);
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v28 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t BNNS.ActivationLayer.__allocating_init(function:axes:input:output:filterParameters:)(uint64_t *a1, uint64_t a2, _OWORD *a3, __int128 *a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  char v24;
  int v25;
  uint64_t *v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  _BYTE __dst[352];
  int v38;
  uint64_t v39;
  uint64_t v40;
  int v41;
  __int128 v42;
  uint64_t v43;
  int v44;
  int v45;
  uint64_t v46;
  uint64_t v47;
  int v48;
  __int128 v49;
  uint64_t v50;
  _OWORD __src[22];
  uint64_t v52;

  v52 = *MEMORY[0x1E0C80C00];
  v11 = a4[8];
  v12 = a4[9];
  v13 = a4[6];
  __src[18] = a4[7];
  __src[19] = v11;
  v14 = a4[10];
  __src[20] = v12;
  __src[21] = v14;
  v15 = a4[4];
  v16 = a4[5];
  v17 = a4[2];
  __src[14] = a4[3];
  __src[15] = v15;
  __src[16] = v16;
  __src[17] = v13;
  v18 = *a4;
  __src[12] = a4[1];
  __src[13] = v17;
  v19 = a3[9];
  __src[8] = a3[8];
  __src[9] = v19;
  __src[10] = a3[10];
  __src[11] = v18;
  v20 = a3[5];
  __src[4] = a3[4];
  __src[5] = v20;
  v21 = a3[7];
  __src[6] = a3[6];
  __src[7] = v21;
  v22 = a3[1];
  __src[0] = *a3;
  __src[1] = v22;
  v23 = a3[3];
  __src[2] = a3[2];
  __src[3] = v23;
  v24 = *((_BYTE *)a1 + 8);
  v33 = *a1;
  LOBYTE(v34) = v24;
  BNNS.ActivationFunction.bnnsActivation.getter();
  v25 = specialized static BNNS.computeAxisFlags(_:)(a2);
  swift_bridgeObjectRelease();
  memcpy(__dst, __src, sizeof(__dst));
  v38 = v45;
  v39 = v46;
  v40 = v47;
  v41 = v48;
  v42 = v49;
  v43 = v50;
  v44 = v25;
  if (a7 == 1)
  {
    v26 = 0;
  }
  else
  {
    LODWORD(v33) = a5;
    v34 = a6;
    v35 = a7;
    v36 = a8;
    v26 = &v33;
  }
  v27 = MEMORY[0x1D1794588](__dst, v26);
  type metadata accessor for BNNS.ActivationLayer();
  v28 = swift_allocObject();
  v29 = v28;
  if (v27)
  {
    *(_QWORD *)(v28 + 16) = v27;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v29;
}

uint64_t static BNNS.applyActivation(activation:axes:input:output:batchSize:filterParameters:)(uint64_t *a1, uint64_t a2, _OWORD *a3, __int128 *a4, uint64_t a5, int a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  char v24;
  int v25;
  unint64_t v26;
  int *v27;
  uint64_t result;
  _BYTE *v29;
  unint64_t v33;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  int v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  _BYTE v47[136];
  unint64_t v48;
  unint64_t v49;
  unint64_t v50;
  unint64_t v51;
  unint64_t v52;
  unint64_t v53;
  unint64_t v54;
  unint64_t v55;
  _BYTE v56[136];
  _BYTE v57[136];
  uint64_t v58;
  char v59;
  _BYTE __dst[352];
  int v61;
  uint64_t v62;
  uint64_t v63;
  int v64;
  __int128 v65;
  uint64_t v66;
  int v67;
  int v68;
  uint64_t v69;
  uint64_t v70;
  int v71;
  __int128 v72;
  uint64_t v73;
  _OWORD __src[22];
  uint64_t v75;

  v75 = *MEMORY[0x1E0C80C00];
  v10 = a4[8];
  v11 = a4[9];
  v12 = a4[6];
  __src[18] = a4[7];
  __src[19] = v10;
  v13 = a4[10];
  __src[20] = v11;
  __src[21] = v13;
  v14 = a4[4];
  v15 = a4[5];
  v16 = a4[2];
  __src[14] = a4[3];
  __src[15] = v14;
  __src[16] = v15;
  __src[17] = v12;
  v17 = *a4;
  __src[12] = a4[1];
  __src[13] = v16;
  v18 = a3[9];
  __src[8] = a3[8];
  __src[9] = v18;
  v19 = a3[10];
  __src[11] = v17;
  __src[10] = v19;
  v20 = a3[5];
  __src[4] = a3[4];
  __src[5] = v20;
  v21 = a3[6];
  __src[7] = a3[7];
  __src[6] = v21;
  v22 = a3[1];
  __src[0] = *a3;
  __src[1] = v22;
  v23 = a3[2];
  __src[3] = a3[3];
  __src[2] = v23;
  v24 = *((_BYTE *)a1 + 8);
  v58 = *a1;
  v59 = v24;
  BNNS.ActivationFunction.bnnsActivation.getter();
  v25 = specialized static BNNS.computeAxisFlags(_:)(a2);
  memcpy(__dst, __src, sizeof(__dst));
  v61 = v68;
  v62 = v69;
  v63 = v70;
  v64 = v71;
  v65 = v72;
  v66 = v73;
  v67 = v25;
  if (a8 == 1)
  {
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v56);
    outlined init with take of BNNS.Shape((uint64_t)v56, (uint64_t)v57);
    outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)&v58);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)&v58);
    BNNS.Shape.stride.getter();
    v33 = specialized static BNNS.calculateBatchStride(size:stride:)(v48, v49, v50, v51, v52, v53, v54, v55, v48, v49, v50, v51, v52, v53, v54, v55);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v48);
    outlined init with take of BNNS.Shape((uint64_t)&v48, (uint64_t)&v58);
    outlined init with take of BNNS.Shape((uint64_t)&v58, (uint64_t)v47);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)&v58, (uint64_t)v47);
    BNNS.Shape.stride.getter();
    v26 = specialized static BNNS.calculateBatchStride(size:stride:)(v35, v36, v37, v38, v39, v40, v41, v42, v35, v36, v37, v38, v39, v40, v41, v42);
    v27 = 0;
  }
  else
  {
    v43 = a6;
    v44 = a7;
    v45 = a8;
    v46 = a9;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v56);
    outlined init with take of BNNS.Shape((uint64_t)v56, (uint64_t)v57);
    outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)&v58);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)&v58);
    BNNS.Shape.stride.getter();
    v33 = specialized static BNNS.calculateBatchStride(size:stride:)(v48, v49, v50, v51, v52, v53, v54, v55, v48, v49, v50, v51, v52, v53, v54, v55);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v48);
    outlined init with take of BNNS.Shape((uint64_t)&v48, (uint64_t)&v58);
    outlined init with take of BNNS.Shape((uint64_t)&v58, (uint64_t)v47);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)&v58, (uint64_t)v47);
    BNNS.Shape.stride.getter();
    v26 = specialized static BNNS.calculateBatchStride(size:stride:)(v35, v36, v37, v38, v39, v40, v41, v42, v35, v36, v37, v38, v39, v40, v41, v42);
    v27 = &v43;
  }
  result = MEMORY[0x1D1794528](__dst, v27, a5, v33, v26);
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v29 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t type metadata accessor for BNNS.ActivationLayer()
{
  return objc_opt_self();
}

uint64_t outlined init with take of BNNS.Shape(uint64_t a1, uint64_t a2)
{
  __swift_memcpy129_8(a2, a1);
  return a2;
}

uint64_t __swift_memcpy9_4(uint64_t result, uint64_t *a2)
{
  uint64_t v2;

  v2 = *a2;
  *(_BYTE *)(result + 8) = *((_BYTE *)a2 + 8);
  *(_QWORD *)result = v2;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.ActivationFunction(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  int v4;

  if (!a2)
    return 0;
  if (a2 >= 0xEE && *(_BYTE *)(a1 + 9))
    return (*(_DWORD *)a1 + 238);
  v3 = *(unsigned __int8 *)(a1 + 8);
  if (v3 <= 0x12)
    v4 = -1;
  else
    v4 = v3 ^ 0xFF;
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.ActivationFunction(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xED)
  {
    *(_BYTE *)(result + 8) = 0;
    *(_QWORD *)result = a2 - 238;
    if (a3 >= 0xEE)
      *(_BYTE *)(result + 9) = 1;
  }
  else
  {
    if (a3 >= 0xEE)
      *(_BYTE *)(result + 9) = 0;
    if (a2)
      *(_BYTE *)(result + 8) = -(char)a2;
  }
  return result;
}

uint64_t getEnumTag for BNNS.ActivationFunction(uint64_t a1)
{
  if (*(unsigned __int8 *)(a1 + 8) <= 0x11u)
    return *(unsigned __int8 *)(a1 + 8);
  else
    return (*(_DWORD *)a1 + 18);
}

uint64_t destructiveInjectEnumTag for BNNS.ActivationFunction(uint64_t result, unsigned int a2)
{
  if (a2 >= 0x12)
  {
    *(_QWORD *)result = a2 - 18;
    LOBYTE(a2) = 18;
  }
  *(_BYTE *)(result + 8) = a2;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.ActivationFunction()
{
  return &type metadata for BNNS.ActivationFunction;
}

uint64_t specialized SetAlgebra<>.init(arrayLiteral:)@<X0>(uint64_t a1@<X0>, _DWORD *a2@<X8>)
{
  uint64_t v3;
  int v4;
  int *v5;
  int v6;
  int v7;
  uint64_t result;

  v3 = *(_QWORD *)(a1 + 16);
  if (v3)
  {
    v4 = 0;
    v5 = (int *)(a1 + 32);
    do
    {
      v7 = *v5++;
      v6 = v7;
      if ((v7 & ~v4) == 0)
        v6 = 0;
      v4 |= v6;
      --v3;
    }
    while (v3);
  }
  else
  {
    v4 = 0;
  }
  result = swift_bridgeObjectRelease();
  *a2 = v4;
  return result;
}

void *static BNNS.DataLayout.allCases.getter()
{
  return &outlined read-only object #0 of static BNNS.DataLayout.allCases.getter;
}

uint64_t BNNS.DataLayout.rank.getter()
{
  char *v0;
  unint64_t v1;

  v1 = *v0;
  if (v1 > 0x14)
    return 8;
  else
    return qword_1CAB5F5F0[v1];
}

BOOL static BNNS.DataLayout.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void BNNS.DataLayout.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance BNNS.DataLayout(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void protocol witness for Hashable.hash(into:) in conformance BNNS.DataLayout()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

void protocol witness for static CaseIterable.allCases.getter in conformance BNNS.DataLayout(_QWORD *a1@<X8>)
{
  *a1 = &outlined read-only object #0 of static BNNS.DataLayout.allCases.getter;
}

uint64_t static BNNS.defaultLayoutForDimensions(_:)@<X0>(uint64_t result@<X0>, char *a2@<X8>)
{
  char v2;

  if ((unint64_t)(result - 1) >= 8)
    v2 = 21;
  else
    v2 = 0x110F0D0B09060300uLL >> (8 * (result - 1));
  *a2 = v2;
  return result;
}

Swift::Int BNNS.DataLayout.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance BNNS.DataLayout()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t BNNSFilterParameters.options.setter(uint64_t result)
{
  _DWORD *v1;

  *v1 = result;
  return result;
}

uint64_t (*BNNSFilterParameters.options.modify(uint64_t a1))(uint64_t result)
{
  _DWORD *v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *v1;
  return BNNSFilterParameters.options.modify;
}

uint64_t BNNSFilterParameters.options.modify(uint64_t result)
{
  **(_DWORD **)result = *(_DWORD *)(result + 8);
  return result;
}

uint64_t BNNSFilterParameters.threadCount.getter(uint64_t a1, uint64_t a2)
{
  return a2;
}

uint64_t BNNSFilterParameters.threadCount.setter(uint64_t result)
{
  uint64_t v1;

  *(_QWORD *)(v1 + 8) = result;
  return result;
}

_QWORD *(*BNNSFilterParameters.threadCount.modify(_QWORD *a1))(_QWORD *result)
{
  uint64_t v1;

  *a1 = *(_QWORD *)(v1 + 8);
  a1[1] = v1;
  return BNNSFilterParameters.threadCount.modify;
}

_QWORD *BNNSFilterParameters.threadCount.modify(_QWORD *result)
{
  *(_QWORD *)(result[1] + 8) = *result;
  return result;
}

uint64_t BNNSFilterParameters.allocator.getter(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return a3;
}

uint64_t BNNSFilterParameters.allocator.setter(uint64_t result)
{
  uint64_t v1;

  *(_QWORD *)(v1 + 16) = result;
  return result;
}

_QWORD *(*BNNSFilterParameters.allocator.modify(_QWORD *a1))(_QWORD *result)
{
  uint64_t v1;

  *a1 = *(_QWORD *)(v1 + 16);
  a1[1] = v1;
  return BNNSFilterParameters.allocator.modify;
}

_QWORD *BNNSFilterParameters.allocator.modify(_QWORD *result)
{
  *(_QWORD *)(result[1] + 16) = *result;
  return result;
}

uint64_t BNNSFilterParameters.deallocator.getter(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return a4;
}

uint64_t BNNSFilterParameters.deallocator.setter(uint64_t result)
{
  uint64_t v1;

  *(_QWORD *)(v1 + 24) = result;
  return result;
}

_QWORD *(*BNNSFilterParameters.deallocator.modify(_QWORD *a1))(_QWORD *result)
{
  uint64_t v1;

  *a1 = *(_QWORD *)(v1 + 24);
  a1[1] = v1;
  return BNNSFilterParameters.deallocator.modify;
}

_QWORD *BNNSFilterParameters.deallocator.modify(_QWORD *result)
{
  *(_QWORD *)(result[1] + 24) = *result;
  return result;
}

unint64_t lazy protocol witness table accessor for type BNNS.DataLayout and conformance BNNS.DataLayout()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNS.DataLayout and conformance BNNS.DataLayout;
  if (!lazy protocol witness table cache variable for type BNNS.DataLayout and conformance BNNS.DataLayout)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNS.DataLayout, &type metadata for BNNS.DataLayout);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.DataLayout and conformance BNNS.DataLayout);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type [BNNS.DataLayout] and conformance [A]()
{
  unint64_t result;
  uint64_t v1;

  result = lazy protocol witness table cache variable for type [BNNS.DataLayout] and conformance [A];
  if (!lazy protocol witness table cache variable for type [BNNS.DataLayout] and conformance [A])
  {
    v1 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for [BNNS.DataLayout]);
    result = MEMORY[0x1D1794D08](MEMORY[0x1E0DEAF50], v1);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type [BNNS.DataLayout] and conformance [A]);
  }
  return result;
}

uint64_t base witness table accessor for RawRepresentable in BNNSFlags()
{
  return lazy protocol witness table accessor for type BNNSFlags and conformance BNNSFlags(&lazy protocol witness table cache variable for type BNNSFlags and conformance BNNSFlags, (uint64_t)&protocol conformance descriptor for BNNSFlags);
}

uint64_t base witness table accessor for SetAlgebra in BNNSFlags()
{
  return lazy protocol witness table accessor for type BNNSFlags and conformance BNNSFlags(&lazy protocol witness table cache variable for type BNNSFlags and conformance BNNSFlags, (uint64_t)&protocol conformance descriptor for BNNSFlags);
}

uint64_t base witness table accessor for Equatable in BNNSFlags()
{
  return lazy protocol witness table accessor for type BNNSFlags and conformance BNNSFlags(&lazy protocol witness table cache variable for type BNNSFlags and conformance BNNSFlags, (uint64_t)&protocol conformance descriptor for BNNSFlags);
}

uint64_t base witness table accessor for ExpressibleByArrayLiteral in BNNSFlags()
{
  return lazy protocol witness table accessor for type BNNSFlags and conformance BNNSFlags(&lazy protocol witness table cache variable for type BNNSFlags and conformance BNNSFlags, (uint64_t)&protocol conformance descriptor for BNNSFlags);
}

uint64_t lazy protocol witness table accessor for type BNNSFlags and conformance BNNSFlags(unint64_t *a1, uint64_t a2)
{
  uint64_t result;
  uint64_t v5;

  result = *a1;
  if (!result)
  {
    type metadata accessor for BNNSFlags(255);
    result = MEMORY[0x1D1794D08](a2, v5);
    atomic_store(result, a1);
  }
  return result;
}

uint64_t protocol witness for ExpressibleByArrayLiteral.init(arrayLiteral:) in conformance BNNSFlags@<X0>(uint64_t a1@<X0>, _DWORD *a2@<X8>)
{
  int v3;
  uint64_t result;

  v3 = specialized SetAlgebra<>.init(arrayLiteral:)(a1);
  result = swift_bridgeObjectRelease();
  *a2 = v3;
  return result;
}

_DWORD *sub_1CAADA098@<X0>(_DWORD *result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *result;
  return result;
}

_DWORD *sub_1CAADA0A4(_DWORD *result, _DWORD *a2)
{
  *a2 = *result;
  return result;
}

uint64_t sub_1CAADA0B0@<X0>(uint64_t result@<X0>, _QWORD *a2@<X8>)
{
  *a2 = *(_QWORD *)(result + 8);
  return result;
}

_QWORD *sub_1CAADA0BC(_QWORD *result, uint64_t a2)
{
  *(_QWORD *)(a2 + 8) = *result;
  return result;
}

uint64_t sub_1CAADA0C8@<X0>(uint64_t result@<X0>, _QWORD *a2@<X8>)
{
  *a2 = *(_QWORD *)(result + 16);
  return result;
}

_QWORD *sub_1CAADA0D4(_QWORD *result, uint64_t a2)
{
  *(_QWORD *)(a2 + 16) = *result;
  return result;
}

uint64_t sub_1CAADA0E0@<X0>(uint64_t result@<X0>, _QWORD *a2@<X8>)
{
  *a2 = *(_QWORD *)(result + 24);
  return result;
}

_QWORD *sub_1CAADA0EC(_QWORD *result, uint64_t a2)
{
  *(_QWORD *)(a2 + 24) = *result;
  return result;
}

ValueMetadata *type metadata accessor for BNNS()
{
  return &type metadata for BNNS;
}

_BYTE *__swift_memcpy1_1(_BYTE *result, _BYTE *a2)
{
  *result = *a2;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.DataLayout(unsigned __int8 *a1, unsigned int a2)
{
  int v2;
  int v3;
  int v4;
  unsigned int v6;
  BOOL v7;
  int v8;

  if (!a2)
    return 0;
  if (a2 < 0xEC)
    goto LABEL_17;
  if (a2 + 20 >= 0xFFFF00)
    v2 = 4;
  else
    v2 = 2;
  if ((a2 + 20) >> 8 < 0xFF)
    v3 = 1;
  else
    v3 = v2;
  if (v3 == 4)
  {
    v4 = *(_DWORD *)(a1 + 1);
    if (v4)
      return (*a1 | (v4 << 8)) - 20;
  }
  else
  {
    if (v3 == 2)
    {
      v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1))
        goto LABEL_17;
      return (*a1 | (v4 << 8)) - 20;
    }
    v4 = a1[1];
    if (a1[1])
      return (*a1 | (v4 << 8)) - 20;
  }
LABEL_17:
  v6 = *a1;
  v7 = v6 >= 0x15;
  v8 = v6 - 21;
  if (!v7)
    v8 = -1;
  return (v8 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.DataLayout(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 20 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 20) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xEC)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xEB)
    return ((uint64_t (*)(void))((char *)&loc_1CAADA1F0 + 4 * byte_1CAB5F29D[v4]))();
  *a1 = a2 + 20;
  return ((uint64_t (*)(void))((char *)sub_1CAADA224 + 4 * byte_1CAB5F298[v4]))();
}

uint64_t sub_1CAADA224(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAADA22C(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAADA234);
  return result;
}

uint64_t sub_1CAADA240(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAADA248);
  *(_BYTE *)result = a2 + 20;
  return result;
}

uint64_t sub_1CAADA24C(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAADA254(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

uint64_t getEnumTag for BNNS.DataLayout(unsigned __int8 *a1)
{
  return *a1;
}

_BYTE *destructiveInjectEnumTag for BNNS.DataLayout(_BYTE *result, char a2)
{
  *result = a2;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.DataLayout()
{
  return &type metadata for BNNS.DataLayout;
}

uint64_t getEnumTagSinglePayload for BNNS.Error(unsigned __int8 *a1, unsigned int a2)
{
  int v2;
  int v3;
  int v4;
  unsigned int v6;
  BOOL v7;
  int v8;

  if (!a2)
    return 0;
  if (a2 < 0xFD)
    goto LABEL_17;
  if (a2 + 3 >= 0xFFFF00)
    v2 = 4;
  else
    v2 = 2;
  if ((a2 + 3) >> 8 < 0xFF)
    v3 = 1;
  else
    v3 = v2;
  if (v3 == 4)
  {
    v4 = *(_DWORD *)(a1 + 1);
    if (v4)
      return (*a1 | (v4 << 8)) - 3;
  }
  else
  {
    if (v3 == 2)
    {
      v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1))
        goto LABEL_17;
      return (*a1 | (v4 << 8)) - 3;
    }
    v4 = a1[1];
    if (a1[1])
      return (*a1 | (v4 << 8)) - 3;
  }
LABEL_17:
  v6 = *a1;
  v7 = v6 >= 4;
  v8 = v6 - 4;
  if (!v7)
    v8 = -1;
  return (v8 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.Error(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 3 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 3) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFD)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFC)
    return ((uint64_t (*)(void))((char *)&loc_1CAADA35C + 4 * byte_1CAB5F2A7[v4]))();
  *a1 = a2 + 3;
  return ((uint64_t (*)(void))((char *)sub_1CAADA390 + 4 * byte_1CAB5F2A2[v4]))();
}

uint64_t sub_1CAADA390(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAADA398(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAADA3A0);
  return result;
}

uint64_t sub_1CAADA3AC(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAADA3B4);
  *(_BYTE *)result = a2 + 3;
  return result;
}

uint64_t sub_1CAADA3B8(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAADA3C0(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.Error()
{
  return &type metadata for BNNS.Error;
}

unint64_t specialized static BNNS.calculateBatchStride(size:stride:)(unint64_t result, unint64_t a2, unint64_t a3, unint64_t a4, unint64_t a5, unint64_t a6, unint64_t a7, unint64_t a8, unint64_t a9, unint64_t a10, unint64_t a11, unint64_t a12, unint64_t a13, unint64_t a14, unint64_t a15, unint64_t a16)
{
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;

  v16 = a9;
  if (result <= 1)
    v17 = 1;
  else
    v17 = result;
  if (a9 <= 1)
    v16 = 1;
  v18 = v17 * v16;
  if ((unsigned __int128)(v17 * (__int128)v16) >> 64 != (v17 * v16) >> 63)
  {
    __break(1u);
LABEL_64:
    __break(1u);
LABEL_65:
    __break(1u);
LABEL_66:
    __break(1u);
LABEL_67:
    __break(1u);
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
LABEL_70:
    __break(1u);
LABEL_71:
    __break(1u);
LABEL_72:
    __break(1u);
LABEL_73:
    __break(1u);
LABEL_74:
    __break(1u);
LABEL_75:
    __break(1u);
LABEL_76:
    __break(1u);
LABEL_77:
    __break(1u);
    return result;
  }
  if (a2 <= 1)
    v19 = 1;
  else
    v19 = a2;
  v20 = v18 * v19;
  if ((unsigned __int128)(v18 * (__int128)v19) >> 64 != (v18 * v19) >> 63)
    goto LABEL_64;
  if (a10 <= 1)
    v21 = 1;
  else
    v21 = a10;
  v22 = v20 * v21;
  if ((unsigned __int128)(v20 * (__int128)v21) >> 64 != (v20 * v21) >> 63)
    goto LABEL_65;
  if (a3 <= 1)
    v23 = 1;
  else
    v23 = a3;
  v24 = v22 * v23;
  if ((unsigned __int128)(v22 * (__int128)v23) >> 64 != (v22 * v23) >> 63)
    goto LABEL_66;
  if (a11 <= 1)
    v25 = 1;
  else
    v25 = a11;
  v26 = v24 * v25;
  if ((unsigned __int128)(v24 * (__int128)v25) >> 64 != (v24 * v25) >> 63)
    goto LABEL_67;
  if (a4 <= 1)
    v27 = 1;
  else
    v27 = a4;
  v28 = v26 * v27;
  if ((unsigned __int128)(v26 * (__int128)v27) >> 64 != (v26 * v27) >> 63)
    goto LABEL_68;
  if (a12 <= 1)
    v29 = 1;
  else
    v29 = a12;
  v30 = v28 * v29;
  if ((unsigned __int128)(v28 * (__int128)v29) >> 64 != (v28 * v29) >> 63)
    goto LABEL_69;
  if (a5 <= 1)
    v31 = 1;
  else
    v31 = a5;
  v32 = v30 * v31;
  if ((unsigned __int128)(v30 * (__int128)v31) >> 64 != (v30 * v31) >> 63)
    goto LABEL_70;
  if (a13 <= 1)
    v33 = 1;
  else
    v33 = a13;
  v34 = v32 * v33;
  if ((unsigned __int128)(v32 * (__int128)v33) >> 64 != (v32 * v33) >> 63)
    goto LABEL_71;
  if (a6 <= 1)
    v35 = 1;
  else
    v35 = a6;
  v36 = v34 * v35;
  if ((unsigned __int128)(v34 * (__int128)v35) >> 64 != (v34 * v35) >> 63)
    goto LABEL_72;
  if (a14 <= 1)
    v37 = 1;
  else
    v37 = a14;
  v38 = v36 * v37;
  if ((unsigned __int128)(v36 * (__int128)v37) >> 64 != (v36 * v37) >> 63)
    goto LABEL_73;
  if (a7 <= 1)
    v39 = 1;
  else
    v39 = a7;
  v40 = v38 * v39;
  if ((unsigned __int128)(v38 * (__int128)v39) >> 64 != (v38 * v39) >> 63)
    goto LABEL_74;
  if (a15 <= 1)
    v41 = 1;
  else
    v41 = a15;
  v42 = v40 * v41;
  if ((unsigned __int128)(v40 * (__int128)v41) >> 64 != (v40 * v41) >> 63)
    goto LABEL_75;
  if (a8 <= 1)
    v43 = 1;
  else
    v43 = a8;
  v44 = v42 * v43;
  if ((unsigned __int128)(v42 * (__int128)v43) >> 64 != (v42 * v43) >> 63)
    goto LABEL_76;
  v45 = a16;
  if (a16 <= 1)
    v45 = 1;
  result = v44 * v45;
  if ((unsigned __int128)(v44 * (__int128)v45) >> 64 != (v44 * v45) >> 63)
    goto LABEL_77;
  return result;
}

uint64_t specialized SetAlgebra<>.init(arrayLiteral:)(uint64_t result)
{
  uint64_t v1;
  uint64_t v2;
  int *v3;
  int v4;
  int v5;

  v1 = *(_QWORD *)(result + 16);
  if (!v1)
    return 0;
  v2 = result;
  LODWORD(result) = 0;
  v3 = (int *)(v2 + 32);
  do
  {
    v5 = *v3++;
    v4 = v5;
    if ((v5 & ~(_DWORD)result) == 0)
      v4 = 0;
    result = v4 | result;
    --v1;
  }
  while (v1);
  return result;
}

void specialized static BNNS.makeArrayDescriptor(shape:data:dataType:)(uint64_t a1)
{
  char *v1;
  _BYTE v2[136];
  _BYTE v3[136];

  outlined init with take of BNNS.Shape(a1, (uint64_t)v2);
  outlined init with take of BNNS.Shape((uint64_t)v2, (uint64_t)v3);
  v1 = (char *)sub_1CAADA644 + 4 * word_1CAB5F2AC[_s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v3)];
  __asm { BR              X10 }
}

double sub_1CAADA644()
{
  uint64_t v0;
  int v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t *v4;
  uint64_t v5;
  __int128 v6;
  double result;
  _BYTE v8[136];
  uint64_t v9;

  destructiveProjectEnumData for BNNS.ActivationFunction(v3 - 200);
  outlined init with take of BNNS.Shape((uint64_t)&v9, (uint64_t)v8);
  v4 = (uint64_t *)destructiveProjectEnumData for BNNS.ActivationFunction(v8);
  v5 = *v4;
  *(_QWORD *)&v6 = v4[1];
  *(_DWORD *)v0 = 0;
  *(_DWORD *)(v0 + 4) = 0x10000;
  *(_QWORD *)(v0 + 8) = v5;
  *(_OWORD *)(v0 + 16) = 0u;
  *(_OWORD *)(v0 + 32) = 0u;
  *(_OWORD *)(v0 + 48) = 0u;
  *(_QWORD *)(v0 + 64) = 0;
  *(_OWORD *)(v0 + 72) = v6;
  *(_OWORD *)(v0 + 88) = 0u;
  *(_OWORD *)(v0 + 104) = 0u;
  *(_OWORD *)(v0 + 120) = 0u;
  *(_QWORD *)(v0 + 136) = v2;
  *(_DWORD *)(v0 + 144) = v1;
  *(_QWORD *)(v0 + 152) = 0;
  *(_DWORD *)(v0 + 160) = v1;
  *(_QWORD *)&result = 1065353216;
  *(_QWORD *)(v0 + 164) = 1065353216;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOg(uint64_t a1)
{
  return *(unsigned __int8 *)(a1 + 128);
}

void BNNS.PoolingType.bnnsPoolingFunction.getter()
{
  _OWORD *v0;
  char *v1;
  _OWORD v2[13];
  _OWORD v3[13];

  outlined init with take of BNNS.PoolingType(v0, v2);
  outlined init with take of BNNS.PoolingType(v2, v3);
  v1 = (char *)sub_1CAADAD34 + 4 * byte_1CAB5F6A0[_s10Accelerate4BNNSO11PoolingTypeOWOg((uint64_t)v3)];
  __asm { BR              X10 }
}

uint64_t sub_1CAADAD34()
{
  uint64_t v1;

  ((void (*)(uint64_t *))destructiveProjectEnumData for BNNS.ActivationFunction)(&v1);
  return 0;
}

_OWORD *outlined init with take of BNNS.PoolingType(_OWORD *a1, _OWORD *a2)
{
  __int128 v2;
  __int128 v3;
  __int128 v4;
  __int128 v5;
  __int128 v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;

  *a2 = *a1;
  v2 = a1[1];
  v3 = a1[2];
  v4 = a1[4];
  a2[3] = a1[3];
  a2[4] = v4;
  a2[1] = v2;
  a2[2] = v3;
  v5 = a1[5];
  v6 = a1[6];
  v7 = a1[8];
  a2[7] = a1[7];
  a2[8] = v7;
  a2[5] = v5;
  a2[6] = v6;
  v8 = a1[9];
  v9 = a1[10];
  v10 = a1[11];
  *(_OWORD *)((char *)a2 + 185) = *(_OWORD *)((char *)a1 + 185);
  a2[10] = v9;
  a2[11] = v10;
  a2[9] = v8;
  return a2;
}

uint64_t _s10Accelerate4BNNSO11PoolingTypeOWOg(uint64_t a1)
{
  if (*(unsigned __int8 *)(a1 + 200) <= 4u)
    return *(unsigned __int8 *)(a1 + 200);
  else
    return (*(_DWORD *)a1 + 5);
}

void BNNS.PoolingLayer.__allocating_init(type:input:output:bias:activation:kernelSize:stride:padding:filterParameters:)(_OWORD *a1, __int128 *a2, _OWORD *a3)
{
  __int128 v3;
  __int128 v4;
  __int128 v5;
  __int128 v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  char *v18;
  _OWORD v19[50];
  _OWORD v20[13];
  uint64_t v21;

  v21 = *MEMORY[0x1E0C80C00];
  v3 = a3[9];
  *(_OWORD *)((char *)&v19[35] + 8) = a3[8];
  *(_OWORD *)((char *)&v19[36] + 8) = v3;
  v4 = a3[5];
  *(_OWORD *)((char *)&v19[31] + 8) = a3[4];
  *(_OWORD *)((char *)&v19[32] + 8) = v4;
  v5 = a3[7];
  *(_OWORD *)((char *)&v19[33] + 8) = a3[6];
  *(_OWORD *)((char *)&v19[34] + 8) = v5;
  v6 = a3[1];
  *(_OWORD *)((char *)&v19[27] + 8) = *a3;
  *(_OWORD *)((char *)&v19[28] + 8) = v6;
  v7 = a3[3];
  *(_OWORD *)((char *)&v19[29] + 8) = a3[2];
  *(_OWORD *)((char *)&v19[30] + 8) = v7;
  v8 = a2[8];
  v9 = a2[9];
  v10 = a2[6];
  *(_OWORD *)((char *)&v19[45] + 8) = a2[7];
  *(_OWORD *)((char *)&v19[46] + 8) = v8;
  v11 = a2[10];
  *(_OWORD *)((char *)&v19[47] + 8) = v9;
  *(_OWORD *)((char *)&v19[48] + 8) = v11;
  v12 = a2[4];
  v13 = a2[5];
  v14 = a2[2];
  *(_OWORD *)((char *)&v19[41] + 8) = a2[3];
  *(_OWORD *)((char *)&v19[42] + 8) = v12;
  v15 = a3[10];
  *(_OWORD *)((char *)&v19[43] + 8) = v13;
  *(_OWORD *)((char *)&v19[44] + 8) = v10;
  v16 = *a2;
  v17 = a2[1];
  *(_OWORD *)((char *)&v19[37] + 8) = v15;
  *(_OWORD *)((char *)&v19[38] + 8) = v16;
  *(_OWORD *)((char *)&v19[39] + 8) = v17;
  *(_OWORD *)((char *)&v19[40] + 8) = v14;
  outlined init with take of BNNS.PoolingType(a1, v20);
  outlined init with take of BNNS.PoolingType(v20, v19);
  v18 = (char *)sub_1CAADAF34 + 4 * byte_1CAB5F6A6[_s10Accelerate4BNNSO11PoolingTypeOWOg((uint64_t)v19)];
  __asm { BR              X10 }
}

uint64_t sub_1CAADAF34(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16)
{
  uint64_t v16;
  char v17;
  unint64_t v18;
  uint64_t v19;
  __int128 v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  __int128 v34;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  unint64_t v46;

  v19 = destructiveProjectEnumData for BNNS.ActivationFunction(&STACK[0x7A8]);
  v32 = *(_QWORD *)(v19 + 32);
  v33 = *(_QWORD *)(v19 + 24);
  HIDWORD(a16) = *(unsigned __int8 *)(v19 + 16);
  v30 = *(_QWORD *)v19;
  v31 = *(_QWORD *)(v19 + 8);
  _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&STACK[0x370]);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&STACK[0x370], (uint64_t)&STACK[0x6F0], &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  outlined init with take of BNNSNDArrayDescriptor?(v16, (uint64_t)&STACK[0x8A8], &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)&STACK[0x8A8]) == 1)
  {
    v45 = 0;
    v46 = 0;
    v43 = 0;
    v44 = 0;
    v41 = 0;
    v42 = 0;
    v39 = 0;
    v40 = 0;
    v37 = 0;
    v38 = 0;
    v35 = 0;
    v36 = 0;
    v34 = 0u;
  }
  else
  {
    v41 = STACK[0x8E8];
    v45 = STACK[0x8F8];
    v46 = STACK[0x8D8];
    v43 = STACK[0x900];
    v44 = STACK[0x8E0];
    v42 = STACK[0x908];
    v39 = STACK[0x918];
    v40 = STACK[0x910];
    v37 = STACK[0x928];
    v38 = STACK[0x920];
    *(_QWORD *)&v20 = STACK[0x938];
    v34 = v20;
    v35 = STACK[0x940];
    v36 = STACK[0x930];
    HIDWORD(a10) = STACK[0x954];
  }
  STACK[0x210] = v18;
  LOBYTE(STACK[0x218]) = v17;
  BNNS.ActivationFunction.bnnsActivation.getter();
  outlined init with take of BNNS.PoolingType(&STACK[0xAC8], &STACK[0x620]);
  v21 = _s10Accelerate4BNNSO11PoolingTypeOWOg((uint64_t)&STACK[0x620]);
  return ((uint64_t (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, _QWORD, _QWORD, unint64_t, unint64_t, unint64_t, unint64_t, unint64_t, unint64_t, unint64_t, unint64_t, unint64_t, unint64_t, unint64_t, unint64_t))((char *)sub_1CAADB188 + 4 * byte_1CAB5F6AC[v21]))(v21, v22, v23, v24, v25, v26, v27, v28, a9, a10, a11, a12, a13, v30, v31, a16, v32, v33, v34,
           *((_QWORD *)&v34 + 1),
           v35,
           v36,
           v37,
           v38,
           v39,
           v40,
           v41,
           v42,
           v43,
           v44,
           v45,
           v46);
}

uint64_t sub_1CAADB188()
{
  _QWORD *v0;
  int v1;
  _OWORD *v2;
  int v3;
  uint64_t v4;
  uint64_t v5;
  __int128 *v6;
  uint64_t v7;
  int v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  int *v29;
  uint64_t v30;
  uint64_t v31;
  int v33;
  uint64_t v34;
  uint64_t v35;
  int v36;
  uint64_t v37;
  uint64_t v38;
  char v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;
  uint64_t v56;
  uint64_t v57;
  uint64_t v58;
  uint64_t v59;
  uint64_t v60;
  uint64_t v61;
  uint64_t v62;
  uint64_t v63;
  uint64_t v64;
  uint64_t v65;
  uint64_t v66;
  uint64_t v67;
  uint64_t v68;
  uint64_t v69;
  uint64_t v70;
  uint64_t v71;
  uint64_t v72;
  int v73;
  uint64_t v74;
  uint64_t v75;
  uint64_t v76;
  _BYTE v77[352];
  _BYTE v78[352];
  uint64_t v79;
  uint64_t v80;
  uint64_t v81;
  uint64_t v82;
  uint64_t v83;
  uint64_t v84;
  uint64_t v85;
  uint64_t v86;
  uint64_t v87;
  uint64_t v88;
  uint64_t v89;
  uint64_t v90;
  uint64_t v91;
  uint64_t v92;
  uint64_t v93;
  uint64_t v94;
  uint64_t v95;
  uint64_t v96;
  uint64_t v97;
  uint64_t v98;
  int v99;
  int v100;
  int v101;
  int v102;
  uint64_t v103;
  uint64_t v104;
  uint64_t v105;
  int v106;
  uint64_t v107;
  uint64_t v108;
  uint64_t v109;
  uint64_t v110;
  uint64_t v111;
  uint64_t v112;
  uint64_t v113;
  uint64_t v114;
  uint64_t v115;
  uint64_t v116;
  uint64_t v117;
  uint64_t v118;
  uint64_t v119;
  uint64_t v120;

  ((void (*)(uint64_t *))destructiveProjectEnumData for BNNS.ActivationFunction)(&v119);
  v12 = v6[18];
  v13 = v6[20];
  v14 = v6[21];
  v2[8] = v6[19];
  v2[9] = v13;
  v15 = v6[14];
  v16 = v6[16];
  v17 = v6[17];
  v2[4] = v6[15];
  v2[5] = v16;
  v2[6] = v17;
  v2[7] = v12;
  v18 = v6[12];
  v19 = v6[13];
  v20 = v6[10];
  *v2 = v6[11];
  v2[1] = v18;
  v2[2] = v19;
  v2[3] = v15;
  v21 = v6[8];
  v22 = v6[9];
  v23 = v6[6];
  v2[18] = v6[7];
  v2[19] = v21;
  v2[20] = v22;
  v2[21] = v20;
  v24 = v6[4];
  v25 = v6[5];
  v26 = v6[2];
  v2[14] = v6[3];
  v2[15] = v24;
  v2[16] = v25;
  v2[17] = v23;
  v27 = *v6;
  v28 = v6[1];
  v2[10] = v14;
  v2[11] = v27;
  v2[12] = v28;
  v2[13] = v26;
  memcpy(v78, v77, sizeof(v78));
  v80 = v61;
  v81 = v60;
  v82 = v58;
  v83 = v57;
  v84 = v56;
  v85 = v54;
  v86 = v52;
  v87 = v49;
  v79 = v59;
  v88 = v55;
  v89 = v53;
  v90 = v51;
  v91 = v50;
  v92 = v48;
  v93 = v47;
  v94 = v46;
  v95 = v45;
  v96 = v44;
  v97 = v42;
  v98 = v43;
  v99 = v3;
  *v0 = v9;
  v100 = v33;
  v101 = v8;
  v0[2] = v10;
  v0[3] = v11;
  v102 = v1;
  v103 = v5;
  v104 = v7;
  v105 = v4;
  v106 = 0;
  v107 = v70;
  v108 = v71;
  v109 = v72;
  v110 = v62;
  v111 = v41;
  v112 = v40;
  v113 = v67;
  v114 = v68;
  v115 = v63;
  v116 = v64;
  v117 = v65;
  v118 = v66;
  if (v69 == 1)
  {
    v29 = 0;
  }
  else
  {
    v73 = v36;
    v74 = v34;
    v75 = v69;
    v76 = v35;
    v29 = &v73;
  }
  v30 = MEMORY[0x1D1794618](v78, v29);
  type metadata accessor for BNNS.PoolingLayer();
  v31 = swift_allocObject();
  *(_QWORD *)(v31 + 24) = 0;
  *(_QWORD *)(v31 + 32) = 0;
  *(_BYTE *)(v31 + 40) = 1;
  _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&v73);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v73, v31 + 48, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  *(_QWORD *)(v31 + 232) = 0;
  if (v30)
  {
    *(_QWORD *)(v31 + 16) = v30;
    *(_QWORD *)(v31 + 24) = v37;
    *(_QWORD *)(v31 + 32) = v38;
    *(_BYTE *)(v31 + 40) = v39;
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v120, v31 + 48, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v31;
}

size_t BNNS.PoolingLayer.apply(batchSize:input:output:)(int64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;

  return specialized static BNNS.poolingFilterApply(_:batchSize:input:output:)(v3, a1, a2, a3);
}

size_t BNNS.PoolingLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingBiasGradient:)(int64_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, _OWORD *a5, uint64_t a6)
{
  uint64_t v6;

  return specialized static BNNS.poolingLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingBiasGradient:)(v6, a1, a2, a3, a4, a5, a6);
}

uint64_t BNNS.PoolingLayer.__allocating_init(bnnsFilter:)(uint64_t a1)
{
  uint64_t v2;
  _BYTE v4[184];

  v2 = swift_allocObject();
  *(_QWORD *)(v2 + 24) = 0;
  *(_QWORD *)(v2 + 32) = 0;
  *(_BYTE *)(v2 + 40) = 1;
  _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)v4);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v4, v2 + 48, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  *(_QWORD *)(v2 + 232) = 0;
  if (a1)
  {
    *(_QWORD *)(v2 + 16) = a1;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v2;
}

uint64_t _sSo21BNNSNDArrayDescriptoraSgWOi_(uint64_t result)
{
  *(_BYTE *)(result + 176) = 0;
  return result;
}

uint64_t _sSo21BNNSNDArrayDescriptoraSgWOg(uint64_t a1)
{
  if (*(_BYTE *)(a1 + 176))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t type metadata accessor for BNNS.PoolingLayer()
{
  return objc_opt_self();
}

double _sSo21BNNSNDArrayDescriptoraSgWOi0_(uint64_t a1)
{
  double result;

  result = 0.0;
  *(_OWORD *)(a1 + 144) = 0u;
  *(_OWORD *)(a1 + 160) = 0u;
  *(_OWORD *)(a1 + 112) = 0u;
  *(_OWORD *)(a1 + 128) = 0u;
  *(_OWORD *)(a1 + 80) = 0u;
  *(_OWORD *)(a1 + 96) = 0u;
  *(_OWORD *)(a1 + 48) = 0u;
  *(_OWORD *)(a1 + 64) = 0u;
  *(_OWORD *)(a1 + 16) = 0u;
  *(_OWORD *)(a1 + 32) = 0u;
  *(_OWORD *)a1 = 0u;
  *(_BYTE *)(a1 + 176) = 1;
  return result;
}

uint64_t BNNS.PoolingLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.PoolingLayer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

size_t closure #1 in closure #1 in closure #1 in static BNNS.poolingLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingBiasGradient:)(BNNSNDArrayDescriptor *a1, uint64_t a2, int64_t a3, uint64_t a4, BNNSNDArrayDescriptor *a5, uint64_t a6, uint64_t a7, BNNSNDArrayDescriptor *a8)
{
  size_t result;
  uint64_t v15;
  size_t v16;
  unint64_t v17;
  unint64_t v18;
  unint64_t v19;
  unint64_t v20;
  unint64_t v21;
  unint64_t v22;
  unint64_t v23;
  unint64_t v24;
  unint64_t v25;
  unint64_t v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  size_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  unint64_t v46;
  unint64_t v47;
  unint64_t v48;
  unint64_t v49;
  size_t v50;
  const size_t *v51;
  const void *v52;
  void *out;
  void *outa;
  const void *v55;
  uint64_t v56;
  size_t v57;
  const void *v58;
  void *filter;
  size_t filtera;
  size_t v61;
  void *v62;
  BNNSNDArrayDescriptor *v63;
  BNNSNDArrayDescriptor *v64;
  size_t v65;
  BNNSNDArrayDescriptor *v66;
  size_t v67;
  size_t v68;
  const void *v70;
  unint64_t v71;
  unint64_t v72;
  unint64_t v73;
  unint64_t v74;
  unint64_t v75;
  unint64_t v76;
  unint64_t v77;
  _QWORD v78[17];
  unint64_t v79;
  unint64_t v80;
  unint64_t v81;
  unint64_t v82;
  unint64_t v83;
  unint64_t v84;
  unint64_t v85;
  unint64_t v86;
  _BYTE v87[136];
  unint64_t v88;
  unint64_t v89;
  unint64_t v90;
  unint64_t v91;
  unint64_t v92;
  unint64_t v93;
  unint64_t v94;
  unint64_t v95;
  _BYTE v96[136];
  unint64_t v97;
  unint64_t v98;
  unint64_t v99;
  unint64_t v100;
  unint64_t v101;
  unint64_t v102;
  unint64_t v103;
  unint64_t v104;
  unint64_t v105;
  unint64_t v106;
  unint64_t v107;
  unint64_t v108;
  unint64_t v109;
  unint64_t v110;
  unint64_t v111;
  unint64_t v112;
  unint64_t v113;
  unint64_t v114;
  unint64_t v115;
  unint64_t v116;
  unint64_t v117;
  unint64_t v118;
  unint64_t v119;
  unint64_t v120;
  _BYTE v121[136];
  _BYTE v122[136];
  _BYTE v123[8];
  _BYTE v124[8];
  _OWORD v125[11];
  const void *v126;
  void *v127;
  _BYTE v128[184];
  _OWORD v129[8];
  __int128 indices;
  __int128 indices_data_type;
  __int128 v132;

  outlined init with take of BNNSNDArrayDescriptor?(a2 + 48, (uint64_t)v128, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v128, (uint64_t)v129, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  result = _sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v129);
  v65 = a3;
  v66 = a5;
  v63 = a1;
  v64 = a8;
  if ((_DWORD)result != 1)
  {
    v125[8] = indices;
    v125[9] = indices_data_type;
    v125[10] = v132;
    v125[4] = v129[4];
    v125[5] = v129[5];
    v125[7] = v129[7];
    v125[6] = v129[6];
    v125[0] = v129[0];
    v125[1] = v129[1];
    v125[3] = v129[3];
    v125[2] = v129[2];
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v121);
    outlined init with take of BNNS.Shape((uint64_t)v121, (uint64_t)v122);
    outlined init with take of BNNS.Shape((uint64_t)v122, (uint64_t)&v113);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v122, (uint64_t)&v113);
    BNNS.Shape.stride.getter();
    v57 = specialized static BNNS.calculateBatchStride(size:stride:)(v105, v106, v107, v108, v109, v110, v111, v112, v105, v106, v107, v108, v109, v110, v111, v112);
    filter = *(void **)(a2 + 16);
    outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v124, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v124, (uint64_t)&v126, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
    v55 = v126;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v105);
    outlined init with take of BNNS.Shape((uint64_t)&v105, (uint64_t)&v113);
    outlined init with take of BNNS.Shape((uint64_t)&v113, (uint64_t)v121);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)&v113, (uint64_t)v121);
    BNNS.Shape.stride.getter();
    v61 = specialized static BNNS.calculateBatchStride(size:stride:)(v97, v98, v99, v100, v101, v102, v103, v104, v97, v98, v99, v100, v101, v102, v103, v104);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v97);
    outlined init with take of BNNS.Shape((uint64_t)&v97, (uint64_t)v121);
    outlined init with take of BNNS.Shape((uint64_t)v121, (uint64_t)v96);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v121, (uint64_t)v96);
    BNNS.Shape.stride.getter();
    v67 = specialized static BNNS.calculateBatchStride(size:stride:)(v88, v89, v90, v91, v92, v93, v94, v95, v88, v89, v90, v91, v92, v93, v94, v95);
    outlined init with take of BNNSNDArrayDescriptor?(a7 + 136, (uint64_t)v123, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v123, (uint64_t)&v127, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
    out = v127;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v87);
    outlined init with take of BNNS.Shape((uint64_t)v87, (uint64_t)&v88);
    outlined init with take of BNNS.Shape((uint64_t)&v88, (uint64_t)v96);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)&v88, (uint64_t)v96);
    BNNS.Shape.stride.getter();
    v16 = specialized static BNNS.calculateBatchStride(size:stride:)(v79, v80, v81, v82, v83, v84, v85, v86, v79, v80, v81, v82, v83, v84, v85, v86);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v79);
    outlined init with take of BNNS.Shape((uint64_t)&v79, (uint64_t)v96);
    outlined init with take of BNNS.Shape((uint64_t)v96, (uint64_t)v78);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v96, (uint64_t)v78);
    BNNS.Shape.stride.getter();
    result = specialized static BNNS.calculateBatchStride(size:stride:)((unint64_t)v70, v71, v72, v73, v74, v75, v76, v77, (unint64_t)v70, v71, v72, v73, v74, v75, v76, v77);
    if (*((_QWORD *)&indices + 1))
      return BNNSPoolingFilterApplyBackwardBatchEx(filter, a3, v55, v61, a5, v67, out, v16, a8, result, a1, (const BNNSDataType)indices_data_type, *((const void **)&indices + 1), v57);
    goto LABEL_16;
  }
  if ((*(_BYTE *)(a2 + 40) & 1) != 0)
    v15 = 1;
  else
    v15 = *(_QWORD *)(a2 + 32);
  if (!a3)
  {
    __break(1u);
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
    return result;
  }
  if (v15 == 0x8000000000000000 && a3 == -1)
    goto LABEL_15;
  filtera = v15 / a3;
  v62 = *(void **)(a2 + 16);
  outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)&v127, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v127, (uint64_t)&v70, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v58 = v70;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v121);
  outlined init with take of BNNS.Shape((uint64_t)v121, (uint64_t)v122);
  outlined init with take of BNNS.Shape((uint64_t)v122, (uint64_t)v125);
  BNNS.Shape.size.getter();
  v17 = v113;
  v18 = v114;
  v19 = v115;
  v20 = v116;
  v21 = v117;
  v22 = v118;
  v56 = a2;
  v23 = v119;
  v24 = v120;
  outlined init with take of BNNS.Shape((uint64_t)v122, (uint64_t)v125);
  BNNS.Shape.stride.getter();
  outa = (void *)specialized static BNNS.calculateBatchStride(size:stride:)(v17, v18, v19, v20, v21, v22, v23, v24, v113, v114, v115, v116, v117, v118, v119, v120);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v113);
  outlined init with take of BNNS.Shape((uint64_t)&v113, (uint64_t)v125);
  outlined init with take of BNNS.Shape((uint64_t)v125, (uint64_t)&v105);
  BNNS.Shape.size.getter();
  v25 = v97;
  v26 = v98;
  v27 = v99;
  v28 = v100;
  v29 = v101;
  v30 = v102;
  v31 = v103;
  v32 = v104;
  outlined init with take of BNNS.Shape((uint64_t)v125, (uint64_t)&v105);
  BNNS.Shape.stride.getter();
  v68 = specialized static BNNS.calculateBatchStride(size:stride:)(v25, v26, v27, v28, v29, v30, v31, v32, v97, v98, v99, v100, v101, v102, v103, v104);
  outlined init with take of BNNSNDArrayDescriptor?(a7 + 136, (uint64_t)&v126, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v126, (uint64_t)v78, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v52 = (const void *)v78[0];
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v96);
  outlined init with take of BNNS.Shape((uint64_t)v96, (uint64_t)&v97);
  outlined init with take of BNNS.Shape((uint64_t)&v97, (uint64_t)&v105);
  BNNS.Shape.size.getter();
  v33 = v88;
  v34 = v89;
  v35 = v90;
  v36 = v91;
  v37 = v92;
  v38 = v93;
  v39 = v94;
  v40 = v95;
  outlined init with take of BNNS.Shape((uint64_t)&v97, (uint64_t)&v105);
  BNNS.Shape.stride.getter();
  v41 = specialized static BNNS.calculateBatchStride(size:stride:)(v33, v34, v35, v36, v37, v38, v39, v40, v88, v89, v90, v91, v92, v93, v94, v95);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v88);
  outlined init with take of BNNS.Shape((uint64_t)&v88, (uint64_t)&v105);
  outlined init with take of BNNS.Shape((uint64_t)&v105, (uint64_t)v87);
  BNNS.Shape.size.getter();
  v42 = v79;
  v43 = v80;
  v44 = v81;
  v45 = v82;
  v46 = v83;
  v47 = v84;
  v49 = v85;
  v48 = v86;
  outlined init with take of BNNS.Shape((uint64_t)&v105, (uint64_t)v87);
  BNNS.Shape.stride.getter();
  v50 = specialized static BNNS.calculateBatchStride(size:stride:)(v42, v43, v44, v45, v46, v47, v49, v48, v79, v80, v81, v82, v83, v84, v85, v86);
  v51 = *(const size_t **)(v56 + 24);
  if (*(_BYTE *)(v56 + 40) & 1 | (v51 == 0))
    v51 = 0;
  return BNNSPoolingFilterApplyBackwardBatch(v62, v65, v58, (size_t)outa, v66, v68, v52, v41, v64, v50, v63, v51, filtera);
}

size_t specialized static BNNS.poolingFilterApply(_:batchSize:input:output:)(uint64_t a1, int64_t a2, uint64_t a3, uint64_t a4)
{
  size_t result;
  uint64_t v9;
  unint64_t v10;
  _BYTE *v11;
  size_t v12;
  size_t *v13;
  void *v14;
  void *in;
  void *ina;
  void *v17;
  int64_t v18;
  uint64_t v19;
  size_t idx_stride;
  const void *v21;
  void *out;
  void *outa;
  unint64_t in_stride;
  unint64_t v25;
  unint64_t v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  unint64_t v46;
  unint64_t v47;
  unint64_t v48;
  unint64_t v49;
  unint64_t v50;
  unint64_t v51;
  unint64_t v52;
  unint64_t v53;
  unint64_t v54;
  unint64_t v55;
  unint64_t v56;
  _BYTE v57[136];
  _BYTE v58[136];
  _BYTE v59[136];
  _BYTE v60[8];
  _BYTE v61[8];
  _BYTE v62[8];
  _BYTE v63[8];
  __int128 v64;
  __int128 v65;
  __int128 v66;
  __int128 v67;
  __int128 v68;
  __int128 v69;
  __int128 v70;
  __int128 v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  const void *v75;
  void *v76;
  void *v77;
  void *v78;
  _BYTE v79[184];
  _OWORD v80[8];
  __int128 v81;
  __int128 indices_data_type;
  __int128 v83;
  int64_t v84;

  outlined init with take of BNNSNDArrayDescriptor?(a1 + 48, (uint64_t)v79, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v79, (uint64_t)v80, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  result = _sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v80);
  v84 = a2;
  if ((_DWORD)result != 1)
  {
    v72 = v81;
    v73 = indices_data_type;
    v74 = v83;
    v68 = v80[4];
    v69 = v80[5];
    v71 = v80[7];
    v70 = v80[6];
    v64 = v80[0];
    v65 = v80[1];
    v67 = v80[3];
    v66 = v80[2];
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v58);
    outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)v59);
    outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v57);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v57);
    BNNS.Shape.stride.getter();
    v10 = specialized static BNNS.calculateBatchStride(size:stride:)(v49, v50, v51, v52, v53, v54, v55, v56, v49, v50, v51, v52, v53, v54, v55, v56);
    v17 = *(void **)(a1 + 16);
    idx_stride = v10;
    outlined init with take of BNNSNDArrayDescriptor?(a3 + 136, (uint64_t)v61, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
    result = outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v61, (uint64_t)&v77, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
    in = v77;
    if (v77)
    {
      BNNSNDArrayDescriptor.shape.getter((uint64_t)v57);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v58);
      outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)&v49);
      BNNS.Shape.size.getter();
      outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)&v49);
      BNNS.Shape.stride.getter();
      in_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v41, v42, v43, v44, v45, v46, v47, v48, v41, v42, v43, v44, v45, v46, v47, v48);
      outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v60, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
      result = outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v60, (uint64_t)&v78, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
      out = v78;
      if (v78)
      {
        BNNSNDArrayDescriptor.shape.getter((uint64_t)&v41);
        outlined init with take of BNNS.Shape((uint64_t)&v41, (uint64_t)&v49);
        outlined init with take of BNNS.Shape((uint64_t)&v49, (uint64_t)&v33);
        BNNS.Shape.size.getter();
        outlined init with take of BNNS.Shape((uint64_t)&v49, (uint64_t)&v33);
        BNNS.Shape.stride.getter();
        result = specialized static BNNS.calculateBatchStride(size:stride:)(v25, v26, v27, v28, v29, v30, v31, v32, v25, v26, v27, v28, v29, v30, v31, v32);
        if (*((_QWORD *)&v81 + 1))
        {
          result = BNNSPoolingFilterApplyBatchEx(v17, v84, in, in_stride, out, result, (const BNNSDataType)indices_data_type, *((void **)&v81 + 1), idx_stride);
          if (!(_DWORD)result)
            return result;
LABEL_8:
          lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
          swift_allocError();
          *v11 = 0;
          return swift_willThrow();
        }
        goto LABEL_24;
      }
LABEL_23:
      __break(1u);
LABEL_24:
      __break(1u);
      goto LABEL_25;
    }
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if ((*(_BYTE *)(a1 + 40) & 1) != 0)
    v9 = 1;
  else
    v9 = *(_QWORD *)(a1 + 32);
  if (!a2)
  {
    __break(1u);
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (v9 == 0x8000000000000000 && a2 == -1)
    goto LABEL_21;
  v18 = v9;
  outa = *(void **)(a1 + 16);
  outlined init with take of BNNSNDArrayDescriptor?(a3 + 136, (uint64_t)v63, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  result = outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v63, (uint64_t)&v75, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v21 = v75;
  if (!v75)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v58);
  outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)v59);
  outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v57);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v57);
  BNNS.Shape.stride.getter();
  ina = (void *)specialized static BNNS.calculateBatchStride(size:stride:)(v49, v50, v51, v52, v53, v54, v55, v56, v49, v50, v51, v52, v53, v54, v55, v56);
  outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v62, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  result = outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v62, (uint64_t)&v76, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v14 = v76;
  if (!v76)
  {
LABEL_26:
    __break(1u);
    return result;
  }
  v19 = v18 / v84;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v49);
  outlined init with take of BNNS.Shape((uint64_t)&v49, (uint64_t)v57);
  outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)&v41);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)&v41);
  BNNS.Shape.stride.getter();
  v12 = specialized static BNNS.calculateBatchStride(size:stride:)(v33, v34, v35, v36, v37, v38, v39, v40, v33, v34, v35, v36, v37, v38, v39, v40);
  if (*(_BYTE *)(a1 + 40))
    v13 = 0;
  else
    v13 = *(size_t **)(a1 + 24);
  result = BNNSPoolingFilterApplyBatch(outa, v84, v21, (size_t)ina, v14, v12, v13, v19);
  if ((_DWORD)result)
    goto LABEL_8;
  return result;
}

size_t specialized static BNNS.poolingLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingBiasGradient:)(uint64_t a1, int64_t a2, uint64_t a3, uint64_t a4, _OWORD *a5, _OWORD *a6, uint64_t a7)
{
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  BNNSNDArrayDescriptor *v22;
  size_t result;
  _BYTE *v24;
  _OWORD v25[11];
  BNNSNDArrayDescriptor v26;
  BNNSNDArrayDescriptor v27;
  _OWORD v28[11];
  uint64_t v29;

  v29 = *MEMORY[0x1E0C80C00];
  v12 = a6[9];
  *(_OWORD *)&v27.stride[7] = a6[8];
  *(_OWORD *)&v27.data_type = v12;
  *(_OWORD *)&v27.table_data_type = a6[10];
  v13 = a6[5];
  *(_OWORD *)&v27.size[7] = a6[4];
  *(_OWORD *)&v27.stride[1] = v13;
  v14 = a6[7];
  *(_OWORD *)&v27.stride[3] = a6[6];
  *(_OWORD *)&v27.stride[5] = v14;
  v15 = a6[1];
  *(_OWORD *)&v27.flags = *a6;
  *(_OWORD *)&v27.size[1] = v15;
  v16 = a6[3];
  *(_OWORD *)&v27.size[3] = a6[2];
  *(_OWORD *)&v27.size[5] = v16;
  v17 = a5[9];
  *(_OWORD *)&v26.stride[7] = a5[8];
  *(_OWORD *)&v26.data_type = v17;
  *(_OWORD *)&v26.table_data_type = a5[10];
  v18 = a5[5];
  *(_OWORD *)&v26.size[7] = a5[4];
  *(_OWORD *)&v26.stride[1] = v18;
  v19 = a5[7];
  *(_OWORD *)&v26.stride[3] = a5[6];
  *(_OWORD *)&v26.stride[5] = v19;
  v20 = a5[1];
  *(_OWORD *)&v26.flags = *a5;
  *(_OWORD *)&v26.size[1] = v20;
  v21 = a5[3];
  *(_OWORD *)&v26.size[3] = a5[2];
  *(_OWORD *)&v26.size[5] = v21;
  outlined init with take of BNNSNDArrayDescriptor?(a7, (uint64_t)v28, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v28) == 1)
  {
    v22 = 0;
  }
  else
  {
    v25[8] = v28[8];
    v25[9] = v28[9];
    v25[10] = v28[10];
    v25[4] = v28[4];
    v25[5] = v28[5];
    v25[6] = v28[6];
    v25[7] = v28[7];
    v25[0] = v28[0];
    v25[1] = v28[1];
    v25[2] = v28[2];
    v25[3] = v28[3];
    v22 = (BNNSNDArrayDescriptor *)v25;
  }
  result = closure #1 in closure #1 in closure #1 in static BNNS.poolingLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingBiasGradient:)(v22, a1, a2, a3, &v27, (uint64_t)a6, a4, &v26);
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v24 = 0;
    return swift_willThrow();
  }
  return result;
}

__n128 __swift_memcpy201_8(uint64_t a1, uint64_t a2)
{
  __int128 v2;
  __int128 v3;
  __int128 v4;
  __int128 v5;
  __int128 v6;
  __int128 v7;
  __n128 result;
  __int128 v9;
  __int128 v10;

  *(_OWORD *)a1 = *(_OWORD *)a2;
  v2 = *(_OWORD *)(a2 + 16);
  v3 = *(_OWORD *)(a2 + 32);
  v4 = *(_OWORD *)(a2 + 64);
  *(_OWORD *)(a1 + 48) = *(_OWORD *)(a2 + 48);
  *(_OWORD *)(a1 + 64) = v4;
  *(_OWORD *)(a1 + 16) = v2;
  *(_OWORD *)(a1 + 32) = v3;
  v5 = *(_OWORD *)(a2 + 80);
  v6 = *(_OWORD *)(a2 + 96);
  v7 = *(_OWORD *)(a2 + 128);
  *(_OWORD *)(a1 + 112) = *(_OWORD *)(a2 + 112);
  *(_OWORD *)(a1 + 128) = v7;
  *(_OWORD *)(a1 + 80) = v5;
  *(_OWORD *)(a1 + 96) = v6;
  result = *(__n128 *)(a2 + 144);
  v9 = *(_OWORD *)(a2 + 160);
  v10 = *(_OWORD *)(a2 + 176);
  *(_OWORD *)(a1 + 185) = *(_OWORD *)(a2 + 185);
  *(_OWORD *)(a1 + 160) = v9;
  *(_OWORD *)(a1 + 176) = v10;
  *(__n128 *)(a1 + 144) = result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.PoolingType(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  int v4;

  if (!a2)
    return 0;
  if (a2 >= 0xFB && *(_BYTE *)(a1 + 201))
    return (*(_DWORD *)a1 + 251);
  v3 = *(unsigned __int8 *)(a1 + 200);
  if (v3 <= 5)
    v4 = -1;
  else
    v4 = v3 ^ 0xFF;
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.PoolingType(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFA)
  {
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_BYTE *)(result + 200) = 0;
    *(_QWORD *)result = a2 - 251;
    if (a3 >= 0xFB)
      *(_BYTE *)(result + 201) = 1;
  }
  else
  {
    if (a3 >= 0xFB)
      *(_BYTE *)(result + 201) = 0;
    if (a2)
      *(_BYTE *)(result + 200) = -(char)a2;
  }
  return result;
}

uint64_t destructiveInjectEnumTag for BNNS.PoolingType(uint64_t result, unsigned int a2)
{
  if (a2 >= 5)
  {
    *(_QWORD *)result = a2 - 5;
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    LOBYTE(a2) = 5;
    *(_OWORD *)(result + 184) = 0u;
  }
  *(_BYTE *)(result + 200) = a2;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.PoolingType()
{
  return &type metadata for BNNS.PoolingType;
}

uint64_t method lookup function for BNNS.PoolingLayer()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of BNNS.PoolingLayer.apply(batchSize:input:output:)(uint64_t a1, uint64_t *a2, uint64_t *a3)
{
  uint64_t v3;
  uint64_t v4;
  int v5;
  uint64_t v6;
  int v7;
  uint64_t v8;
  int v9;
  uint64_t v10;
  int v11;
  uint64_t (*v12)(uint64_t, uint64_t *, uint64_t *);
  uint64_t v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  uint64_t v23;
  int v24;
  uint64_t v25;
  int v26;
  uint64_t v27;
  uint64_t v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  uint64_t v37;
  int v38;
  uint64_t v39;
  int v40;
  uint64_t v41;

  v4 = a2[17];
  v5 = *((_DWORD *)a2 + 36);
  v6 = a2[19];
  v7 = *((_DWORD *)a2 + 40);
  v8 = a3[17];
  v9 = *((_DWORD *)a3 + 36);
  v10 = a3[19];
  v11 = *((_DWORD *)a3 + 40);
  v12 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *))(*(_QWORD *)v3 + 192);
  v28 = *a2;
  v29 = *(_OWORD *)(a2 + 1);
  v30 = *(_OWORD *)(a2 + 3);
  v31 = *(_OWORD *)(a2 + 5);
  v32 = *(_OWORD *)(a2 + 7);
  v33 = *(_OWORD *)(a2 + 9);
  v34 = *(_OWORD *)(a2 + 11);
  v35 = *(_OWORD *)(a2 + 13);
  v36 = *(_OWORD *)(a2 + 15);
  v37 = v4;
  v38 = v5;
  v39 = v6;
  v40 = v7;
  v41 = *(uint64_t *)((char *)a2 + 164);
  v14 = *a3;
  v15 = *(_OWORD *)(a3 + 1);
  v16 = *(_OWORD *)(a3 + 3);
  v17 = *(_OWORD *)(a3 + 5);
  v18 = *(_OWORD *)(a3 + 7);
  v19 = *(_OWORD *)(a3 + 9);
  v20 = *(_OWORD *)(a3 + 11);
  v21 = *(_OWORD *)(a3 + 13);
  v22 = *(_OWORD *)(a3 + 15);
  v23 = v8;
  v24 = v9;
  v25 = v10;
  v26 = v11;
  v27 = *(uint64_t *)((char *)a3 + 164);
  return v12(a1, &v28, &v14);
}

uint64_t dispatch thunk of BNNS.PoolingLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingBiasGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5, uint64_t a6)
{
  uint64_t v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  uint64_t v35;
  int v36;
  uint64_t v37;
  int v38;
  uint64_t v39;
  int v40;
  uint64_t v41;
  int v42;
  uint64_t v43;
  int v44;
  uint64_t v45;
  int v46;
  uint64_t v47;
  int v48;
  uint64_t v49;
  int v50;
  char v51;
  uint64_t (*v52)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, _OWORD *);
  _OWORD v54[11];
  char v55;
  uint64_t v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  __int128 v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  uint64_t v65;
  int v66;
  uint64_t v67;
  int v68;
  uint64_t v69;
  uint64_t v70;
  __int128 v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  __int128 v76;
  __int128 v77;
  __int128 v78;
  uint64_t v79;
  int v80;
  uint64_t v81;
  int v82;
  uint64_t v83;
  uint64_t v84;
  __int128 v85;
  __int128 v86;
  __int128 v87;
  __int128 v88;
  __int128 v89;
  __int128 v90;
  __int128 v91;
  __int128 v92;
  uint64_t v93;
  int v94;
  uint64_t v95;
  int v96;
  uint64_t v97;
  uint64_t v98;
  __int128 v99;
  __int128 v100;
  __int128 v101;
  __int128 v102;
  __int128 v103;
  __int128 v104;
  __int128 v105;
  __int128 v106;
  uint64_t v107;
  int v108;
  uint64_t v109;
  int v110;
  uint64_t v111;

  v98 = *a2;
  v99 = *(_OWORD *)(a2 + 1);
  v100 = *(_OWORD *)(a2 + 3);
  v101 = *(_OWORD *)(a2 + 5);
  v102 = *(_OWORD *)(a2 + 7);
  v103 = *(_OWORD *)(a2 + 9);
  v104 = *(_OWORD *)(a2 + 11);
  v105 = *(_OWORD *)(a2 + 13);
  v106 = *(_OWORD *)(a2 + 15);
  v111 = *(uint64_t *)((char *)a2 + 164);
  v84 = *a3;
  v7 = *(_OWORD *)(a3 + 3);
  v85 = *(_OWORD *)(a3 + 1);
  v8 = *(_OWORD *)(a3 + 5);
  v86 = v7;
  v9 = *(_OWORD *)(a3 + 7);
  v87 = v8;
  v10 = *(_OWORD *)(a3 + 9);
  v88 = v9;
  v11 = *(_OWORD *)(a3 + 11);
  v89 = v10;
  v12 = *(_OWORD *)(a3 + 13);
  v90 = v11;
  v13 = *(_OWORD *)(a3 + 15);
  v91 = v12;
  v14 = *(_OWORD *)(a4 + 1);
  v92 = v13;
  v97 = *(uint64_t *)((char *)a3 + 164);
  v70 = *a4;
  v15 = *(_OWORD *)(a4 + 3);
  v71 = v14;
  v16 = *(_OWORD *)(a4 + 5);
  v72 = v15;
  v17 = *(_OWORD *)(a4 + 7);
  v73 = v16;
  v18 = *(_OWORD *)(a4 + 9);
  v74 = v17;
  v19 = *(_OWORD *)(a4 + 11);
  v75 = v18;
  v20 = *(_OWORD *)(a4 + 13);
  v76 = v19;
  v21 = *(_OWORD *)(a4 + 15);
  v77 = v20;
  v22 = *(_OWORD *)(a5 + 1);
  v78 = v21;
  v83 = *(uint64_t *)((char *)a4 + 164);
  v56 = *a5;
  v23 = *(_OWORD *)(a5 + 3);
  v57 = v22;
  v24 = *(_OWORD *)(a5 + 5);
  v58 = v23;
  v25 = *(_OWORD *)(a5 + 7);
  v59 = v24;
  v26 = *(_OWORD *)(a5 + 9);
  v60 = v25;
  v27 = *(_OWORD *)(a5 + 11);
  v61 = v26;
  v28 = *(_OWORD *)(a5 + 13);
  v62 = v27;
  v29 = *(_OWORD *)(a5 + 15);
  v63 = v28;
  *(_QWORD *)&v28 = *(uint64_t *)((char *)a5 + 164);
  v64 = v29;
  v69 = v28;
  v30 = *(_OWORD *)(a6 + 16);
  v54[0] = *(_OWORD *)a6;
  v54[1] = v30;
  v31 = *(_OWORD *)(a6 + 48);
  v54[2] = *(_OWORD *)(a6 + 32);
  v54[3] = v31;
  v32 = *(_OWORD *)(a6 + 80);
  v54[4] = *(_OWORD *)(a6 + 64);
  v54[5] = v32;
  v33 = *(_OWORD *)(a6 + 112);
  v54[6] = *(_OWORD *)(a6 + 96);
  v54[7] = v33;
  v34 = *(_OWORD *)(a6 + 144);
  v54[8] = *(_OWORD *)(a6 + 128);
  v54[9] = v34;
  v54[10] = *(_OWORD *)(a6 + 160);
  v35 = a2[17];
  v36 = *((_DWORD *)a2 + 36);
  v37 = a2[19];
  v38 = *((_DWORD *)a2 + 40);
  v39 = a3[17];
  v40 = *((_DWORD *)a3 + 36);
  v41 = a3[19];
  v42 = *((_DWORD *)a3 + 40);
  v43 = a4[17];
  v44 = *((_DWORD *)a4 + 36);
  v45 = a4[19];
  v46 = *((_DWORD *)a4 + 40);
  v47 = a5[17];
  v48 = *((_DWORD *)a5 + 36);
  v49 = a5[19];
  v50 = *((_DWORD *)a5 + 40);
  v51 = *(_BYTE *)(a6 + 176);
  v52 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, _OWORD *))(*(_QWORD *)v6 + 200);
  v107 = v35;
  v108 = v36;
  v109 = v37;
  v110 = v38;
  v93 = v39;
  v94 = v40;
  v95 = v41;
  v96 = v42;
  v79 = v43;
  v80 = v44;
  v81 = v45;
  v82 = v46;
  v65 = v47;
  v66 = v48;
  v67 = v49;
  v68 = v50;
  v55 = v51;
  return v52(a1, &v98, &v84, &v70, &v56, v54);
}

uint64_t outlined init with take of BNNSNDArrayDescriptor?(uint64_t a1, uint64_t a2, uint64_t *a3)
{
  uint64_t v5;

  v5 = __swift_instantiateConcreteTypeFromMangledName(a3);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(_QWORD *)(v5 - 8) + 32))(a2, a1, v5);
  return a2;
}

uint64_t BNNS.ConvolutionLayer.__allocating_init(type:input:weights:output:bias:padding:activation:groupCount:stride:dilationStride:filterParameters:)(char *a1, _OWORD *a2, __int128 *a3, _OWORD *a4, uint64_t a5, uint64_t a6, uint64_t *a7, size_t a8, size_t a9, size_t a10, size_t a11, size_t a12, uint32_t a13, size_t a14, int (__cdecl *a15)(void **, size_t, size_t), void (__cdecl *a16)(void *))
{
  uint64_t v23;
  char v24;
  void *v25;
  void *v26;
  BNNSDataType v27;
  uint64_t v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  BNNSFilterParameters *p_filter_params;
  void *v50;
  BNNSFilterParameters *v51;
  int v53;
  uint64_t v54;
  size_t v55;
  size_t v56;
  size_t v57;
  size_t v58;
  size_t v59;
  size_t v60;
  size_t v61;
  size_t v62;
  size_t v63;
  size_t v64;
  size_t v65;
  size_t v66;
  size_t v67;
  uint64_t v68;
  size_t v69;
  size_t v70;
  size_t v71;
  char v72;
  size_t v73;
  size_t v74;
  size_t v75;
  size_t v76;
  size_t v77;
  size_t v78;
  uint64_t v80;
  BNNSFilterParameters filter_params;
  _OWORD __src[33];
  BNNSLayerParametersConvolution __dst;
  BNNSActivation v84;
  _BYTE v85[184];
  _BYTE v86[184];
  _BYTE v87[184];
  uint64_t v88;

  v88 = *MEMORY[0x1E0C80C00];
  outlined init with take of BNNSNDArrayDescriptor?(a5, (uint64_t)v85);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v85, (uint64_t)v87);
  if (*(_BYTE *)(a6 + 32) == 1)
  {
    v75 = *(_QWORD *)(a6 + 16);
    v76 = *(_QWORD *)(a6 + 24);
    v74 = *(_QWORD *)(a6 + 8);
    v73 = *(_QWORD *)a6;
    v77 = 0;
    v78 = 0;
  }
  else
  {
    v77 = *(_QWORD *)(a6 + 8);
    v78 = *(_QWORD *)a6;
    v75 = 0;
    v76 = 0;
    v74 = 0;
    v73 = 0;
  }
  v72 = *a1;
  v23 = *a7;
  v24 = *((_BYTE *)a7 + 8);
  outlined init with take of BNNSNDArrayDescriptor?(a5, (uint64_t)v86);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v86) == 1)
  {
    v70 = 0;
    v71 = 0;
    v69 = 0;
    v66 = 0;
    v67 = 0;
    v64 = 0;
    v65 = 0;
    v62 = 0;
    v63 = 0;
    v60 = 0;
    v61 = 0;
    v58 = 0;
    v59 = 0;
    v56 = 0;
    v57 = 0;
    v55 = 0;
    v25 = 0;
    v26 = 0;
    v27 = 0;
    v68 = 0;
    v28 = 0;
    v54 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v87, (uint64_t)__src);
    v68 = *(_QWORD *)&__src[0];
    v70 = *(_QWORD *)&__src[1];
    v71 = *((_QWORD *)&__src[0] + 1);
    v69 = *((_QWORD *)&__src[1] + 1);
    v66 = *(_QWORD *)&__src[2];
    v67 = *(_QWORD *)&__src[3];
    v64 = *((_QWORD *)&__src[2] + 1);
    v65 = *((_QWORD *)&__src[3] + 1);
    v62 = *((_QWORD *)&__src[4] + 1);
    v63 = *(_QWORD *)&__src[4];
    v60 = *((_QWORD *)&__src[5] + 1);
    v61 = *(_QWORD *)&__src[5];
    v58 = *((_QWORD *)&__src[6] + 1);
    v59 = *(_QWORD *)&__src[6];
    v56 = *((_QWORD *)&__src[7] + 1);
    v57 = *(_QWORD *)&__src[7];
    v25 = (void *)*((_QWORD *)&__src[8] + 1);
    v55 = *(_QWORD *)&__src[8];
    v26 = (void *)*((_QWORD *)&__src[9] + 1);
    v54 = *(_QWORD *)&__src[9];
    v27 = __src[10];
    v28 = *(_QWORD *)((char *)&__src[10] + 4);
    v53 = HIDWORD(__src[10]);
  }
  *(_QWORD *)&__src[0] = v23;
  BYTE8(__src[0]) = v24;
  BNNS.ActivationFunction.bnnsActivation.getter();
  v29 = a2[9];
  __src[8] = a2[8];
  __src[9] = v29;
  v30 = a2[5];
  __src[4] = a2[4];
  __src[5] = v30;
  v31 = a2[7];
  __src[6] = a2[6];
  __src[7] = v31;
  v32 = a2[1];
  __src[0] = *a2;
  __src[1] = v32;
  v33 = a2[3];
  __src[2] = a2[2];
  __src[3] = v33;
  v34 = a3[8];
  v35 = a3[9];
  v36 = a3[6];
  __src[18] = a3[7];
  __src[19] = v34;
  v37 = a3[10];
  __src[20] = v35;
  __src[21] = v37;
  v38 = a3[4];
  v39 = a3[5];
  v40 = a3[2];
  __src[14] = a3[3];
  __src[15] = v38;
  v41 = a2[10];
  __src[16] = v39;
  __src[17] = v36;
  v42 = *a3;
  v43 = a3[1];
  __src[10] = v41;
  __src[11] = v42;
  __src[12] = v43;
  __src[13] = v40;
  v44 = a4[9];
  __src[30] = a4[8];
  __src[31] = v44;
  __src[32] = a4[10];
  v45 = a4[5];
  __src[26] = a4[4];
  __src[27] = v45;
  v46 = a4[7];
  __src[28] = a4[6];
  __src[29] = v46;
  v47 = a4[1];
  __src[22] = *a4;
  __src[23] = v47;
  v48 = a4[3];
  __src[24] = a4[2];
  __src[25] = v48;
  memcpy(&__dst, __src, 0x210uLL);
  __dst.bias.size[0] = v71;
  __dst.bias.size[1] = v70;
  __dst.bias.size[2] = v69;
  __dst.bias.size[3] = v66;
  __dst.bias.size[4] = v64;
  *(_QWORD *)&__dst.bias.flags = v68;
  __dst.bias.size[5] = v67;
  __dst.bias.size[6] = v65;
  __dst.bias.size[7] = v63;
  __dst.bias.stride[0] = v62;
  __dst.bias.stride[1] = v61;
  __dst.bias.stride[2] = v60;
  __dst.bias.stride[3] = v59;
  __dst.bias.stride[4] = v58;
  __dst.bias.stride[5] = v57;
  __dst.bias.stride[6] = v56;
  __dst.bias.stride[7] = v55;
  __dst.bias.data = v25;
  *(_QWORD *)&__dst.bias.data_type = v54;
  __dst.bias.table_data = v26;
  __dst.bias.table_data_type = v27;
  *(_QWORD *)&__dst.bias.data_scale = v28;
  *((_DWORD *)&__dst.bias.data_bias + 1) = v53;
  __dst.activation = v84;
  __dst.x_stride = a9;
  __dst.y_stride = a10;
  __dst.x_dilation_stride = a11;
  __dst.y_dilation_stride = a12;
  __dst.x_padding = v78;
  __dst.y_padding = v77;
  __dst.groups = a8;
  __dst.pad[0] = v73;
  __dst.pad[1] = v74;
  __dst.pad[2] = v75;
  __dst.pad[3] = v76;
  if (a15 != (int (__cdecl *)(void **, size_t, size_t))1)
  {
    filter_params.flags = a13;
    filter_params.n_threads = a14;
    filter_params.alloc_memory = a15;
    filter_params.free_memory = a16;
    if ((v72 & 1) != 0)
    {
      p_filter_params = &filter_params;
      goto LABEL_12;
    }
    v51 = &filter_params;
LABEL_15:
    v50 = (void *)MEMORY[0x1D17945AC](&__dst, v51);
    return (*(uint64_t (**)(void *))(v80 + 88))(v50);
  }
  if ((v72 & 1) == 0)
  {
    v51 = 0;
    goto LABEL_15;
  }
  p_filter_params = 0;
LABEL_12:
  v50 = BNNSFilterCreateLayerTransposedConvolution(&__dst, p_filter_params);
  return (*(uint64_t (**)(void *))(v80 + 88))(v50);
}

uint64_t outlined init with take of BNNSNDArrayDescriptor?(uint64_t a1, uint64_t a2)
{
  uint64_t v4;

  v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(_QWORD *)(v4 - 8) + 32))(a2, a1, v4);
  return a2;
}

uint64_t BNNS.ConvolutionLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(size_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, _OWORD *a5, uint64_t a6, uint64_t a7)
{
  uint64_t v7;

  return specialized static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(v7, a1, a2, a3, a4, a5, a6, a7);
}

uint64_t BNNS.ConvolutionLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.ConvolutionLayer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

double static BNNS.ConvolutionPadding.zero.getter@<D0>(uint64_t a1@<X8>)
{
  double result;

  *(_BYTE *)(a1 + 32) = 0;
  result = 0.0;
  *(_OWORD *)a1 = 0u;
  *(_OWORD *)(a1 + 16) = 0u;
  return result;
}

BOOL static BNNS.ConvolutionType.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void BNNS.ConvolutionType.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

Swift::Int BNNS.ConvolutionType.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance BNNS.ConvolutionType(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

unint64_t lazy protocol witness table accessor for type BNNS.ConvolutionType and conformance BNNS.ConvolutionType()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNS.ConvolutionType and conformance BNNS.ConvolutionType;
  if (!lazy protocol witness table cache variable for type BNNS.ConvolutionType and conformance BNNS.ConvolutionType)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNS.ConvolutionType, &type metadata for BNNS.ConvolutionType);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.ConvolutionType and conformance BNNS.ConvolutionType);
  }
  return result;
}

uint64_t type metadata accessor for BNNS.ConvolutionLayer()
{
  return objc_opt_self();
}

uint64_t method lookup function for BNNS.ConvolutionLayer()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of BNNS.ConvolutionLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5, uint64_t a6, uint64_t a7)
{
  uint64_t v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  _OWORD v43[11];
  char v44;
  _OWORD v45[11];
  char v46;
  uint64_t v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  uint64_t v56;
  int v57;
  uint64_t v58;
  int v59;
  uint64_t v60;
  uint64_t v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  __int128 v65;
  __int128 v66;
  __int128 v67;
  __int128 v68;
  __int128 v69;
  uint64_t v70;
  int v71;
  uint64_t v72;
  int v73;
  uint64_t v74;
  uint64_t v75;
  __int128 v76;
  __int128 v77;
  __int128 v78;
  __int128 v79;
  __int128 v80;
  __int128 v81;
  __int128 v82;
  __int128 v83;
  uint64_t v84;
  int v85;
  uint64_t v86;
  int v87;
  uint64_t v88;
  uint64_t v89;
  __int128 v90;
  __int128 v91;
  __int128 v92;
  __int128 v93;
  __int128 v94;
  __int128 v95;
  __int128 v96;
  __int128 v97;
  uint64_t v98;
  int v99;
  uint64_t v100;
  int v101;
  uint64_t v102;

  v8 = *(_OWORD *)(a2 + 13);
  v95 = *(_OWORD *)(a2 + 11);
  v9 = *(_OWORD *)(a2 + 15);
  v96 = v8;
  v97 = v9;
  v102 = *(uint64_t *)((char *)a2 + 164);
  v90 = *(_OWORD *)(a2 + 1);
  v91 = *(_OWORD *)(a2 + 3);
  v92 = *(_OWORD *)(a2 + 5);
  v93 = *(_OWORD *)(a2 + 7);
  v94 = *(_OWORD *)(a2 + 9);
  v10 = *(_OWORD *)(a3 + 13);
  v81 = *(_OWORD *)(a3 + 11);
  v11 = *(_OWORD *)(a3 + 15);
  v82 = v10;
  *(_QWORD *)&v10 = *(uint64_t *)((char *)a3 + 164);
  v83 = v11;
  v88 = v10;
  v12 = *(_OWORD *)(a3 + 3);
  v76 = *(_OWORD *)(a3 + 1);
  v13 = *(_OWORD *)(a3 + 5);
  v77 = v12;
  v14 = *(_OWORD *)(a3 + 7);
  v78 = v13;
  v15 = *(_OWORD *)(a3 + 9);
  v79 = v14;
  v16 = *(_OWORD *)(a4 + 11);
  v80 = v15;
  v17 = *(_OWORD *)(a4 + 13);
  v67 = v16;
  v18 = *(_OWORD *)(a4 + 15);
  v68 = v17;
  *(_QWORD *)&v17 = *(uint64_t *)((char *)a4 + 164);
  v69 = v18;
  v74 = v17;
  v89 = *a2;
  v19 = *(_OWORD *)(a4 + 1);
  v75 = *a3;
  v61 = *a4;
  v20 = *(_OWORD *)(a4 + 3);
  v62 = v19;
  v21 = *(_OWORD *)(a4 + 5);
  v63 = v20;
  v22 = *(_OWORD *)(a4 + 7);
  v64 = v21;
  v23 = *(_OWORD *)(a4 + 9);
  v65 = v22;
  v24 = *(_OWORD *)(a5 + 1);
  v66 = v23;
  v47 = *a5;
  v25 = *(_OWORD *)(a5 + 3);
  v48 = v24;
  v26 = *(_OWORD *)(a5 + 5);
  v49 = v25;
  v27 = *(_OWORD *)(a5 + 7);
  v50 = v26;
  v28 = *(_OWORD *)(a5 + 9);
  v51 = v27;
  v29 = *(_OWORD *)(a5 + 11);
  v52 = v28;
  v30 = *(_OWORD *)(a5 + 13);
  v53 = v29;
  v31 = *(_OWORD *)(a5 + 15);
  v54 = v30;
  *(_QWORD *)&v30 = *(uint64_t *)((char *)a5 + 164);
  v55 = v31;
  v60 = v30;
  v32 = *(_OWORD *)(a6 + 16);
  v45[0] = *(_OWORD *)a6;
  v45[1] = v32;
  v33 = *(_OWORD *)(a6 + 48);
  v45[2] = *(_OWORD *)(a6 + 32);
  v45[3] = v33;
  v34 = *(_OWORD *)(a6 + 80);
  v45[4] = *(_OWORD *)(a6 + 64);
  v45[5] = v34;
  v35 = *(_OWORD *)(a6 + 112);
  v45[6] = *(_OWORD *)(a6 + 96);
  v45[7] = v35;
  v36 = *(_OWORD *)(a6 + 144);
  v45[8] = *(_OWORD *)(a6 + 128);
  v45[9] = v36;
  v45[10] = *(_OWORD *)(a6 + 160);
  v37 = *(_OWORD *)(a7 + 16);
  v43[0] = *(_OWORD *)a7;
  v43[1] = v37;
  v38 = *(_OWORD *)(a7 + 48);
  v43[2] = *(_OWORD *)(a7 + 32);
  v43[3] = v38;
  v39 = *(_OWORD *)(a7 + 80);
  v43[4] = *(_OWORD *)(a7 + 64);
  v43[5] = v39;
  v40 = *(_OWORD *)(a7 + 112);
  v43[6] = *(_OWORD *)(a7 + 96);
  v43[7] = v40;
  v41 = *(_OWORD *)(a7 + 144);
  v43[8] = *(_OWORD *)(a7 + 128);
  v43[9] = v41;
  v43[10] = *(_OWORD *)(a7 + 160);
  v98 = a2[17];
  v99 = *((_DWORD *)a2 + 36);
  v100 = a2[19];
  v101 = *((_DWORD *)a2 + 40);
  v84 = a3[17];
  v85 = *((_DWORD *)a3 + 36);
  v86 = a3[19];
  v87 = *((_DWORD *)a3 + 40);
  v70 = a4[17];
  v71 = *((_DWORD *)a4 + 36);
  v72 = a4[19];
  v73 = *((_DWORD *)a4 + 40);
  v56 = a5[17];
  v57 = *((_DWORD *)a5 + 36);
  v58 = a5[19];
  v59 = *((_DWORD *)a5 + 40);
  v46 = *(_BYTE *)(a6 + 176);
  v44 = *(_BYTE *)(a7 + 176);
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, _OWORD *, _OWORD *))(*(_QWORD *)v7 + 112))(a1, &v89, &v75, &v61, &v47, v45, v43);
}

__n128 __swift_memcpy33_8(uint64_t a1, uint64_t a2)
{
  __n128 result;
  __int128 v3;

  result = *(__n128 *)a2;
  v3 = *(_OWORD *)(a2 + 16);
  *(_BYTE *)(a1 + 32) = *(_BYTE *)(a2 + 32);
  *(__n128 *)a1 = result;
  *(_OWORD *)(a1 + 16) = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.ConvolutionPadding(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  int v4;

  if (!a2)
    return 0;
  if (a2 >= 0xFF && *(_BYTE *)(a1 + 33))
    return (*(_DWORD *)a1 + 255);
  v3 = *(unsigned __int8 *)(a1 + 32);
  if (v3 <= 1)
    v4 = -1;
  else
    v4 = v3 ^ 0xFF;
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.ConvolutionPadding(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(_QWORD *)(result + 16) = 0;
    *(_QWORD *)(result + 24) = 0;
    *(_BYTE *)(result + 32) = 0;
    *(_QWORD *)result = a2 - 255;
    *(_QWORD *)(result + 8) = 0;
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 33) = 1;
  }
  else
  {
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 33) = 0;
    if (a2)
      *(_BYTE *)(result + 32) = -(char)a2;
  }
  return result;
}

uint64_t getEnumTag for BNNS.ConvolutionPadding(uint64_t a1)
{
  return *(unsigned __int8 *)(a1 + 32);
}

uint64_t destructiveInjectEnumTag for BNNS.ConvolutionPadding(uint64_t result, char a2)
{
  *(_BYTE *)(result + 32) = a2 & 1;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.ConvolutionPadding()
{
  return &type metadata for BNNS.ConvolutionPadding;
}

uint64_t getEnumTagSinglePayload for BNNS.ConvolutionType(unsigned __int8 *a1, unsigned int a2)
{
  int v2;
  int v3;
  int v4;
  unsigned int v6;
  BOOL v7;
  int v8;

  if (!a2)
    return 0;
  if (a2 < 0xFF)
    goto LABEL_17;
  if (a2 + 1 >= 0xFFFF00)
    v2 = 4;
  else
    v2 = 2;
  if ((a2 + 1) >> 8 < 0xFF)
    v3 = 1;
  else
    v3 = v2;
  if (v3 == 4)
  {
    v4 = *(_DWORD *)(a1 + 1);
    if (v4)
      return (*a1 | (v4 << 8)) - 1;
  }
  else
  {
    if (v3 == 2)
    {
      v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1))
        goto LABEL_17;
      return (*a1 | (v4 << 8)) - 1;
    }
    v4 = a1[1];
    if (a1[1])
      return (*a1 | (v4 << 8)) - 1;
  }
LABEL_17:
  v6 = *a1;
  v7 = v6 >= 2;
  v8 = v6 - 2;
  if (!v7)
    v8 = -1;
  return (v8 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.ConvolutionType(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 1 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 1) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFF)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFE)
    return ((uint64_t (*)(void))((char *)&loc_1CAADD590 + 4 * byte_1CAB5F725[v4]))();
  *a1 = a2 + 1;
  return ((uint64_t (*)(void))((char *)sub_1CAADD5C4 + 4 * byte_1CAB5F720[v4]))();
}

uint64_t sub_1CAADD5C4(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAADD5CC(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAADD5D4);
  return result;
}

uint64_t sub_1CAADD5E0(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAADD5E8);
  *(_BYTE *)result = a2 + 1;
  return result;
}

uint64_t sub_1CAADD5EC(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAADD5F4(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

_BYTE *destructiveInjectEnumTag for BNNS.ConvolutionType(_BYTE *result, char a2)
{
  *result = a2 & 1;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.ConvolutionType()
{
  return &type metadata for BNNS.ConvolutionType;
}

uint64_t static vDSP.convert(splitComplexVector:toInterleavedComplexVector:)(uint64_t a1, uint64_t a2, uint64_t *a3)
{
  return static vDSP.convert(splitComplexVector:toInterleavedComplexVector:)(a1, a2, a3, (uint64_t (*)(_QWORD, uint64_t, _QWORD, uint64_t))specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:), (uint64_t (*)(_QWORD *, uint64_t, uint64_t, uint64_t, uint64_t))MEMORY[0x1E0C8CA50]);
}

{
  return static vDSP.convert(splitComplexVector:toInterleavedComplexVector:)(a1, a2, a3, (uint64_t (*)(_QWORD, uint64_t, _QWORD, uint64_t))specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:), (uint64_t (*)(_QWORD *, uint64_t, uint64_t, uint64_t, uint64_t))MEMORY[0x1E0C8CA58]);
}

void static vDSP.convert(interleavedComplexVector:toSplitComplexVector:)(uint64_t a1, DSPSplitComplex *__Z)
{
  vDSP_ctoz((const DSPComplex *)(a1 + 32), 2, __Z, 1, *(_QWORD *)(a1 + 16));
}

uint64_t static vDSP.convert(splitComplexVector:toInterleavedComplexVector:)(uint64_t a1, uint64_t a2, uint64_t *a3, uint64_t (*a4)(_QWORD, uint64_t, _QWORD, uint64_t), uint64_t (*a5)(_QWORD *, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v8;
  uint64_t v9;
  _QWORD v11[3];

  v11[2] = *MEMORY[0x1E0C80C00];
  v11[0] = a1;
  v11[1] = a2;
  v8 = *a3;
  v9 = *(_QWORD *)(*a3 + 16);
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    v8 = a4(0, v9, 0, v8);
  *a3 = v8;
  return a5(v11, 1, v8 + 32, 2, v9);
}

void static vDSP.convert(interleavedComplexVector:toSplitComplexVector:)(uint64_t a1, DSPDoubleSplitComplex *__Z)
{
  vDSP_ctozD((const DSPDoubleComplex *)(a1 + 32), 2, __Z, 1, *(_QWORD *)(a1 + 16));
}

BOOL static BNNS.LearningPhase.== infix(_:_:)(int a1, int a2)
{
  return ((a2 ^ a1) & 1) == 0;
}

void BNNS.LearningPhase.hash(into:)(uint64_t a1, char a2)
{
  Hasher._combine(_:)(a2 & 1);
}

Swift::Int BNNS.LearningPhase.hashValue.getter(char a1)
{
  Hasher.init(_seed:)();
  Hasher._combine(_:)(a1 & 1);
  return Hasher._finalize()();
}

uint64_t BNNS.FusedLayer.apply(batchSize:input:output:for:)(size_t a1, uint64_t a2, uint64_t a3, char a4)
{
  uint64_t v4;

  return specialized static BNNS.fusedLayerApply(_:batchSize:input:output:for:)(v4, a1, a2, a3, a4 & 1);
}

uint64_t BNNS.FusedLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingParameterGradients:)(size_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, __int128 *a5, uint64_t a6)
{
  uint64_t v6;

  return specialized static BNNS.fusedLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingParameterGradients:)(v6, a1, a2, a3, a4, a5, a6);
}

uint64_t BNNS.FusedLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t closure #1 in closure #2 in static BNNS.fusedLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingParameterGradients:)@<X0>(BNNSNDArrayDescriptor *a1@<X0>, uint64_t a2@<X1>, size_t a3@<X2>, uint64_t a4@<X3>, BNNSNDArrayDescriptor *a5@<X4>, uint64_t a6@<X6>, _DWORD *a7@<X8>, char **a8)
{
  size_t v8;
  char *v9;
  char isUniquelyReferenced_nonNull_native;
  uint64_t result;
  unint64_t out_stride;
  const void *v14;
  size_t v15;
  unint64_t in_stride;
  void *in;
  void *v18;
  unint64_t v23;
  unint64_t v24;
  unint64_t v25;
  unint64_t v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  _BYTE v31[136];
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  _BYTE v40[136];
  unint64_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  unint64_t v46;
  unint64_t v47;
  unint64_t v48;
  _BYTE v49[136];
  unint64_t v50;
  unint64_t v51;
  unint64_t v52;
  unint64_t v53;
  unint64_t v54;
  unint64_t v55;
  unint64_t v56;
  unint64_t v57;
  _BYTE v58[136];
  _BYTE v59[136];
  _BYTE v60[136];
  _BYTE v61[8];
  _BYTE v62[8];
  void *v63;
  const void *v64;

  v18 = *(void **)(a2 + 16);
  outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v62, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v62, (uint64_t)&v63, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  in = v63;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v58);
  outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)v59);
  outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v60);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v60);
  BNNS.Shape.stride.getter();
  in_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v50, v51, v52, v53, v54, v55, v56, v57, v50, v51, v52, v53, v54, v55, v56, v57);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v50);
  outlined init with take of BNNS.Shape((uint64_t)&v50, (uint64_t)v60);
  outlined init with take of BNNS.Shape((uint64_t)v60, (uint64_t)v49);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v60, (uint64_t)v49);
  BNNS.Shape.stride.getter();
  v15 = specialized static BNNS.calculateBatchStride(size:stride:)(v41, v42, v43, v44, v45, v46, v47, v48, v41, v42, v43, v44, v45, v46, v47, v48);
  outlined init with take of BNNSNDArrayDescriptor?(a6 + 136, (uint64_t)v61, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v61, (uint64_t)&v64, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v14 = v64;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v40);
  outlined init with take of BNNS.Shape((uint64_t)v40, (uint64_t)&v41);
  outlined init with take of BNNS.Shape((uint64_t)&v41, (uint64_t)v49);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)&v41, (uint64_t)v49);
  BNNS.Shape.stride.getter();
  out_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v32, v33, v34, v35, v36, v37, v38, v39, v32, v33, v34, v35, v36, v37, v38, v39);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v32);
  outlined init with take of BNNS.Shape((uint64_t)&v32, (uint64_t)v49);
  outlined init with take of BNNS.Shape((uint64_t)v49, (uint64_t)v31);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v49, (uint64_t)v31);
  BNNS.Shape.stride.getter();
  v8 = specialized static BNNS.calculateBatchStride(size:stride:)(v23, v24, v25, v26, v27, v28, v29, v30, v23, v24, v25, v26, v27, v28, v29, v30);
  v9 = *a8;
  isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
  *a8 = v9;
  if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    v9 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v9 + 2), 0, v9);
  *a8 = v9;
  result = BNNSFusedFilterApplyBackwardBatch(v18, a3, in, in_stride, a5, v15, v14, out_stride, a1, v8, (BNNSNDArrayDescriptor **)v9 + 4);
  *a7 = result;
  return result;
}

void BNNS.FusedConvolutionNormalizationLayer.__allocating_init(input:output:convolutionWeights:convolutionBias:convolutionStride:convolutionDilationStride:convolutionPadding:normalization:normalizationBeta:normalizationGamma:normalizationMomentum:normalizationEpsilon:normalizationActivation:filterParameters:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, const void *a10, __int128 *a11, _OWORD *a12)
{
  _OWORD *v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  _OWORD *v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 *v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  char *v50;
  _BYTE v51[368];
  _OWORD v52[124];
  uint64_t v53;

  v12 = (_OWORD *)MEMORY[0x1E0C80A78](a1);
  v53 = *MEMORY[0x1E0C80C00];
  v13 = a12[9];
  v52[77] = a12[8];
  v52[78] = v13;
  v14 = a12[10];
  v15 = a12[5];
  v52[73] = a12[4];
  v52[74] = v15;
  v16 = a12[7];
  v52[75] = a12[6];
  v52[76] = v16;
  v17 = a12[1];
  v52[69] = *a12;
  v52[70] = v17;
  v18 = a12[3];
  v52[71] = a12[2];
  v52[72] = v18;
  v19 = a11[8];
  v20 = a11[9];
  v21 = a11[6];
  v52[87] = a11[7];
  v52[88] = v19;
  v22 = a11[10];
  v52[89] = v20;
  v52[90] = v22;
  v23 = a11[4];
  v24 = a11[5];
  v25 = a11[2];
  v52[83] = a11[3];
  v52[84] = v23;
  v52[85] = v24;
  v52[86] = v21;
  v26 = *a11;
  v27 = a11[1];
  v52[79] = v14;
  v52[80] = v26;
  v52[81] = v27;
  v52[82] = v25;
  v29 = v28[9];
  v52[99] = v28[8];
  v52[100] = v29;
  v30 = v28[5];
  v52[95] = v28[4];
  v52[96] = v30;
  v31 = v28[7];
  v52[97] = v28[6];
  v52[98] = v31;
  v32 = v28[1];
  v52[91] = *v28;
  v52[92] = v32;
  v33 = v28[3];
  v52[93] = v28[2];
  v52[94] = v33;
  v35 = v34[8];
  v36 = v34[9];
  v37 = v34[6];
  v52[109] = v34[7];
  v52[110] = v35;
  v38 = v34[10];
  v52[111] = v36;
  v52[112] = v38;
  v39 = v34[4];
  v40 = v34[5];
  v41 = v34[2];
  v52[105] = v34[3];
  v52[106] = v39;
  v42 = v28[10];
  v52[107] = v40;
  v52[108] = v37;
  v43 = *v34;
  v44 = v34[1];
  v52[101] = v42;
  v52[102] = v43;
  v52[103] = v44;
  v52[104] = v41;
  v45 = v12[9];
  v52[121] = v12[8];
  v52[122] = v45;
  v52[123] = v12[10];
  v46 = v12[5];
  v52[117] = v12[4];
  v52[118] = v46;
  v47 = v12[7];
  v52[119] = v12[6];
  v52[120] = v47;
  v48 = v12[1];
  v52[113] = *v12;
  v52[114] = v48;
  v49 = v12[3];
  v52[115] = v12[2];
  v52[116] = v49;
  outlined init with take of BNNS.NormalizationType(a10, v51);
  outlined init with take of BNNS.NormalizationType(v51, v52);
  v50 = (char *)sub_1CAADDE60
      + 4 * byte_1CAB5F810[_s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v52)];
  __asm { BR              X10 }
}

uint64_t sub_1CAADDE60()
{
  char v0;
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  __int128 *v6;
  uint64_t v7;
  uint64_t v8;
  int v9;
  int v10;
  const void *v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  int v15;
  uint64_t v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  uint64_t v35;
  uint64_t v36;
  int v37;
  int v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  __int128 v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  __int128 v65;
  __int128 v66;
  __int128 v67;
  __int128 v68;
  __int128 v69;
  __int128 v70;
  __int128 v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  uint64_t v75;
  const BNNSFilterParameters *v76;
  void *v77;
  uint64_t v78;
  uint64_t v79;
  int v81;
  int v82;
  int v83;
  int v84;
  uint64_t v85;
  uint64_t v86;
  uint64_t v87;
  uint64_t v88;
  int v89;
  uint64_t v90;
  uint64_t v91;
  uint64_t v92;
  uint64_t v93;
  uint64_t v94;
  uint64_t v95;
  uint64_t v96;
  uint64_t v97;
  uint64_t v98;
  uint64_t v99;
  uint64_t v100;
  uint64_t v101;
  uint64_t v102;
  uint64_t v103;
  uint64_t v104;
  uint64_t v105;
  uint64_t v106;
  uint64_t v107;
  uint64_t v108;
  uint64_t v109;
  uint64_t v110;
  uint64_t v111;
  uint64_t v112;
  uint64_t v113;
  int v114;
  uint64_t v115;
  uint64_t v116;
  uint64_t v117;
  uint64_t v118;
  uint64_t v119;
  uint64_t v120;
  uint64_t v121;
  uint64_t v122;
  uint64_t v123;
  uint64_t v124;
  uint64_t v125;
  uint64_t v126;
  uint64_t v127;
  uint64_t v128;
  uint64_t v129;
  uint64_t v130;
  uint64_t v131;
  uint64_t v132;
  uint64_t v133;
  uint64_t v134;
  uint64_t v135;
  uint64_t v136;
  uint64_t v137;
  uint64_t v138;
  uint64_t v139;
  uint64_t v140;
  uint64_t v141;
  uint64_t v142;
  uint64_t v143;
  uint64_t v144;
  uint64_t v145;
  uint64_t v146;
  uint64_t v147;
  uint64_t v148;
  uint64_t v149;
  uint64_t v150;
  int v151;
  uint64_t v152;
  _QWORD *v153;
  uint64_t v154;
  uint64_t v155;
  uint64_t v156;
  uint64_t v157;
  uint64_t v158;
  uint64_t v159;
  uint64_t v160;
  char *v161;
  int v162;
  uint64_t v163;
  uint64_t v164;
  uint64_t v165;
  _QWORD v166[108];
  int v167;
  int v168;
  uint64_t v169;
  uint64_t v170;
  uint64_t v171;
  uint64_t v172;
  uint64_t v173;
  uint64_t v174;
  uint64_t v175;
  uint64_t v176;
  uint64_t v177;
  uint64_t v178;
  uint64_t v179;
  uint64_t v180;
  uint64_t v181;
  uint64_t v182;
  uint64_t v183;
  uint64_t v184;
  uint64_t v185;
  uint64_t v186;
  uint64_t v187;
  uint64_t v188;
  int v189;
  int v190;
  int v191;
  int v192;
  int v193;
  int v194;
  uint64_t v195;
  uint64_t v196;
  uint64_t v197;
  uint64_t v198;
  uint64_t v199;
  _QWORD v200[86];
  int v201;
  uint64_t v202;
  int v203;
  uint64_t v204;
  uint64_t v205;
  __int128 v206;
  __int128 v207;
  uint64_t v208;
  uint64_t v209;
  uint64_t v210;
  uint64_t v211;
  uint64_t v212;
  uint64_t v213;
  uint64_t v214;
  uint64_t v215;
  uint64_t v216;
  uint64_t v217;
  uint64_t v218;
  _QWORD v219[88];
  _BYTE v220[528];
  int v221;
  uint64_t v222;
  uint64_t v223;
  int v224;
  uint64_t v225;
  uint64_t v226;
  uint64_t v227;
  _QWORD v228[20];
  int v229;
  uint64_t v230;
  int v231;
  _BYTE v232[368];
  uint64_t v233;
  _BYTE v234[368];
  _BYTE v235[184];
  _BYTE v236[1256];

  v11 = (const void *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)&v233);
  memcpy(v166, v11, 0x169uLL);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v166, (uint64_t)v235, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v166[23], (uint64_t)v236, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v235, (uint64_t)v166, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  v12 = 0;
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v166) == 1)
  {
    v114 = 0;
    v115 = 0;
    v116 = 0;
    v109 = 0;
    v110 = 0;
    v111 = 0;
    v112 = 0;
    v113 = 0;
    v117 = 0;
    v118 = 0;
    v119 = 0;
    v120 = 0;
    v121 = 0;
    v122 = 0;
    v123 = 0;
    v125 = 0;
    v126 = 0;
    v127 = 0;
    v128 = 0;
    v124 = 0;
    v13 = 0;
    v92 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v235, (uint64_t)v200, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    v124 = v200[0];
    v127 = v200[2];
    v128 = v200[1];
    v125 = v200[4];
    v126 = v200[3];
    v122 = v200[6];
    v123 = v200[5];
    v120 = v200[8];
    v121 = v200[7];
    v118 = v200[10];
    v119 = v200[9];
    v115 = v200[12];
    v112 = v200[14];
    v113 = v200[13];
    v110 = v200[16];
    v111 = v200[15];
    v109 = v200[17];
    v92 = v200[18];
    v116 = v200[19];
    v117 = v200[11];
    v114 = v200[20];
    v13 = *(_QWORD *)((char *)&v200[20] + 4);
    v82 = HIDWORD(v200[21]);
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v236, (uint64_t)v200, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v200) == 1)
  {
    v89 = 0;
    v90 = 0;
    v91 = 0;
    v93 = 0;
    v94 = 0;
    v95 = 0;
    v96 = 0;
    v97 = 0;
    v98 = 0;
    v99 = 0;
    v100 = 0;
    v101 = 0;
    v102 = 0;
    v103 = 0;
    v104 = 0;
    v105 = 0;
    v106 = 0;
    v107 = 0;
    v108 = 0;
    v87 = 0;
    v88 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v236, (uint64_t)v219, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    v88 = v219[0];
    v107 = v219[2];
    v108 = v219[1];
    v105 = v219[4];
    v106 = v219[3];
    v103 = v219[6];
    v104 = v219[5];
    v101 = v219[8];
    v102 = v219[7];
    v99 = v219[10];
    v100 = v219[9];
    v97 = v219[12];
    v98 = v219[11];
    v95 = v219[14];
    v96 = v219[13];
    v93 = v219[16];
    v94 = v219[15];
    v87 = v219[18];
    v90 = v219[19];
    v91 = v219[17];
    v12 = *(_QWORD *)(v7 + 164);
    v89 = v219[20];
    v81 = HIDWORD(v219[21]);
  }
  if (v151)
  {
    v149 = v4;
    v148 = v8;
    v4 = 0;
    v8 = 0;
  }
  else
  {
    v3 = 0;
    v2 = 0;
    v149 = 0;
    v148 = 0;
  }
  v150 = v8;
  v160 = v3;
  outlined init with take of BNNSNDArrayDescriptor?(v1, (uint64_t)v228, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  v159 = v2;
  v152 = v4;
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v228) == 1)
  {
    v146 = 0;
    v145 = 0;
    v144 = 0;
    v143 = 0;
    v142 = 0;
    v141 = 0;
    v140 = 0;
    v139 = 0;
    v138 = 0;
    v137 = 0;
    v136 = 0;
    v134 = 0;
    v135 = 0;
    v132 = 0;
    v133 = 0;
    v130 = 0;
    v131 = 0;
    v14 = 0;
    v15 = 0;
    v147 = 0;
    v16 = 0;
    v129 = 0;
  }
  else
  {
    v147 = v228[0];
    v146 = v228[1];
    v145 = v228[2];
    v144 = v228[3];
    v143 = v228[4];
    v142 = v228[5];
    v141 = v228[6];
    v140 = v228[7];
    v139 = v228[8];
    v138 = v228[9];
    v137 = v228[10];
    v136 = v228[11];
    v134 = v228[13];
    v135 = v228[12];
    v132 = v228[15];
    v133 = v228[14];
    v130 = v228[17];
    v131 = v228[16];
    v129 = v228[18];
    v14 = v228[19];
    v16 = v230;
    v15 = v229;
    v83 = v231;
  }
  v17 = v6[53];
  *(_OWORD *)(v7 + 832) = v6[52];
  *(_OWORD *)(v7 + 848) = v17;
  v18 = v6[54];
  v19 = v6[49];
  *(_OWORD *)(v7 + 768) = v6[48];
  *(_OWORD *)(v7 + 784) = v19;
  v20 = v6[51];
  *(_OWORD *)(v7 + 800) = v6[50];
  *(_OWORD *)(v7 + 816) = v20;
  v21 = v6[45];
  *(_OWORD *)(v7 + 704) = v6[44];
  *(_OWORD *)(v7 + 720) = v21;
  v22 = v6[47];
  *(_OWORD *)(v7 + 736) = v6[46];
  *(_OWORD *)(v7 + 752) = v22;
  v23 = v6[41];
  *(_OWORD *)(v7 + 992) = v6[40];
  *(_OWORD *)(v7 + 1008) = v23;
  v24 = v6[43];
  *(_OWORD *)(v7 + 1024) = v6[42];
  *(_OWORD *)(v7 + 1040) = v24;
  v25 = v6[37];
  *(_OWORD *)(v7 + 928) = v6[36];
  *(_OWORD *)(v7 + 944) = v25;
  v26 = v6[39];
  *(_OWORD *)(v7 + 960) = v6[38];
  *(_OWORD *)(v7 + 976) = v26;
  v28 = v6[32];
  v27 = v6[33];
  *(_OWORD *)(v7 + 864) = v18;
  *(_OWORD *)(v7 + 880) = v27;
  v29 = v6[35];
  *(_OWORD *)(v7 + 896) = v6[34];
  *(_OWORD *)(v7 + 912) = v29;
  v30 = v6[28];
  *(_OWORD *)(v7 + 1168) = v6[29];
  v31 = v6[31];
  *(_OWORD *)(v7 + 1184) = v6[30];
  *(_OWORD *)(v7 + 1200) = v31;
  *(_OWORD *)(v7 + 1216) = v28;
  v32 = v6[24];
  *(_OWORD *)(v7 + 1104) = v6[25];
  v33 = v6[27];
  *(_OWORD *)(v7 + 1120) = v6[26];
  *(_OWORD *)(v7 + 1136) = v33;
  *(_OWORD *)(v7 + 1152) = v30;
  v34 = v6[23];
  *(_OWORD *)(v7 + 1056) = v6[22];
  *(_OWORD *)(v7 + 1072) = v34;
  *(_OWORD *)(v7 + 1088) = v32;
  v166[0] = v5;
  LOBYTE(v166[1]) = v0;
  BNNS.ActivationFunction.bnnsActivation.getter();
  v35 = v222;
  v36 = v223;
  v37 = v221;
  v38 = v224;
  v39 = v225;
  v40 = v226;
  v41 = v227;
  outlined init with take of BNNS.NormalizationType(v232, v234);
  if (_s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v234) == 2)
    v42 = *(_QWORD *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v234);
  else
    v42 = 0;
  v43 = v6[29];
  v45 = v6[30];
  v44 = v6[31];
  *(_OWORD *)(v7 + 128) = v45;
  *(_OWORD *)(v7 + 144) = v44;
  v46 = v6[25];
  v48 = v6[26];
  v47 = v6[27];
  *(_OWORD *)(v7 + 64) = v48;
  *(_OWORD *)(v7 + 80) = v47;
  v49 = v6[27];
  v51 = v6[28];
  v50 = v6[29];
  *(_OWORD *)(v7 + 96) = v51;
  *(_OWORD *)(v7 + 112) = v50;
  v52 = v6[23];
  *(_OWORD *)v7 = v6[22];
  *(_OWORD *)(v7 + 16) = v52;
  v53 = v6[25];
  v55 = v6[22];
  v54 = v6[23];
  v56 = v6[24];
  *(_OWORD *)(v7 + 32) = v56;
  *(_OWORD *)(v7 + 48) = v53;
  *(_OWORD *)(v7 + 288) = v43;
  *(_OWORD *)(v7 + 304) = v45;
  v57 = v6[32];
  *(_OWORD *)(v7 + 320) = v6[31];
  *(_OWORD *)(v7 + 336) = v57;
  *(_OWORD *)(v7 + 224) = v46;
  *(_OWORD *)(v7 + 240) = v48;
  *(_OWORD *)(v7 + 256) = v49;
  *(_OWORD *)(v7 + 272) = v51;
  *(_OWORD *)(v7 + 160) = v57;
  *(_OWORD *)(v7 + 176) = v55;
  *(_OWORD *)(v7 + 192) = v54;
  *(_OWORD *)(v7 + 208) = v56;
  v58 = v6[18];
  v59 = v6[20];
  v60 = v6[21];
  *(_OWORD *)(v7 + 480) = v6[19];
  *(_OWORD *)(v7 + 496) = v59;
  v61 = v6[14];
  v62 = v6[16];
  v63 = v6[17];
  *(_OWORD *)(v7 + 416) = v6[15];
  *(_OWORD *)(v7 + 432) = v62;
  *(_OWORD *)(v7 + 448) = v63;
  *(_OWORD *)(v7 + 464) = v58;
  v64 = v6[12];
  v65 = v6[13];
  v66 = v6[10];
  *(_OWORD *)(v7 + 352) = v6[11];
  *(_OWORD *)(v7 + 368) = v64;
  *(_OWORD *)(v7 + 384) = v65;
  *(_OWORD *)(v7 + 400) = v61;
  v67 = v6[8];
  v68 = v6[9];
  v69 = v6[6];
  *(_OWORD *)(v7 + 640) = v6[7];
  *(_OWORD *)(v7 + 656) = v67;
  *(_OWORD *)(v7 + 672) = v68;
  *(_OWORD *)(v7 + 688) = v66;
  v70 = v6[4];
  v71 = v6[5];
  v72 = v6[2];
  *(_OWORD *)(v7 + 576) = v6[3];
  *(_OWORD *)(v7 + 592) = v70;
  *(_OWORD *)(v7 + 608) = v71;
  *(_OWORD *)(v7 + 624) = v69;
  v73 = *v6;
  v74 = v6[1];
  *(_OWORD *)(v7 + 512) = v60;
  *(_OWORD *)(v7 + 528) = v73;
  *(_OWORD *)(v7 + 544) = v74;
  *(_OWORD *)(v7 + 560) = v72;
  memcpy(v200, v220, 0x210uLL);
  v200[67] = v146;
  v200[68] = v145;
  v200[69] = v144;
  v200[70] = v143;
  v200[71] = v142;
  v200[72] = v141;
  v200[73] = v140;
  v200[74] = v139;
  v200[75] = v138;
  v200[76] = v137;
  v200[77] = v136;
  v200[78] = v135;
  v200[79] = v134;
  v200[80] = v133;
  v200[81] = v132;
  v200[82] = v131;
  v200[83] = v130;
  v200[85] = v14;
  v201 = v15;
  v202 = v16;
  v203 = v83;
  v204 = 0x7FC0000000000000;
  v205 = 0x17FC00000;
  v208 = v155;
  v209 = v156;
  v210 = v157;
  v211 = v158;
  v212 = v150;
  v213 = v152;
  v215 = v148;
  v216 = v149;
  v217 = v159;
  v218 = v160;
  v200[66] = v147;
  v200[84] = v129;
  v207 = 0u;
  v206 = 0u;
  v214 = 0;
  memcpy(v166, v219, 0x2C0uLL);
  v166[89] = v128;
  v166[90] = v127;
  v166[91] = v126;
  v166[92] = v125;
  v166[93] = v123;
  v166[94] = v122;
  v166[95] = v121;
  v166[96] = v120;
  v166[97] = v119;
  v166[98] = v118;
  v166[99] = v117;
  v166[100] = v115;
  v166[101] = v113;
  v166[102] = v112;
  v166[103] = v111;
  v166[104] = v110;
  v166[105] = v109;
  v166[88] = v124;
  v166[106] = v92;
  v166[107] = v116;
  v167 = v114;
  *v153 = v13;
  v168 = v82;
  v169 = v88;
  v170 = v108;
  v171 = v107;
  v172 = v106;
  v173 = v105;
  v174 = v104;
  v175 = v103;
  v176 = v102;
  v177 = v101;
  v178 = v100;
  v179 = v99;
  v180 = v98;
  v181 = v97;
  v182 = v96;
  v183 = v95;
  v184 = v94;
  v185 = v93;
  v186 = v91;
  v187 = v87;
  v188 = v90;
  v189 = v89;
  v153[22] = v12;
  v190 = v81;
  v191 = v10;
  v192 = v9;
  v193 = v37;
  v153[25] = v35;
  v153[26] = v36;
  v194 = v38;
  v195 = v39;
  v196 = v40;
  v197 = v41;
  v198 = 0;
  v199 = v42;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
  v75 = swift_allocObject();
  *(_OWORD *)(v75 + 16) = xmmword_1CAB5E440;
  *(_QWORD *)(v75 + 32) = v200;
  *(_QWORD *)(v75 + 40) = v166;
  v161 = (char *)v75;
  if (v154 == 1)
  {
    v76 = 0;
  }
  else
  {
    v162 = v84;
    v163 = v85;
    v164 = v154;
    v165 = v86;
    v76 = (const BNNSFilterParameters *)&v162;
  }
  v77 = closure #1 in closure #1 in closure #1 in BNNS.FusedConvolutionNormalizationLayer.init(input:output:convolutionWeights:convolutionBias:convolutionStride:convolutionDilationStride:convolutionPadding:normalization:normalizationBeta:normalizationGamma:normalizationMomentum:normalizationEpsilon:normalizationActivation:filterParameters:)(v76, v232, &v161, 0);
  swift_bridgeObjectRelease();
  type metadata accessor for BNNS.FusedConvolutionNormalizationLayer();
  v78 = swift_allocObject();
  v79 = v78;
  if (v77)
  {
    *(_QWORD *)(v78 + 16) = v77;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v79;
}

void BNNS.FusedFullyConnectedNormalizationLayer.__allocating_init(input:output:fullyConnectedWeights:fullyConnectedBias:normalization:normalizationBeta:normalizationGamma:normalizationMomentum:normalizationEpsilon:normalizationActivation:filterParameters:)(uint64_t a1)
{
  uint64_t v1;
  uint64_t v2;
  _OWORD *v3;
  __int128 v4;
  __int128 v5;
  __int128 v6;
  __int128 v7;
  __int128 v8;
  _OWORD *v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  _OWORD *v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  const void *v21;
  char *v22;
  uint64_t v23;
  _BYTE v24[368];
  _OWORD v25[79];
  uint64_t v26;

  MEMORY[0x1E0C80A78](a1);
  v2 = v1;
  v26 = *MEMORY[0x1E0C80C00];
  v4 = v3[9];
  v25[54] = v3[8];
  v25[55] = v4;
  v25[56] = v3[10];
  v5 = v3[5];
  v25[50] = v3[4];
  v25[51] = v5;
  v6 = v3[6];
  v25[53] = v3[7];
  v25[52] = v6;
  v7 = v3[1];
  v25[46] = *v3;
  v25[47] = v7;
  v8 = v3[2];
  v25[49] = v3[3];
  v25[48] = v8;
  v10 = v9[9];
  v25[65] = v9[8];
  v25[66] = v10;
  v25[67] = v9[10];
  v11 = v9[5];
  v25[61] = v9[4];
  v25[62] = v11;
  v12 = v9[6];
  v25[64] = v9[7];
  v25[63] = v12;
  v13 = v9[1];
  v25[57] = *v9;
  v25[58] = v13;
  v14 = v9[2];
  v25[60] = v9[3];
  v25[59] = v14;
  v16 = v15[9];
  v25[76] = v15[8];
  v25[77] = v16;
  v25[78] = v15[10];
  v17 = v15[5];
  v25[72] = v15[4];
  v25[73] = v17;
  v18 = v15[6];
  v25[75] = v15[7];
  v25[74] = v18;
  v19 = v15[1];
  v25[68] = *v15;
  v25[69] = v19;
  v20 = v15[2];
  v25[71] = v15[3];
  v25[70] = v20;
  outlined init with take of BNNS.NormalizationType(v21, v24);
  outlined init with take of BNNSNDArrayDescriptor?(v2, (uint64_t)&v23, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  _sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)&v23);
  outlined init with take of BNNS.NormalizationType(v24, v25);
  v22 = (char *)sub_1CAADECE4
      + 4 * byte_1CAB5F814[_s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v25)];
  __asm { BR              X10 }
}

uint64_t sub_1CAADECE4()
{
  _QWORD *v0;
  uint64_t v1;
  uint64_t v2;
  int v3;
  int v4;
  uint64_t v5;
  const void *v6;
  uint64_t *v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  int v13;
  int v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  const BNNSFilterParameters *v20;
  void *v21;
  uint64_t v22;
  uint64_t v23;
  int v25;
  int v26;
  uint64_t v27;
  uint64_t v28;
  int v29;
  int v30;
  uint64_t v31;
  uint64_t v32;
  int v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;
  uint64_t v56;
  uint64_t v57;
  int v58;
  uint64_t v59;
  uint64_t v60;
  uint64_t v61;
  uint64_t v62;
  uint64_t v63;
  uint64_t v64;
  uint64_t v65;
  uint64_t v66;
  uint64_t v67;
  uint64_t v68;
  uint64_t v69;
  uint64_t v70;
  uint64_t v71;
  uint64_t v72;
  char v73;
  uint64_t v74;
  int v75;
  uint64_t v76;
  uint64_t v77;
  uint64_t v78;
  uint64_t v79;
  uint64_t v80;
  uint64_t v81;
  uint64_t v82;
  uint64_t v83;
  uint64_t v84;
  uint64_t v85;
  uint64_t v86;
  uint64_t v87;
  uint64_t v88;
  uint64_t v89;
  uint64_t v90;
  uint64_t v91;
  uint64_t v92;
  uint64_t v93;
  uint64_t v94;
  char *v95;
  int v96;
  uint64_t v97;
  uint64_t v98;
  uint64_t v99;
  _QWORD v100[108];
  int v101;
  int v102;
  uint64_t v103;
  uint64_t v104;
  uint64_t v105;
  uint64_t v106;
  uint64_t v107;
  uint64_t v108;
  uint64_t v109;
  uint64_t v110;
  uint64_t v111;
  uint64_t v112;
  uint64_t v113;
  uint64_t v114;
  uint64_t v115;
  uint64_t v116;
  uint64_t v117;
  uint64_t v118;
  uint64_t v119;
  uint64_t v120;
  uint64_t v121;
  uint64_t v122;
  int v123;
  int v124;
  int v125;
  int v126;
  int v127;
  int v128;
  uint64_t v129;
  uint64_t v130;
  uint64_t v131;
  uint64_t v132;
  uint64_t v133;
  _QWORD v134[86];
  int v135;
  uint64_t v136;
  int v137;
  uint64_t v138;
  uint64_t v139;
  __int128 v140;
  __int128 v141;
  _OWORD v142[44];
  _BYTE v143[184];
  _BYTE v144[184];
  _BYTE v145[528];
  int v146;
  uint64_t v147;
  uint64_t v148;
  int v149;
  uint64_t v150;
  uint64_t v151;
  uint64_t v152;
  _BYTE v153[368];
  uint64_t v154;
  _OWORD v155[45];
  __int128 v156;
  __int128 v157;
  __int128 v158;
  __int128 v159;
  __int128 v160;
  __int128 v161;
  __int128 v162;
  __int128 v163;
  __int128 v164;
  __int128 v165;
  __int128 v166;

  v6 = (const void *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)&v154);
  memcpy(v100, v6, 0x169uLL);
  v7 = &demangling cache variable for type metadata for BNNSNDArrayDescriptor?;
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v100, (uint64_t)v143, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v100[23], (uint64_t)v144, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v143, (uint64_t)v100, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  v8 = 0;
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v100) == 1)
  {
    v58 = 0;
    v59 = 0;
    v60 = 0;
    v53 = 0;
    v54 = 0;
    v55 = 0;
    v56 = 0;
    v57 = 0;
    v61 = 0;
    v62 = 0;
    v63 = 0;
    v64 = 0;
    v65 = 0;
    v66 = 0;
    v67 = 0;
    v69 = 0;
    v70 = 0;
    v71 = 0;
    v72 = 0;
    v68 = 0;
    v9 = 0;
    v36 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v143, (uint64_t)v134, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    v68 = v134[0];
    v71 = v134[2];
    v72 = v134[1];
    v69 = v134[4];
    v70 = v134[3];
    v66 = v134[6];
    v67 = v134[5];
    v64 = v134[8];
    v65 = v134[7];
    v62 = v134[10];
    v63 = v134[9];
    v59 = v134[12];
    v56 = v134[14];
    v57 = v134[13];
    v54 = v134[16];
    v55 = v134[15];
    v53 = v134[17];
    v36 = v134[18];
    v60 = v134[19];
    v61 = v134[11];
    v58 = v134[20];
    v9 = *(_QWORD *)((char *)&v134[20] + 4);
    LODWORD(v7) = HIDWORD(v134[21]);
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v144, (uint64_t)v134, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  v26 = (int)v7;
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v134) == 1)
  {
    v33 = 0;
    v34 = 0;
    v35 = 0;
    v37 = 0;
    v38 = 0;
    v39 = 0;
    v40 = 0;
    v41 = 0;
    v42 = 0;
    v43 = 0;
    v44 = 0;
    v45 = 0;
    v46 = 0;
    v47 = 0;
    v48 = 0;
    v49 = 0;
    v50 = 0;
    v51 = 0;
    v52 = 0;
    v31 = 0;
    v32 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v144, (uint64_t)v142, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    v32 = *(_QWORD *)&v142[0];
    v51 = *(_QWORD *)&v142[1];
    v52 = *((_QWORD *)&v142[0] + 1);
    v49 = *(_QWORD *)&v142[2];
    v50 = *((_QWORD *)&v142[1] + 1);
    v47 = *(_QWORD *)&v142[3];
    v48 = *((_QWORD *)&v142[2] + 1);
    v45 = *(_QWORD *)&v142[4];
    v46 = *((_QWORD *)&v142[3] + 1);
    v43 = *(_QWORD *)&v142[5];
    v44 = *((_QWORD *)&v142[4] + 1);
    v41 = *(_QWORD *)&v142[6];
    v42 = *((_QWORD *)&v142[5] + 1);
    v39 = *(_QWORD *)&v142[7];
    v40 = *((_QWORD *)&v142[6] + 1);
    v37 = *(_QWORD *)&v142[8];
    v38 = *((_QWORD *)&v142[7] + 1);
    v34 = *((_QWORD *)&v142[9] + 1);
    v31 = *(_QWORD *)&v142[9];
    v35 = *((_QWORD *)&v142[8] + 1);
    v33 = v142[10];
    v8 = *(_QWORD *)((char *)&v142[10] + 4);
    v25 = HIDWORD(v142[10]);
  }
  v10 = v1;
  v100[0] = v2;
  LOBYTE(v100[1]) = v73;
  BNNS.ActivationFunction.bnnsActivation.getter();
  v11 = v147;
  v12 = v148;
  v13 = v146;
  v14 = v149;
  v15 = v150;
  v16 = v151;
  v17 = v152;
  outlined init with take of BNNS.NormalizationType(v153, v155);
  if (_s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v155) == 2)
    v18 = *(_QWORD *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v155);
  else
    v18 = 0;
  v142[8] = v164;
  v142[9] = v165;
  v142[4] = v160;
  v142[5] = v161;
  v142[7] = v163;
  v142[6] = v162;
  v142[0] = v156;
  v142[1] = v157;
  v142[3] = v159;
  v142[2] = v158;
  v142[18] = v163;
  v142[19] = v164;
  v142[20] = v165;
  v142[21] = v166;
  v142[14] = v159;
  v142[15] = v160;
  v142[16] = v161;
  v142[17] = v162;
  v142[10] = v166;
  v142[11] = v156;
  v142[12] = v157;
  v142[13] = v158;
  v142[30] = v155[42];
  v142[31] = v155[43];
  v142[26] = v155[38];
  v142[27] = v155[39];
  v142[29] = v155[41];
  v142[28] = v155[40];
  v142[22] = v155[34];
  v142[23] = v155[35];
  v142[25] = v155[37];
  v142[24] = v155[36];
  v142[40] = v155[30];
  v142[41] = v155[31];
  v142[42] = v155[32];
  v142[43] = v155[33];
  v142[36] = v155[26];
  v142[37] = v155[27];
  v142[38] = v155[28];
  v142[39] = v155[29];
  v142[32] = v155[44];
  v142[33] = v155[23];
  v142[34] = v155[24];
  v142[35] = v155[25];
  memcpy(v134, v145, 0x210uLL);
  v134[66] = v88;
  v134[67] = v92;
  v134[68] = v93;
  v134[69] = v10;
  v134[70] = v91;
  v134[71] = v90;
  v134[72] = v89;
  v134[73] = v87;
  v134[74] = v86;
  v134[75] = v85;
  v134[76] = v84;
  v134[77] = v83;
  v134[78] = v82;
  v134[79] = v81;
  v134[80] = v80;
  v134[81] = v79;
  v134[82] = v78;
  v134[83] = v77;
  v134[84] = v74;
  v134[85] = v76;
  v135 = v75;
  v136 = v5;
  v137 = v29;
  v138 = 0x7FC0000000000000;
  v139 = 0x17FC00000;
  v141 = 0u;
  v140 = 0u;
  memcpy(v100, v142, 0x2C0uLL);
  v100[89] = v72;
  v100[90] = v71;
  v100[91] = v70;
  v100[92] = v69;
  v100[93] = v67;
  v100[94] = v66;
  v100[95] = v65;
  v100[96] = v64;
  v100[97] = v63;
  v100[98] = v62;
  v100[99] = v61;
  v100[100] = v59;
  v100[101] = v57;
  v100[102] = v56;
  v100[103] = v55;
  v100[104] = v54;
  v100[105] = v53;
  v100[88] = v68;
  v100[106] = v36;
  v100[107] = v60;
  v101 = v58;
  *v0 = v9;
  v102 = v26;
  v103 = v32;
  v104 = v52;
  v105 = v51;
  v106 = v50;
  v107 = v49;
  v108 = v48;
  v109 = v47;
  v110 = v46;
  v111 = v45;
  v112 = v44;
  v113 = v43;
  v114 = v42;
  v115 = v41;
  v116 = v40;
  v117 = v39;
  v118 = v38;
  v119 = v37;
  v120 = v35;
  v121 = v31;
  v122 = v34;
  v123 = v33;
  v0[22] = v8;
  v124 = v25;
  v125 = v4;
  v126 = v3;
  v127 = v13;
  v0[25] = v11;
  v0[26] = v12;
  v128 = v14;
  v129 = v15;
  v130 = v16;
  v131 = v17;
  v132 = 0;
  v133 = v18;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
  v19 = swift_allocObject();
  *(_OWORD *)(v19 + 16) = xmmword_1CAB5E440;
  *(_QWORD *)(v19 + 32) = v134;
  *(_QWORD *)(v19 + 40) = v100;
  v95 = (char *)v19;
  if (v94 == 1)
  {
    v20 = 0;
  }
  else
  {
    v96 = v30;
    v97 = v27;
    v98 = v94;
    v99 = v28;
    v20 = (const BNNSFilterParameters *)&v96;
  }
  v21 = closure #1 in closure #1 in closure #1 in BNNS.FusedConvolutionNormalizationLayer.init(input:output:convolutionWeights:convolutionBias:convolutionStride:convolutionDilationStride:convolutionPadding:normalization:normalizationBeta:normalizationGamma:normalizationMomentum:normalizationEpsilon:normalizationActivation:filterParameters:)(v20, v153, &v95, 1);
  swift_bridgeObjectRelease();
  type metadata accessor for BNNS.FusedFullyConnectedNormalizationLayer();
  v22 = swift_allocObject();
  v23 = v22;
  if (v21)
  {
    *(_QWORD *)(v22 + 16) = v21;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v23;
}

void *closure #1 in closure #1 in closure #1 in BNNS.FusedConvolutionNormalizationLayer.init(input:output:convolutionWeights:convolutionBias:convolutionStride:convolutionDilationStride:convolutionPadding:normalization:normalizationBeta:normalizationGamma:normalizationMomentum:normalizationEpsilon:normalizationActivation:filterParameters:)(const BNNSFilterParameters *a1, const void *a2, char **a3, int a4)
{
  uint64_t v8;
  int v9;
  char *v10;
  char isUniquelyReferenced_nonNull_native;
  void *FusedLayer;
  _BYTE v14[368];

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<BNNSFilterType>);
  v8 = swift_allocObject();
  *(_DWORD *)(v8 + 32) = a4;
  outlined init with take of BNNS.NormalizationType(a2, v14);
  v9 = _s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v14) + 2;
  _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v14);
  *(_DWORD *)(v8 + 36) = v9;
  v10 = *a3;
  isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
  *a3 = v10;
  if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    v10 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v10 + 2), 0, v10);
  *a3 = v10;
  FusedLayer = BNNSFilterCreateFusedLayer(2uLL, (const BNNSFilterType *)(v8 + 32), (const void **)v10 + 4, a1);
  swift_setDeallocating();
  swift_deallocClassInstance();
  return FusedLayer;
}

uint64_t BNNS.FusedLayer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

uint64_t specialized static BNNS.fusedLayerApply(_:batchSize:input:output:for:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4, char a5)
{
  uint64_t result;
  size_t v6;
  _BYTE *v7;
  void *v8;
  size_t v9;
  void *in;
  void *v12;
  unint64_t v14;
  unint64_t v15;
  unint64_t v16;
  unint64_t v17;
  unint64_t v18;
  unint64_t v19;
  unint64_t v20;
  unint64_t v21;
  _BYTE v22[136];
  unint64_t v23;
  unint64_t v24;
  unint64_t v25;
  unint64_t v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  _BYTE v31[136];
  _BYTE v32[136];
  _BYTE v33[136];
  _BYTE v34[8];
  _BYTE v35[8];
  void *v36;
  void *v37;
  uint64_t v38;

  v38 = a4;
  v12 = *(void **)(a1 + 16);
  outlined init with take of BNNSNDArrayDescriptor?(a3 + 136, (uint64_t)v35, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  result = outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v35, (uint64_t)&v36, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  in = v36;
  if (!v36)
  {
    __break(1u);
    goto LABEL_7;
  }
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v32);
  outlined init with take of BNNS.Shape((uint64_t)v32, (uint64_t)v33);
  outlined init with take of BNNS.Shape((uint64_t)v33, (uint64_t)v31);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v33, (uint64_t)v31);
  BNNS.Shape.stride.getter();
  v9 = specialized static BNNS.calculateBatchStride(size:stride:)(v23, v24, v25, v26, v27, v28, v29, v30, v23, v24, v25, v26, v27, v28, v29, v30);
  outlined init with take of BNNSNDArrayDescriptor?(v38 + 136, (uint64_t)v34, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  result = outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v34, (uint64_t)&v37, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v8 = v37;
  if (!v37)
  {
LABEL_7:
    __break(1u);
    return result;
  }
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v23);
  outlined init with take of BNNS.Shape((uint64_t)&v23, (uint64_t)v31);
  outlined init with take of BNNS.Shape((uint64_t)v31, (uint64_t)v22);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v31, (uint64_t)v22);
  BNNS.Shape.stride.getter();
  v6 = specialized static BNNS.calculateBatchStride(size:stride:)(v14, v15, v16, v17, v18, v19, v20, v21, v14, v15, v16, v17, v18, v19, v20, v21);
  result = BNNSFusedFilterApplyBatch(v12, a2, in, v9, v8, v6, (a5 & 1) == 0);
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v7 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t specialized static BNNS.fusedLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingParameterGradients:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4, _OWORD *a5, __int128 *a6, uint64_t a7)
{
  uint64_t v13;
  int64_t v14;
  uint64_t v15;
  _OWORD *v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  _OWORD *v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  unint64_t v28;
  unint64_t v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  _BYTE *v43;
  char *v44;
  uint64_t v45;
  uint64_t i;
  uint64_t v47;
  char *v48;
  uint64_t v49;
  uint64_t j;
  uint64_t v51;
  size_t v54;
  uint64_t v55;
  uint64_t v56;
  int v57;
  char *v58;
  BNNSNDArrayDescriptor v59;
  BNNSNDArrayDescriptor v60;
  uint64_t v61;

  v13 = a1;
  v61 = *MEMORY[0x1E0C80C00];
  v14 = *(_QWORD *)(a7 + 16);
  v15 = MEMORY[0x1E0DEE9D8];
  if (v14)
  {
    v54 = a2;
    v55 = a3;
    v56 = a4;
    *(_QWORD *)&v59.flags = MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v14, 0);
    v16 = (_OWORD *)(a7 + 32);
    v15 = *(_QWORD *)&v59.flags;
    do
    {
      v17 = v16[9];
      *(_OWORD *)&v60.stride[7] = v16[8];
      *(_OWORD *)&v60.data_type = v17;
      *(_OWORD *)&v60.table_data_type = v16[10];
      v18 = v16[5];
      *(_OWORD *)&v60.size[7] = v16[4];
      *(_OWORD *)&v60.stride[1] = v18;
      v19 = v16[7];
      *(_OWORD *)&v60.stride[3] = v16[6];
      *(_OWORD *)&v60.stride[5] = v19;
      v20 = v16[1];
      *(_OWORD *)&v60.flags = *v16;
      *(_OWORD *)&v60.size[1] = v20;
      v21 = v16[3];
      *(_OWORD *)&v60.size[3] = v16[2];
      *(_OWORD *)&v60.size[5] = v21;
      v22 = (_OWORD *)swift_slowAlloc();
      v23 = *(_OWORD *)&v60.data_type;
      v22[8] = *(_OWORD *)&v60.stride[7];
      v22[9] = v23;
      v22[10] = *(_OWORD *)&v60.table_data_type;
      v24 = *(_OWORD *)&v60.stride[1];
      v22[4] = *(_OWORD *)&v60.size[7];
      v22[5] = v24;
      v25 = *(_OWORD *)&v60.stride[5];
      v22[6] = *(_OWORD *)&v60.stride[3];
      v22[7] = v25;
      v26 = *(_OWORD *)&v60.size[1];
      *v22 = *(_OWORD *)&v60.flags;
      v22[1] = v26;
      v27 = *(_OWORD *)&v60.size[5];
      v22[2] = *(_OWORD *)&v60.size[3];
      v22[3] = v27;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(_QWORD *)(v15 + 16) + 1, 1);
        v15 = *(_QWORD *)&v59.flags;
      }
      v29 = *(_QWORD *)(v15 + 16);
      v28 = *(_QWORD *)(v15 + 24);
      if (v29 >= v28 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v28 > 1), v29 + 1, 1);
        v15 = *(_QWORD *)&v59.flags;
      }
      *(_QWORD *)(v15 + 16) = v29 + 1;
      *(_QWORD *)(v15 + 8 * v29 + 32) = v22;
      v16 += 11;
      --v14;
    }
    while (v14);
    a4 = v56;
    a2 = v54;
    a3 = v55;
    v13 = a1;
  }
  v30 = a6[8];
  v31 = a6[9];
  v32 = a6[6];
  *(_OWORD *)&v60.stride[5] = a6[7];
  *(_OWORD *)&v60.stride[7] = v30;
  v33 = a6[10];
  *(_OWORD *)&v60.data_type = v31;
  *(_OWORD *)&v60.table_data_type = v33;
  v34 = a6[4];
  v35 = a6[5];
  v36 = a6[2];
  *(_OWORD *)&v60.size[5] = a6[3];
  *(_OWORD *)&v60.size[7] = v34;
  v58 = (char *)v15;
  *(_OWORD *)&v60.stride[1] = v35;
  *(_OWORD *)&v60.stride[3] = v32;
  v37 = *a6;
  *(_OWORD *)&v60.size[1] = a6[1];
  *(_OWORD *)&v60.size[3] = v36;
  v38 = a5[9];
  *(_OWORD *)&v59.stride[7] = a5[8];
  *(_OWORD *)&v59.data_type = v38;
  *(_OWORD *)&v59.table_data_type = a5[10];
  *(_OWORD *)&v60.flags = v37;
  v39 = a5[5];
  *(_OWORD *)&v59.size[7] = a5[4];
  *(_OWORD *)&v59.stride[1] = v39;
  v40 = a5[7];
  *(_OWORD *)&v59.stride[3] = a5[6];
  *(_OWORD *)&v59.stride[5] = v40;
  v41 = a5[1];
  *(_OWORD *)&v59.flags = *a5;
  *(_OWORD *)&v59.size[1] = v41;
  v42 = a5[3];
  *(_OWORD *)&v59.size[3] = a5[2];
  *(_OWORD *)&v59.size[5] = v42;
  closure #1 in closure #2 in static BNNS.fusedLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingParameterGradients:)(&v59, v13, a2, a3, &v60, a4, &v57, &v58);
  if (v57)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v43 = 0;
    swift_willThrow();
    v44 = v58;
    v45 = *((_QWORD *)v58 + 2);
    if (v45)
    {
      swift_bridgeObjectRetain();
      for (i = 0; i != v45; ++i)
      {
        v47 = *(_QWORD *)&v44[8 * i + 32];
        if (v47)
          MEMORY[0x1D1794DA4](v47, -1, -1);
      }
LABEL_20:
      swift_bridgeObjectRelease();
    }
  }
  else
  {
    v48 = v58;
    v49 = *((_QWORD *)v58 + 2);
    if (v49)
    {
      swift_bridgeObjectRetain();
      for (j = 0; j != v49; ++j)
      {
        v51 = *(_QWORD *)&v48[8 * j + 32];
        if (v51)
          MEMORY[0x1D1794DA4](v51, -1, -1);
      }
      goto LABEL_20;
    }
  }
  return swift_bridgeObjectRelease();
}

void *outlined init with take of BNNS.NormalizationType(const void *a1, void *a2)
{
  __swift_memcpy361_8(a2, a1);
  return a2;
}

uint64_t _s10Accelerate4BNNSO17NormalizationTypeOWOg(uint64_t a1)
{
  return *(unsigned __int8 *)(a1 + 360) >> 6;
}

uint64_t type metadata accessor for BNNS.FusedConvolutionNormalizationLayer()
{
  return objc_opt_self();
}

uint64_t _s10Accelerate4BNNSO17NormalizationTypeOWOj0_(uint64_t result)
{
  *(_BYTE *)(result + 360) &= 0x3Fu;
  return result;
}

uint64_t type metadata accessor for BNNS.FusedFullyConnectedNormalizationLayer()
{
  return objc_opt_self();
}

unint64_t lazy protocol witness table accessor for type BNNS.LearningPhase and conformance BNNS.LearningPhase()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNS.LearningPhase and conformance BNNS.LearningPhase;
  if (!lazy protocol witness table cache variable for type BNNS.LearningPhase and conformance BNNS.LearningPhase)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNS.LearningPhase, &type metadata for BNNS.LearningPhase);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.LearningPhase and conformance BNNS.LearningPhase);
  }
  return result;
}

uint64_t storeEnumTagSinglePayload for BNNS.LearningPhase(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 1 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 1) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFF)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFE)
    return ((uint64_t (*)(void))((char *)&loc_1CAADFD8C + 4 * byte_1CAB5F81D[v4]))();
  *a1 = a2 + 1;
  return ((uint64_t (*)(void))((char *)sub_1CAADFDC0 + 4 * byte_1CAB5F818[v4]))();
}

uint64_t sub_1CAADFDC0(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAADFDC8(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAADFDD0);
  return result;
}

uint64_t sub_1CAADFDDC(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAADFDE4);
  *(_BYTE *)result = a2 + 1;
  return result;
}

uint64_t sub_1CAADFDE8(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAADFDF0(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.LearningPhase()
{
  return &type metadata for BNNS.LearningPhase;
}

uint64_t type metadata accessor for BNNS.FusedLayer()
{
  return objc_opt_self();
}

uint64_t method lookup function for BNNS.FusedLayer()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of BNNS.FusedLayer.apply(batchSize:input:output:for:)(uint64_t a1, uint64_t *a2, uint64_t *a3, char a4)
{
  uint64_t v4;
  uint64_t v5;
  int v6;
  uint64_t v7;
  int v8;
  uint64_t v9;
  int v10;
  uint64_t v11;
  int v12;
  uint64_t (*v13)(uint64_t, uint64_t *, uint64_t *, _QWORD);
  uint64_t v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  uint64_t v24;
  int v25;
  uint64_t v26;
  int v27;
  uint64_t v28;
  uint64_t v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  uint64_t v38;
  int v39;
  uint64_t v40;
  int v41;
  uint64_t v42;

  v5 = a2[17];
  v6 = *((_DWORD *)a2 + 36);
  v7 = a2[19];
  v8 = *((_DWORD *)a2 + 40);
  v9 = a3[17];
  v10 = *((_DWORD *)a3 + 36);
  v11 = a3[19];
  v12 = *((_DWORD *)a3 + 40);
  v13 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, _QWORD))(*(_QWORD *)v4 + 96);
  v29 = *a2;
  v30 = *(_OWORD *)(a2 + 1);
  v31 = *(_OWORD *)(a2 + 3);
  v32 = *(_OWORD *)(a2 + 5);
  v33 = *(_OWORD *)(a2 + 7);
  v34 = *(_OWORD *)(a2 + 9);
  v35 = *(_OWORD *)(a2 + 11);
  v36 = *(_OWORD *)(a2 + 13);
  v37 = *(_OWORD *)(a2 + 15);
  v38 = v5;
  v39 = v6;
  v40 = v7;
  v41 = v8;
  v42 = *(uint64_t *)((char *)a2 + 164);
  v15 = *a3;
  v16 = *(_OWORD *)(a3 + 1);
  v17 = *(_OWORD *)(a3 + 3);
  v18 = *(_OWORD *)(a3 + 5);
  v19 = *(_OWORD *)(a3 + 7);
  v20 = *(_OWORD *)(a3 + 9);
  v21 = *(_OWORD *)(a3 + 11);
  v22 = *(_OWORD *)(a3 + 13);
  v23 = *(_OWORD *)(a3 + 15);
  v24 = v9;
  v25 = v10;
  v26 = v11;
  v27 = v12;
  v28 = *(uint64_t *)((char *)a3 + 164);
  return v13(a1, &v29, &v15, a4 & 1);
}

uint64_t dispatch thunk of BNNS.FusedLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingParameterGradients:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5)
{
  uint64_t v5;
  uint64_t v6;
  int v7;
  uint64_t v8;
  int v9;
  uint64_t v10;
  int v11;
  uint64_t v12;
  int v13;
  uint64_t v14;
  int v15;
  uint64_t v16;
  int v17;
  uint64_t v18;
  int v19;
  uint64_t v20;
  int v21;
  uint64_t (*v22)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *);
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  uint64_t v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  uint64_t v40;
  __int128 v41;
  uint64_t v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  uint64_t v52;
  int v53;
  uint64_t v54;
  int v55;
  uint64_t v56;
  uint64_t v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  __int128 v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  __int128 v65;
  uint64_t v66;
  int v67;
  uint64_t v68;
  int v69;
  uint64_t v70;
  uint64_t v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  __int128 v76;
  __int128 v77;
  __int128 v78;
  __int128 v79;
  uint64_t v80;
  int v81;
  uint64_t v82;
  int v83;
  uint64_t v84;
  uint64_t v85;
  __int128 v86;
  __int128 v87;
  __int128 v88;
  __int128 v89;
  __int128 v90;
  __int128 v91;
  __int128 v92;
  __int128 v93;
  uint64_t v94;
  int v95;
  uint64_t v96;
  int v97;
  uint64_t v98;

  v6 = a2[17];
  v7 = *((_DWORD *)a2 + 36);
  v8 = a2[19];
  v9 = *((_DWORD *)a2 + 40);
  v10 = a3[17];
  v11 = *((_DWORD *)a3 + 36);
  v12 = a3[19];
  v13 = *((_DWORD *)a3 + 40);
  v14 = a4[17];
  v15 = *((_DWORD *)a4 + 36);
  v16 = a4[19];
  v17 = *((_DWORD *)a4 + 40);
  v18 = a5[17];
  v19 = *((_DWORD *)a5 + 36);
  v20 = a5[19];
  v21 = *((_DWORD *)a5 + 40);
  v22 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(_QWORD *)v5 + 104);
  v85 = *a2;
  v86 = *(_OWORD *)(a2 + 1);
  v87 = *(_OWORD *)(a2 + 3);
  v88 = *(_OWORD *)(a2 + 5);
  v89 = *(_OWORD *)(a2 + 7);
  v90 = *(_OWORD *)(a2 + 9);
  v91 = *(_OWORD *)(a2 + 11);
  v92 = *(_OWORD *)(a2 + 13);
  v93 = *(_OWORD *)(a2 + 15);
  v94 = v6;
  v95 = v7;
  v96 = v8;
  v97 = v9;
  v98 = *(uint64_t *)((char *)a2 + 164);
  v71 = *a3;
  v72 = *(_OWORD *)(a3 + 1);
  v73 = *(_OWORD *)(a3 + 3);
  v74 = *(_OWORD *)(a3 + 5);
  v75 = *(_OWORD *)(a3 + 7);
  v76 = *(_OWORD *)(a3 + 9);
  v77 = *(_OWORD *)(a3 + 11);
  v23 = *(_OWORD *)(a3 + 15);
  v78 = *(_OWORD *)(a3 + 13);
  v24 = *(_OWORD *)(a4 + 1);
  v25 = *(_OWORD *)(a4 + 3);
  v26 = *(_OWORD *)(a4 + 5);
  v27 = *(_OWORD *)(a4 + 7);
  v28 = *(_OWORD *)(a4 + 9);
  v29 = *(_OWORD *)(a4 + 11);
  v30 = *(_OWORD *)(a4 + 13);
  v31 = *a4;
  v32 = *(_OWORD *)(a4 + 15);
  v33 = *(_OWORD *)(a5 + 1);
  v34 = *(_OWORD *)(a5 + 3);
  v35 = *(_OWORD *)(a5 + 5);
  v36 = *(_OWORD *)(a5 + 7);
  v37 = *(_OWORD *)(a5 + 9);
  v38 = *(_OWORD *)(a5 + 11);
  v39 = *(_OWORD *)(a5 + 13);
  v40 = *a5;
  v41 = *(_OWORD *)(a5 + 15);
  v79 = v23;
  v80 = v10;
  v81 = v11;
  v82 = v12;
  v83 = v13;
  v84 = *(uint64_t *)((char *)a3 + 164);
  *(_QWORD *)&v23 = *(uint64_t *)((char *)a5 + 164);
  v57 = v31;
  v58 = v24;
  *(_QWORD *)&v24 = *(uint64_t *)((char *)a4 + 164);
  v59 = v25;
  v60 = v26;
  v61 = v27;
  v62 = v28;
  v63 = v29;
  v64 = v30;
  v65 = v32;
  v66 = v14;
  v67 = v15;
  v68 = v16;
  v69 = v17;
  v70 = v24;
  v43 = v40;
  v44 = v33;
  v45 = v34;
  v46 = v35;
  v47 = v36;
  v48 = v37;
  v49 = v38;
  v50 = v39;
  v51 = v41;
  v52 = v18;
  v53 = v19;
  v54 = v20;
  v55 = v21;
  v56 = v23;
  return v22(a1, &v85, &v71, &v57, &v43);
}

uint64_t vImage.PixelBuffer<>.flatten(channelOrdering:backgroundColor:isPremultiplied:destination:)(_BYTE *a1, char a2, char a3, char a4, char a5, char a6, uint64_t a7)
{
  uint64_t v7;
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  _QWORD *v11;
  uint64_t v12;
  uint64_t v13;
  vImage_Error (*v14)(const vImage_Buffer *, const vImage_Buffer *, const uint8_t *, char, vImage_Flags);
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  _BYTE v20[8];
  _QWORD v21[4];
  _QWORD v22[5];

  v22[4] = *MEMORY[0x1E0C80C00];
  v8 = *(_QWORD **)v7;
  if (!*(_QWORD *)(*(_QWORD *)v7 + 16))
  {
    __break(1u);
    goto LABEL_17;
  }
  v9 = v8[6];
  if (v9 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v10 = v8[5];
  if (v10 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v9)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v10)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v11 = *(_QWORD **)a7;
  if (!*(_QWORD *)(*(_QWORD *)a7 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v12 = v11[6];
  if (v12 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v13 = v11[5];
  if (v13 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v12)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v13)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v9 != v12)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v10 != v13)
    goto LABEL_27;
  v14 = @nonobjc vImageFlatten_ARGB8888ToRGB888(_:_:_:_:_:);
  v15 = v8[4];
  v16 = v8[7];
  if (*a1)
    v14 = @nonobjc vImageFlatten_RGBA8888ToRGB888(_:_:_:_:_:);
  v22[0] = v15;
  v22[1] = v10;
  v22[2] = v9;
  v22[3] = v16;
  v17 = v11[4];
  v18 = v11[7];
  v21[0] = v17;
  v21[1] = v10;
  v21[2] = v9;
  v21[3] = v18;
  v20[0] = a2;
  v20[1] = a3;
  v20[2] = a4;
  v20[3] = a5;
  return v14((const vImage_Buffer *)v22, (const vImage_Buffer *)v21, v20, a6 & 1, 0);
}

vImage_Error @nonobjc vImageFlatten_ARGB8888ToRGB888(_:_:_:_:_:)(const vImage_Buffer *a1, const vImage_Buffer *a2, const uint8_t *a3, char a4, vImage_Flags a5)
{
  return vImageFlatten_ARGB8888ToRGB888(a1, a2, a3, a4 & 1, a5);
}

vImage_Error @nonobjc vImageFlatten_RGBA8888ToRGB888(_:_:_:_:_:)(const vImage_Buffer *a1, const vImage_Buffer *a2, const uint8_t *a3, char a4, vImage_Flags a5)
{
  return vImageFlatten_RGBA8888ToRGB888(a1, a2, a3, a4 & 1, a5);
}

uint64_t vImage.PixelBuffer<>.flatten(channelOrdering:backgroundColor:isPremultiplied:destination:)(_BYTE *a1, char a2, uint64_t a3, float a4, float a5, float a6, float a7)
{
  uint64_t v7;
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  _QWORD *v11;
  uint64_t v12;
  uint64_t v13;
  vImage_Error (*v14)(const vImage_Buffer *, const vImage_Buffer *, const float *, char, vImage_Flags);
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  _DWORD v20[4];
  _QWORD v21[4];
  _QWORD v22[5];

  v22[4] = *MEMORY[0x1E0C80C00];
  v8 = *(_QWORD **)v7;
  if (!*(_QWORD *)(*(_QWORD *)v7 + 16))
  {
    __break(1u);
    goto LABEL_17;
  }
  v9 = v8[6];
  if (v9 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v10 = v8[5];
  if (v10 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v9)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v10)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v11 = *(_QWORD **)a3;
  if (!*(_QWORD *)(*(_QWORD *)a3 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v12 = v11[6];
  if (v12 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v13 = v11[5];
  if (v13 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v12)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v13)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v9 != v12)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v10 != v13)
    goto LABEL_27;
  v14 = @nonobjc vImageFlatten_ARGBFFFFToRGBFFF(_:_:_:_:_:);
  v15 = v8[4];
  v16 = v8[7];
  if (*a1)
    v14 = @nonobjc vImageFlatten_RGBAFFFFToRGBFFF(_:_:_:_:_:);
  v22[0] = v15;
  v22[1] = v10;
  v22[2] = v9;
  v22[3] = v16;
  v17 = v11[4];
  v18 = v11[7];
  v21[0] = v17;
  v21[1] = v10;
  v21[2] = v9;
  v21[3] = v18;
  *(float *)v20 = a4;
  *(float *)&v20[1] = a5;
  *(float *)&v20[2] = a6;
  *(float *)&v20[3] = a7;
  return v14((const vImage_Buffer *)v22, (const vImage_Buffer *)v21, (const float *)v20, a2 & 1, 0);
}

vImage_Error @nonobjc vImageFlatten_ARGBFFFFToRGBFFF(_:_:_:_:_:)(const vImage_Buffer *a1, const vImage_Buffer *a2, const float *a3, char a4, vImage_Flags a5)
{
  return vImageFlatten_ARGBFFFFToRGBFFF(a1, a2, a3, a4 & 1, a5);
}

vImage_Error @nonobjc vImageFlatten_RGBAFFFFToRGBFFF(_:_:_:_:_:)(const vImage_Buffer *a1, const vImage_Buffer *a2, const float *a3, char a4, vImage_Flags a5)
{
  return vImageFlatten_RGBAFFFFToRGBFFF(a1, a2, a3, a4 & 1, a5);
}

BOOL static vImage.ChannelOrdering.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void vImage.ChannelOrdering.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

Swift::Int vImage.ChannelOrdering.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

unint64_t lazy protocol witness table accessor for type vImage.ChannelOrdering and conformance vImage.ChannelOrdering()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImage.ChannelOrdering and conformance vImage.ChannelOrdering;
  if (!lazy protocol witness table cache variable for type vImage.ChannelOrdering and conformance vImage.ChannelOrdering)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImage.ChannelOrdering, &type metadata for vImage.ChannelOrdering);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.ChannelOrdering and conformance vImage.ChannelOrdering);
  }
  return result;
}

uint64_t storeEnumTagSinglePayload for vImage.ChannelOrdering(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 1 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 1) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFF)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFE)
    return ((uint64_t (*)(void))((char *)&loc_1CAAE0530 + 4 * byte_1CAB5F93D[v4]))();
  *a1 = a2 + 1;
  return ((uint64_t (*)(void))((char *)sub_1CAAE0564 + 4 * byte_1CAB5F938[v4]))();
}

uint64_t sub_1CAAE0564(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAAE056C(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAAE0574);
  return result;
}

uint64_t sub_1CAAE0580(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAAE0588);
  *(_BYTE *)result = a2 + 1;
  return result;
}

uint64_t sub_1CAAE058C(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAAE0594(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vImage.ChannelOrdering()
{
  return &type metadata for vImage.ChannelOrdering;
}

uint64_t static BNNS.tile(input:output:filterParameters:)(__int128 *a1, __int128 *a2, int a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  __int128 v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  uint64_t result;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  _BYTE *v27;
  int v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  __int128 v52;
  __int128 v53;
  uint64_t v54;

  v54 = *MEMORY[0x1E0C80C00];
  if (a5 != 1)
  {
    v28 = a3;
    v29 = a4;
    v30 = a5;
    v31 = a6;
    v17 = a1[9];
    v51 = a1[8];
    v52 = v17;
    v53 = a1[10];
    v18 = a1[5];
    v47 = a1[4];
    v48 = v18;
    v19 = a1[7];
    v49 = a1[6];
    v50 = v19;
    v20 = a1[1];
    v43 = *a1;
    v44 = v20;
    v21 = a1[3];
    v45 = a1[2];
    v46 = v21;
    v22 = a2[9];
    v40 = a2[8];
    v41 = v22;
    v42 = a2[10];
    v23 = a2[5];
    v36 = a2[4];
    v37 = v23;
    v24 = a2[7];
    v38 = a2[6];
    v39 = v24;
    v25 = a2[1];
    v32 = *a2;
    v33 = v25;
    v26 = a2[3];
    v34 = a2[2];
    v35 = v26;
    result = MEMORY[0x1D17948C4](&v43, &v32, &v28);
    if (!(_DWORD)result)
      return result;
    goto LABEL_5;
  }
  v6 = a1[9];
  v51 = a1[8];
  v52 = v6;
  v53 = a1[10];
  v7 = a1[5];
  v47 = a1[4];
  v48 = v7;
  v8 = a1[7];
  v49 = a1[6];
  v50 = v8;
  v9 = a1[1];
  v43 = *a1;
  v44 = v9;
  v10 = a1[3];
  v45 = a1[2];
  v46 = v10;
  v11 = a2[9];
  v40 = a2[8];
  v41 = v11;
  v42 = a2[10];
  v12 = a2[5];
  v36 = a2[4];
  v37 = v12;
  v13 = a2[7];
  v38 = a2[6];
  v39 = v13;
  v14 = a2[1];
  v32 = *a2;
  v33 = v14;
  v15 = a2[3];
  v34 = a2[2];
  v35 = v15;
  result = MEMORY[0x1D17948C4](&v43, &v32, 0);
  if ((_DWORD)result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v27 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.tileBackward(outputGradient:generatingInputGradient:filterParameters:)(__int128 *a1, __int128 *a2, int a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  __int128 v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  uint64_t result;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  _BYTE *v27;
  int v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  __int128 v52;
  __int128 v53;
  uint64_t v54;

  v54 = *MEMORY[0x1E0C80C00];
  if (a5 != 1)
  {
    v28 = a3;
    v29 = a4;
    v30 = a5;
    v31 = a6;
    v17 = a1[9];
    v51 = a1[8];
    v52 = v17;
    v53 = a1[10];
    v18 = a1[5];
    v47 = a1[4];
    v48 = v18;
    v19 = a1[7];
    v49 = a1[6];
    v50 = v19;
    v20 = a1[1];
    v43 = *a1;
    v44 = v20;
    v21 = a1[3];
    v45 = a1[2];
    v46 = v21;
    v22 = a2[9];
    v40 = a2[8];
    v41 = v22;
    v42 = a2[10];
    v23 = a2[5];
    v36 = a2[4];
    v37 = v23;
    v24 = a2[7];
    v38 = a2[6];
    v39 = v24;
    v25 = a2[1];
    v32 = *a2;
    v33 = v25;
    v26 = a2[3];
    v34 = a2[2];
    v35 = v26;
    result = MEMORY[0x1D17948D0](&v32, &v43, &v28);
    if (!(_DWORD)result)
      return result;
    goto LABEL_5;
  }
  v6 = a1[9];
  v51 = a1[8];
  v52 = v6;
  v53 = a1[10];
  v7 = a1[5];
  v47 = a1[4];
  v48 = v7;
  v8 = a1[7];
  v49 = a1[6];
  v50 = v8;
  v9 = a1[1];
  v43 = *a1;
  v44 = v9;
  v10 = a1[3];
  v45 = a1[2];
  v46 = v10;
  v11 = a2[9];
  v40 = a2[8];
  v41 = v11;
  v42 = a2[10];
  v12 = a2[5];
  v36 = a2[4];
  v37 = v12;
  v13 = a2[7];
  v38 = a2[6];
  v39 = v13;
  v14 = a2[1];
  v32 = *a2;
  v33 = v14;
  v15 = a2[3];
  v34 = a2[2];
  v35 = v15;
  result = MEMORY[0x1D17948D0](&v32, &v43, 0);
  if ((_DWORD)result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v27 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.copyBandPart(_:to:lowerBandCount:upperBandCount:filterParameters:)(_OWORD *a1, _OWORD *a2, uint64_t a3, char a4, uint64_t a5, char a6, uint32_t a7, size_t a8, int (__cdecl *a9)(void **, size_t, size_t), void (__cdecl *a10)(void *))
{
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  uint64_t result;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  _BYTE *v35;
  BNNSFilterParameters v36;
  BNNSNDArrayDescriptor output;
  BNNSNDArrayDescriptor input;
  uint64_t v39;

  v39 = *MEMORY[0x1E0C80C00];
  if ((a4 & 1) != 0)
    a3 = -1;
  if (a3 < (uint64_t)0xFFFFFFFF80000000)
  {
    __break(1u);
    goto LABEL_18;
  }
  if (a3 > 0x7FFFFFFF)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
LABEL_20:
    __break(1u);
  }
  if ((a6 & 1) != 0)
    a5 = -1;
  if (a5 < (uint64_t)0xFFFFFFFF80000000)
    goto LABEL_19;
  if (a5 > 0x7FFFFFFF)
    goto LABEL_20;
  if (a9 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    v14 = a1[9];
    *(_OWORD *)&input.stride[7] = a1[8];
    *(_OWORD *)&input.data_type = v14;
    *(_OWORD *)&input.table_data_type = a1[10];
    v15 = a1[5];
    *(_OWORD *)&input.size[7] = a1[4];
    *(_OWORD *)&input.stride[1] = v15;
    v16 = a1[7];
    *(_OWORD *)&input.stride[3] = a1[6];
    *(_OWORD *)&input.stride[5] = v16;
    v17 = a1[1];
    *(_OWORD *)&input.flags = *a1;
    *(_OWORD *)&input.size[1] = v17;
    v18 = a1[3];
    *(_OWORD *)&input.size[3] = a1[2];
    *(_OWORD *)&input.size[5] = v18;
    v19 = a2[9];
    *(_OWORD *)&output.stride[7] = a2[8];
    *(_OWORD *)&output.data_type = v19;
    *(_OWORD *)&output.table_data_type = a2[10];
    v20 = a2[5];
    *(_OWORD *)&output.size[7] = a2[4];
    *(_OWORD *)&output.stride[1] = v20;
    v21 = a2[7];
    *(_OWORD *)&output.stride[3] = a2[6];
    *(_OWORD *)&output.stride[5] = v21;
    v22 = a2[1];
    *(_OWORD *)&output.flags = *a2;
    *(_OWORD *)&output.size[1] = v22;
    v23 = a2[3];
    *(_OWORD *)&output.size[3] = a2[2];
    *(_OWORD *)&output.size[5] = v23;
    result = BNNSBandPart(a3, a5, &input, &output, 0);
    if (!(_DWORD)result)
      return result;
  }
  else
  {
    v36.flags = a7;
    v36.n_threads = a8;
    v36.alloc_memory = a9;
    v36.free_memory = a10;
    v25 = a1[9];
    *(_OWORD *)&input.stride[7] = a1[8];
    *(_OWORD *)&input.data_type = v25;
    *(_OWORD *)&input.table_data_type = a1[10];
    v26 = a1[5];
    *(_OWORD *)&input.size[7] = a1[4];
    *(_OWORD *)&input.stride[1] = v26;
    v27 = a1[7];
    *(_OWORD *)&input.stride[3] = a1[6];
    *(_OWORD *)&input.stride[5] = v27;
    v28 = a1[1];
    *(_OWORD *)&input.flags = *a1;
    *(_OWORD *)&input.size[1] = v28;
    v29 = a1[3];
    *(_OWORD *)&input.size[3] = a1[2];
    *(_OWORD *)&input.size[5] = v29;
    v30 = a2[9];
    *(_OWORD *)&output.stride[7] = a2[8];
    *(_OWORD *)&output.data_type = v30;
    *(_OWORD *)&output.table_data_type = a2[10];
    v31 = a2[5];
    *(_OWORD *)&output.size[7] = a2[4];
    *(_OWORD *)&output.stride[1] = v31;
    v32 = a2[7];
    *(_OWORD *)&output.stride[3] = a2[6];
    *(_OWORD *)&output.stride[5] = v32;
    v33 = a2[1];
    *(_OWORD *)&output.flags = *a2;
    *(_OWORD *)&output.size[1] = v33;
    v34 = a2[3];
    *(_OWORD *)&output.size[3] = a2[2];
    *(_OWORD *)&output.size[5] = v34;
    result = BNNSBandPart(a3, a5, &input, &output, &v36);
    if (!(_DWORD)result)
      return result;
  }
  lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
  swift_allocError();
  *v35 = 0;
  return swift_willThrow();
}

BOOL static BNNS.CropResizeLayer.BoxCoordinateMode.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void BNNS.CropResizeLayer.BoxCoordinateMode.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

uint64_t BNNS.CropResizeLayer.__allocating_init(coordinatesAreNormalized:spatialScale:extrapolationValue:samplingMode:boxCoordinateMode:)(char a1, unsigned __int8 *a2, unsigned __int8 *a3, float a4, float a5)
{
  uint64_t result;
  int v11;
  int v12;

  result = swift_allocObject();
  v11 = *a2;
  v12 = *a3;
  *(_BYTE *)(result + 16) = a1;
  *(float *)(result + 20) = a4;
  *(float *)(result + 24) = a5;
  *(_DWORD *)(result + 28) = v11;
  *(_DWORD *)(result + 32) = v12;
  *(_DWORD *)(result + 36) = 1;
  return result;
}

uint64_t BNNS.CropResizeLayer.init(coordinatesAreNormalized:spatialScale:extrapolationValue:samplingMode:boxCoordinateMode:)(char a1, unsigned __int8 *a2, unsigned __int8 *a3, float a4, float a5)
{
  uint64_t v5;
  int v6;
  int v7;

  v6 = *a2;
  v7 = *a3;
  *(_BYTE *)(v5 + 16) = a1;
  *(float *)(v5 + 20) = a4;
  *(float *)(v5 + 24) = a5;
  *(_DWORD *)(v5 + 28) = v6;
  *(_DWORD *)(v5 + 32) = v7;
  *(_DWORD *)(v5 + 36) = 1;
  return v5;
}

uint64_t BNNS.CropResizeLayer.apply(input:regionOfInterest:output:filterParameters:)(__int128 *a1, __int128 *a2, __int128 *a3, int a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  int v27;
  __int128 v28;
  __int128 v29;
  uint64_t result;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  int v50;
  __int128 v51;
  __int128 v52;
  _BYTE *v53;
  _BYTE v54[4];
  uint64_t v55;
  uint64_t v56;
  int v57;
  int v58;
  _BYTE v59[20];
  uint64_t v60;
  __int128 v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  __int128 v65;
  __int128 v66;
  __int128 v67;
  __int128 v68;
  __int128 v69;
  __int128 v70;
  __int128 v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  __int128 v76;
  __int128 v77;
  __int128 v78;
  __int128 v79;
  __int128 v80;
  __int128 v81;
  __int128 v82;
  __int128 v83;
  __int128 v84;
  __int128 v85;
  __int128 v86;
  __int128 v87;
  __int128 v88;
  __int128 v89;
  __int128 v90;
  __int128 v91;
  __int128 v92;
  __int128 v93;
  uint64_t v94;

  v94 = *MEMORY[0x1E0C80C00];
  if (a6 != 1)
  {
    v55 = *(_QWORD *)(v7 + 20);
    v56 = *(_QWORD *)(v7 + 28);
    v31 = a1[8];
    v32 = a1[9];
    v33 = a1[6];
    v90 = a1[7];
    v91 = v31;
    v34 = a1[10];
    v92 = v32;
    v93 = v34;
    v35 = a1[4];
    v36 = a1[5];
    v37 = a1[2];
    v86 = a1[3];
    v87 = v35;
    v88 = v36;
    v89 = v33;
    v38 = *a1;
    v84 = a1[1];
    v85 = v37;
    v39 = a2[9];
    v80 = a2[8];
    v81 = v39;
    v82 = a2[10];
    v83 = v38;
    v40 = a2[5];
    v76 = a2[4];
    v77 = v40;
    v41 = a2[7];
    v78 = a2[6];
    v79 = v41;
    v42 = a2[1];
    v72 = *a2;
    v73 = v42;
    v43 = a2[3];
    v74 = a2[2];
    v75 = v43;
    v44 = a3[8];
    v45 = a3[9];
    v46 = a3[6];
    v68 = a3[7];
    v69 = v44;
    v47 = a3[10];
    v70 = v45;
    v71 = v47;
    v49 = a3[4];
    v48 = a3[5];
    *(_QWORD *)&v59[4] = a5;
    *(_QWORD *)&v59[12] = a6;
    v60 = a7;
    v50 = *(_DWORD *)(v7 + 36);
    v54[0] = *(_BYTE *)(v7 + 16);
    v57 = v50;
    v58 = a4;
    v66 = v48;
    v67 = v46;
    v51 = a3[1];
    v61 = *a3;
    v62 = v51;
    v52 = a3[2];
    v64 = a3[3];
    v65 = v49;
    v63 = v52;
    result = MEMORY[0x1D17944F8](v54, &v83, &v72, &v61, &v58);
    if (!(_DWORD)result)
      return result;
    goto LABEL_5;
  }
  *(_QWORD *)v59 = *(_QWORD *)(v7 + 20);
  *(_QWORD *)&v59[8] = *(_QWORD *)(v7 + 28);
  v8 = a1[8];
  v9 = a1[9];
  v10 = a1[6];
  v90 = a1[7];
  v91 = v8;
  v11 = a1[10];
  v92 = v9;
  v93 = v11;
  v12 = a1[4];
  v13 = a1[5];
  v14 = a1[2];
  v86 = a1[3];
  v87 = v12;
  v88 = v13;
  v89 = v10;
  v15 = *a1;
  v84 = a1[1];
  v85 = v14;
  v16 = a2[9];
  v80 = a2[8];
  v81 = v16;
  v82 = a2[10];
  v83 = v15;
  v17 = a2[5];
  v76 = a2[4];
  v77 = v17;
  v18 = a2[7];
  v78 = a2[6];
  v79 = v18;
  v19 = a2[1];
  v72 = *a2;
  v73 = v19;
  v20 = a2[3];
  v74 = a2[2];
  v75 = v20;
  v21 = a3[8];
  v22 = a3[9];
  v23 = a3[6];
  v68 = a3[7];
  v69 = v21;
  v24 = a3[10];
  v70 = v22;
  v71 = v24;
  v26 = a3[4];
  v25 = a3[5];
  v27 = *(_DWORD *)(v7 + 36);
  LOBYTE(v58) = *(_BYTE *)(v7 + 16);
  *(_DWORD *)&v59[16] = v27;
  v66 = v25;
  v67 = v23;
  v28 = a3[1];
  v61 = *a3;
  v62 = v28;
  v29 = a3[2];
  v64 = a3[3];
  v65 = v26;
  v63 = v29;
  result = MEMORY[0x1D17944F8](&v58, &v83, &v72, &v61, 0);
  if ((_DWORD)result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v53 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t BNNS.CropResizeLayer.applyBackward(regionOfInterest:outputGradient:generatingInputGradient:filterParameters:)(__int128 *a1, __int128 *a2, __int128 *a3, int a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  int v27;
  __int128 v28;
  __int128 v29;
  uint64_t result;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  int v50;
  __int128 v51;
  __int128 v52;
  _BYTE *v53;
  _BYTE v54[4];
  uint64_t v55;
  uint64_t v56;
  int v57;
  int v58;
  _BYTE v59[20];
  uint64_t v60;
  __int128 v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  __int128 v65;
  __int128 v66;
  __int128 v67;
  __int128 v68;
  __int128 v69;
  __int128 v70;
  __int128 v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  __int128 v76;
  __int128 v77;
  __int128 v78;
  __int128 v79;
  __int128 v80;
  __int128 v81;
  __int128 v82;
  __int128 v83;
  __int128 v84;
  __int128 v85;
  __int128 v86;
  __int128 v87;
  __int128 v88;
  __int128 v89;
  __int128 v90;
  __int128 v91;
  __int128 v92;
  __int128 v93;
  uint64_t v94;

  v94 = *MEMORY[0x1E0C80C00];
  if (a6 != 1)
  {
    v55 = *(_QWORD *)(v7 + 20);
    v56 = *(_QWORD *)(v7 + 28);
    v31 = a3[8];
    v32 = a3[9];
    v33 = a3[6];
    v90 = a3[7];
    v91 = v31;
    v34 = a3[10];
    v92 = v32;
    v93 = v34;
    v35 = a3[4];
    v36 = a3[5];
    v37 = a3[2];
    v86 = a3[3];
    v87 = v35;
    v88 = v36;
    v89 = v33;
    v38 = *a3;
    v84 = a3[1];
    v85 = v37;
    v39 = a1[9];
    v80 = a1[8];
    v81 = v39;
    v82 = a1[10];
    v83 = v38;
    v40 = a1[5];
    v76 = a1[4];
    v77 = v40;
    v41 = a1[7];
    v78 = a1[6];
    v79 = v41;
    v42 = a1[1];
    v72 = *a1;
    v73 = v42;
    v43 = a1[3];
    v74 = a1[2];
    v75 = v43;
    v44 = a2[8];
    v45 = a2[9];
    v46 = a2[6];
    v68 = a2[7];
    v69 = v44;
    v47 = a2[10];
    v70 = v45;
    v71 = v47;
    v49 = a2[4];
    v48 = a2[5];
    *(_QWORD *)&v59[4] = a5;
    *(_QWORD *)&v59[12] = a6;
    v60 = a7;
    v50 = *(_DWORD *)(v7 + 36);
    v54[0] = *(_BYTE *)(v7 + 16);
    v57 = v50;
    v58 = a4;
    v66 = v48;
    v67 = v46;
    v51 = a2[1];
    v61 = *a2;
    v62 = v51;
    v52 = a2[2];
    v64 = a2[3];
    v65 = v49;
    v63 = v52;
    result = MEMORY[0x1D1794504](v54, &v83, &v72, &v61, &v58);
    if (!(_DWORD)result)
      return result;
    goto LABEL_5;
  }
  *(_QWORD *)v59 = *(_QWORD *)(v7 + 20);
  *(_QWORD *)&v59[8] = *(_QWORD *)(v7 + 28);
  v8 = a3[8];
  v9 = a3[9];
  v10 = a3[6];
  v90 = a3[7];
  v91 = v8;
  v11 = a3[10];
  v92 = v9;
  v93 = v11;
  v12 = a3[4];
  v13 = a3[5];
  v14 = a3[2];
  v86 = a3[3];
  v87 = v12;
  v88 = v13;
  v89 = v10;
  v15 = *a3;
  v84 = a3[1];
  v85 = v14;
  v16 = a1[9];
  v80 = a1[8];
  v81 = v16;
  v82 = a1[10];
  v83 = v15;
  v17 = a1[5];
  v76 = a1[4];
  v77 = v17;
  v18 = a1[7];
  v78 = a1[6];
  v79 = v18;
  v19 = a1[1];
  v72 = *a1;
  v73 = v19;
  v20 = a1[3];
  v74 = a1[2];
  v75 = v20;
  v21 = a2[8];
  v22 = a2[9];
  v23 = a2[6];
  v68 = a2[7];
  v69 = v21;
  v24 = a2[10];
  v70 = v22;
  v71 = v24;
  v26 = a2[4];
  v25 = a2[5];
  v27 = *(_DWORD *)(v7 + 36);
  LOBYTE(v58) = *(_BYTE *)(v7 + 16);
  *(_DWORD *)&v59[16] = v27;
  v66 = v25;
  v67 = v23;
  v28 = a2[1];
  v61 = *a2;
  v62 = v28;
  v29 = a2[2];
  v64 = a2[3];
  v65 = v26;
  v63 = v29;
  result = MEMORY[0x1D1794504](&v58, &v83, &v72, &v61, 0);
  if ((_DWORD)result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v53 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t BNNS.CropResizeLayer.deinit()
{
  uint64_t v0;

  return v0;
}

uint64_t BNNS.CropResizeLayer.__deallocating_deinit()
{
  return swift_deallocClassInstance();
}

BOOL static BNNS.ShuffleType.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void BNNS.ShuffleType.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

Swift::Int BNNS.ShuffleType.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t static BNNS.shuffle(_:input:output:filterParameters:)(unsigned __int8 *a1, _OWORD *a2, _OWORD *a3, uint32_t a4, size_t a5, int (__cdecl *a6)(void **, size_t, size_t), void (__cdecl *a7)(void *))
{
  BNNSShuffleType v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  uint64_t result;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  _BYTE *v29;
  BNNSFilterParameters v30;
  BNNSNDArrayDescriptor output;
  BNNSNDArrayDescriptor input;
  uint64_t v33;

  v33 = *MEMORY[0x1E0C80C00];
  v7 = *a1;
  if (a6 != (int (__cdecl *)(void **, size_t, size_t))1)
  {
    v30.flags = a4;
    v30.n_threads = a5;
    v30.alloc_memory = a6;
    v30.free_memory = a7;
    v19 = a2[9];
    *(_OWORD *)&input.stride[7] = a2[8];
    *(_OWORD *)&input.data_type = v19;
    *(_OWORD *)&input.table_data_type = a2[10];
    v20 = a2[5];
    *(_OWORD *)&input.size[7] = a2[4];
    *(_OWORD *)&input.stride[1] = v20;
    v21 = a2[7];
    *(_OWORD *)&input.stride[3] = a2[6];
    *(_OWORD *)&input.stride[5] = v21;
    v22 = a2[1];
    *(_OWORD *)&input.flags = *a2;
    *(_OWORD *)&input.size[1] = v22;
    v23 = a2[3];
    *(_OWORD *)&input.size[3] = a2[2];
    *(_OWORD *)&input.size[5] = v23;
    v24 = a3[9];
    *(_OWORD *)&output.stride[7] = a3[8];
    *(_OWORD *)&output.data_type = v24;
    *(_OWORD *)&output.table_data_type = a3[10];
    v25 = a3[5];
    *(_OWORD *)&output.size[7] = a3[4];
    *(_OWORD *)&output.stride[1] = v25;
    v26 = a3[7];
    *(_OWORD *)&output.stride[3] = a3[6];
    *(_OWORD *)&output.stride[5] = v26;
    v27 = a3[1];
    *(_OWORD *)&output.flags = *a3;
    *(_OWORD *)&output.size[1] = v27;
    v28 = a3[3];
    *(_OWORD *)&output.size[3] = a3[2];
    *(_OWORD *)&output.size[5] = v28;
    result = BNNSShuffle(v7, &input, &output, &v30);
    if (!(_DWORD)result)
      return result;
    goto LABEL_5;
  }
  v8 = a2[9];
  *(_OWORD *)&input.stride[7] = a2[8];
  *(_OWORD *)&input.data_type = v8;
  *(_OWORD *)&input.table_data_type = a2[10];
  v9 = a2[5];
  *(_OWORD *)&input.size[7] = a2[4];
  *(_OWORD *)&input.stride[1] = v9;
  v10 = a2[7];
  *(_OWORD *)&input.stride[3] = a2[6];
  *(_OWORD *)&input.stride[5] = v10;
  v11 = a2[1];
  *(_OWORD *)&input.flags = *a2;
  *(_OWORD *)&input.size[1] = v11;
  v12 = a2[3];
  *(_OWORD *)&input.size[3] = a2[2];
  *(_OWORD *)&input.size[5] = v12;
  v13 = a3[9];
  *(_OWORD *)&output.stride[7] = a3[8];
  *(_OWORD *)&output.data_type = v13;
  *(_OWORD *)&output.table_data_type = a3[10];
  v14 = a3[5];
  *(_OWORD *)&output.size[7] = a3[4];
  *(_OWORD *)&output.stride[1] = v14;
  v15 = a3[7];
  *(_OWORD *)&output.stride[3] = a3[6];
  *(_OWORD *)&output.stride[5] = v15;
  v16 = a3[1];
  *(_OWORD *)&output.flags = *a3;
  *(_OWORD *)&output.size[1] = v16;
  v17 = a3[3];
  *(_OWORD *)&output.size[3] = a3[2];
  *(_OWORD *)&output.size[5] = v17;
  result = BNNSShuffle(v7, &input, &output, 0);
  if ((_DWORD)result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v29 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t BNNSNDArrayDescriptor.dataSize.getter()
{
  _OWORD *v0;
  __int128 v1;
  __int128 v2;
  __int128 v3;
  __int128 v4;
  __int128 v5;
  _OWORD v7[11];
  uint64_t v8;

  v8 = *MEMORY[0x1E0C80C00];
  v1 = v0[9];
  v7[8] = v0[8];
  v7[9] = v1;
  v7[10] = v0[10];
  v2 = v0[5];
  v7[4] = v0[4];
  v7[5] = v2;
  v3 = v0[7];
  v7[6] = v0[6];
  v7[7] = v3;
  v4 = v0[1];
  v7[0] = *v0;
  v7[1] = v4;
  v5 = v0[3];
  v7[2] = v0[2];
  v7[3] = v5;
  return MEMORY[0x1D17947E0](v7);
}

uint64_t static BNNS.gather(input:indices:output:axis:filterParameters:)(_OWORD *a1, _OWORD *a2, _OWORD *a3, uint64_t a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  uint64_t result;
  _BYTE *v24;
  int v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  _OWORD v29[11];
  _OWORD v30[11];
  _OWORD v31[11];
  uint64_t v32;

  v32 = *MEMORY[0x1E0C80C00];
  v8 = a1[9];
  v31[8] = a1[8];
  v31[9] = v8;
  v31[10] = a1[10];
  v9 = a1[5];
  v31[4] = a1[4];
  v31[5] = v9;
  v10 = a1[7];
  v31[6] = a1[6];
  v31[7] = v10;
  v11 = a1[1];
  v31[0] = *a1;
  v31[1] = v11;
  v12 = a1[3];
  v31[2] = a1[2];
  v31[3] = v12;
  v13 = a2[9];
  v30[8] = a2[8];
  v30[9] = v13;
  v30[10] = a2[10];
  v14 = a2[5];
  v30[4] = a2[4];
  v30[5] = v14;
  v15 = a2[7];
  v30[6] = a2[6];
  v30[7] = v15;
  v16 = a2[1];
  v30[0] = *a2;
  v30[1] = v16;
  v17 = a2[3];
  v30[2] = a2[2];
  v30[3] = v17;
  v18 = a3[9];
  v29[8] = a3[8];
  v29[9] = v18;
  v29[10] = a3[10];
  v19 = a3[5];
  v29[4] = a3[4];
  v29[5] = v19;
  v20 = a3[7];
  v29[6] = a3[6];
  v29[7] = v20;
  v21 = a3[1];
  v29[0] = *a3;
  v29[1] = v21;
  v22 = a3[3];
  v29[2] = a3[2];
  v29[3] = v22;
  if (a7 != 1)
  {
    v25 = a5;
    v26 = a6;
    v27 = a7;
    v28 = a8;
    result = MEMORY[0x1D1794684](a4, v31, v30, v29, &v25);
    if (!(_DWORD)result)
      return result;
    goto LABEL_5;
  }
  result = MEMORY[0x1D1794684](a4, v31, v30, v29, 0);
  if ((_DWORD)result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v24 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.gatherND(input:indices:output:filterParameters:)(_OWORD *a1, _OWORD *a2, _OWORD *a3, int a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  uint64_t result;
  _BYTE *v23;
  int v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  _OWORD v28[11];
  _OWORD v29[11];
  _OWORD v30[11];
  uint64_t v31;

  v31 = *MEMORY[0x1E0C80C00];
  v7 = a1[9];
  v30[8] = a1[8];
  v30[9] = v7;
  v30[10] = a1[10];
  v8 = a1[5];
  v30[4] = a1[4];
  v30[5] = v8;
  v9 = a1[7];
  v30[6] = a1[6];
  v30[7] = v9;
  v10 = a1[1];
  v30[0] = *a1;
  v30[1] = v10;
  v11 = a1[3];
  v30[2] = a1[2];
  v30[3] = v11;
  v12 = a2[9];
  v29[8] = a2[8];
  v29[9] = v12;
  v29[10] = a2[10];
  v13 = a2[5];
  v29[4] = a2[4];
  v29[5] = v13;
  v14 = a2[7];
  v29[6] = a2[6];
  v29[7] = v14;
  v15 = a2[1];
  v29[0] = *a2;
  v29[1] = v15;
  v16 = a2[3];
  v29[2] = a2[2];
  v29[3] = v16;
  v17 = a3[9];
  v28[8] = a3[8];
  v28[9] = v17;
  v28[10] = a3[10];
  v18 = a3[5];
  v28[4] = a3[4];
  v28[5] = v18;
  v19 = a3[7];
  v28[6] = a3[6];
  v28[7] = v19;
  v20 = a3[1];
  v28[0] = *a3;
  v28[1] = v20;
  v21 = a3[3];
  v28[2] = a3[2];
  v28[3] = v21;
  if (a6 != 1)
  {
    v24 = a4;
    v25 = a5;
    v26 = a6;
    v27 = a7;
    result = MEMORY[0x1D1794690](v30, v29, v28, &v24);
    if (!(_DWORD)result)
      return result;
    goto LABEL_5;
  }
  result = MEMORY[0x1D1794690](v30, v29, v28, 0);
  if ((_DWORD)result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v23 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.scatter(input:indices:output:axis:reductionFunction:filterParameters:)(_OWORD *a1, _OWORD *a2, _OWORD *a3, uint64_t a4, int *a5, int a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  uint64_t v9;
  int v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t result;
  _BYTE *v29;
  int v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  _OWORD v34[11];
  _OWORD v35[11];
  _OWORD v36[11];
  uint64_t v37;

  v37 = *MEMORY[0x1E0C80C00];
  v9 = *a5;
  v10 = *((unsigned __int8 *)a5 + 4);
  v11 = a1[9];
  v36[8] = a1[8];
  v36[9] = v11;
  v36[10] = a1[10];
  v12 = a1[5];
  v36[4] = a1[4];
  v36[5] = v12;
  v13 = a1[7];
  v36[6] = a1[6];
  v36[7] = v13;
  v14 = a1[1];
  v36[0] = *a1;
  v36[1] = v14;
  v15 = a1[3];
  v36[2] = a1[2];
  v36[3] = v15;
  v16 = a2[9];
  v35[8] = a2[8];
  v35[9] = v16;
  v35[10] = a2[10];
  v17 = a2[5];
  v35[4] = a2[4];
  v35[5] = v17;
  v18 = a2[7];
  v35[6] = a2[6];
  v35[7] = v18;
  v19 = a2[1];
  v35[0] = *a2;
  v35[1] = v19;
  v20 = a2[3];
  v35[2] = a2[2];
  v35[3] = v20;
  v21 = a3[9];
  v34[8] = a3[8];
  v34[9] = v21;
  v34[10] = a3[10];
  v22 = a3[5];
  v34[4] = a3[4];
  v34[5] = v22;
  v23 = a3[7];
  v34[6] = a3[6];
  v34[7] = v23;
  v24 = a3[1];
  v34[0] = *a3;
  v34[1] = v24;
  v25 = a3[3];
  v34[2] = a3[2];
  v34[3] = v25;
  if (a8 == 1)
  {
    if (v10)
      v26 = dword_1CAB5FBD0[v9];
    else
      v26 = 8;
    result = MEMORY[0x1D17948A0](a4, v26, v36, v35, v34, 0);
    if (!(_DWORD)result)
      return result;
LABEL_11:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v29 = 0;
    return swift_willThrow();
  }
  v30 = a6;
  v31 = a7;
  v32 = a8;
  v33 = a9;
  if (v10)
    v27 = dword_1CAB5FBD0[v9];
  else
    v27 = 8;
  result = MEMORY[0x1D17948A0](a4, v27, v36, v35, v34, &v30);
  if ((_DWORD)result)
    goto LABEL_11;
  return result;
}

uint64_t static BNNS.scatterND(input:indices:output:reductionFunction:filterParameters:)(_OWORD *a1, _OWORD *a2, _OWORD *a3, int *a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v8;
  int v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t result;
  _BYTE *v28;
  int v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  _OWORD v33[11];
  _OWORD v34[11];
  _OWORD v35[11];
  uint64_t v36;

  v36 = *MEMORY[0x1E0C80C00];
  v8 = *a4;
  v9 = *((unsigned __int8 *)a4 + 4);
  v10 = a1[9];
  v35[8] = a1[8];
  v35[9] = v10;
  v35[10] = a1[10];
  v11 = a1[5];
  v35[4] = a1[4];
  v35[5] = v11;
  v12 = a1[7];
  v35[6] = a1[6];
  v35[7] = v12;
  v13 = a1[1];
  v35[0] = *a1;
  v35[1] = v13;
  v14 = a1[3];
  v35[2] = a1[2];
  v35[3] = v14;
  v15 = a2[9];
  v34[8] = a2[8];
  v34[9] = v15;
  v34[10] = a2[10];
  v16 = a2[5];
  v34[4] = a2[4];
  v34[5] = v16;
  v17 = a2[7];
  v34[6] = a2[6];
  v34[7] = v17;
  v18 = a2[1];
  v34[0] = *a2;
  v34[1] = v18;
  v19 = a2[3];
  v34[2] = a2[2];
  v34[3] = v19;
  v20 = a3[9];
  v33[8] = a3[8];
  v33[9] = v20;
  v33[10] = a3[10];
  v21 = a3[5];
  v33[4] = a3[4];
  v33[5] = v21;
  v22 = a3[7];
  v33[6] = a3[6];
  v33[7] = v22;
  v23 = a3[1];
  v33[0] = *a3;
  v33[1] = v23;
  v24 = a3[3];
  v33[2] = a3[2];
  v33[3] = v24;
  if (a7 == 1)
  {
    if (v9)
      v25 = dword_1CAB5FBD0[v8];
    else
      v25 = 8;
    result = MEMORY[0x1D17948AC](v25, v35, v34, v33, 0);
    if (!(_DWORD)result)
      return result;
LABEL_11:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v28 = 0;
    return swift_willThrow();
  }
  v29 = a5;
  v30 = a6;
  v31 = a7;
  v32 = a8;
  if (v9)
    v26 = dword_1CAB5FBD0[v8];
  else
    v26 = 8;
  result = MEMORY[0x1D17948AC](v26, v35, v34, v33, &v29);
  if ((_DWORD)result)
    goto LABEL_11;
  return result;
}

uint64_t static BNNS.scatter(input:indices:output:axis:filterParameters:)(_OWORD *a1, _OWORD *a2, _OWORD *a3, uint64_t a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  uint64_t result;
  _BYTE *v24;
  int v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  _OWORD v29[11];
  _OWORD v30[11];
  _OWORD v31[11];
  uint64_t v32;

  v32 = *MEMORY[0x1E0C80C00];
  v8 = a1[9];
  v31[8] = a1[8];
  v31[9] = v8;
  v31[10] = a1[10];
  v9 = a1[5];
  v31[4] = a1[4];
  v31[5] = v9;
  v10 = a1[7];
  v31[6] = a1[6];
  v31[7] = v10;
  v11 = a1[1];
  v31[0] = *a1;
  v31[1] = v11;
  v12 = a1[3];
  v31[2] = a1[2];
  v31[3] = v12;
  v13 = a2[9];
  v30[8] = a2[8];
  v30[9] = v13;
  v30[10] = a2[10];
  v14 = a2[5];
  v30[4] = a2[4];
  v30[5] = v14;
  v15 = a2[7];
  v30[6] = a2[6];
  v30[7] = v15;
  v16 = a2[1];
  v30[0] = *a2;
  v30[1] = v16;
  v17 = a2[3];
  v30[2] = a2[2];
  v30[3] = v17;
  v18 = a3[9];
  v29[8] = a3[8];
  v29[9] = v18;
  v29[10] = a3[10];
  v19 = a3[5];
  v29[4] = a3[4];
  v29[5] = v19;
  v20 = a3[7];
  v29[6] = a3[6];
  v29[7] = v20;
  v21 = a3[1];
  v29[0] = *a3;
  v29[1] = v21;
  v22 = a3[3];
  v29[2] = a3[2];
  v29[3] = v22;
  if (a7 != 1)
  {
    v25 = a5;
    v26 = a6;
    v27 = a7;
    v28 = a8;
    result = MEMORY[0x1D17948A0](a4, 15, v31, v30, v29, &v25);
    if (!(_DWORD)result)
      return result;
    goto LABEL_5;
  }
  result = MEMORY[0x1D17948A0](a4, 15, v31, v30, v29, 0);
  if ((_DWORD)result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v24 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.scatterND(input:indices:output:filterParameters:)(_OWORD *a1, _OWORD *a2, _OWORD *a3, int a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  uint64_t result;
  _BYTE *v23;
  int v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  _OWORD v28[11];
  _OWORD v29[11];
  _OWORD v30[11];
  uint64_t v31;

  v31 = *MEMORY[0x1E0C80C00];
  v7 = a1[9];
  v30[8] = a1[8];
  v30[9] = v7;
  v30[10] = a1[10];
  v8 = a1[5];
  v30[4] = a1[4];
  v30[5] = v8;
  v9 = a1[7];
  v30[6] = a1[6];
  v30[7] = v9;
  v10 = a1[1];
  v30[0] = *a1;
  v30[1] = v10;
  v11 = a1[3];
  v30[2] = a1[2];
  v30[3] = v11;
  v12 = a2[9];
  v29[8] = a2[8];
  v29[9] = v12;
  v29[10] = a2[10];
  v13 = a2[5];
  v29[4] = a2[4];
  v29[5] = v13;
  v14 = a2[7];
  v29[6] = a2[6];
  v29[7] = v14;
  v15 = a2[1];
  v29[0] = *a2;
  v29[1] = v15;
  v16 = a2[3];
  v29[2] = a2[2];
  v29[3] = v16;
  v17 = a3[9];
  v28[8] = a3[8];
  v28[9] = v17;
  v28[10] = a3[10];
  v18 = a3[5];
  v28[4] = a3[4];
  v28[5] = v18;
  v19 = a3[7];
  v28[6] = a3[6];
  v28[7] = v19;
  v20 = a3[1];
  v28[0] = *a3;
  v28[1] = v20;
  v21 = a3[3];
  v28[2] = a3[2];
  v28[3] = v21;
  if (a6 != 1)
  {
    v24 = a4;
    v25 = a5;
    v26 = a6;
    v27 = a7;
    result = MEMORY[0x1D17948AC](15, v30, v29, v28, &v24);
    if (!(_DWORD)result)
      return result;
    goto LABEL_5;
  }
  result = MEMORY[0x1D17948AC](15, v30, v29, v28, 0);
  if ((_DWORD)result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v23 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNSNDArrayDescriptor.allocate<A>(randomNormalUsing:mean:standardDeviation:shape:batchSize:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X4>, uint64_t a5@<X5>, uint64_t a6@<X6>, uint64_t a7@<X8>)
{
  uint64_t v12;
  uint64_t v13;
  char *v14;
  uint64_t v15;
  void *v16;
  void (*v17)(char *, uint64_t, uint64_t);
  float v18;
  uint64_t v20;
  uint64_t v21;
  BNNSNDArrayDescriptor v22;
  BNNSNDArrayDescriptor v23;
  _BYTE v24[136];
  _DWORD v25[46];
  uint64_t v26;

  v20 = a3;
  v21 = a7;
  v26 = *MEMORY[0x1E0C80C00];
  v12 = *(_QWORD *)(a5 - 8);
  MEMORY[0x1E0C80A78](a1);
  v14 = (char *)&v20 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  outlined init with take of BNNS.Shape(v15, (uint64_t)v24);
  (*(void (**)(uint64_t, uint64_t))(a6 + 8))(a5, a6);
  helper #1 <A>(_:) in static BNNSNDArrayDescriptor.allocateUninitialized(scalarType:shape:batchSize:)(a4, (uint64_t)v24, &v23);
  v16 = *(void **)(a1 + 16);
  v17 = *(void (**)(char *, uint64_t, uint64_t))(v12 + 16);
  v17(v14, a2, a5);
  lazy protocol witness table accessor for type Float and conformance Float();
  BinaryFloatingPoint.init<A>(_:)();
  v18 = *(float *)v25;
  v17(v14, v20, a5);
  BinaryFloatingPoint.init<A>(_:)();
  if (BNNSRandomFillNormalFloat(v16, &v23, v18, *(float *)&v22.flags))
  {
    if (v23.data)
      MEMORY[0x1D1794DA4](v23.data, -1, -1);
    _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&v22);
  }
  else
  {
    v22 = v23;
    _sSo21BNNSNDArrayDescriptoraSgWOi_((uint64_t)&v22);
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v22, (uint64_t)v25);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v25, v21);
}

unint64_t lazy protocol witness table accessor for type Float and conformance Float()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type Float and conformance Float;
  if (!lazy protocol witness table cache variable for type Float and conformance Float)
  {
    result = MEMORY[0x1D1794D08](MEMORY[0x1E0DEB190], MEMORY[0x1E0DEB188]);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Float and conformance Float);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type BNNS.CropResizeLayer.BoxCoordinateMode and conformance BNNS.CropResizeLayer.BoxCoordinateMode()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNS.CropResizeLayer.BoxCoordinateMode and conformance BNNS.CropResizeLayer.BoxCoordinateMode;
  if (!lazy protocol witness table cache variable for type BNNS.CropResizeLayer.BoxCoordinateMode and conformance BNNS.CropResizeLayer.BoxCoordinateMode)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNS.CropResizeLayer.BoxCoordinateMode, &type metadata for BNNS.CropResizeLayer.BoxCoordinateMode);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.CropResizeLayer.BoxCoordinateMode and conformance BNNS.CropResizeLayer.BoxCoordinateMode);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type BNNS.CropResizeLayer.LinearSamplingMode and conformance BNNS.CropResizeLayer.LinearSamplingMode()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNS.CropResizeLayer.LinearSamplingMode and conformance BNNS.CropResizeLayer.LinearSamplingMode;
  if (!lazy protocol witness table cache variable for type BNNS.CropResizeLayer.LinearSamplingMode and conformance BNNS.CropResizeLayer.LinearSamplingMode)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNS.CropResizeLayer.LinearSamplingMode, &type metadata for BNNS.CropResizeLayer.LinearSamplingMode);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.CropResizeLayer.LinearSamplingMode and conformance BNNS.CropResizeLayer.LinearSamplingMode);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type BNNS.ShuffleType and conformance BNNS.ShuffleType()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNS.ShuffleType and conformance BNNS.ShuffleType;
  if (!lazy protocol witness table cache variable for type BNNS.ShuffleType and conformance BNNS.ShuffleType)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNS.ShuffleType, &type metadata for BNNS.ShuffleType);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.ShuffleType and conformance BNNS.ShuffleType);
  }
  return result;
}

uint64_t type metadata accessor for BNNS.CropResizeLayer()
{
  return objc_opt_self();
}

uint64_t method lookup function for BNNS.CropResizeLayer()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of BNNS.CropResizeLayer.__allocating_init(coordinatesAreNormalized:spatialScale:extrapolationValue:samplingMode:boxCoordinateMode:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(v0 + 88))();
}

uint64_t dispatch thunk of BNNS.CropResizeLayer.apply(input:regionOfInterest:output:filterParameters:)(uint64_t *a1, uint64_t *a2, uint64_t *a3)
{
  uint64_t v3;
  uint64_t v4;
  int v5;
  uint64_t v6;
  int v7;
  uint64_t v8;
  int v9;
  uint64_t v10;
  int v11;
  uint64_t v12;
  int v13;
  uint64_t v14;
  int v15;
  uint64_t (*v16)(uint64_t *, uint64_t *, uint64_t *);
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  uint64_t v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  uint64_t v32;
  int v33;
  uint64_t v34;
  int v35;
  uint64_t v36;
  uint64_t v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  uint64_t v46;
  int v47;
  uint64_t v48;
  int v49;
  uint64_t v50;
  uint64_t v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  uint64_t v60;
  int v61;
  uint64_t v62;
  int v63;
  uint64_t v64;

  v4 = a1[17];
  v5 = *((_DWORD *)a1 + 36);
  v6 = a1[19];
  v7 = *((_DWORD *)a1 + 40);
  v8 = a2[17];
  v9 = *((_DWORD *)a2 + 36);
  v10 = a2[19];
  v11 = *((_DWORD *)a2 + 40);
  v12 = a3[17];
  v13 = *((_DWORD *)a3 + 36);
  v14 = a3[19];
  v15 = *((_DWORD *)a3 + 40);
  v16 = *(uint64_t (**)(uint64_t *, uint64_t *, uint64_t *))(*(_QWORD *)v3 + 96);
  v51 = *a1;
  v52 = *(_OWORD *)(a1 + 1);
  v53 = *(_OWORD *)(a1 + 3);
  v54 = *(_OWORD *)(a1 + 5);
  v55 = *(_OWORD *)(a1 + 7);
  v56 = *(_OWORD *)(a1 + 9);
  v57 = *(_OWORD *)(a1 + 11);
  v58 = *(_OWORD *)(a1 + 13);
  v59 = *(_OWORD *)(a1 + 15);
  v60 = v4;
  v61 = v5;
  v62 = v6;
  v63 = v7;
  v64 = *(uint64_t *)((char *)a1 + 164);
  v37 = *a2;
  v38 = *(_OWORD *)(a2 + 1);
  v39 = *(_OWORD *)(a2 + 3);
  v40 = *(_OWORD *)(a2 + 5);
  v41 = *(_OWORD *)(a2 + 7);
  v42 = *(_OWORD *)(a2 + 9);
  v43 = *(_OWORD *)(a2 + 11);
  v44 = *(_OWORD *)(a2 + 13);
  v45 = *(_OWORD *)(a2 + 15);
  v46 = v8;
  v47 = v9;
  v48 = v10;
  v49 = v11;
  v50 = *(uint64_t *)((char *)a2 + 164);
  v23 = *a3;
  v24 = *(_OWORD *)(a3 + 1);
  v25 = *(_OWORD *)(a3 + 3);
  v17 = *(_OWORD *)(a3 + 7);
  v18 = *(_OWORD *)(a3 + 9);
  v19 = *(_OWORD *)(a3 + 11);
  v20 = *(_OWORD *)(a3 + 13);
  v21 = *(_OWORD *)(a3 + 15);
  v26 = *(_OWORD *)(a3 + 5);
  v27 = v17;
  v28 = v18;
  v29 = v19;
  v30 = v20;
  v31 = v21;
  v32 = v12;
  v33 = v13;
  v34 = v14;
  v35 = v15;
  v36 = *(uint64_t *)((char *)a3 + 164);
  return v16(&v51, &v37, &v23);
}

uint64_t dispatch thunk of BNNS.CropResizeLayer.applyBackward(regionOfInterest:outputGradient:generatingInputGradient:filterParameters:)(uint64_t *a1, uint64_t *a2, uint64_t *a3)
{
  uint64_t v3;
  uint64_t v4;
  int v5;
  uint64_t v6;
  int v7;
  uint64_t v8;
  int v9;
  uint64_t v10;
  int v11;
  uint64_t v12;
  int v13;
  uint64_t v14;
  int v15;
  uint64_t (*v16)(uint64_t *, uint64_t *, uint64_t *);
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  uint64_t v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  uint64_t v32;
  int v33;
  uint64_t v34;
  int v35;
  uint64_t v36;
  uint64_t v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  uint64_t v46;
  int v47;
  uint64_t v48;
  int v49;
  uint64_t v50;
  uint64_t v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  uint64_t v60;
  int v61;
  uint64_t v62;
  int v63;
  uint64_t v64;

  v4 = a1[17];
  v5 = *((_DWORD *)a1 + 36);
  v6 = a1[19];
  v7 = *((_DWORD *)a1 + 40);
  v8 = a2[17];
  v9 = *((_DWORD *)a2 + 36);
  v10 = a2[19];
  v11 = *((_DWORD *)a2 + 40);
  v12 = a3[17];
  v13 = *((_DWORD *)a3 + 36);
  v14 = a3[19];
  v15 = *((_DWORD *)a3 + 40);
  v16 = *(uint64_t (**)(uint64_t *, uint64_t *, uint64_t *))(*(_QWORD *)v3 + 104);
  v51 = *a1;
  v52 = *(_OWORD *)(a1 + 1);
  v53 = *(_OWORD *)(a1 + 3);
  v54 = *(_OWORD *)(a1 + 5);
  v55 = *(_OWORD *)(a1 + 7);
  v56 = *(_OWORD *)(a1 + 9);
  v57 = *(_OWORD *)(a1 + 11);
  v58 = *(_OWORD *)(a1 + 13);
  v59 = *(_OWORD *)(a1 + 15);
  v60 = v4;
  v61 = v5;
  v62 = v6;
  v63 = v7;
  v64 = *(uint64_t *)((char *)a1 + 164);
  v37 = *a2;
  v38 = *(_OWORD *)(a2 + 1);
  v39 = *(_OWORD *)(a2 + 3);
  v40 = *(_OWORD *)(a2 + 5);
  v41 = *(_OWORD *)(a2 + 7);
  v42 = *(_OWORD *)(a2 + 9);
  v43 = *(_OWORD *)(a2 + 11);
  v44 = *(_OWORD *)(a2 + 13);
  v45 = *(_OWORD *)(a2 + 15);
  v46 = v8;
  v47 = v9;
  v48 = v10;
  v49 = v11;
  v50 = *(uint64_t *)((char *)a2 + 164);
  v23 = *a3;
  v24 = *(_OWORD *)(a3 + 1);
  v25 = *(_OWORD *)(a3 + 3);
  v17 = *(_OWORD *)(a3 + 7);
  v18 = *(_OWORD *)(a3 + 9);
  v19 = *(_OWORD *)(a3 + 11);
  v20 = *(_OWORD *)(a3 + 13);
  v21 = *(_OWORD *)(a3 + 15);
  v26 = *(_OWORD *)(a3 + 5);
  v27 = v17;
  v28 = v18;
  v29 = v19;
  v30 = v20;
  v31 = v21;
  v32 = v12;
  v33 = v13;
  v34 = v14;
  v35 = v15;
  v36 = *(uint64_t *)((char *)a3 + 164);
  return v16(&v51, &v37, &v23);
}

uint64_t storeEnumTagSinglePayload for BNNS.CropResizeLayer.BoxCoordinateMode(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 3 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 3) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFD)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFC)
    return ((uint64_t (*)(void))((char *)&loc_1CAAE21E8 + 4 * byte_1CAB5F9D5[v4]))();
  *a1 = a2 + 3;
  return ((uint64_t (*)(void))((char *)sub_1CAAE221C + 4 * byte_1CAB5F9D0[v4]))();
}

uint64_t sub_1CAAE221C(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAAE2224(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAAE222CLL);
  return result;
}

uint64_t sub_1CAAE2238(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAAE2240);
  *(_BYTE *)result = a2 + 3;
  return result;
}

uint64_t sub_1CAAE2244(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAAE224C(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.CropResizeLayer.BoxCoordinateMode()
{
  return &type metadata for BNNS.CropResizeLayer.BoxCoordinateMode;
}

uint64_t getEnumTagSinglePayload for BNNS.CropResizeLayer.LinearSamplingMode(unsigned __int8 *a1, unsigned int a2)
{
  int v2;
  int v3;
  int v4;
  unsigned int v6;
  BOOL v7;
  int v8;

  if (!a2)
    return 0;
  if (a2 < 0xFC)
    goto LABEL_17;
  if (a2 + 4 >= 0xFFFF00)
    v2 = 4;
  else
    v2 = 2;
  if ((a2 + 4) >> 8 < 0xFF)
    v3 = 1;
  else
    v3 = v2;
  if (v3 == 4)
  {
    v4 = *(_DWORD *)(a1 + 1);
    if (v4)
      return (*a1 | (v4 << 8)) - 4;
  }
  else
  {
    if (v3 == 2)
    {
      v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1))
        goto LABEL_17;
      return (*a1 | (v4 << 8)) - 4;
    }
    v4 = a1[1];
    if (a1[1])
      return (*a1 | (v4 << 8)) - 4;
  }
LABEL_17:
  v6 = *a1;
  v7 = v6 >= 5;
  v8 = v6 - 5;
  if (!v7)
    v8 = -1;
  return (v8 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.CropResizeLayer.LinearSamplingMode(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 4 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 4) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFC)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFB)
    return ((uint64_t (*)(void))((char *)&loc_1CAAE2344 + 4 * byte_1CAB5F9DF[v4]))();
  *a1 = a2 + 4;
  return ((uint64_t (*)(void))((char *)sub_1CAAE2378 + 4 * byte_1CAB5F9DA[v4]))();
}

uint64_t sub_1CAAE2378(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAAE2380(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAAE2388);
  return result;
}

uint64_t sub_1CAAE2394(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAAE239CLL);
  *(_BYTE *)result = a2 + 4;
  return result;
}

uint64_t sub_1CAAE23A0(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAAE23A8(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.CropResizeLayer.LinearSamplingMode()
{
  return &type metadata for BNNS.CropResizeLayer.LinearSamplingMode;
}

uint64_t storeEnumTagSinglePayload for BNNS.ShuffleType(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 1 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 1) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFF)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFE)
    return ((uint64_t (*)(void))((char *)&loc_1CAAE2410 + 4 * byte_1CAB5F9E9[v4]))();
  *a1 = a2 + 1;
  return ((uint64_t (*)(void))((char *)sub_1CAAE2444 + 4 * byte_1CAB5F9E4[v4]))();
}

uint64_t sub_1CAAE2444(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAAE244C(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAAE2454);
  return result;
}

uint64_t sub_1CAAE2460(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAAE2468);
  *(_BYTE *)result = a2 + 1;
  return result;
}

uint64_t sub_1CAAE246C(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAAE2474(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.ShuffleType()
{
  return &type metadata for BNNS.ShuffleType;
}

__n128 BNNS.FusedNormalizationParameters.layerParameters(input:output:)(uint64_t a1)
{
  uint64_t v1;
  __int128 *v2;
  uint64_t *v3;
  __int128 *v4;
  __int128 v5;
  __int128 v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  uint64_t v15;
  uint64_t v16;
  char v17;
  unsigned int v18;
  uint64_t v19;
  uint64_t v20;
  _BYTE *v21;
  _BYTE *v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  int v27;
  __n128 v28;
  uint64_t v29;
  uint64_t v30;
  int v31;
  int v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  __n128 result;
  int v40;
  int v41;
  int v42;
  int v43;
  __n128 v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;
  uint64_t v56;
  uint64_t v57;
  uint64_t v58;
  uint64_t v59;
  uint64_t v60;
  uint64_t v61;
  uint64_t v62;
  uint64_t v63;
  uint64_t v64;
  uint64_t v65;
  uint64_t v66;
  uint64_t v67;
  uint64_t v68;
  uint64_t v69;
  uint64_t v70;
  uint64_t v71;
  uint64_t v72;
  uint64_t v73;
  unint64_t v74;
  uint64_t v75;
  uint64_t v76;
  uint64_t v77;
  uint64_t v78;
  uint64_t v79;
  uint64_t v80;
  uint64_t v81;
  uint64_t v82;
  int v83;
  uint64_t v84;
  int v85;
  uint64_t v86;
  uint64_t v87;
  uint64_t v88;
  unint64_t v89;
  uint64_t v90;
  uint64_t v91;
  uint64_t v92;
  uint64_t v93;
  uint64_t v94;
  uint64_t v95;
  uint64_t v96;
  uint64_t v97;
  uint64_t v98;
  uint64_t v99;
  uint64_t v100;
  uint64_t v101;
  uint64_t v102;
  uint64_t v103;
  uint64_t v104;
  uint64_t v105;
  int v106;
  uint64_t v107;
  unint64_t v108;
  uint64_t v109;
  uint64_t v110;
  uint64_t v111;
  uint64_t v112;
  uint64_t v113;
  uint64_t v114;
  uint64_t v115;
  uint64_t v116;
  uint64_t v117;
  uint64_t v118;
  uint64_t v119;
  uint64_t v120;
  uint64_t v121;
  uint64_t v122;
  uint64_t v123;
  uint64_t v124;
  uint64_t v125;
  uint64_t v126;
  uint64_t v127;
  uint64_t *v128;
  _OWORD __src[22];
  uint64_t v130;
  uint64_t v131;
  uint64_t v132;
  uint64_t v133;
  uint64_t v134;
  uint64_t v135;
  uint64_t v136;
  uint64_t v137;
  uint64_t v138;
  uint64_t v139;
  uint64_t v140;
  uint64_t v141;
  uint64_t v142;
  uint64_t v143;
  uint64_t v144;
  uint64_t v145;
  uint64_t v146;
  uint64_t v147;
  unint64_t v148;
  uint64_t v149;
  int v150;
  uint64_t v151;
  int v152;
  _BYTE v153[184];
  _BYTE v154[184];
  _BYTE v155[184];
  _BYTE v156[184];
  int v157;
  uint64_t v158;
  uint64_t v159;
  int v160;
  uint64_t v161;
  uint64_t v162;
  uint64_t v163;
  _BYTE v164[184];
  _BYTE v165[184];
  _BYTE v166[184];
  _BYTE v167[184];
  __int128 v168;
  __int128 v169;
  __int128 v170;
  __int128 v171;
  __int128 v172;
  __int128 v173;
  __int128 v174;
  __int128 v175;
  __int128 v176;
  __int128 v177;
  __int128 v178;
  __int128 v179;
  __int128 v180;
  __int128 v181;
  __int128 v182;
  __int128 v183;
  __int128 v184;
  __int128 v185;
  __int128 v186;
  __int128 v187;
  __int128 v188;
  __int128 v189;
  _BYTE v190[368];
  _BYTE v191[184];
  _BYTE v192[184];
  _BYTE v193[184];
  _BYTE v194[384];

  v2 = (__int128 *)MEMORY[0x1E0C80A78](a1);
  v128 = v3;
  v5 = v4[9];
  v176 = v4[8];
  v177 = v5;
  v178 = v4[10];
  v6 = v4[5];
  v172 = v4[4];
  v173 = v6;
  v7 = v4[6];
  v175 = v4[7];
  v174 = v7;
  v8 = v4[1];
  v168 = *v4;
  v169 = v8;
  v9 = v4[2];
  v171 = v4[3];
  v170 = v9;
  v10 = v2[9];
  v187 = v2[8];
  v188 = v10;
  v189 = v2[10];
  v11 = v2[5];
  v183 = v2[4];
  v184 = v11;
  v12 = v2[6];
  v186 = v2[7];
  v185 = v12;
  v13 = v2[1];
  v179 = *v2;
  v180 = v13;
  v14 = v2[2];
  v182 = v2[3];
  v181 = v14;
  outlined init with take of BNNS.NormalizationType((const void *)v1, v190);
  outlined init with take of BNNSNDArrayDescriptor?(v1 + 368, (uint64_t)v191);
  outlined init with take of BNNSNDArrayDescriptor?(v1 + 552, (uint64_t)v192);
  v15 = *(_QWORD *)(v1 + 732);
  v16 = *(_QWORD *)(v1 + 740);
  v17 = *(_BYTE *)(v1 + 748);
  _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)v193);
  outlined init with take of BNNS.NormalizationType(v190, v194);
  v18 = _s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v194);
  if (v18 >= 2)
  {
    if (v18 == 2)
    {
      _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v194);
      v127 = 0;
    }
    else
    {
      v127 = *(_QWORD *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v194);
    }
    v22 = v193;
    v21 = v193;
  }
  else
  {
    v19 = _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v194);
    v20 = v19 + 184;
    v21 = v164;
    outlined init with take of BNNSNDArrayDescriptor?(v19, (uint64_t)v164);
    v22 = v165;
    outlined init with take of BNNSNDArrayDescriptor?(v20, (uint64_t)v165);
    v127 = 0;
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v22, (uint64_t)v166);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v21, (uint64_t)v167);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v191, (uint64_t)v156);
  v23 = 0;
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v156) == 1)
  {
    v125 = 0;
    v124 = 0;
    v123 = 0;
    v122 = 0;
    v121 = 0;
    v120 = 0;
    v119 = 0;
    v118 = 0;
    v117 = 0;
    v116 = 0;
    v115 = 0;
    v114 = 0;
    v113 = 0;
    v112 = 0;
    v111 = 0;
    v110 = 0;
    v109 = 0;
    v107 = 0;
    v106 = 0;
    v126 = 0;
    v24 = 0;
    v108 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v191, (uint64_t)&v130);
    v126 = v130;
    v125 = v131;
    v124 = v132;
    v123 = v133;
    v122 = v134;
    v121 = v135;
    v120 = v136;
    v119 = v137;
    v118 = v138;
    v117 = v139;
    v116 = v140;
    v115 = v141;
    v114 = v142;
    v113 = v143;
    v112 = v144;
    v111 = v145;
    v110 = v146;
    v109 = v147;
    v108 = v148;
    v107 = v149;
    v24 = v151;
    v106 = v150;
    v43 = v152;
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v192, (uint64_t)v155);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v155) == 1)
  {
    v104 = 0;
    v103 = 0;
    v102 = 0;
    v101 = 0;
    v100 = 0;
    v99 = 0;
    v98 = 0;
    v96 = 0;
    v97 = 0;
    v94 = 0;
    v95 = 0;
    v92 = 0;
    v93 = 0;
    v90 = 0;
    v91 = 0;
    v87 = 0;
    v88 = 0;
    v86 = 0;
    v85 = 0;
    v105 = 0;
    v89 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v192, (uint64_t)&v130);
    v105 = v130;
    v104 = v131;
    v103 = v132;
    v102 = v133;
    v101 = v134;
    v100 = v135;
    v99 = v136;
    v98 = v137;
    v96 = v139;
    v97 = v138;
    v94 = v141;
    v95 = v140;
    v92 = v143;
    v93 = v142;
    v90 = v145;
    v91 = v144;
    v87 = v147;
    v88 = v146;
    v89 = v148;
    v86 = v149;
    v23 = v151;
    v85 = v150;
    v42 = v152;
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v167, (uint64_t)v154);
  v25 = 0;
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v154) == 1)
  {
    v81 = 0;
    v82 = 0;
    v79 = 0;
    v80 = 0;
    v77 = 0;
    v78 = 0;
    v75 = 0;
    v76 = 0;
    v72 = 0;
    v73 = 0;
    v70 = 0;
    v71 = 0;
    v68 = 0;
    v69 = 0;
    v66 = 0;
    v67 = 0;
    v64 = 0;
    v65 = 0;
    v83 = 0;
    v84 = 0;
    v26 = 0;
    v74 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v167, (uint64_t)&v130);
    v84 = v130;
    v81 = v132;
    v82 = v131;
    v79 = v134;
    v80 = v133;
    v77 = v136;
    v78 = v135;
    v75 = v138;
    v76 = v137;
    v72 = v140;
    v73 = v139;
    v70 = v142;
    v71 = v141;
    v68 = v144;
    v69 = v143;
    v66 = v146;
    v67 = v145;
    v74 = v148;
    v64 = v149;
    v65 = v147;
    v26 = v151;
    v83 = v150;
    v41 = v152;
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v166, (uint64_t)v153);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v153) == 1)
  {
    v27 = 0;
    v45 = 0;
    v46 = 0;
    v47 = 0;
    v48 = 0;
    v49 = 0;
    v50 = 0;
    v51 = 0;
    v52 = 0;
    v53 = 0;
    v54 = 0;
    v55 = 0;
    v56 = 0;
    v57 = 0;
    v58 = 0;
    v59 = 0;
    v60 = 0;
    v61 = 0;
    v63 = 0;
    v62 = 0;
    v44 = 0u;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v166, (uint64_t)&v130);
    v62 = v130;
    v63 = v131;
    v60 = v133;
    v61 = v132;
    v58 = v135;
    v59 = v134;
    v56 = v137;
    v57 = v136;
    v54 = v139;
    v55 = v138;
    v52 = v141;
    v53 = v140;
    v50 = v143;
    v51 = v142;
    v48 = v145;
    v49 = v144;
    v46 = v147;
    v47 = v146;
    v28.n128_u64[0] = v148;
    v44 = v28;
    v45 = v149;
    v25 = v151;
    v27 = v150;
    v40 = v152;
  }
  v130 = v16;
  LOBYTE(v131) = v17;
  BNNS.ActivationFunction.bnnsActivation.getter();
  v29 = v158;
  v30 = v159;
  v31 = v157;
  v32 = v160;
  v33 = v161;
  v34 = v162;
  v35 = v163;
  outlined init with take of BNNS.NormalizationType(v190, &v130);
  if (_s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)&v130) == 2)
    v36 = *(_QWORD *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)&v130);
  else
    v36 = 0;
  __src[8] = v187;
  __src[9] = v188;
  __src[4] = v183;
  __src[5] = v184;
  __src[6] = v185;
  __src[7] = v186;
  __src[0] = v179;
  __src[1] = v180;
  __src[2] = v181;
  __src[3] = v182;
  __src[18] = v175;
  __src[19] = v176;
  __src[20] = v177;
  __src[21] = v178;
  __src[13] = v170;
  __src[14] = v171;
  __src[15] = v172;
  __src[16] = v173;
  __src[17] = v174;
  __src[10] = v189;
  __src[11] = v168;
  __src[12] = v169;
  type metadata accessor for BNNSLayerParametersNormalization(0);
  v128[3] = v37;
  v128[4] = (uint64_t)&protocol witness table for BNNSLayerParametersNormalization;
  v38 = swift_allocObject();
  *v128 = v38;
  memcpy((void *)(v38 + 16), __src, 0x160uLL);
  *(_QWORD *)(v38 + 376) = v125;
  *(_QWORD *)(v38 + 384) = v124;
  *(_QWORD *)(v38 + 392) = v123;
  *(_QWORD *)(v38 + 400) = v122;
  *(_QWORD *)(v38 + 408) = v121;
  *(_QWORD *)(v38 + 416) = v120;
  *(_QWORD *)(v38 + 424) = v119;
  *(_QWORD *)(v38 + 432) = v118;
  *(_QWORD *)(v38 + 440) = v117;
  *(_QWORD *)(v38 + 448) = v116;
  *(_QWORD *)(v38 + 456) = v115;
  *(_QWORD *)(v38 + 464) = v114;
  *(_QWORD *)(v38 + 472) = v113;
  *(_QWORD *)(v38 + 480) = v112;
  *(_QWORD *)(v38 + 488) = v111;
  *(_QWORD *)(v38 + 496) = v110;
  *(_QWORD *)(v38 + 504) = v109;
  *(_QWORD *)(v38 + 520) = v107;
  *(_DWORD *)(v38 + 528) = v106;
  *(_DWORD *)(v38 + 540) = v43;
  *(_QWORD *)(v38 + 552) = v104;
  *(_QWORD *)(v38 + 560) = v103;
  *(_QWORD *)(v38 + 568) = v102;
  *(_QWORD *)(v38 + 576) = v101;
  *(_QWORD *)(v38 + 584) = v100;
  *(_QWORD *)(v38 + 592) = v99;
  *(_QWORD *)(v38 + 600) = v98;
  *(_QWORD *)(v38 + 608) = v97;
  *(_QWORD *)(v38 + 616) = v96;
  *(_QWORD *)(v38 + 624) = v95;
  *(_QWORD *)(v38 + 632) = v94;
  *(_QWORD *)(v38 + 640) = v93;
  *(_QWORD *)(v38 + 648) = v92;
  *(_QWORD *)(v38 + 656) = v91;
  *(_QWORD *)(v38 + 664) = v90;
  *(_QWORD *)(v38 + 672) = v88;
  *(_QWORD *)(v38 + 680) = v87;
  *(_QWORD *)(v38 + 696) = v86;
  *(_DWORD *)(v38 + 704) = v85;
  *(_DWORD *)(v38 + 716) = v42;
  *(_QWORD *)(v38 + 728) = v82;
  *(_QWORD *)(v38 + 736) = v81;
  *(_QWORD *)(v38 + 744) = v80;
  *(_QWORD *)(v38 + 752) = v79;
  *(_QWORD *)(v38 + 760) = v78;
  *(_QWORD *)(v38 + 768) = v77;
  *(_QWORD *)(v38 + 776) = v76;
  *(_QWORD *)(v38 + 784) = v75;
  *(_QWORD *)(v38 + 792) = v73;
  *(_QWORD *)(v38 + 800) = v72;
  *(_QWORD *)(v38 + 808) = v71;
  *(_QWORD *)(v38 + 816) = v70;
  *(_QWORD *)(v38 + 824) = v69;
  *(_QWORD *)(v38 + 832) = v68;
  *(_QWORD *)(v38 + 840) = v67;
  *(_QWORD *)(v38 + 848) = v66;
  *(_QWORD *)(v38 + 856) = v65;
  *(_QWORD *)(v38 + 872) = v64;
  *(_QWORD *)(v38 + 368) = v126;
  *(_QWORD *)(v38 + 512) = v108;
  *(_QWORD *)(v38 + 532) = v24;
  *(_QWORD *)(v38 + 544) = v105;
  *(_QWORD *)(v38 + 688) = v89;
  *(_QWORD *)(v38 + 708) = v23;
  *(_QWORD *)(v38 + 720) = v84;
  *(_QWORD *)(v38 + 864) = v74;
  *(_DWORD *)(v38 + 880) = v83;
  *(_QWORD *)(v38 + 884) = v26;
  *(_DWORD *)(v38 + 892) = v41;
  *(_QWORD *)(v38 + 896) = v62;
  *(_QWORD *)(v38 + 904) = v63;
  *(_QWORD *)(v38 + 912) = v61;
  *(_QWORD *)(v38 + 920) = v60;
  *(_QWORD *)(v38 + 928) = v59;
  *(_QWORD *)(v38 + 936) = v58;
  *(_QWORD *)(v38 + 944) = v57;
  *(_QWORD *)(v38 + 952) = v56;
  *(_QWORD *)(v38 + 960) = v55;
  *(_QWORD *)(v38 + 968) = v54;
  *(_QWORD *)(v38 + 976) = v53;
  *(_QWORD *)(v38 + 984) = v52;
  *(_QWORD *)(v38 + 992) = v51;
  *(_QWORD *)(v38 + 1000) = v50;
  *(_QWORD *)(v38 + 1008) = v49;
  *(_QWORD *)(v38 + 1016) = v48;
  *(_QWORD *)(v38 + 1024) = v47;
  *(_QWORD *)(v38 + 1032) = v46;
  result = v44;
  *(_QWORD *)(v38 + 1040) = v44.n128_u64[0];
  *(_QWORD *)(v38 + 1048) = v45;
  *(_DWORD *)(v38 + 1056) = v27;
  *(_QWORD *)(v38 + 1060) = v25;
  *(_DWORD *)(v38 + 1068) = v40;
  *(_QWORD *)(v38 + 1072) = v15;
  *(_DWORD *)(v38 + 1080) = v31;
  *(_QWORD *)(v38 + 1084) = v29;
  *(_QWORD *)(v38 + 1092) = v30;
  *(_DWORD *)(v38 + 1100) = v32;
  *(_QWORD *)(v38 + 1104) = v33;
  *(_QWORD *)(v38 + 1112) = v34;
  *(_QWORD *)(v38 + 1120) = v35;
  *(_QWORD *)(v38 + 1128) = v127;
  *(_QWORD *)(v38 + 1136) = v36;
  return result;
}

void *BNNS.FusedNormalizationParameters.init(type:beta:gamma:momentum:epsilon:activation:)@<X0>(const void *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t *a4@<X3>, void *a5@<X8>, float a6@<S0>, float a7@<S1>)
{
  uint64_t v13;
  _BYTE v15[184];
  _BYTE v16[184];
  _BYTE v17[184];
  _BYTE v18[184];
  _BYTE v19[368];
  _DWORD __src[188];

  outlined init with take of BNNSNDArrayDescriptor?(a3, (uint64_t)v16);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v16, (uint64_t)v17);
  outlined init with take of BNNSNDArrayDescriptor?(a2, (uint64_t)v15);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v15, (uint64_t)v18);
  outlined init with take of BNNS.NormalizationType(a1, v19);
  v13 = *a4;
  LOBYTE(a4) = *((_BYTE *)a4 + 8);
  outlined init with take of BNNS.NormalizationType(v19, __src);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v18, (uint64_t)&__src[92]);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v17, (uint64_t)&__src[138]);
  *(float *)&__src[183] = a6;
  *(float *)&__src[184] = a7;
  *(_QWORD *)&__src[185] = v13;
  LOBYTE(__src[187]) = (_BYTE)a4;
  return memcpy(a5, __src, 0x2EDuLL);
}

void *BNNS.FusedNormalizationParameters.type.getter@<X0>(void *a1@<X8>)
{
  const void *v1;
  _BYTE v4[376];

  outlined init with take of BNNS.NormalizationType(v1, v4);
  return outlined init with take of BNNS.NormalizationType(v4, a1);
}

void *BNNS.FusedNormalizationParameters.type.setter(const void *a1)
{
  void *v1;
  _BYTE v3[368];

  outlined init with take of BNNS.NormalizationType(a1, v3);
  return outlined init with take of BNNS.NormalizationType(v3, v1);
}

uint64_t (*BNNS.FusedNormalizationParameters.type.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedNormalizationParameters.beta.getter@<X0>(uint64_t a1@<X8>)
{
  uint64_t v1;
  _BYTE v4[184];

  outlined init with take of BNNSNDArrayDescriptor?(v1 + 368, (uint64_t)v4);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v4, a1);
}

uint64_t BNNS.FusedNormalizationParameters.beta.setter(uint64_t a1)
{
  uint64_t v1;

  return outlined init with take of BNNSNDArrayDescriptor?(a1, v1 + 368);
}

uint64_t (*BNNS.FusedNormalizationParameters.beta.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedNormalizationParameters.gamma.getter@<X0>(uint64_t a1@<X8>)
{
  uint64_t v1;
  _BYTE v4[184];

  outlined init with take of BNNSNDArrayDescriptor?(v1 + 552, (uint64_t)v4);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v4, a1);
}

uint64_t BNNS.FusedNormalizationParameters.gamma.setter(uint64_t a1)
{
  uint64_t v1;

  return outlined init with take of BNNSNDArrayDescriptor?(a1, v1 + 552);
}

uint64_t (*BNNS.FusedNormalizationParameters.gamma.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

float BNNS.FusedNormalizationParameters.momentum.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 732);
}

void BNNS.FusedNormalizationParameters.momentum.setter(float a1)
{
  uint64_t v1;

  *(float *)(v1 + 732) = a1;
}

uint64_t (*BNNS.FusedNormalizationParameters.momentum.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

float BNNS.FusedNormalizationParameters.epsilon.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 736);
}

void BNNS.FusedNormalizationParameters.epsilon.setter(float a1)
{
  uint64_t v1;

  *(float *)(v1 + 736) = a1;
}

uint64_t (*BNNS.FusedNormalizationParameters.epsilon.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedNormalizationParameters.activation.getter(uint64_t a1@<X8>)
{
  uint64_t v1;
  char v2;

  v2 = *(_BYTE *)(v1 + 748);
  *(_QWORD *)a1 = *(_QWORD *)(v1 + 740);
  *(_BYTE *)(a1 + 8) = v2;
}

uint64_t BNNS.FusedNormalizationParameters.activation.setter(uint64_t result)
{
  uint64_t v1;
  char v2;

  v2 = *(_BYTE *)(result + 8);
  *(_QWORD *)(v1 + 740) = *(_QWORD *)result;
  *(_BYTE *)(v1 + 748) = v2;
  return result;
}

uint64_t (*BNNS.FusedNormalizationParameters.activation.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t protocol witness for FusableLayerParametersWrapper.filterType.getter in conformance BNNS.FusedNormalizationParameters()
{
  const void *v0;
  uint64_t v1;
  _BYTE v3[368];
  _BYTE v4[376];

  outlined init with take of BNNS.NormalizationType(v0, v3);
  outlined init with take of BNNS.NormalizationType(v3, v4);
  v1 = _s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v4) + 2;
  _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v4);
  return v1;
}

void *__swift_memcpy749_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x2EDuLL);
}

uint64_t getEnumTagSinglePayload for BNNS.FusedNormalizationParameters(uint64_t a1, int a2)
{
  uint64_t v2;
  int v3;

  if (!a2)
    return 0;
  if (a2 < 0 && *(_BYTE *)(a1 + 749))
    return *(_DWORD *)a1 + 0x80000000;
  v2 = *(_QWORD *)(a1 + 176) >> 1;
  if (v2 > 0x80000000)
    v3 = ~(_DWORD)v2;
  else
    v3 = -1;
  return (v3 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.FusedNormalizationParameters(uint64_t result, int a2, int a3)
{
  if (a2 < 0)
  {
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_BYTE *)(result + 748) = 0;
    *(_DWORD *)(result + 744) = 0;
    *(_OWORD *)(result + 728) = 0u;
    *(_OWORD *)(result + 712) = 0u;
    *(_OWORD *)(result + 696) = 0u;
    *(_OWORD *)(result + 680) = 0u;
    *(_OWORD *)(result + 664) = 0u;
    *(_OWORD *)(result + 648) = 0u;
    *(_OWORD *)(result + 632) = 0u;
    *(_OWORD *)(result + 616) = 0u;
    *(_OWORD *)(result + 600) = 0u;
    *(_OWORD *)(result + 584) = 0u;
    *(_OWORD *)(result + 568) = 0u;
    *(_OWORD *)(result + 552) = 0u;
    *(_OWORD *)(result + 536) = 0u;
    *(_OWORD *)(result + 520) = 0u;
    *(_OWORD *)(result + 504) = 0u;
    *(_OWORD *)(result + 488) = 0u;
    *(_OWORD *)(result + 472) = 0u;
    *(_OWORD *)(result + 456) = 0u;
    *(_OWORD *)(result + 440) = 0u;
    *(_OWORD *)(result + 424) = 0u;
    *(_OWORD *)(result + 408) = 0u;
    *(_OWORD *)(result + 392) = 0u;
    *(_OWORD *)(result + 376) = 0u;
    *(_OWORD *)(result + 360) = 0u;
    *(_OWORD *)(result + 344) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(_QWORD *)result = a2 ^ 0x80000000;
    if (a3 < 0)
      *(_BYTE *)(result + 749) = 1;
  }
  else
  {
    if ((a3 & 0x80000000) == 0)
    {
      if (!a2)
        return result;
LABEL_8:
      *(_OWORD *)(result + 144) = 0u;
      *(_OWORD *)(result + 160) = 0u;
      *(_OWORD *)(result + 112) = 0u;
      *(_OWORD *)(result + 128) = 0u;
      *(_OWORD *)(result + 80) = 0u;
      *(_OWORD *)(result + 96) = 0u;
      *(_OWORD *)(result + 48) = 0u;
      *(_OWORD *)(result + 64) = 0u;
      *(_OWORD *)(result + 16) = 0u;
      *(_OWORD *)(result + 32) = 0u;
      *(_OWORD *)result = 0u;
      *(_QWORD *)(result + 176) = 2 * -a2;
      *(_OWORD *)(result + 200) = 0u;
      *(_OWORD *)(result + 216) = 0u;
      *(_OWORD *)(result + 232) = 0u;
      *(_OWORD *)(result + 248) = 0u;
      *(_BYTE *)(result + 360) = 0;
      *(_OWORD *)(result + 184) = 0u;
      result += 184;
      *(_OWORD *)(result + 80) = 0u;
      *(_OWORD *)(result + 96) = 0u;
      *(_OWORD *)(result + 112) = 0u;
      *(_OWORD *)(result + 128) = 0u;
      *(_OWORD *)(result + 144) = 0u;
      *(_OWORD *)(result + 160) = 0u;
      return result;
    }
    *(_BYTE *)(result + 749) = 0;
    if (a2)
      goto LABEL_8;
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.FusedNormalizationParameters()
{
  return &type metadata for BNNS.FusedNormalizationParameters;
}

uint64_t sub_1CAAE3368()
{
  return swift_deallocObject();
}

float static vDSP.maximum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.maximum<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.maximum<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C1B0]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C1B8]);
}

double static vDSP.maximum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.maximum<A>(_:));
}

float static vDSP.maximumMagnitude<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.maximumMagnitude<A>(_:));
}

float static vDSP.maximum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v6;

  v6 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  if (v6 < 0)
    __break(1u);
  MEMORY[0x1E0C80A78](v6);
  (*(void (**)(uint64_t))(a3 + 24))(a4);
  return NAN;
}

uint64_t partial apply for closure #1 in static vDSP.maximumMagnitude<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C188]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C190]);
}

double static vDSP.maximumMagnitude<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.maximumMagnitude<A>(_:));
}

double static vDSP.maximum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v6;

  v6 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  if (v6 < 0)
    __break(1u);
  MEMORY[0x1E0C80A78](v6);
  (*(void (**)(uint64_t))(a3 + 24))(a4);
  return NAN;
}

float static vDSP.minimum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.minimum<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.minimum<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C220]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C228]);
}

double static vDSP.minimum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.minimum<A>(_:));
}

float static vDSP.sum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.sum<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.sum<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C2E8]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C2F8]);
}

double static vDSP.sum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.sum<A>(_:));
}

float static vDSP.sumOfSquares<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.sumOfSquares<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.sumOfSquares<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C328]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C338]);
}

double static vDSP.sumOfSquares<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.sumOfSquares<A>(_:));
}

float static vDSP.sumAndSumOfSquares<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4;

  v4 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  if (v4 < 0)
    __break(1u);
  MEMORY[0x1E0C80A78](v4);
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t)))(a3 + 24))(partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:));
  return NAN;
}

uint64_t partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(a1, a2, MEMORY[0x1E0C8C300]);
}

{
  return partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(a1, a2, MEMORY[0x1E0C8C308]);
}

double static vDSP.sumAndSumOfSquares<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4;

  v4 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  if (v4 < 0)
    __break(1u);
  MEMORY[0x1E0C80A78](v4);
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t)))(a3 + 24))(partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:));
  return NAN;
}

float static vDSP.sumOfMagnitudes<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.sumOfMagnitudes<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.sumOfMagnitudes<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C318]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C320]);
}

double static vDSP.sumOfMagnitudes<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.sumOfMagnitudes<A>(_:));
}

float static vDSP.indexOfMaximum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.indexOfMaximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.indexOfMaximum<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.indexOfMaximum<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(a1, a2, MEMORY[0x1E0C8C1C0]);
}

{
  return partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(a1, a2, MEMORY[0x1E0C8C1D0]);
}

double static vDSP.indexOfMaximum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.indexOfMaximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.indexOfMaximum<A>(_:));
}

float static vDSP.indexOfMaximumMagnitude<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.indexOfMaximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.indexOfMaximumMagnitude<A>(_:));
}

float static vDSP.indexOfMaximum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v6;

  v6 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  if (v6 < 0)
    __break(1u);
  MEMORY[0x1E0C80A78](v6);
  (*(void (**)(uint64_t))(a3 + 24))(a4);
  return NAN;
}

uint64_t partial apply for closure #1 in static vDSP.indexOfMaximumMagnitude<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(a1, a2, MEMORY[0x1E0C8C198]);
}

{
  return partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(a1, a2, MEMORY[0x1E0C8C1A0]);
}

double static vDSP.indexOfMaximumMagnitude<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.indexOfMaximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.indexOfMaximumMagnitude<A>(_:));
}

double static vDSP.indexOfMaximum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v6;

  v6 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  if (v6 < 0)
    __break(1u);
  MEMORY[0x1E0C80A78](v6);
  (*(void (**)(uint64_t))(a3 + 24))(a4);
  return NAN;
}

float static vDSP.indexOfMinimum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.indexOfMaximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.indexOfMinimum<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.indexOfMinimum<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(a1, a2, MEMORY[0x1E0C8C230]);
}

{
  return partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(a1, a2, MEMORY[0x1E0C8C240]);
}

double static vDSP.indexOfMinimum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.indexOfMaximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.indexOfMinimum<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(uint64_t result, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD))
{
  _QWORD *v3;

  if (result)
    return a3(result, 1, v3[2], v3[3], v3[4]);
  __break(1u);
  return result;
}

float static vDSP.meanSquare<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.meanSquare<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.meanSquare<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C1F8]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C200]);
}

double static vDSP.meanSquare<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.meanSquare<A>(_:));
}

float static vDSP.meanMagnitude<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.meanMagnitude<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.meanMagnitude<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C1D8]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C1E0]);
}

double static vDSP.meanMagnitude<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.meanMagnitude<A>(_:));
}

float static vDSP.mean<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.mean<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.mean<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C1E8]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C1F0]);
}

double static vDSP.mean<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.mean<A>(_:));
}

float static vDSP.rootMeanSquare<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.rootMeanSquare<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.rootMeanSquare<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C2C0]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E0C8C2C8]);
}

double static vDSP.rootMeanSquare<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.rootMeanSquare<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.maximum<A>(_:)(uint64_t result, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, _QWORD, _QWORD))
{
  uint64_t v3;

  if (result)
    return a3(result, 1, *(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24));
  __break(1u);
  return result;
}

uint64_t BNNS.BroadcastMatrixMultiplyLayer.__allocating_init(inputA:transposed:isWeights:inputB:transposed:isWeights:output:alpha:accumulatesToOutput:isQuadratic:filterParameters:)(_OWORD *a1, char a2, char a3, _OWORD *a4, char a5, char a6, __int128 *a7, char a8, float a9, char a10, int a11, int a12, uint64_t a13, uint64_t a14, uint64_t a15)
{
  __int128 v15;
  float v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  char v36;
  char v37;
  const BNNSFilterParameters *v38;
  void *v39;
  uint64_t v40;
  uint64_t v41;
  int v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  _BYTE v47[179];
  BNNSLayerParametersBroadcastMatMul layer_params;
  uint64_t v49;

  v49 = *MEMORY[0x1E0C80C00];
  if ((a3 & 1) != 0 && (a6 & 1) != 0)
    return 0;
  v15 = a1[6];
  *(_OWORD *)&v47[115] = a1[7];
  if ((a8 & 1) != 0)
    v16 = 1.0;
  else
    v16 = 0.0;
  v17 = a1[9];
  *(_OWORD *)&v47[131] = a1[8];
  *(_OWORD *)&v47[147] = v17;
  *(_OWORD *)&v47[163] = a1[10];
  v18 = a1[2];
  *(_OWORD *)&v47[51] = a1[3];
  v19 = a1[5];
  *(_OWORD *)&v47[67] = a1[4];
  *(_OWORD *)&v47[83] = v19;
  *(_OWORD *)&v47[99] = v15;
  v20 = a1[1];
  *(_OWORD *)&v47[3] = *a1;
  *(_OWORD *)&v47[19] = v20;
  *(_OWORD *)&v47[35] = v18;
  v21 = a4[9];
  *(_OWORD *)&layer_params.iB_desc.stride[7] = a4[8];
  *(_OWORD *)&layer_params.iB_desc.data_type = v21;
  v22 = a4[5];
  *(_OWORD *)&layer_params.iB_desc.size[7] = a4[4];
  *(_OWORD *)&layer_params.iB_desc.stride[1] = v22;
  v23 = a4[7];
  *(_OWORD *)&layer_params.iB_desc.stride[3] = a4[6];
  *(_OWORD *)&layer_params.iB_desc.stride[5] = v23;
  v24 = a4[1];
  *(_OWORD *)&layer_params.iB_desc.flags = *a4;
  *(_OWORD *)&layer_params.iB_desc.size[1] = v24;
  v25 = a4[3];
  *(_OWORD *)&layer_params.iB_desc.size[3] = a4[2];
  *(_OWORD *)&layer_params.iB_desc.size[5] = v25;
  v26 = a7[8];
  v27 = a7[9];
  v28 = a7[6];
  *(_OWORD *)&layer_params.o_desc.stride[5] = a7[7];
  *(_OWORD *)&layer_params.o_desc.stride[7] = v26;
  v29 = a7[10];
  *(_OWORD *)&layer_params.o_desc.data_type = v27;
  *(_OWORD *)&layer_params.o_desc.table_data_type = v29;
  v30 = a7[4];
  v31 = a7[5];
  v32 = a7[2];
  *(_OWORD *)&layer_params.o_desc.size[5] = a7[3];
  *(_OWORD *)&layer_params.o_desc.size[7] = v30;
  v33 = a4[10];
  *(_OWORD *)&layer_params.o_desc.stride[1] = v31;
  *(_OWORD *)&layer_params.o_desc.stride[3] = v28;
  v34 = *a7;
  v35 = a7[1];
  *(_OWORD *)&layer_params.iB_desc.table_data_type = v33;
  *(_OWORD *)&layer_params.o_desc.flags = v34;
  v36 = a3 & 1;
  v37 = a6 & 1;
  *(_OWORD *)&layer_params.o_desc.size[1] = v35;
  *(_OWORD *)&layer_params.o_desc.size[3] = v32;
  layer_params.alpha = a9;
  layer_params.beta = v16;
  layer_params.transA = a2 & 1;
  layer_params.transB = a5 & 1;
  layer_params.quadratic = a10 & 1;
  layer_params.a_is_weights = a3 & 1;
  layer_params.b_is_weights = a6 & 1;
  *(_OWORD *)((char *)&layer_params.iA_desc.stride[6] + 5) = *(_OWORD *)&v47[128];
  *(_OWORD *)((char *)&layer_params.iA_desc.data + 5) = *(_OWORD *)&v47[144];
  *(_OWORD *)((char *)&layer_params.iA_desc.table_data + 5) = *(_OWORD *)&v47[160];
  *((_DWORD *)&layer_params.iA_desc.data_bias + 1) = *(_DWORD *)&v47[175];
  *(_OWORD *)((char *)&layer_params.iA_desc.size[6] + 5) = *(_OWORD *)&v47[64];
  *(_OWORD *)((char *)layer_params.iA_desc.stride + 5) = *(_OWORD *)&v47[80];
  *(_OWORD *)((char *)&layer_params.iA_desc.stride[2] + 5) = *(_OWORD *)&v47[96];
  *(_OWORD *)((char *)&layer_params.iA_desc.stride[4] + 5) = *(_OWORD *)&v47[112];
  *(_OWORD *)(&layer_params.b_is_weights + 1) = *(_OWORD *)v47;
  *(_OWORD *)((char *)layer_params.iA_desc.size + 5) = *(_OWORD *)&v47[16];
  *(_OWORD *)((char *)&layer_params.iA_desc.size[2] + 5) = *(_OWORD *)&v47[32];
  *(_OWORD *)((char *)&layer_params.iA_desc.size[4] + 5) = *(_OWORD *)&v47[48];
  if (a14 == 1)
  {
    v38 = 0;
  }
  else
  {
    v43 = a12;
    v44 = a13;
    v45 = a14;
    v46 = a15;
    v38 = (const BNNSFilterParameters *)&v43;
  }
  v39 = BNNSFilterCreateLayerBroadcastMatMul(&layer_params, v38);
  type metadata accessor for BNNS.BroadcastMatrixMultiplyLayer();
  v40 = swift_allocObject();
  v41 = v40;
  *(_BYTE *)(v40 + 24) = v36;
  *(_BYTE *)(v40 + 25) = v37;
  if (!v39)
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  *(_QWORD *)(v40 + 16) = v39;
  return v41;
}

uint64_t type metadata accessor for BNNS.BroadcastMatrixMultiplyLayer()
{
  return objc_opt_self();
}

uint64_t BNNS.BroadcastMatrixMultiplyLayer.apply(batchSize:inputA:inputB:output:)(void *a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4;
  uint64_t v8;
  const void *v9;
  unint64_t v10;
  const void *v11;
  const void *v12;
  size_t v13;
  uint64_t result;
  _BYTE *v15;
  size_t v16;
  _BYTE *v17;
  unint64_t inB_stride;
  size_t v19;
  void *v20;
  size_t v21;
  void *v22;
  void *v23;
  unint64_t v24;
  unint64_t v25;
  unint64_t v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  unint64_t v46;
  unint64_t v47;
  _BYTE v48[136];
  unint64_t v49;
  unint64_t v50;
  unint64_t v51;
  unint64_t v52;
  unint64_t v53;
  unint64_t v54;
  unint64_t v55;
  unint64_t v56;
  _BYTE v57[136];
  _BYTE v58[136];
  _BYTE v59[136];
  _BYTE v60[8];
  _BYTE v61[8];
  _BYTE v62[8];
  _BYTE v63[8];
  const void *v64;
  const void *v65;
  void *v66;
  void *v67;
  void *v68;

  v8 = a3;
  if ((*(_BYTE *)(v4 + 24) & 1) != 0 || (v8 = a2, (*(_BYTE *)(v4 + 25) & 1) != 0))
  {
    v9 = *(const void **)(v8 + 136);
    if (v9)
    {
      outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v60);
      outlined init with take of UnsafeMutableRawPointer?((uint64_t)v60, (uint64_t)&v67);
      if (v67)
      {
        v22 = v67;
        v68 = a1;
        BNNSNDArrayDescriptor.shape.getter((uint64_t)v58);
        outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)v59);
        outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v57);
        BNNS.Shape.size.getter();
        outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v57);
        BNNS.Shape.stride.getter();
        v10 = specialized static BNNS.calculateBatchStride(size:stride:)(v49, v50, v51, v52, v53, v54, v55, v56, v49, v50, v51, v52, v53, v54, v55, v56);
        v20 = *(void **)(v4 + 16);
        v21 = v10;
        BNNSNDArrayDescriptor.shape.getter((uint64_t)v58);
        outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)v59);
        outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v57);
        BNNS.Shape.size.getter();
        outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v57);
        BNNS.Shape.stride.getter();
        v16 = specialized static BNNS.calculateBatchStride(size:stride:)(v49, v50, v51, v52, v53, v54, v55, v56, v49, v50, v51, v52, v53, v54, v55, v56);
        result = BNNSFilterApplyBatch(v20, (size_t)v68, v9, v21, v22, v16);
        if ((_DWORD)result)
          goto LABEL_13;
        return result;
      }
    }
LABEL_11:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v15 = 2;
    return swift_willThrow();
  }
  outlined init with take of UnsafeMutableRawPointer?(a2 + 136, (uint64_t)v63);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v63, (uint64_t)&v64);
  v11 = v64;
  if (!v64)
    goto LABEL_11;
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v62);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v62, (uint64_t)&v65);
  v12 = v65;
  if (!v65)
    goto LABEL_11;
  outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v61);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v61, (uint64_t)&v66);
  if (!v66)
    goto LABEL_11;
  v23 = *(void **)(v4 + 16);
  v68 = v66;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v49);
  outlined init with take of BNNS.Shape((uint64_t)&v49, (uint64_t)v57);
  outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v48);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v48);
  BNNS.Shape.stride.getter();
  v19 = specialized static BNNS.calculateBatchStride(size:stride:)(v40, v41, v42, v43, v44, v45, v46, v47, v40, v41, v42, v43, v44, v45, v46, v47);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v48);
  outlined init with take of BNNS.Shape((uint64_t)v48, (uint64_t)v58);
  outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)&v40);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)&v40);
  BNNS.Shape.stride.getter();
  inB_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v32, v33, v34, v35, v36, v37, v38, v39, v32, v33, v34, v35, v36, v37, v38, v39);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v40);
  outlined init with take of BNNS.Shape((uint64_t)&v40, (uint64_t)v59);
  outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)&v32);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)&v32);
  BNNS.Shape.stride.getter();
  v13 = specialized static BNNS.calculateBatchStride(size:stride:)(v24, v25, v26, v27, v28, v29, v30, v31, v24, v25, v26, v27, v28, v29, v30, v31);
  result = BNNSFilterApplyTwoInputBatch(v23, (size_t)a1, v11, v19, v12, inB_stride, v68, v13);
  if ((_DWORD)result)
  {
LABEL_13:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v17 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t outlined init with take of UnsafeMutableRawPointer?(uint64_t a1, uint64_t a2)
{
  uint64_t v4;

  v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(_QWORD *)(v4 - 8) + 32))(a2, a1, v4);
  return a2;
}

uint64_t BNNS.BroadcastMatrixMultiplyLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _OWORD *a5, __int128 *a6, _OWORD *a7)
{
  uint64_t v7;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  size_t v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  uint64_t result;
  _BYTE *v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  size_t v59;
  const void *v60;
  uint64_t v61;
  size_t v62;
  uint64_t v63;
  unint64_t v64;
  unint64_t v65;
  unint64_t v66;
  unint64_t v67;
  unint64_t v68;
  unint64_t v69;
  unint64_t v70;
  unint64_t v71;
  unint64_t v72;
  unint64_t v73;
  unint64_t v74;
  unint64_t v75;
  unint64_t v76;
  unint64_t v77;
  unint64_t v78;
  unint64_t v79;
  BNNSNDArrayDescriptor v80;
  BNNSNDArrayDescriptor v81;
  BNNSNDArrayDescriptor v82;
  BNNSNDArrayDescriptor v83;
  BNNSNDArrayDescriptor v84;
  __int128 v85;
  __int128 v86;
  __int128 v87;
  __int128 v88;
  __int128 v89;
  __int128 v90;
  __int128 v91;
  __int128 v92;
  __int128 v93;
  __int128 v94;
  __int128 v95;
  BNNSNDArrayDescriptor v96;
  uint64_t v97;

  v97 = *MEMORY[0x1E0C80C00];
  v11 = a6[9];
  v93 = a6[8];
  v94 = v11;
  v95 = a6[10];
  v12 = a6[5];
  v89 = a6[4];
  v90 = v12;
  v13 = a6[7];
  v91 = a6[6];
  v92 = v13;
  v14 = a6[1];
  v85 = *a6;
  v86 = v14;
  v15 = a6[3];
  v87 = a6[2];
  v88 = v15;
  v16 = a7[6];
  *(_OWORD *)&v96.stride[5] = a7[7];
  v17 = a7[9];
  *(_OWORD *)&v96.stride[7] = a7[8];
  *(_OWORD *)&v96.data_type = v17;
  *(_OWORD *)&v96.table_data_type = a7[10];
  v18 = a7[2];
  *(_OWORD *)&v96.size[5] = a7[3];
  v19 = a7[5];
  *(_OWORD *)&v96.size[7] = a7[4];
  *(_OWORD *)&v96.stride[1] = v19;
  *(_OWORD *)&v96.stride[3] = v16;
  v20 = a7[1];
  *(_OWORD *)&v96.flags = *a7;
  *(_OWORD *)&v96.size[1] = v20;
  *(_OWORD *)&v96.size[3] = v18;
  if ((*(_BYTE *)(v7 + 24) & 1) != 0)
  {
    v61 = a4;
    v62 = a1;
    v60 = *(const void **)(a3 + 136);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v82);
    outlined init with take of BNNS.Shape((uint64_t)&v82, (uint64_t)&v81);
    outlined init with take of BNNS.Shape((uint64_t)&v81, (uint64_t)&v80);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)&v81, (uint64_t)&v80);
    BNNS.Shape.stride.getter();
    v59 = specialized static BNNS.calculateBatchStride(size:stride:)(v72, v73, v74, v75, v76, v77, v78, v79, v72, v73, v74, v75, v76, v77, v78, v79);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v80);
    outlined init with take of BNNS.Shape((uint64_t)&v80, (uint64_t)&v82);
    outlined init with take of BNNS.Shape((uint64_t)&v82, (uint64_t)&v72);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)&v82, (uint64_t)&v72);
    BNNS.Shape.stride.getter();
    v21 = specialized static BNNS.calculateBatchStride(size:stride:)(v64, v65, v66, v67, v68, v69, v70, v71, v64, v65, v66, v67, v68, v69, v70, v71);
    v22 = a6[9];
    *(_OWORD *)&v83.stride[7] = a6[8];
    *(_OWORD *)&v83.data_type = v22;
    v23 = a6[10];
    v24 = a6[5];
    *(_OWORD *)&v83.size[7] = a6[4];
    *(_OWORD *)&v83.stride[1] = v24;
    v25 = a6[7];
    *(_OWORD *)&v83.stride[3] = a6[6];
    *(_OWORD *)&v83.stride[5] = v25;
    v26 = a6[1];
    *(_OWORD *)&v83.flags = *a6;
    *(_OWORD *)&v83.size[1] = v26;
    v27 = a6[3];
    *(_OWORD *)&v83.size[3] = a6[2];
    *(_OWORD *)&v83.size[5] = v27;
    v84 = v96;
    *(_OWORD *)&v83.table_data_type = v23;
    goto LABEL_5;
  }
  if ((*(_BYTE *)(v7 + 25) & 1) != 0)
  {
    v61 = a4;
    v62 = a1;
    v63 = v7;
    v60 = *(const void **)(a2 + 136);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v82);
    outlined init with take of BNNS.Shape((uint64_t)&v82, (uint64_t)&v81);
    outlined init with take of BNNS.Shape((uint64_t)&v81, (uint64_t)&v80);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)&v81, (uint64_t)&v80);
    BNNS.Shape.stride.getter();
    v59 = specialized static BNNS.calculateBatchStride(size:stride:)(v72, v73, v74, v75, v76, v77, v78, v79, v72, v73, v74, v75, v76, v77, v78, v79);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v80);
    outlined init with take of BNNS.Shape((uint64_t)&v80, (uint64_t)&v82);
    outlined init with take of BNNS.Shape((uint64_t)&v82, (uint64_t)&v72);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)&v82, (uint64_t)&v72);
    BNNS.Shape.stride.getter();
    v21 = specialized static BNNS.calculateBatchStride(size:stride:)(v64, v65, v66, v67, v68, v69, v70, v71, v64, v65, v66, v67, v68, v69, v70, v71);
    v28 = a7[9];
    *(_OWORD *)&v83.stride[7] = a7[8];
    *(_OWORD *)&v83.data_type = v28;
    v29 = a7[5];
    *(_OWORD *)&v83.size[7] = a7[4];
    *(_OWORD *)&v83.stride[1] = v29;
    v30 = a7[7];
    *(_OWORD *)&v83.stride[3] = a7[6];
    *(_OWORD *)&v83.stride[5] = v30;
    v31 = a7[1];
    *(_OWORD *)&v83.flags = *a7;
    *(_OWORD *)&v83.size[1] = v31;
    v32 = a7[3];
    *(_OWORD *)&v83.size[3] = a7[2];
    *(_OWORD *)&v83.size[5] = v32;
    *(_OWORD *)&v84.stride[5] = v92;
    *(_OWORD *)&v84.stride[7] = v93;
    *(_OWORD *)&v84.data_type = v94;
    *(_OWORD *)&v84.table_data_type = v95;
    *(_OWORD *)&v84.size[5] = v88;
    *(_OWORD *)&v84.size[7] = v89;
    v33 = a7[10];
    *(_OWORD *)&v84.stride[1] = v90;
    *(_OWORD *)&v84.stride[3] = v91;
    *(_OWORD *)&v83.table_data_type = v33;
    *(_OWORD *)&v84.flags = v85;
    *(_OWORD *)&v84.size[1] = v86;
    *(_OWORD *)&v84.size[3] = v87;
LABEL_5:
    v34 = a5[9];
    *(_OWORD *)&v82.stride[7] = a5[8];
    *(_OWORD *)&v82.data_type = v34;
    *(_OWORD *)&v82.table_data_type = a5[10];
    v35 = a5[5];
    *(_OWORD *)&v82.size[7] = a5[4];
    *(_OWORD *)&v82.stride[1] = v35;
    v36 = a5[7];
    *(_OWORD *)&v82.stride[3] = a5[6];
    *(_OWORD *)&v82.stride[5] = v36;
    v37 = a5[1];
    *(_OWORD *)&v82.flags = *a5;
    *(_OWORD *)&v82.size[1] = v37;
    v38 = a5[3];
    *(_OWORD *)&v82.size[3] = a5[2];
    *(_OWORD *)&v82.size[5] = v38;
    v81 = v84;
    v80 = v83;
    result = closure #1 in closure #1 in closure #2 in BNNS.BroadcastMatrixMultiplyLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(&v80, v63, v62, v60, v59, &v81, v21, v61, &v82);
    if (!(_DWORD)result)
      return result;
    goto LABEL_6;
  }
  v41 = a5[9];
  *(_OWORD *)&v82.stride[7] = a5[8];
  *(_OWORD *)&v82.data_type = v41;
  *(_OWORD *)&v82.table_data_type = a5[10];
  v42 = a5[5];
  *(_OWORD *)&v82.size[7] = a5[4];
  *(_OWORD *)&v82.stride[1] = v42;
  v43 = a5[7];
  *(_OWORD *)&v82.stride[3] = a5[6];
  *(_OWORD *)&v82.stride[5] = v43;
  v44 = a5[1];
  *(_OWORD *)&v82.flags = *a5;
  *(_OWORD *)&v82.size[1] = v44;
  v45 = a5[3];
  *(_OWORD *)&v82.size[3] = a5[2];
  *(_OWORD *)&v82.size[5] = v45;
  v46 = a6[8];
  v47 = a6[9];
  v48 = a6[6];
  *(_OWORD *)&v81.stride[5] = a6[7];
  *(_OWORD *)&v81.stride[7] = v46;
  v49 = a6[10];
  *(_OWORD *)&v81.data_type = v47;
  *(_OWORD *)&v81.table_data_type = v49;
  v50 = a6[4];
  v51 = a6[5];
  v52 = a6[2];
  *(_OWORD *)&v81.size[5] = a6[3];
  *(_OWORD *)&v81.size[7] = v50;
  *(_OWORD *)&v81.stride[1] = v51;
  *(_OWORD *)&v81.stride[3] = v48;
  v53 = *a6;
  *(_OWORD *)&v81.size[1] = a6[1];
  *(_OWORD *)&v81.size[3] = v52;
  v54 = a7[9];
  *(_OWORD *)&v80.stride[7] = a7[8];
  *(_OWORD *)&v80.data_type = v54;
  *(_OWORD *)&v80.table_data_type = a7[10];
  *(_OWORD *)&v81.flags = v53;
  v55 = a7[5];
  *(_OWORD *)&v80.size[7] = a7[4];
  *(_OWORD *)&v80.stride[1] = v55;
  v56 = a7[7];
  *(_OWORD *)&v80.stride[3] = a7[6];
  *(_OWORD *)&v80.stride[5] = v56;
  v57 = a7[1];
  *(_OWORD *)&v80.flags = *a7;
  *(_OWORD *)&v80.size[1] = v57;
  v58 = a7[3];
  *(_OWORD *)&v80.size[3] = a7[2];
  *(_OWORD *)&v80.size[5] = v58;
  result = closure #1 in closure #1 in closure #1 in BNNS.BroadcastMatrixMultiplyLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(&v80, v7, a1, a2, &v81, (uint64_t)a6, a3, (uint64_t)a7, a4, &v82);
  if ((_DWORD)result)
  {
LABEL_6:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v40 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in BNNS.BroadcastMatrixMultiplyLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(BNNSNDArrayDescriptor *a1, uint64_t a2, size_t a3, uint64_t a4, BNNSNDArrayDescriptor *a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, const BNNSNDArrayDescriptor *out_delta)
{
  size_t v10;
  size_t v11;
  const void *out;
  size_t v15;
  size_t v16;
  void *inB;
  size_t v18;
  size_t v19;
  const void *v20;
  void *v21;
  unint64_t v25;
  unint64_t v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  _BYTE v33[136];
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  _BYTE v42[136];
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  unint64_t v46;
  unint64_t v47;
  unint64_t v48;
  unint64_t v49;
  unint64_t v50;
  _BYTE v51[136];
  unint64_t v52;
  unint64_t v53;
  unint64_t v54;
  unint64_t v55;
  unint64_t v56;
  unint64_t v57;
  unint64_t v58;
  unint64_t v59;
  _BYTE v60[136];
  unint64_t v61;
  unint64_t v62;
  unint64_t v63;
  unint64_t v64;
  unint64_t v65;
  unint64_t v66;
  unint64_t v67;
  unint64_t v68;
  _BYTE v69[136];
  unint64_t v70;
  unint64_t v71;
  unint64_t v72;
  unint64_t v73;
  unint64_t v74;
  unint64_t v75;
  unint64_t v76;
  unint64_t v77;
  _BYTE v78[136];
  _BYTE v79[136];
  _BYTE v80[136];
  _BYTE v81[8];
  _BYTE v82[8];
  _BYTE v83[8];
  const void *v84;
  void *v85;
  const void *v86;

  v21 = *(void **)(a2 + 16);
  outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v83);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v83, (uint64_t)&v84);
  v20 = v84;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v78);
  outlined init with take of BNNS.Shape((uint64_t)v78, (uint64_t)v79);
  outlined init with take of BNNS.Shape((uint64_t)v79, (uint64_t)v80);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v79, (uint64_t)v80);
  BNNS.Shape.stride.getter();
  v19 = specialized static BNNS.calculateBatchStride(size:stride:)(v70, v71, v72, v73, v74, v75, v76, v77, v70, v71, v72, v73, v74, v75, v76, v77);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v70);
  outlined init with take of BNNS.Shape((uint64_t)&v70, (uint64_t)v80);
  outlined init with take of BNNS.Shape((uint64_t)v80, (uint64_t)v69);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v80, (uint64_t)v69);
  BNNS.Shape.stride.getter();
  v18 = specialized static BNNS.calculateBatchStride(size:stride:)(v61, v62, v63, v64, v65, v66, v67, v68, v61, v62, v63, v64, v65, v66, v67, v68);
  outlined init with take of UnsafeMutableRawPointer?(a7 + 136, (uint64_t)v82);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v82, (uint64_t)&v85);
  inB = v85;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v60);
  outlined init with take of BNNS.Shape((uint64_t)v60, (uint64_t)&v61);
  outlined init with take of BNNS.Shape((uint64_t)&v61, (uint64_t)v69);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)&v61, (uint64_t)v69);
  BNNS.Shape.stride.getter();
  v15 = specialized static BNNS.calculateBatchStride(size:stride:)(v52, v53, v54, v55, v56, v57, v58, v59, v52, v53, v54, v55, v56, v57, v58, v59);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v52);
  outlined init with take of BNNS.Shape((uint64_t)&v52, (uint64_t)v69);
  outlined init with take of BNNS.Shape((uint64_t)v69, (uint64_t)v51);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v69, (uint64_t)v51);
  BNNS.Shape.stride.getter();
  v16 = specialized static BNNS.calculateBatchStride(size:stride:)(v43, v44, v45, v46, v47, v48, v49, v50, v43, v44, v45, v46, v47, v48, v49, v50);
  outlined init with take of UnsafeMutableRawPointer?(a9 + 136, (uint64_t)v81);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v81, (uint64_t)&v86);
  out = v86;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v42);
  outlined init with take of BNNS.Shape((uint64_t)v42, (uint64_t)&v43);
  outlined init with take of BNNS.Shape((uint64_t)&v43, (uint64_t)v51);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)&v43, (uint64_t)v51);
  BNNS.Shape.stride.getter();
  v10 = specialized static BNNS.calculateBatchStride(size:stride:)(v34, v35, v36, v37, v38, v39, v40, v41, v34, v35, v36, v37, v38, v39, v40, v41);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v34);
  outlined init with take of BNNS.Shape((uint64_t)&v34, (uint64_t)v51);
  outlined init with take of BNNS.Shape((uint64_t)v51, (uint64_t)v33);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v51, (uint64_t)v33);
  BNNS.Shape.stride.getter();
  v11 = specialized static BNNS.calculateBatchStride(size:stride:)(v25, v26, v27, v28, v29, v30, v31, v32, v25, v26, v27, v28, v29, v30, v31, v32);
  return BNNSFilterApplyBackwardTwoInputBatch(v21, a3, v20, v19, a5, v18, inB, v15, a1, v16, out, v10, out_delta, v11, 0, 0);
}

uint64_t closure #1 in closure #1 in closure #2 in BNNS.BroadcastMatrixMultiplyLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(BNNSNDArrayDescriptor *a1, uint64_t a2, size_t a3, const void *a4, size_t a5, BNNSNDArrayDescriptor *a6, size_t a7, uint64_t a8, const BNNSNDArrayDescriptor *out_delta)
{
  size_t v9;
  size_t v10;
  const void *v12;
  void *v13;
  unint64_t v20;
  unint64_t v21;
  unint64_t v22;
  unint64_t v23;
  unint64_t v24;
  unint64_t v25;
  unint64_t v26;
  unint64_t v27;
  _BYTE v28[136];
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  _BYTE v37[136];
  _BYTE v38[136];
  _BYTE v39[136];
  _BYTE v40[8];
  const void *v41;

  v13 = *(void **)(a2 + 16);
  outlined init with take of UnsafeMutableRawPointer?(a8 + 136, (uint64_t)v40);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v40, (uint64_t)&v41);
  v12 = v41;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v37);
  outlined init with take of BNNS.Shape((uint64_t)v37, (uint64_t)v38);
  outlined init with take of BNNS.Shape((uint64_t)v38, (uint64_t)v39);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v38, (uint64_t)v39);
  BNNS.Shape.stride.getter();
  v9 = specialized static BNNS.calculateBatchStride(size:stride:)(v29, v30, v31, v32, v33, v34, v35, v36, v29, v30, v31, v32, v33, v34, v35, v36);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v29);
  outlined init with take of BNNS.Shape((uint64_t)&v29, (uint64_t)v39);
  outlined init with take of BNNS.Shape((uint64_t)v39, (uint64_t)v28);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v39, (uint64_t)v28);
  BNNS.Shape.stride.getter();
  v10 = specialized static BNNS.calculateBatchStride(size:stride:)(v20, v21, v22, v23, v24, v25, v26, v27, v20, v21, v22, v23, v24, v25, v26, v27);
  return BNNSFilterApplyBackwardBatch(v13, a3, a4, a5, a6, a7, v12, v9, out_delta, v10, a1, 0);
}

void BNNS.BroadcastMatrixMultiplyLayer.__allocating_init(bnnsFilter:)()
{
  _swift_stdlib_reportUnimplementedInitializer();
  __break(1u);
}

uint64_t BNNS.BroadcastMatrixMultiplyLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.BroadcastMatrixMultiplyLayer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

uint64_t method lookup function for BNNS.BroadcastMatrixMultiplyLayer()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of BNNS.BroadcastMatrixMultiplyLayer.apply(batchSize:inputA:inputB:output:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4)
{
  uint64_t v4;
  uint64_t v5;
  int v6;
  uint64_t v7;
  int v8;
  uint64_t v9;
  int v10;
  uint64_t v11;
  int v12;
  uint64_t v13;
  int v14;
  uint64_t v15;
  int v16;
  uint64_t (*v17)(uint64_t, uint64_t *, uint64_t *, uint64_t *);
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  uint64_t v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  uint64_t v33;
  int v34;
  uint64_t v35;
  int v36;
  uint64_t v37;
  uint64_t v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  uint64_t v47;
  int v48;
  uint64_t v49;
  int v50;
  uint64_t v51;
  uint64_t v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  uint64_t v61;
  int v62;
  uint64_t v63;
  int v64;
  uint64_t v65;

  v5 = a2[17];
  v6 = *((_DWORD *)a2 + 36);
  v7 = a2[19];
  v8 = *((_DWORD *)a2 + 40);
  v9 = a3[17];
  v10 = *((_DWORD *)a3 + 36);
  v11 = a3[19];
  v12 = *((_DWORD *)a3 + 40);
  v13 = a4[17];
  v14 = *((_DWORD *)a4 + 36);
  v15 = a4[19];
  v16 = *((_DWORD *)a4 + 40);
  v17 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *))(*(_QWORD *)v4 + 120);
  v52 = *a2;
  v53 = *(_OWORD *)(a2 + 1);
  v54 = *(_OWORD *)(a2 + 3);
  v55 = *(_OWORD *)(a2 + 5);
  v56 = *(_OWORD *)(a2 + 7);
  v57 = *(_OWORD *)(a2 + 9);
  v58 = *(_OWORD *)(a2 + 11);
  v59 = *(_OWORD *)(a2 + 13);
  v60 = *(_OWORD *)(a2 + 15);
  v61 = v5;
  v62 = v6;
  v63 = v7;
  v64 = v8;
  v65 = *(uint64_t *)((char *)a2 + 164);
  v38 = *a3;
  v39 = *(_OWORD *)(a3 + 1);
  v40 = *(_OWORD *)(a3 + 3);
  v41 = *(_OWORD *)(a3 + 5);
  v42 = *(_OWORD *)(a3 + 7);
  v43 = *(_OWORD *)(a3 + 9);
  v44 = *(_OWORD *)(a3 + 11);
  v45 = *(_OWORD *)(a3 + 13);
  v46 = *(_OWORD *)(a3 + 15);
  v47 = v9;
  v48 = v10;
  v49 = v11;
  v50 = v12;
  v51 = *(uint64_t *)((char *)a3 + 164);
  v24 = *a4;
  v25 = *(_OWORD *)(a4 + 1);
  v26 = *(_OWORD *)(a4 + 3);
  v18 = *(_OWORD *)(a4 + 7);
  v19 = *(_OWORD *)(a4 + 9);
  v20 = *(_OWORD *)(a4 + 11);
  v21 = *(_OWORD *)(a4 + 13);
  v22 = *(_OWORD *)(a4 + 15);
  v27 = *(_OWORD *)(a4 + 5);
  v28 = v18;
  v29 = v19;
  v30 = v20;
  v31 = v21;
  v32 = v22;
  v33 = v13;
  v34 = v14;
  v35 = v15;
  v36 = v16;
  v37 = *(uint64_t *)((char *)a4 + 164);
  return v17(a1, &v52, &v38, &v24);
}

uint64_t dispatch thunk of BNNS.BroadcastMatrixMultiplyLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5, uint64_t *a6, uint64_t *a7)
{
  uint64_t v7;
  uint64_t v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  uint64_t v18;
  int v19;
  uint64_t v20;
  int v21;
  uint64_t v22;
  uint64_t v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  uint64_t v32;
  int v33;
  uint64_t v34;
  int v35;
  uint64_t v36;
  uint64_t v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  uint64_t v46;
  int v47;
  uint64_t v48;
  int v49;
  uint64_t v50;
  uint64_t v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  uint64_t v60;
  int v61;
  uint64_t v62;
  int v63;
  uint64_t v64;
  uint64_t v65;
  __int128 v66;
  __int128 v67;
  __int128 v68;
  __int128 v69;
  __int128 v70;
  __int128 v71;
  __int128 v72;
  __int128 v73;
  uint64_t v74;
  int v75;
  uint64_t v76;
  int v77;
  uint64_t v78;
  uint64_t v79;
  __int128 v80;
  __int128 v81;
  __int128 v82;
  __int128 v83;
  __int128 v84;
  __int128 v85;
  __int128 v86;
  __int128 v87;
  uint64_t v88;
  int v89;
  uint64_t v90;
  int v91;
  uint64_t v92;

  v85 = *(_OWORD *)(a2 + 11);
  v86 = *(_OWORD *)(a2 + 13);
  v87 = *(_OWORD *)(a2 + 15);
  v92 = *(uint64_t *)((char *)a2 + 164);
  v80 = *(_OWORD *)(a2 + 1);
  v81 = *(_OWORD *)(a2 + 3);
  v82 = *(_OWORD *)(a2 + 5);
  v83 = *(_OWORD *)(a2 + 7);
  v84 = *(_OWORD *)(a2 + 9);
  v71 = *(_OWORD *)(a3 + 11);
  v72 = *(_OWORD *)(a3 + 13);
  v73 = *(_OWORD *)(a3 + 15);
  v78 = *(uint64_t *)((char *)a3 + 164);
  v66 = *(_OWORD *)(a3 + 1);
  v67 = *(_OWORD *)(a3 + 3);
  v68 = *(_OWORD *)(a3 + 5);
  v69 = *(_OWORD *)(a3 + 7);
  v70 = *(_OWORD *)(a3 + 9);
  v57 = *(_OWORD *)(a4 + 11);
  v58 = *(_OWORD *)(a4 + 13);
  v59 = *(_OWORD *)(a4 + 15);
  v64 = *(uint64_t *)((char *)a4 + 164);
  v52 = *(_OWORD *)(a4 + 1);
  v53 = *(_OWORD *)(a4 + 3);
  v54 = *(_OWORD *)(a4 + 5);
  v55 = *(_OWORD *)(a4 + 7);
  v56 = *(_OWORD *)(a4 + 9);
  v43 = *(_OWORD *)(a5 + 11);
  v44 = *(_OWORD *)(a5 + 13);
  v45 = *(_OWORD *)(a5 + 15);
  v50 = *(uint64_t *)((char *)a5 + 164);
  v38 = *(_OWORD *)(a5 + 1);
  v39 = *(_OWORD *)(a5 + 3);
  v40 = *(_OWORD *)(a5 + 5);
  v41 = *(_OWORD *)(a5 + 7);
  v42 = *(_OWORD *)(a5 + 9);
  v29 = *(_OWORD *)(a6 + 11);
  v30 = *(_OWORD *)(a6 + 13);
  v31 = *(_OWORD *)(a6 + 15);
  v36 = *(uint64_t *)((char *)a6 + 164);
  v79 = *a2;
  v65 = *a3;
  v51 = *a4;
  v37 = *a5;
  v23 = *a6;
  v24 = *(_OWORD *)(a6 + 1);
  v25 = *(_OWORD *)(a6 + 3);
  v26 = *(_OWORD *)(a6 + 5);
  v27 = *(_OWORD *)(a6 + 7);
  v28 = *(_OWORD *)(a6 + 9);
  v9 = *a7;
  v10 = *(_OWORD *)(a7 + 1);
  v11 = *(_OWORD *)(a7 + 3);
  v12 = *(_OWORD *)(a7 + 5);
  v13 = *(_OWORD *)(a7 + 7);
  v14 = *(_OWORD *)(a7 + 9);
  v15 = *(_OWORD *)(a7 + 11);
  v16 = *(_OWORD *)(a7 + 13);
  v17 = *(_OWORD *)(a7 + 15);
  v22 = *(uint64_t *)((char *)a7 + 164);
  v88 = a2[17];
  v89 = *((_DWORD *)a2 + 36);
  v90 = a2[19];
  v91 = *((_DWORD *)a2 + 40);
  v74 = a3[17];
  v75 = *((_DWORD *)a3 + 36);
  v76 = a3[19];
  v77 = *((_DWORD *)a3 + 40);
  v60 = a4[17];
  v61 = *((_DWORD *)a4 + 36);
  v62 = a4[19];
  v63 = *((_DWORD *)a4 + 40);
  v46 = a5[17];
  v47 = *((_DWORD *)a5 + 36);
  v48 = a5[19];
  v49 = *((_DWORD *)a5 + 40);
  v32 = a6[17];
  v33 = *((_DWORD *)a6 + 36);
  v34 = a6[19];
  v35 = *((_DWORD *)a6 + 40);
  v18 = a7[17];
  v19 = *((_DWORD *)a7 + 36);
  v20 = a7[19];
  v21 = *((_DWORD *)a7 + 40);
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(_QWORD *)v7 + 128))(a1, &v79, &v65, &v51, &v37, &v23, &v9);
}

uint64_t static vDSP.float16ToFloat<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.float16ToFloat<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vDSP.float16ToFloat<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.float16ToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:), static vDSP.convertElements<A, B>(of:to:));
}

uint64_t closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t *v6;
  uint64_t v7;
  uint64_t inited;
  _QWORD v9[11];

  if (!a2)
    goto LABEL_10;
  if (a4 + 0x4000000000000000 < 0)
  {
    __break(1u);
    goto LABEL_8;
  }
  v6 = (uint64_t *)result;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  result = swift_initStackObject();
  *(_OWORD *)(result + 16) = xmmword_1CAB5E430;
  if (a4 < 0)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  *(_QWORD *)(result + 32) = a2;
  *(_QWORD *)(result + 40) = 1;
  *(_QWORD *)(result + 48) = a4;
  *(_QWORD *)(result + 56) = 2 * a4;
  *(_QWORD *)(result + 64) = 0;
  v9[10] = result;
  v7 = *v6;
  if (v7)
  {
    if ((unint64_t)(a4 - 0x2000000000000000) >> 62 == 3)
    {
      inited = swift_initStackObject();
      *(_OWORD *)(inited + 16) = xmmword_1CAB5E430;
      *(_QWORD *)(inited + 32) = v7;
      *(_QWORD *)(inited + 40) = 1;
      *(_QWORD *)(inited + 48) = a4;
      *(_QWORD *)(inited + 56) = 4 * a4;
      *(_QWORD *)(inited + 64) = 0;
      v9[0] = inited;
      vImage.PixelBuffer<>.convert(to:)((uint64_t)v9);
      swift_bridgeObjectRelease();
      return swift_bridgeObjectRelease();
    }
    goto LABEL_9;
  }
LABEL_11:
  __break(1u);
  return result;
}

{
  uint64_t *v6;
  uint64_t v7;
  uint64_t inited;
  _QWORD v9[12];

  if (!a2)
    goto LABEL_8;
  if ((unint64_t)(a4 - 0x2000000000000000) >> 62 != 3)
  {
    __break(1u);
    goto LABEL_7;
  }
  v6 = (uint64_t *)result;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  result = swift_initStackObject();
  *(_OWORD *)(result + 16) = xmmword_1CAB5E430;
  if (a4 < 0)
  {
LABEL_7:
    __break(1u);
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  *(_QWORD *)(result + 32) = a2;
  *(_QWORD *)(result + 40) = 1;
  *(_QWORD *)(result + 48) = a4;
  *(_QWORD *)(result + 56) = 4 * a4;
  *(_QWORD *)(result + 64) = 0;
  v9[11] = result;
  v7 = *v6;
  if (*v6)
  {
    inited = swift_initStackObject();
    *(_OWORD *)(inited + 16) = xmmword_1CAB5E430;
    *(_QWORD *)(inited + 32) = v7;
    *(_QWORD *)(inited + 40) = 1;
    *(_QWORD *)(inited + 48) = a4;
    *(_QWORD *)(inited + 56) = 2 * a4;
    *(_QWORD *)(inited + 64) = 0;
    v9[0] = inited;
    vImage.PixelBuffer<>.convert(to:)((uint64_t)v9);
    swift_bridgeObjectRelease();
    return swift_bridgeObjectRelease();
  }
LABEL_9:
  __break(1u);
  return result;
}

uint64_t static vDSP.convertElements<A, B>(of:to:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return back deployment fallback for static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return back deployment fallback for static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

uint64_t back deployment fallback for static vDSP.convertElements<A, B>(of:to:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v12;
  uint64_t v13;
  char *v14;
  uint64_t (*v15)(uint64_t, uint64_t);
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t result;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;

  v26 = a7;
  v27 = a4;
  v12 = *(_QWORD *)(a3 - 8);
  MEMORY[0x1E0C80A78](a1);
  v14 = (char *)&v25 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(char *, uint64_t))(v12 + 16))(v14, a1);
  v15 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  v16 = v15(a3, a5);
  v25 = a6;
  v17 = *(_QWORD *)(a6 + 8);
  v18 = a2;
  v19 = v27;
  v20 = (*(uint64_t (**)(uint64_t))(v17 + 16))(v27);
  result = (*(uint64_t (**)(char *, uint64_t))(v12 + 8))(v14, a3);
  if (v16 == v20)
  {
    v22 = v15(a3, a5);
    v23 = MEMORY[0x1E0C80A78](v22);
    *(&v25 - 6) = a3;
    *(&v25 - 5) = v19;
    v24 = v25;
    *(&v25 - 4) = a5;
    *(&v25 - 3) = v24;
    *(&v25 - 2) = v18;
    *(&v25 - 1) = v23;
    return (*(uint64_t (**)(uint64_t))(a5 + 24))(v26);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in static vDSP.float16ToFloat<A>(_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t *a6, unint64_t *a7, uint64_t a8, uint64_t (*a9)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16;
  uint64_t v17;
  uint64_t result;

  v16 = __swift_instantiateConcreteTypeFromMangledName(a6);
  v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a7, a6);
  static vDSP.convertElements<A, B>(of:to:)(a3, a1, a4, v16, a5, v17, a8, a9);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.convertElements<A, B>(of:to:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  if (__isPlatformVersionAtLeast(2, 18, 0, 0))
    return a8(a1, a2, a3, a4, a5, a6);
  else
    return back deployment fallback for static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, a7);
}

uint64_t static vDSP.floatToFloat16<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.floatToFloat16<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.convertElements<A, B>(of:to:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v12;
  uint64_t v13;
  char *v14;
  uint64_t (*v15)(uint64_t, uint64_t);
  uint64_t v16;
  uint64_t (*v17)(uint64_t);
  uint64_t v18;
  uint64_t v19;
  uint64_t result;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;

  v23 = a7;
  v24 = a4;
  v12 = *(_QWORD *)(a3 - 8);
  MEMORY[0x1E0C80A78](a1);
  v14 = (char *)&v22 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(char *, uint64_t))(v12 + 16))(v14, a1);
  v15 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  v16 = v15(a3, a5);
  v17 = *(uint64_t (**)(uint64_t))(*(_QWORD *)(a6 + 8) + 16);
  v22 = a2;
  v18 = v24;
  v19 = v17(v24);
  result = (*(uint64_t (**)(char *, uint64_t))(v12 + 8))(v14, a3);
  if (v16 == v19)
  {
    result = v15(a3, a5);
    if ((result & 0x8000000000000000) == 0)
    {
      v21 = MEMORY[0x1E0C80A78](result);
      *(&v22 - 6) = a3;
      *(&v22 - 5) = v18;
      *(&v22 - 4) = a5;
      *(&v22 - 3) = a6;
      *(&v22 - 2) = a1;
      *(&v22 - 1) = v21;
      return (*(uint64_t (**)(uint64_t))(a6 + 16))(v23);
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

{
  uint64_t v10;
  uint64_t result;
  uint64_t v12;

  v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  result = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a6 + 8) + 16))(a4);
  if (result >= v10)
    v12 = v10;
  else
    v12 = result;
  if (v12 < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1E0C80A78](result);
    return (*(uint64_t (**)(uint64_t))(a6 + 16))(a7);
  }
  return result;
}

BOOL static vDSP.RoundingMode.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void vDSP.RoundingMode.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

Swift::Int vDSP.RoundingMode.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t static vDSP.convertElements<A, B>(of:to:rounding:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

uint64_t static vDSP.convertElements<A, B>(of:to:rounding:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v11;
  uint64_t result;
  uint64_t v13;

  v11 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  result = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a7 + 8) + 16))(a5);
  if (result >= v11)
    v13 = v11;
  else
    v13 = result;
  if (v13 < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1E0C80A78](result);
    return (*(uint64_t (**)(uint64_t))(a7 + 16))(a8);
  }
  return result;
}

uint64_t closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(uint64_t result, uint64_t a2, char a3, _QWORD *a4, uint64_t a5, uint64_t (*a6)(void), uint64_t (*a7)(void))
{
  if ((a3 & 1) == 0)
  {
    if (!result)
    {
      __break(1u);
      goto LABEL_9;
    }
    if (*a4)
      return a6();
    __break(1u);
  }
  if (!result)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  a6 = a7;
  if (*a4)
    return a6();
LABEL_10:
  __break(1u);
  return result;
}

uint64_t static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:), (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:));
}

{
  return static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:), (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:));
}

{
  return static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:), (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:));
}

{
  return static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:), (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:));
}

{
  return static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:), (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:));
}

{
  return static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:), (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:));
}

uint64_t closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t *a8, unint64_t *a9, void (*a10)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16;
  uint64_t v17;
  uint64_t result;

  v16 = __swift_instantiateConcreteTypeFromMangledName(a8);
  v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a9, a8);
  a10(a3, a1, a4, v16, a6, v17);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  *a2 = result;
  return result;
}

uint64_t static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(_QWORD *, uint64_t *), uint64_t (*a8)(_QWORD *, uint64_t *))
{
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t result;
  Swift::String v18;

  if (swift_dynamicCastMetatype())
  {
    v12 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
    v13 = MEMORY[0x1E0C80A78](v12);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v13, a7);
LABEL_5:
    v16 = _arrayForceCast<A, B>(_:)();
    swift_bridgeObjectRelease();
    return v16;
  }
  if (swift_dynamicCastMetatype())
  {
    v14 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
    v15 = MEMORY[0x1E0C80A78](v14);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v15, a8);
    goto LABEL_5;
  }
  _StringGuts.grow(_:)(39);
  _typeName(_:qualified:)();
  swift_bridgeObjectRelease();
  v18._object = (void *)0x80000001CAB657C0;
  v18._countAndFlagsBits = 0xD000000000000025;
  String.append(_:)(v18);
  result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t result;
  Swift::String v22;

  if (swift_dynamicCastMetatype())
  {
    v8 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    v9 = MEMORY[0x1E0C80A78](v8);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v9, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
LABEL_13:
    v20 = _arrayForceCast<A, B>(_:)();
    swift_bridgeObjectRelease();
    return v20;
  }
  if (swift_dynamicCastMetatype())
  {
    v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    v11 = MEMORY[0x1E0C80A78](v10);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v11, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #2 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  if (swift_dynamicCastMetatype())
  {
    v12 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    v13 = MEMORY[0x1E0C80A78](v12);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v13, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #3 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  if (swift_dynamicCastMetatype())
  {
    v14 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    v15 = MEMORY[0x1E0C80A78](v14);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v15, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #4 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  if (swift_dynamicCastMetatype())
  {
    v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    v17 = MEMORY[0x1E0C80A78](v16);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v17, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #5 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  if (swift_dynamicCastMetatype())
  {
    v18 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    v19 = MEMORY[0x1E0C80A78](v18);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v19, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  _StringGuts.grow(_:)(39);
  _typeName(_:qualified:)();
  swift_bridgeObjectRelease();
  v22._object = (void *)0x80000001CAB657C0;
  v22._countAndFlagsBits = 0xD000000000000025;
  String.append(_:)(v22);
  result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

{
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t result;
  Swift::String v22;

  if (swift_dynamicCastMetatype())
  {
    v8 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    v9 = MEMORY[0x1E0C80A78](v8);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v9, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
LABEL_13:
    v20 = _arrayForceCast<A, B>(_:)();
    swift_bridgeObjectRelease();
    return v20;
  }
  if (swift_dynamicCastMetatype())
  {
    v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    v11 = MEMORY[0x1E0C80A78](v10);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v11, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #2 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  if (swift_dynamicCastMetatype())
  {
    v12 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    v13 = MEMORY[0x1E0C80A78](v12);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v13, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #3 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  if (swift_dynamicCastMetatype())
  {
    v14 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    v15 = MEMORY[0x1E0C80A78](v14);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v15, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #4 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  if (swift_dynamicCastMetatype())
  {
    v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    v17 = MEMORY[0x1E0C80A78](v16);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v17, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #5 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  if (swift_dynamicCastMetatype())
  {
    v18 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    v19 = MEMORY[0x1E0C80A78](v18);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v19, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  _StringGuts.grow(_:)(39);
  _typeName(_:qualified:)();
  swift_bridgeObjectRelease();
  v22._object = (void *)0x80000001CAB657C0;
  v22._countAndFlagsBits = 0xD000000000000025;
  String.append(_:)(v22);
  result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t closure #1 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, unint64_t *a10, void (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16;
  uint64_t v17;
  uint64_t result;

  v16 = __swift_instantiateConcreteTypeFromMangledName(a9);
  v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a10, a9);
  a11(a3, a1, a4, a5, v16, a7, v17);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  *a2 = result;
  return result;
}

uint64_t static vDSP.floatToDouble<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.floatToDouble<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t closure #1 in static vDSP.doubleToFloat<A>(_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t *a6, unint64_t *a7, void (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16;
  uint64_t v17;
  uint64_t result;

  v16 = __swift_instantiateConcreteTypeFromMangledName(a6);
  v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a7, a6);
  a8(a3, a1, a4, v16, a5, v17);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.doubleToFloat<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.doubleToFloat<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.float16ToFloat<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(uint64_t, uint64_t, _QWORD *))
{
  uint64_t v10;
  _QWORD v12[6];

  v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  v12[2] = a2;
  v12[3] = a3;
  v12[4] = a1;
  return a5(v10, a4, v12);
}

uint64_t partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  _QWORD v6[3];

  v3 = *(_QWORD *)(v2 + 16);
  v4 = *(_QWORD *)(v2 + 32);
  v6[2] = a1;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v4 + 24))(a2, v6, MEMORY[0x1E0DEE9C0] + 8, v3);
}

uint64_t partial apply for closure #1 in static vDSP.floatToFloat16<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.float16ToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float16>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float16> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:), static vDSP.convertElements<A, B>(of:to:));
}

uint64_t partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

uint64_t partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

uint64_t partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  _BYTE v7[24];
  uint64_t v8;
  uint64_t v9;

  v3 = *(_QWORD *)(v2 + 16);
  v4 = *(_QWORD *)(v2 + 32);
  v5 = *(_QWORD *)(v2 + 64);
  v7[16] = *(_BYTE *)(v2 + 56);
  v8 = a1;
  v9 = v5;
  return (*(uint64_t (**)(uint64_t, _BYTE *, uint64_t, uint64_t))(v4 + 24))(a2, v7, MEMORY[0x1E0DEE9C0] + 8, v3);
}

uint64_t partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

uint64_t partial apply for closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

uint64_t partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t *v5;

  return closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, v5[6], v5[2], v5[3], v5[4], v5[5], a3, a4, a5);
}

uint64_t partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Int32>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Int32> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Int32>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Int32> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

uint64_t partial apply for closure #5 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Int16>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Int16> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Int16>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Int16> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

uint64_t partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t *v5;

  return closure #1 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, v5[6], v5[7], v5[2], v5[3], v5[4], v5[5], a3, a4, a5);
}

uint64_t partial apply for closure #4 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Int8>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Int8> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Int8>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Int8> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

uint64_t partial apply for closure #3 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<UInt32>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<UInt32> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<UInt32>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<UInt32> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

uint64_t partial apply for closure #2 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<UInt16>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<UInt16> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<UInt16>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<UInt16> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

uint64_t partial apply for closure #1 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<UInt8>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<UInt8> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<UInt8>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<UInt8> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

uint64_t partial apply for closure #1 in static vDSP.floatToDouble<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

uint64_t partial apply for closure #1 in static vDSP.doubleToFloat<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

unint64_t lazy protocol witness table accessor for type vDSP.RoundingMode and conformance vDSP.RoundingMode()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vDSP.RoundingMode and conformance vDSP.RoundingMode;
  if (!lazy protocol witness table cache variable for type vDSP.RoundingMode and conformance vDSP.RoundingMode)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vDSP.RoundingMode, &type metadata for vDSP.RoundingMode);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.RoundingMode and conformance vDSP.RoundingMode);
  }
  return result;
}

uint64_t storeEnumTagSinglePayload for vDSP.RoundingMode(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 1 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 1) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFF)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFE)
    return ((uint64_t (*)(void))((char *)&loc_1CAAE7FEC + 4 * byte_1CAB5FCC5[v4]))();
  *a1 = a2 + 1;
  return ((uint64_t (*)(void))((char *)sub_1CAAE8020 + 4 * byte_1CAB5FCC0[v4]))();
}

uint64_t sub_1CAAE8020(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAAE8028(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAAE8030);
  return result;
}

uint64_t sub_1CAAE803C(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAAE8044);
  *(_BYTE *)result = a2 + 1;
  return result;
}

uint64_t sub_1CAAE8048(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAAE8050(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vDSP.RoundingMode()
{
  return &type metadata for vDSP.RoundingMode;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C458]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C918]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C5C0]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C5B8]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C5A0]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C598]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C5D8]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C5C8]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C600]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C5F8]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C5F0]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C5E8]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C618]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C608]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(_BYTE *)(v2 + 16), *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C588], MEMORY[0x1E0C8C540]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(_BYTE *)(v2 + 16), *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C580], MEMORY[0x1E0C8C530]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(_BYTE *)(v2 + 16), *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C4C8], MEMORY[0x1E0C8C508]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(_BYTE *)(v2 + 16), *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C4B8], MEMORY[0x1E0C8C4F8]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(_BYTE *)(v2 + 16), *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C498], MEMORY[0x1E0C8C4D8]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(_BYTE *)(v2 + 16), *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C488], MEMORY[0x1E0C8C4D0]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(_BYTE *)(v2 + 16), *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C570], MEMORY[0x1E0C8C528]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(_BYTE *)(v2 + 16), *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C568], MEMORY[0x1E0C8C520]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(_BYTE *)(v2 + 16), *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C558], MEMORY[0x1E0C8C518]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(_BYTE *)(v2 + 16), *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C550], MEMORY[0x1E0C8C510]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(_BYTE *)(v2 + 16), *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C4B0], MEMORY[0x1E0C8C4F0]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(_BYTE *)(v2 + 16), *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C4A8], MEMORY[0x1E0C8C4E0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(uint64_t result, uint64_t a2, uint64_t (*a3)(void))
{
  uint64_t v3;

  if (result)
  {
    if (**(_QWORD **)(v3 + 16))
      return a3();
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(uint64_t a1)
{
  uint64_t *v1;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, v1[2], v1[3], v1[4]);
}

{
  uint64_t *v1;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, v1[2], v1[3], v1[4]);
}

uint64_t partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  _QWORD *v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD v8[6];

  v4 = v3[3];
  v5 = v3[5];
  v6 = v3[7];
  v8[2] = a1;
  v8[3] = a2;
  v8[4] = v6;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v5 + 16))(a3, v8, MEMORY[0x1E0DEE9C0] + 8, v4);
}

uint64_t static BNNS.applyTopK(k:input:bestValues:bestIndices:axis:batchSize:filterParameters:)(uint64_t a1, _OWORD *a2, _OWORD *a3, _OWORD *a4, uint64_t a5, uint64_t a6, int a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return static BNNS.applyTopK(k:input:bestValues:bestIndices:axis:batchSize:filterParameters:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, MEMORY[0x1E0C8B5A0]);
}

uint64_t static BNNS.applyInTopK(k:input:testIndices:output:axis:batchSize:filterParameters:)(uint64_t a1, _OWORD *a2, _OWORD *a3, _OWORD *a4, uint64_t a5, uint64_t a6, int a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return static BNNS.applyTopK(k:input:bestValues:bestIndices:axis:batchSize:filterParameters:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, MEMORY[0x1E0C8B570]);
}

uint64_t static BNNS.applyTopK(k:input:bestValues:bestIndices:axis:batchSize:filterParameters:)(uint64_t a1, _OWORD *a2, _OWORD *a3, _OWORD *a4, uint64_t a5, uint64_t a6, int a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, unint64_t, uint64_t, unint64_t, uint64_t, unint64_t, uint64_t))
{
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  uint64_t result;
  _BYTE *v27;
  int v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  _OWORD v32[11];
  _OWORD v33[11];
  _OWORD v34[11];
  uint64_t v35;

  v35 = *MEMORY[0x1E0C80C00];
  v11 = a2[9];
  v34[8] = a2[8];
  v34[9] = v11;
  v34[10] = a2[10];
  v12 = a2[5];
  v34[4] = a2[4];
  v34[5] = v12;
  v13 = a2[7];
  v34[6] = a2[6];
  v34[7] = v13;
  v14 = a2[1];
  v34[0] = *a2;
  v34[1] = v14;
  v15 = a2[3];
  v34[2] = a2[2];
  v34[3] = v15;
  v16 = a3[9];
  v33[8] = a3[8];
  v33[9] = v16;
  v33[10] = a3[10];
  v17 = a3[5];
  v33[4] = a3[4];
  v33[5] = v17;
  v18 = a3[7];
  v33[6] = a3[6];
  v33[7] = v18;
  v19 = a3[1];
  v33[0] = *a3;
  v33[1] = v19;
  v20 = a3[3];
  v33[2] = a3[2];
  v33[3] = v20;
  v21 = a4[9];
  v32[8] = a4[8];
  v32[9] = v21;
  v32[10] = a4[10];
  v22 = a4[5];
  v32[4] = a4[4];
  v32[5] = v22;
  v23 = a4[7];
  v32[6] = a4[6];
  v32[7] = v23;
  v24 = a4[1];
  v32[0] = *a4;
  v32[1] = v24;
  v25 = a4[3];
  v32[2] = a4[2];
  v32[3] = v25;
  if (a9 == 1)
  {
    result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.applyTopK(k:input:bestValues:bestIndices:axis:batchSize:filterParameters:)(0, a1, a5, a6, (uint64_t)v34, (uint64_t)a2, (uint64_t)v33, (uint64_t)a3, (uint64_t)v32, (uint64_t)a4, a11);
  }
  else
  {
    v28 = a7;
    v29 = a8;
    v30 = a9;
    v31 = a10;
    result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.applyTopK(k:input:bestValues:bestIndices:axis:batchSize:filterParameters:)((uint64_t)&v28, a1, a5, a6, (uint64_t)v34, (uint64_t)a2, (uint64_t)v33, (uint64_t)a3, (uint64_t)v32, (uint64_t)a4, a11);
  }
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v27 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.applyTopK(k:input:bestValues:bestIndices:axis:batchSize:filterParameters:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, unint64_t, uint64_t, unint64_t, uint64_t, unint64_t, uint64_t))
{
  unint64_t v11;
  unint64_t v12;
  unint64_t v13;
  unint64_t v14;
  unint64_t v15;
  unint64_t v16;
  unint64_t v17;
  unint64_t v18;
  unint64_t v19;
  unint64_t v20;
  unint64_t v22;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  _BYTE v37[136];
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  _BYTE v46[136];
  _BYTE v47[136];
  _BYTE v48[136];
  unint64_t v49;
  unint64_t v50;
  unint64_t v51;
  unint64_t v52;
  unint64_t v53;
  unint64_t v54;
  unint64_t v55;
  unint64_t v56;
  _BYTE v57[144];

  BNNSNDArrayDescriptor.shape.getter((uint64_t)v47);
  outlined init with take of BNNS.Shape((uint64_t)v47, (uint64_t)v48);
  outlined init with take of BNNS.Shape((uint64_t)v48, (uint64_t)v57);
  BNNS.Shape.size.getter();
  v11 = v49;
  v12 = v50;
  v13 = v51;
  v14 = v52;
  v15 = v53;
  v16 = v54;
  v17 = v55;
  v18 = v56;
  outlined init with take of BNNS.Shape((uint64_t)v48, (uint64_t)v57);
  BNNS.Shape.stride.getter();
  v22 = specialized static BNNS.calculateBatchStride(size:stride:)(v11, v12, v13, v14, v15, v16, v17, v18, v49, v50, v51, v52, v53, v54, v55, v56);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v46);
  outlined init with take of BNNS.Shape((uint64_t)v46, (uint64_t)&v49);
  outlined init with take of BNNS.Shape((uint64_t)&v49, (uint64_t)v57);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)&v49, (uint64_t)v57);
  BNNS.Shape.stride.getter();
  v19 = specialized static BNNS.calculateBatchStride(size:stride:)(v38, v39, v40, v41, v42, v43, v44, v45, v38, v39, v40, v41, v42, v43, v44, v45);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v38);
  outlined init with take of BNNS.Shape((uint64_t)&v38, (uint64_t)v57);
  outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v37);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v37);
  BNNS.Shape.stride.getter();
  v20 = specialized static BNNS.calculateBatchStride(size:stride:)(v29, v30, v31, v32, v33, v34, v35, v36, v29, v30, v31, v32, v33, v34, v35, v36);
  return a11(a2, a3, a4, a5, v22, a7, v19, a9, v20, a1);
}

uint64_t static vDSP.taperedMerge<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  char *v24;
  void (*v25)(char *, uint64_t, uint64_t);
  uint64_t v26;
  void (*v27)(char *, uint64_t, uint64_t);
  uint64_t (*v28)(uint64_t, uint64_t);
  uint64_t v29;
  uint64_t (*v30)(uint64_t, uint64_t);
  uint64_t v31;
  void (*v32)(char *, uint64_t);
  uint64_t (*v33)(char *, uint64_t);
  uint64_t result;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  void (*v46)(char *, uint64_t, uint64_t);
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t (*v51)(uint64_t, uint64_t);

  v47 = a3;
  v48 = a6;
  v14 = *(_QWORD *)(a5 - 8);
  v15 = MEMORY[0x1E0C80A78](a1);
  v17 = (char *)&v43 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = *(_QWORD *)(v18 - 8);
  v20 = MEMORY[0x1E0C80A78](v15);
  v22 = (char *)&v43 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1E0C80A78](v20);
  v24 = (char *)&v43 - v23;
  v25 = *(void (**)(char *, uint64_t, uint64_t))(v19 + 16);
  v50 = v26;
  v46 = v25;
  ((void (*)(char *))v25)((char *)&v43 - v23);
  v27 = *(void (**)(char *, uint64_t, uint64_t))(v14 + 16);
  v44 = a2;
  v27(v17, a2, a5);
  v28 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  v49 = a7;
  v51 = v28;
  v29 = v28(a4, a7);
  v30 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  v45 = a8;
  v31 = v30(a5, a8);
  v32 = *(void (**)(char *, uint64_t))(v14 + 8);
  v43 = a5;
  v32(v17, a5);
  v33 = *(uint64_t (**)(char *, uint64_t))(v19 + 8);
  result = v33(v24, a4);
  if (v29 != v31)
  {
    __break(1u);
    goto LABEL_6;
  }
  v46(v22, v50, a4);
  v35 = v49;
  v36 = v51(a4, v49);
  v37 = v47;
  v38 = v48;
  v39 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a9 + 8) + 16))(v48);
  result = v33(v22, a4);
  if (v36 != v39)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  result = v51(a4, v35);
  if ((result & 0x8000000000000000) == 0)
  {
    v40 = MEMORY[0x1E0C80A78](result);
    v41 = v43;
    *(&v43 - 10) = a4;
    *(&v43 - 9) = v41;
    *(&v43 - 8) = v38;
    *(&v43 - 7) = v35;
    *(&v43 - 6) = v45;
    *(&v43 - 5) = a9;
    *(&v43 - 4) = v44;
    *(&v43 - 3) = v37;
    *(&v43 - 2) = v40;
    return (*(uint64_t (**)(uint64_t))(v35 + 24))(v42);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  _QWORD v7[3];
  __int128 v8;
  uint64_t v9;
  __int128 v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;

  v4 = *(_QWORD *)(v3 + 40);
  v5 = *(_QWORD *)(v3 + 72);
  v7[2] = *(_QWORD *)(v3 + 16);
  v8 = *(_OWORD *)(v3 + 24);
  v9 = v4;
  v10 = *(_OWORD *)(v3 + 48);
  v11 = v5;
  v12 = a1;
  v13 = a2;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, _QWORD))(v10 + 24))(a3, v7, MEMORY[0x1E0DEE9C0] + 8, v8);
}

_QWORD *closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(_QWORD *result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, uint64_t, uint64_t, _QWORD, uint64_t, uint64_t))
{
  if (!a2)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!a4)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*result)
    return (_QWORD *)a7(a2, 1, a4, 1, *result, 1, a6);
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vDSP.taperedMerge<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.taperedMerge<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.taperedMerge<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.taperedMerge<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.taperedMerge<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vDSP.taperedMerge<A, B>(_:_:)(uint64_t a1, _QWORD *a2)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vDSP.taperedMerge<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vDSP.taperedMerge<A, B, C>(_:_:result:));
}

uint64_t static vDSP.taperedMerge<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, _QWORD *))
{
  uint64_t v16;
  _QWORD v18[10];

  v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  v18[2] = a3;
  v18[3] = a4;
  v18[4] = a5;
  v18[5] = a6;
  v18[6] = a1;
  v18[7] = a2;
  v18[8] = v16;
  return a8(v16, a7, v18);
}

uint64_t static vDSP.swapElements<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.swapElements<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.swapElements<A, B>(_:_:));
}

{
  return static vDSP.swapElements<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.swapElements<A, B>(_:_:));
}

uint64_t static vDSP.swapElements<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v11;
  uint64_t (*v12)(uint64_t, uint64_t);
  uint64_t v13;
  uint64_t result;

  v11 = *(_QWORD *)(a5 + 8);
  v12 = *(uint64_t (**)(uint64_t, uint64_t))(v11 + 16);
  v13 = v12(a3, v11);
  result = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a6 + 8) + 16))(a4);
  if (v13 == result)
  {
    result = v12(a3, v11);
    if ((result & 0x8000000000000000) == 0)
    {
      MEMORY[0x1E0C80A78](result);
      return (*(uint64_t (**)(uint64_t))(a5 + 16))(a7);
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in static vDSP.swapElements<A, B>(_:_:)(_QWORD *a1, uint64_t *a2, uint64_t a3, uint64_t (*a4)(void))
{
  uint64_t result;

  result = *a2;
  if (*a2)
  {
    if (*a1)
      return a4();
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t static vDSP.gather<A, B, C>(_:indices:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.gather<A, B, C>(_:indices:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vDSP.gather<A, B, C>(_:indices:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v11;
  uint64_t v12;
  char *v13;
  void (*v14)(char *);
  uint64_t v15;
  uint64_t (*v16)(uint64_t, uint64_t);
  uint64_t v17;
  uint64_t v18;
  uint64_t (*v19)(uint64_t, uint64_t);
  uint64_t v20;
  uint64_t v21;
  uint64_t result;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;

  v31 = a7;
  v33 = a6;
  v28 = a4;
  v29 = a1;
  v11 = *(_QWORD *)(a5 - 8);
  MEMORY[0x1E0C80A78](a1);
  v13 = (char *)&v27 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  v14 = *(void (**)(char *))(v11 + 16);
  v30 = v15;
  v14(v13);
  v16 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  v32 = a8;
  v17 = v16(a5, a8);
  v18 = *(_QWORD *)(a9 + 8);
  v19 = *(uint64_t (**)(uint64_t, uint64_t))(v18 + 16);
  v20 = v33;
  v21 = v19(v33, v18);
  result = (*(uint64_t (**)(char *, uint64_t))(v11 + 8))(v13, a5);
  if (v17 == v21)
  {
    result = v19(v20, v18);
    if ((result & 0x8000000000000000) == 0)
    {
      v23 = MEMORY[0x1E0C80A78](result);
      *(&v27 - 10) = v28;
      *(&v27 - 9) = a5;
      v24 = v31;
      *(&v27 - 8) = v20;
      *(&v27 - 7) = v24;
      *(&v27 - 6) = v32;
      *(&v27 - 5) = a9;
      v25 = v30;
      *(&v27 - 4) = v29;
      *(&v27 - 3) = v25;
      *(&v27 - 2) = v23;
      return (*(uint64_t (**)(uint64_t))(a9 + 16))(v26);
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, _QWORD *a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, uint64_t, _QWORD, uint64_t))
{
  if (!a3)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!result)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*a5)
    return a7(a3, result, 1, *a5, 1);
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vDSP.gather<A, B>(_:indices:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.gather<A, B>(_:indices:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.gather<A, B>(_:indices:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.gather<A, B>(_:indices:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.gather<A, B>(_:indices:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.gather<A, B>(_:indices:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, _QWORD *))
{
  uint64_t v16;
  _QWORD v18[10];

  v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  v18[2] = a3;
  v18[3] = a4;
  v18[4] = a5;
  v18[5] = a6;
  v18[6] = a1;
  v18[7] = a2;
  v18[8] = v16;
  return a8(v16, a7, v18);
}

uint64_t static vDSP.compress<A, B, C>(_:gatingVector:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  return static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, a3, a4, a5, a6, a7, a8);
}

{
  return static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, a3, a4, a5, a6, a7, a8);
}

{
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  void (*v21)(char *);
  uint64_t v22;
  void (*v23)(char *, uint64_t, uint64_t);
  uint64_t (*v24)(uint64_t, uint64_t);
  uint64_t v25;
  uint64_t (*v26)(uint64_t, uint64_t);
  uint64_t v27;
  uint64_t result;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;

  v38 = a3;
  v39 = a6;
  v13 = *(_QWORD *)(a5 - 8);
  v14 = MEMORY[0x1E0C80A78](a1);
  v16 = (char *)&v35 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  v18 = *(_QWORD *)(v17 - 8);
  MEMORY[0x1E0C80A78](v14);
  v20 = (char *)&v35 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  v21 = *(void (**)(char *))(v18 + 16);
  v40 = v22;
  v21(v20);
  v23 = *(void (**)(char *, uint64_t, uint64_t))(v13 + 16);
  v36 = a2;
  v23(v16, a2, a5);
  v24 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  v41 = a7;
  v25 = v24(a4, a7);
  v26 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  v37 = a8;
  v27 = v26(a5, a8);
  (*(void (**)(char *, uint64_t))(v13 + 8))(v16, a5);
  result = (*(uint64_t (**)(char *, uint64_t))(v18 + 8))(v20, a4);
  if (v25 == v27)
  {
    v29 = v40;
    v30 = v41;
    result = v24(a4, v41);
    if ((result & 0x8000000000000000) == 0)
    {
      v31 = MEMORY[0x1E0C80A78](result);
      *(&v35 - 10) = a4;
      *(&v35 - 9) = a5;
      *(&v35 - 8) = v39;
      *(&v35 - 7) = v30;
      *(&v35 - 6) = v37;
      *(&v35 - 5) = v32;
      v33 = v36;
      *(&v35 - 4) = v29;
      *(&v35 - 3) = v33;
      *(&v35 - 2) = v31;
      return (*(uint64_t (**)(uint64_t))(v32 + 16))(v34);
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t *a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v7;

  if (!a3)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!result)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  v7 = *a5;
  if (v7)
    return a7(a3, 1, result, 1, v7, 1, a6);
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(uint64_t a1, uint64_t a2, uint64_t a3, char a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;

  if ((a4 & 1) != 0)
  {
    v8 = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
    v9 = MEMORY[0x1E0C80A78](v8);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v9, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:));
    a1 = swift_bridgeObjectRelease();
  }
  MEMORY[0x1E0C80A78](a1);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v10, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #2 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:));
}

{
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;

  if ((a4 & 1) != 0)
  {
    v8 = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
    v9 = MEMORY[0x1E0C80A78](v8);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v9, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:));
    a1 = swift_bridgeObjectRelease();
  }
  MEMORY[0x1E0C80A78](a1);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v10, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #2 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:));
}

void closure #1 in closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(const float *a1, uint64_t a2, const float *a3, float **a4, uint64_t a5, vDSP_Length *a6, vDSP_Length *a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  float *v11;
  vDSP_Length v16;

  if (!a1)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  v11 = *a4;
  if (*a4)
  {
    v16 = (*(uint64_t (**)(uint64_t))(a11 + 16))(a9);
    if ((v16 & 0x8000000000000000) == 0)
    {
      vDSP_vclipc(a1, 1, a3, a3, v11, 1, v16, a6, a7);
      return;
    }
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
}

void closure #1 in closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(const double *a1, uint64_t a2, const double *a3, double **a4, uint64_t a5, vDSP_Length *a6, vDSP_Length *a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  double *v11;
  vDSP_Length v16;

  if (!a1)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  v11 = *a4;
  if (*a4)
  {
    v16 = (*(uint64_t (**)(uint64_t))(a11 + 16))(a9);
    if ((v16 & 0x8000000000000000) == 0)
    {
      vDSP_vclipcD(a1, 1, a3, a3, v11, 1, v16, a6, a7);
      return;
    }
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
}

uint64_t partial apply for closure #1 in static vDSP.swapElements<A, B>(_:_:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.swapElements<A, B>(_:_:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.swapElements<A, B>(_:_:));
}

{
  return partial apply for closure #1 in static vDSP.swapElements<A, B>(_:_:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.swapElements<A, B>(_:_:));
}

uint64_t partial apply for closure #1 in static vDSP.swapElements<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  _QWORD v6[3];

  v3 = *(_QWORD *)(v2 + 24);
  v4 = *(_QWORD *)(v2 + 40);
  v6[2] = a1;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v4 + 16))(a2, v6, MEMORY[0x1E0DEE9C0] + 8, v3);
}

uint64_t partial apply for closure #1 in static vDSP.gather<A, B, C>(_:indices:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:));
}

uint64_t partial apply for closure #1 in static vDSP.gather<A, B>(_:indices:)(uint64_t a1, _QWORD *a2)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vDSP.gather<A, B, C>(_:indices:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vDSP.gather<A, B, C>(_:indices:result:));
}

uint64_t partial apply for closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:));
}

uint64_t partial apply for closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:));
}

{
  return partial apply for closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:));
}

uint64_t partial apply for closure #2 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(uint64_t a1, _QWORD *a2)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.compress<A, B, C>(_:gatingVector:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.compress<A, B, C>(_:gatingVector:result:));
}

uint64_t partial apply for closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD v8[3];
  __int128 v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  __int128 v14;

  v4 = *(_QWORD *)(v3 + 40);
  v5 = *(_QWORD *)(v3 + 48);
  v6 = *(_QWORD *)(v3 + 56);
  v8[2] = *(_QWORD *)(v3 + 16);
  v9 = *(_OWORD *)(v3 + 24);
  v10 = v4;
  v11 = v6;
  v12 = a1;
  v13 = v5;
  v14 = *(_OWORD *)(v3 + 64);
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, _QWORD))(v4 + 24))(a3, v8, MEMORY[0x1E0DEE9C0] + 8, v9);
}

void partial apply for closure #1 in closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(const double *a1, uint64_t a2)
{
  uint64_t v2;

  closure #1 in closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(a1, a2, *(const double **)(v2 + 48), *(double ***)(v2 + 56), *(_QWORD *)(v2 + 64), *(vDSP_Length **)(v2 + 72), *(vDSP_Length **)(v2 + 80), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40));
}

void partial apply for closure #1 in closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(const float *a1, uint64_t a2)
{
  uint64_t v2;

  closure #1 in closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(a1, a2, *(const float **)(v2 + 48), *(float ***)(v2 + 56), *(_QWORD *)(v2 + 64), *(vDSP_Length **)(v2 + 72), *(vDSP_Length **)(v2 + 80), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(uint64_t **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C418]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(uint64_t **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C410]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C638]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C630]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  _QWORD *v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD v8[5];

  v4 = v3[3];
  v5 = v3[6];
  v6 = v3[9];
  v8[2] = a1;
  v8[3] = a2;
  v8[4] = v6;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E0DEE9C0] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.swapElements<A, B>(_:_:)(_QWORD *a1)
{
  uint64_t v1;

  return closure #1 in closure #1 in static vDSP.swapElements<A, B>(_:_:)(a1, *(uint64_t **)(v1 + 16), *(_QWORD *)(v1 + 24), MEMORY[0x1E0C8C968]);
}

{
  uint64_t v1;

  return closure #1 in closure #1 in static vDSP.swapElements<A, B>(_:_:)(a1, *(uint64_t **)(v1 + 16), *(_QWORD *)(v1 + 24), MEMORY[0x1E0C8C960]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:));
}

_QWORD *partial apply for closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(_QWORD *a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, MEMORY[0x1E0C8C9E8]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, MEMORY[0x1E0C8C9E0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _OWORD v8[2];
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;

  v4 = *(_QWORD *)(v3 + 32);
  v5 = *(_QWORD *)(v3 + 56);
  v6 = *(_QWORD *)(v3 + 88);
  v8[1] = *(_OWORD *)(v3 + 72);
  v9 = a1;
  v10 = a2;
  v11 = v6;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, uint64_t))(v5 + 16))(a3, v8, MEMORY[0x1E0DEE9C0] + 8, v4);
}

_QWORD *partial apply for closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(_QWORD *a1, uint64_t (*a2)(uint64_t, uint64_t, uint64_t, uint64_t, _QWORD, uint64_t, uint64_t))
{
  uint64_t *v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, v2[2], v2[3], v2[4], v2[5], v2[6], a2);
}

unint64_t BNNS.Shape.batchStride.getter()
{
  uint64_t v0;
  unint64_t v2;
  unint64_t v3;
  unint64_t v4;
  unint64_t v5;
  unint64_t v6;
  unint64_t v7;
  unint64_t v8;
  unint64_t v9;
  _BYTE v10[136];
  _BYTE v11[136];

  outlined init with take of BNNS.Shape(v0, (uint64_t)v11);
  outlined init with take of BNNS.Shape((uint64_t)v11, (uint64_t)v10);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v11, (uint64_t)v10);
  BNNS.Shape.stride.getter();
  return specialized static BNNS.calculateBatchStride(size:stride:)(v2, v3, v4, v5, v6, v7, v8, v9, v2, v3, v4, v5, v6, v7, v8, v9);
}

uint64_t BNNS.Shape.layout.getter()
{
  uint64_t v0;
  uint64_t v1;
  _BYTE v3[136];
  _BYTE v4[144];

  outlined init with take of BNNS.Shape(v0, (uint64_t)v3);
  outlined init with take of BNNS.Shape((uint64_t)v3, (uint64_t)v4);
  v1 = dword_1CAB60020[(int)_s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v4)];
  destructiveProjectEnumData for BNNS.ActivationFunction(v4);
  return v1;
}

void BNNS.Shape.size.getter()
{
  uint64_t v0;
  char *v1;
  _BYTE v2[136];
  _BYTE v3[144];

  outlined init with take of BNNS.Shape(v0, (uint64_t)v2);
  outlined init with take of BNNS.Shape((uint64_t)v2, (uint64_t)v3);
  v1 = (char *)sub_1CAAE9FBC + 4 * byte_1CAB5FE20[_s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v3)];
  __asm { BR              X10 }
}

uint64_t sub_1CAAE9FBC()
{
  uint64_t v0;
  uint64_t result;
  __int128 v2;
  uint64_t v3;

  result = ((uint64_t (*)(uint64_t *))destructiveProjectEnumData for BNNS.ActivationFunction)(&v3);
  v2 = *(_OWORD *)(result + 8);
  *(_QWORD *)v0 = *(_QWORD *)result;
  *(_OWORD *)(v0 + 8) = v2;
  *(_OWORD *)(v0 + 24) = 0u;
  *(_OWORD *)(v0 + 40) = 0u;
  *(_QWORD *)(v0 + 56) = 0;
  return result;
}

void BNNS.Shape.stride.getter()
{
  uint64_t v0;
  char *v1;
  _BYTE v2[136];
  _BYTE v3[144];

  outlined init with take of BNNS.Shape(v0, (uint64_t)v2);
  outlined init with take of BNNS.Shape((uint64_t)v2, (uint64_t)v3);
  v1 = (char *)sub_1CAAEA0F8 + 4 * byte_1CAB5FE35[_s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v3)];
  __asm { BR              X10 }
}

uint64_t sub_1CAAEA0F8()
{
  uint64_t v0;
  uint64_t result;
  __int128 v2;
  uint64_t v3;

  result = ((uint64_t (*)(uint64_t *))destructiveProjectEnumData for BNNS.ActivationFunction)(&v3);
  v2 = *(_OWORD *)(result + 32);
  *(_QWORD *)v0 = *(_QWORD *)(result + 24);
  *(_OWORD *)(v0 + 8) = v2;
  *(_OWORD *)(v0 + 24) = 0u;
  *(_OWORD *)(v0 + 40) = 0u;
  *(_QWORD *)(v0 + 56) = 0;
  return result;
}

uint64_t BNNS.Shape.init(_:dataLayout:stride:)@<X0>(uint64_t a1@<X0>, unsigned __int8 *a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X8>)
{
  unsigned int v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t result;
  _BYTE v11[135];
  char v12;
  char v13[136];

  v7 = *a2;
  if (v7 == 21)
  {
    static BNNS.defaultLayoutForDimensions(_:)(*(_QWORD *)(a1 + 16), v13);
    v7 = v13[0];
    if (v13[0] == 21)
      goto LABEL_13;
  }
  if (!a3)
  {
    v8 = *(_QWORD *)(a1 + 16);
    if (v7 <= 0x14)
      goto LABEL_6;
    goto LABEL_8;
  }
  v8 = *(_QWORD *)(a1 + 16);
  if (v8 == *(_QWORD *)(a3 + 16))
  {
    if (v7 <= 0x14)
    {
LABEL_6:
      v9 = qword_1CAB5FEE0[(char)v7];
LABEL_9:
      if (v8 == v9)
      {
        v12 = v7;
        static BNNS.makeShape(size:dataLayout:stride:)(a1, (uint64_t)&v12);
        swift_bridgeObjectRelease();
        swift_bridgeObjectRelease();
        outlined init with take of BNNS.Shape((uint64_t)v13, (uint64_t)v11);
        return outlined init with take of BNNS.Shape((uint64_t)v11, a4);
      }
      __break(1u);
      goto LABEL_12;
    }
LABEL_8:
    v9 = 8;
    goto LABEL_9;
  }
LABEL_12:
  __break(1u);
LABEL_13:
  result = swift_bridgeObjectRelease();
  __break(1u);
  return result;
}

void static BNNS.makeShape(size:dataLayout:stride:)(uint64_t a1, uint64_t a2)
{
  __asm { BR              X10 }
}

uint64_t sub_1CAAEA330(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  _QWORD v6[21];

  if (!*(_QWORD *)(a1 + 16))
  {
    __break(1u);
    goto LABEL_5;
  }
  if (!*(_QWORD *)(a3 + 16))
  {
LABEL_5:
    __break(1u);
    JUMPOUT(0x1CAAEAC1CLL);
  }
  v4 = *(_QWORD *)(a3 + 32);
  v6[0] = *(_QWORD *)(a1 + 32);
  v6[1] = v4;
  _s10Accelerate4BNNSO5ShapeOWOi_((uint64_t)v6);
  return outlined init with take of BNNS.Shape((uint64_t)v6, v3);
}

uint64_t BNNS.Shape.rank.getter()
{
  uint64_t v0;
  unsigned int v1;
  _BYTE v3[136];

  outlined init with take of BNNS.Shape(v0, (uint64_t)v3);
  v1 = _s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v3);
  if (v1 > 0x12)
    return 8;
  else
    return qword_1CAB5FF88[v1];
}

uint64_t BNNS.Shape.init(arrayLiteral:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  _BYTE v4[136];
  unsigned __int8 v5[136];

  v5[0] = 21;
  BNNS.Shape.init(_:dataLayout:stride:)(a1, v5, 0, (uint64_t)v4);
  outlined init with take of BNNS.Shape((uint64_t)v4, (uint64_t)v5);
  return outlined init with take of BNNS.Shape((uint64_t)v5, a2);
}

uint64_t protocol witness for ExpressibleByArrayLiteral.init(arrayLiteral:) in conformance BNNS.Shape@<X0>(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  _BYTE v4[136];
  unsigned __int8 v5[136];

  v5[0] = 21;
  BNNS.Shape.init(_:dataLayout:stride:)(a1, v5, 0, (uint64_t)v4);
  outlined init with take of BNNS.Shape((uint64_t)v4, (uint64_t)v5);
  return outlined init with take of BNNS.Shape((uint64_t)v5, a2);
}

uint64_t BNNS.Shape.denseTensorSize.getter()
{
  uint64_t v0;
  uint64_t v1;
  uint64_t result;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  _BYTE v15[136];
  _BYTE v16[136];

  outlined init with take of BNNS.Shape(v0, (uint64_t)v16);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  v1 = swift_allocObject();
  *(_OWORD *)(v1 + 16) = xmmword_1CAB60080;
  outlined init with take of BNNS.Shape((uint64_t)v16, (uint64_t)v15);
  BNNS.Shape.size.getter();
  *(_QWORD *)(v1 + 32) = v7;
  outlined init with take of BNNS.Shape((uint64_t)v16, (uint64_t)v15);
  BNNS.Shape.size.getter();
  *(_QWORD *)(v1 + 40) = v8;
  outlined init with take of BNNS.Shape((uint64_t)v16, (uint64_t)v15);
  BNNS.Shape.size.getter();
  *(_QWORD *)(v1 + 48) = v9;
  outlined init with take of BNNS.Shape((uint64_t)v16, (uint64_t)v15);
  BNNS.Shape.size.getter();
  *(_QWORD *)(v1 + 56) = v10;
  outlined init with take of BNNS.Shape((uint64_t)v16, (uint64_t)v15);
  BNNS.Shape.size.getter();
  *(_QWORD *)(v1 + 64) = v11;
  outlined init with take of BNNS.Shape((uint64_t)v16, (uint64_t)v15);
  BNNS.Shape.size.getter();
  *(_QWORD *)(v1 + 72) = v12;
  outlined init with take of BNNS.Shape((uint64_t)v16, (uint64_t)v15);
  BNNS.Shape.size.getter();
  *(_QWORD *)(v1 + 80) = v13;
  outlined init with take of BNNS.Shape((uint64_t)v16, (uint64_t)v15);
  BNNS.Shape.size.getter();
  *(_QWORD *)(v1 + 88) = v14;
  result = _s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v16);
  if (result > 0x12)
    v3 = 8;
  else
    v3 = qword_1CAB5FF88[(int)result];
  v4 = 0;
  v5 = 1;
  while (1)
  {
    v6 = *(_QWORD *)(v1 + 8 * v4 + 32);
    if ((unsigned __int128)(v5 * (__int128)v6) >> 64 != (v5 * v6) >> 63)
      break;
    v5 *= v6;
    if (v3 == ++v4)
    {
      swift_bridgeObjectRelease();
      return v5;
    }
  }
  __break(1u);
  return result;
}

__n128 __swift_memcpy129_8(uint64_t a1, uint64_t a2)
{
  __int128 v2;
  __int128 v3;
  __int128 v4;
  __n128 result;
  __int128 v6;
  __int128 v7;

  *(_OWORD *)a1 = *(_OWORD *)a2;
  v2 = *(_OWORD *)(a2 + 16);
  v3 = *(_OWORD *)(a2 + 32);
  v4 = *(_OWORD *)(a2 + 64);
  *(_OWORD *)(a1 + 48) = *(_OWORD *)(a2 + 48);
  *(_OWORD *)(a1 + 64) = v4;
  *(_OWORD *)(a1 + 16) = v2;
  *(_OWORD *)(a1 + 32) = v3;
  result = *(__n128 *)(a2 + 80);
  v6 = *(_OWORD *)(a2 + 96);
  v7 = *(_OWORD *)(a2 + 112);
  *(_BYTE *)(a1 + 128) = *(_BYTE *)(a2 + 128);
  *(_OWORD *)(a1 + 96) = v6;
  *(_OWORD *)(a1 + 112) = v7;
  *(__n128 *)(a1 + 80) = result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.Shape(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  int v4;

  if (!a2)
    return 0;
  if (a2 >= 0xEC && *(_BYTE *)(a1 + 129))
    return (*(_DWORD *)a1 + 236);
  v3 = *(unsigned __int8 *)(a1 + 128);
  if (v3 <= 0x14)
    v4 = -1;
  else
    v4 = v3 ^ 0xFF;
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.Shape(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xEB)
  {
    *(_QWORD *)(result + 120) = 0;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_BYTE *)(result + 128) = 0;
    *(_QWORD *)result = a2 - 236;
    if (a3 >= 0xEC)
      *(_BYTE *)(result + 129) = 1;
  }
  else
  {
    if (a3 >= 0xEC)
      *(_BYTE *)(result + 129) = 0;
    if (a2)
      *(_BYTE *)(result + 128) = -(char)a2;
  }
  return result;
}

uint64_t destructiveInjectEnumTag for BNNS.Shape(uint64_t result, char a2)
{
  *(_BYTE *)(result + 128) = a2;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.Shape()
{
  return &type metadata for BNNS.Shape;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi19_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 20;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi18_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 19;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi17_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 18;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi16_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 17;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi15_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 16;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi14_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 15;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi13_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 14;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi12_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 13;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi11_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 12;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi10_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 11;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi9_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 10;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi8_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 9;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi7_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 8;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi6_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 7;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi5_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 6;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi4_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 5;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi3_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 4;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi2_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 3;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi0_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 1;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi1_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 2;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi_(uint64_t result)
{
  *(_BYTE *)(result + 128) = 0;
  return result;
}

unint64_t specialized static BNNS.arrayToTuple<A>(_:fillValue:)(_QWORD *a1, _QWORD *a2, _QWORD *a3, _QWORD *a4, _QWORD *a5, unint64_t *a6, _QWORD *a7, char *a8, unint64_t *a9)
{
  char *isUniquelyReferenced_nonNull_native;
  int64_t v18;
  unint64_t v19;
  unint64_t v20;
  unint64_t v21;
  unint64_t v22;
  unint64_t v23;
  unint64_t v24;
  unint64_t v25;
  unint64_t v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  BOOL v35;
  uint64_t v36;
  _QWORD *v38;
  unint64_t *v39;

  swift_bridgeObjectRetain();
  isUniquelyReferenced_nonNull_native = (char *)swift_isUniquelyReferenced_nonNull_native();
  if (!(_DWORD)isUniquelyReferenced_nonNull_native || *((_QWORD *)a8 + 3) <= 0xFuLL)
  {
    if (*((_QWORD *)a8 + 2) <= 8uLL)
      v18 = 8;
    else
      v18 = *((_QWORD *)a8 + 2);
    a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(isUniquelyReferenced_nonNull_native, v18, 0, a8);
  }
  v19 = *((_QWORD *)a8 + 2);
  if (v19 > 7)
    goto LABEL_33;
  v39 = a6;
  a6 = a9;
  v20 = *((_QWORD *)a8 + 3);
  v21 = v19 + 1;
  if (v19 >= v20 >> 1)
    goto LABEL_46;
  while (1)
  {
    *((_QWORD *)a8 + 2) = v21;
    *(_QWORD *)&a8[8 * v19 + 32] = a6;
    if (v19 <= 6)
    {
      v38 = a7;
      v22 = *((_QWORD *)a8 + 3);
      v23 = v19 + 2;
      if (v21 >= v22 >> 1)
        a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v22 > 1), v19 + 2, 1, a8);
      *((_QWORD *)a8 + 2) = v23;
      *(_QWORD *)&a8[8 * v21 + 32] = a6;
      if (v19 <= 5)
      {
        v24 = *((_QWORD *)a8 + 3);
        v25 = v19 + 3;
        if (v23 >= v24 >> 1)
          a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v24 > 1), v19 + 3, 1, a8);
        *((_QWORD *)a8 + 2) = v25;
        *(_QWORD *)&a8[8 * v23 + 32] = a6;
        if (v19 <= 4)
        {
          v26 = *((_QWORD *)a8 + 3);
          v27 = v19 + 4;
          if (v25 >= v26 >> 1)
            a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v26 > 1), v19 + 4, 1, a8);
          *((_QWORD *)a8 + 2) = v27;
          *(_QWORD *)&a8[8 * v25 + 32] = a6;
          if (v19 <= 3)
          {
            v28 = *((_QWORD *)a8 + 3);
            v29 = v19 + 5;
            if (v27 >= v28 >> 1)
              a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v28 > 1), v19 + 5, 1, a8);
            *((_QWORD *)a8 + 2) = v29;
            *(_QWORD *)&a8[8 * v27 + 32] = a6;
            if (v19 <= 2)
            {
              v30 = *((_QWORD *)a8 + 3);
              v31 = v19 + 6;
              if (v29 >= v30 >> 1)
                a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v30 > 1), v19 + 6, 1, a8);
              *((_QWORD *)a8 + 2) = v31;
              *(_QWORD *)&a8[8 * v29 + 32] = a6;
              if (v19 <= 1)
              {
                v32 = *((_QWORD *)a8 + 3);
                v33 = v19 + 7;
                if (v31 >= v32 >> 1)
                  a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v32 > 1), v19 + 7, 1, a8);
                *((_QWORD *)a8 + 2) = v33;
                *(_QWORD *)&a8[8 * v31 + 32] = a6;
                if (!v19)
                {
                  v34 = *((_QWORD *)a8 + 3);
                  if (v33 >= v34 >> 1)
                    a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v34 > 1), 8, 1, a8);
                  *((_QWORD *)a8 + 2) = 8;
                  *(_QWORD *)&a8[8 * v33 + 32] = a6;
                }
              }
            }
          }
        }
      }
      a7 = v38;
    }
    a6 = v39;
LABEL_33:
    v21 = *((_QWORD *)a8 + 4);
    *a1 = *((_QWORD *)a8 + 5);
    v20 = *((_QWORD *)a8 + 2);
    if (v20 < 3)
    {
      __break(1u);
LABEL_41:
      __break(1u);
LABEL_42:
      __break(1u);
LABEL_43:
      __break(1u);
LABEL_44:
      __break(1u);
      goto LABEL_45;
    }
    *a2 = *((_QWORD *)a8 + 6);
    if (v20 == 3)
      goto LABEL_41;
    *a3 = *((_QWORD *)a8 + 7);
    if (v20 < 5)
      goto LABEL_42;
    *a4 = *((_QWORD *)a8 + 8);
    if (v20 == 5)
      goto LABEL_43;
    *a5 = *((_QWORD *)a8 + 9);
    v35 = v20 == 7;
    if (v20 < 7)
      goto LABEL_44;
    v20 = *((_QWORD *)a8 + 10);
    *a6 = v20;
    if (!v35)
      break;
LABEL_45:
    __break(1u);
LABEL_46:
    a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v20 > 1), v21, 1, a8);
  }
  v36 = *((_QWORD *)a8 + 11);
  swift_bridgeObjectRelease();
  *a7 = v36;
  return v21;
}

unint64_t specialized static BNNS.arrayToTuple<A>(_:fillValue:)(_OWORD *a1, _OWORD *a2, _OWORD *a3, _OWORD *a4, _OWORD *a5, _OWORD *a6, _QWORD *a7, char *a8, _OWORD *a9, _OWORD *a10)
{
  char *isUniquelyReferenced_nonNull_native;
  int64_t v19;
  unint64_t v20;
  unint64_t v21;
  unint64_t v22;
  _OWORD *v23;
  _OWORD *v24;
  char *v25;
  unint64_t v26;
  unint64_t v27;
  char *v28;
  unint64_t v29;
  unint64_t v30;
  char *v31;
  unint64_t v32;
  unint64_t v33;
  char *v34;
  unint64_t v35;
  unint64_t v36;
  char *v37;
  unint64_t v38;
  unint64_t v39;
  char *v40;
  unint64_t v41;
  unint64_t v42;
  char *v43;
  _OWORD *v44;
  uint64_t v45;
  uint64_t v46;
  unint64_t v48;
  char *v49;
  _OWORD *v50;
  _OWORD *v51;
  _OWORD *v52;

  swift_bridgeObjectRetain();
  isUniquelyReferenced_nonNull_native = (char *)swift_isUniquelyReferenced_nonNull_native();
  v52 = a1;
  if (!(_DWORD)isUniquelyReferenced_nonNull_native || *((_QWORD *)a8 + 3) <= 0xFuLL)
  {
    if (*((_QWORD *)a8 + 2) <= 8uLL)
      v19 = 8;
    else
      v19 = *((_QWORD *)a8 + 2);
    a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(isUniquelyReferenced_nonNull_native, v19, 0, a8);
  }
  v20 = *((_QWORD *)a8 + 2);
  if (v20 > 7)
  {
    v23 = a3;
    v24 = a4;
    goto LABEL_30;
  }
  v50 = a5;
  v51 = a6;
  a5 = a9;
  a6 = a10;
  v21 = *((_QWORD *)a8 + 3);
  v22 = v20 + 1;
  v23 = a3;
  if (v20 >= v21 >> 1)
    goto LABEL_47;
  while (1)
  {
    v24 = a4;
    *((_QWORD *)a8 + 2) = v22;
    v25 = &a8[16 * v20];
    *((_QWORD *)v25 + 4) = a5;
    *((_QWORD *)v25 + 5) = a6;
    if (v20 > 6)
      goto LABEL_28;
    v26 = *((_QWORD *)a8 + 3);
    v27 = v20 + 2;
    if (v22 >= v26 >> 1)
      a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v26 > 1), v20 + 2, 1, a8);
    *((_QWORD *)a8 + 2) = v27;
    v28 = &a8[16 * v22];
    *((_QWORD *)v28 + 4) = a5;
    *((_QWORD *)v28 + 5) = a6;
    if (v20 > 5)
      goto LABEL_28;
    v29 = *((_QWORD *)a8 + 3);
    v30 = v20 + 3;
    if (v27 >= v29 >> 1)
      a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v29 > 1), v20 + 3, 1, a8);
    *((_QWORD *)a8 + 2) = v30;
    v31 = &a8[16 * v27];
    *((_QWORD *)v31 + 4) = a5;
    *((_QWORD *)v31 + 5) = a6;
    if (v20 > 4)
      goto LABEL_28;
    v32 = *((_QWORD *)a8 + 3);
    v33 = v20 + 4;
    if (v30 >= v32 >> 1)
      a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v32 > 1), v20 + 4, 1, a8);
    *((_QWORD *)a8 + 2) = v33;
    v34 = &a8[16 * v30];
    *((_QWORD *)v34 + 4) = a5;
    *((_QWORD *)v34 + 5) = a6;
    if (v20 > 3)
      goto LABEL_28;
    v35 = *((_QWORD *)a8 + 3);
    v36 = v20 + 5;
    if (v33 >= v35 >> 1)
      a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v35 > 1), v20 + 5, 1, a8);
    *((_QWORD *)a8 + 2) = v36;
    v37 = &a8[16 * v33];
    *((_QWORD *)v37 + 4) = a5;
    *((_QWORD *)v37 + 5) = a6;
    if (v20 > 2)
      goto LABEL_28;
    v38 = *((_QWORD *)a8 + 3);
    v39 = v20 + 6;
    if (v36 >= v38 >> 1)
      a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v38 > 1), v20 + 6, 1, a8);
    *((_QWORD *)a8 + 2) = v39;
    v40 = &a8[16 * v36];
    *((_QWORD *)v40 + 4) = a5;
    *((_QWORD *)v40 + 5) = a6;
    if (v20 > 1)
      goto LABEL_28;
    v41 = *((_QWORD *)a8 + 3);
    v42 = v20 + 7;
    if (v39 >= v41 >> 1)
      a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v41 > 1), v20 + 7, 1, a8);
    *((_QWORD *)a8 + 2) = v42;
    v43 = &a8[16 * v39];
    *((_QWORD *)v43 + 4) = a5;
    *((_QWORD *)v43 + 5) = a6;
    if (v20)
    {
LABEL_28:
      a5 = v50;
      a6 = v51;
LABEL_30:
      v44 = v52;
    }
    else
    {
      v48 = *((_QWORD *)a8 + 3);
      if (v42 >= v48 >> 1)
        a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v48 > 1), 8, 1, a8);
      *((_QWORD *)a8 + 2) = 8;
      v49 = &a8[16 * v42];
      *((_QWORD *)v49 + 4) = a5;
      *((_QWORD *)v49 + 5) = a6;
      a6 = v51;
      v44 = v52;
      a5 = v50;
    }
    v22 = *((_QWORD *)a8 + 4);
    a4 = (_OWORD *)*((_QWORD *)a8 + 5);
    *v44 = *((_OWORD *)a8 + 3);
    v21 = *((_QWORD *)a8 + 2);
    if (v21 < 3)
    {
      __break(1u);
LABEL_42:
      __break(1u);
LABEL_43:
      __break(1u);
LABEL_44:
      __break(1u);
LABEL_45:
      __break(1u);
      goto LABEL_46;
    }
    *a2 = *((_OWORD *)a8 + 4);
    if (v21 == 3)
      goto LABEL_42;
    *v23 = *((_OWORD *)a8 + 5);
    if (v21 < 5)
      goto LABEL_43;
    *v24 = *((_OWORD *)a8 + 6);
    if (v21 == 5)
      goto LABEL_44;
    *a5 = *((_OWORD *)a8 + 7);
    if (v21 < 7)
      goto LABEL_45;
    *a6 = *((_OWORD *)a8 + 8);
    if (v21 != 7)
      break;
LABEL_46:
    __break(1u);
LABEL_47:
    a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v21 > 1), v22, 1, a8);
  }
  v45 = *((_QWORD *)a8 + 18);
  v46 = *((_QWORD *)a8 + 19);
  swift_bridgeObjectRelease();
  *a7 = v45;
  a7[1] = v46;
  return v22;
}

uint64_t BNNS.PermuteLayer.__allocating_init(input:output:permutation:filterParameters:)(_OWORD *a1, __int128 *a2, char *a3, int a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  unint64_t v23;
  _QWORD *v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v30;
  unint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  _QWORD v35[4];
  _BYTE __dst[352];
  unint64_t v37;
  _QWORD v38[7];
  _OWORD __src[22];
  uint64_t v40;

  v40 = *MEMORY[0x1E0C80C00];
  v10 = a2[8];
  v11 = a2[9];
  v12 = a2[6];
  __src[18] = a2[7];
  __src[19] = v10;
  v13 = a2[10];
  __src[20] = v11;
  __src[21] = v13;
  v14 = a2[4];
  v15 = a2[5];
  v16 = a2[2];
  __src[14] = a2[3];
  __src[15] = v14;
  __src[16] = v15;
  __src[17] = v12;
  v17 = *a2;
  __src[12] = a2[1];
  __src[13] = v16;
  v18 = a1[9];
  __src[8] = a1[8];
  __src[9] = v18;
  __src[10] = a1[10];
  __src[11] = v17;
  v19 = a1[5];
  __src[4] = a1[4];
  __src[5] = v19;
  v20 = a1[7];
  __src[6] = a1[6];
  __src[7] = v20;
  v21 = a1[1];
  __src[0] = *a1;
  __src[1] = v21;
  v22 = a1[3];
  __src[2] = a1[2];
  __src[3] = v22;
  v23 = specialized static BNNS.arrayToTuple<A>(_:fillValue:)(v38, v35, &v34, &v33, &v32, &v31, &v30, a3, 0);
  swift_bridgeObjectRelease();
  memcpy(__dst, __src, sizeof(__dst));
  v37 = v23;
  v38[1] = v35[0];
  v38[2] = v34;
  v38[3] = v33;
  v38[4] = v32;
  v38[5] = v31;
  v38[6] = v30;
  if (a6 == 1)
  {
    v24 = 0;
  }
  else
  {
    LODWORD(v35[0]) = a4;
    v35[1] = a5;
    v35[2] = a6;
    v35[3] = a7;
    v24 = v35;
  }
  v25 = MEMORY[0x1D179460C](__dst, v24);
  type metadata accessor for BNNS.PermuteLayer();
  v26 = swift_allocObject();
  v27 = v26;
  if (v25)
  {
    *(_QWORD *)(v26 + 16) = v25;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v27;
}

uint64_t type metadata accessor for BNNS.PermuteLayer()
{
  return objc_opt_self();
}

uint64_t BNNS.PermuteLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.PermuteLayer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

uint64_t BNNS.Layer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.UnaryLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.UnaryLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:)(size_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, _OWORD *a5)
{
  uint64_t v5;
  __int128 v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  uint64_t result;
  _BYTE *v17;
  BNNSNDArrayDescriptor v18;
  BNNSNDArrayDescriptor v19;
  uint64_t v20;

  v20 = *MEMORY[0x1E0C80C00];
  v6 = a4[9];
  *(_OWORD *)&v19.stride[7] = a4[8];
  *(_OWORD *)&v19.data_type = v6;
  *(_OWORD *)&v19.table_data_type = a4[10];
  v7 = a4[5];
  *(_OWORD *)&v19.size[7] = a4[4];
  *(_OWORD *)&v19.stride[1] = v7;
  v8 = a4[7];
  *(_OWORD *)&v19.stride[3] = a4[6];
  *(_OWORD *)&v19.stride[5] = v8;
  v9 = a4[1];
  *(_OWORD *)&v19.flags = *a4;
  *(_OWORD *)&v19.size[1] = v9;
  v10 = a4[3];
  *(_OWORD *)&v19.size[3] = a4[2];
  *(_OWORD *)&v19.size[5] = v10;
  v11 = a5[9];
  *(_OWORD *)&v18.stride[7] = a5[8];
  *(_OWORD *)&v18.data_type = v11;
  *(_OWORD *)&v18.table_data_type = a5[10];
  v12 = a5[5];
  *(_OWORD *)&v18.size[7] = a5[4];
  *(_OWORD *)&v18.stride[1] = v12;
  v13 = a5[7];
  *(_OWORD *)&v18.stride[3] = a5[6];
  *(_OWORD *)&v18.stride[5] = v13;
  v14 = a5[1];
  *(_OWORD *)&v18.flags = *a5;
  *(_OWORD *)&v18.size[1] = v14;
  v15 = a5[3];
  *(_OWORD *)&v18.size[3] = a5[2];
  *(_OWORD *)&v18.size[5] = v15;
  result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(0, v5, a1, a2, &v18, (uint64_t)a5, a3, &v19, (uint64_t)a4, 0);
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v17 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t BNNS.Layer.__allocating_init(bnnsFilter:)(uint64_t a1)
{
  uint64_t result;

  result = swift_allocObject();
  if (a1)
  {
    *(_QWORD *)(result + 16) = a1;
  }
  else
  {
    swift_deallocPartialClassInstance();
    return 0;
  }
  return result;
}

uint64_t BNNS.Layer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

uint64_t BNNS.Layer.bnnsFilter.getter()
{
  uint64_t v0;

  return *(_QWORD *)(v0 + 16);
}

uint64_t BNNS.BinaryLayer.apply(batchSize:inputA:inputB:output:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4;

  return specialized static BNNS.layerApply(_:batchSize:inputA:inputB:output:)(v4, a1, a2, a3, a4);
}

uint64_t BNNS.BinaryLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _OWORD *a5, _OWORD *a6, _OWORD *a7)
{
  uint64_t v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  uint64_t result;
  _BYTE *v24;
  BNNSNDArrayDescriptor v25;
  BNNSNDArrayDescriptor v26;
  BNNSNDArrayDescriptor v27;
  uint64_t v28;

  v28 = *MEMORY[0x1E0C80C00];
  v8 = a5[9];
  *(_OWORD *)&v27.stride[7] = a5[8];
  *(_OWORD *)&v27.data_type = v8;
  *(_OWORD *)&v27.table_data_type = a5[10];
  v9 = a5[5];
  *(_OWORD *)&v27.size[7] = a5[4];
  *(_OWORD *)&v27.stride[1] = v9;
  v10 = a5[7];
  *(_OWORD *)&v27.stride[3] = a5[6];
  *(_OWORD *)&v27.stride[5] = v10;
  v11 = a5[1];
  *(_OWORD *)&v27.flags = *a5;
  *(_OWORD *)&v27.size[1] = v11;
  v12 = a5[3];
  *(_OWORD *)&v27.size[3] = a5[2];
  *(_OWORD *)&v27.size[5] = v12;
  v13 = a6[9];
  *(_OWORD *)&v26.stride[7] = a6[8];
  *(_OWORD *)&v26.data_type = v13;
  *(_OWORD *)&v26.table_data_type = a6[10];
  v14 = a6[5];
  *(_OWORD *)&v26.size[7] = a6[4];
  *(_OWORD *)&v26.stride[1] = v14;
  v15 = a6[7];
  *(_OWORD *)&v26.stride[3] = a6[6];
  *(_OWORD *)&v26.stride[5] = v15;
  v16 = a6[1];
  *(_OWORD *)&v26.flags = *a6;
  *(_OWORD *)&v26.size[1] = v16;
  v17 = a6[3];
  *(_OWORD *)&v26.size[3] = a6[2];
  *(_OWORD *)&v26.size[5] = v17;
  v18 = a7[9];
  *(_OWORD *)&v25.stride[7] = a7[8];
  *(_OWORD *)&v25.data_type = v18;
  *(_OWORD *)&v25.table_data_type = a7[10];
  v19 = a7[5];
  *(_OWORD *)&v25.size[7] = a7[4];
  *(_OWORD *)&v25.stride[1] = v19;
  v20 = a7[7];
  *(_OWORD *)&v25.stride[3] = a7[6];
  *(_OWORD *)&v25.stride[5] = v20;
  v21 = a7[1];
  *(_OWORD *)&v25.flags = *a7;
  *(_OWORD *)&v25.size[1] = v21;
  v22 = a7[3];
  *(_OWORD *)&v25.size[3] = a7[2];
  *(_OWORD *)&v25.size[5] = v22;
  result = closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingWeightsGradient:generatingBiasGradient:)(0, v7, a1, a2, &v26, (uint64_t)a6, a3, &v25, (uint64_t)a7, a4, &v27, (uint64_t)a5, 0);
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v24 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t BNNS.UnaryLayer.__allocating_init(bnnsFilter:)(uint64_t a1)
{
  uint64_t v2;
  uint64_t v3;

  v2 = swift_allocObject();
  v3 = v2;
  if (a1)
  {
    *(_QWORD *)(v2 + 16) = a1;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v3;
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(BNNSNDArrayDescriptor *a1, uint64_t a2, size_t a3, uint64_t a4, BNNSNDArrayDescriptor *a5, uint64_t a6, uint64_t a7, const BNNSNDArrayDescriptor *a8, uint64_t a9, BNNSNDArrayDescriptor *weights_delta)
{
  size_t v10;
  size_t v11;
  const void *v13;
  size_t v14;
  unint64_t in_stride;
  const void *v17;
  void *filter;
  unint64_t v23;
  unint64_t v24;
  unint64_t v25;
  unint64_t v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  _BYTE v31[136];
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  _BYTE v40[136];
  unint64_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  unint64_t v46;
  unint64_t v47;
  unint64_t v48;
  _BYTE v49[136];
  unint64_t v50;
  unint64_t v51;
  unint64_t v52;
  unint64_t v53;
  unint64_t v54;
  unint64_t v55;
  unint64_t v56;
  unint64_t v57;
  _BYTE v58[136];
  _BYTE v59[136];
  _BYTE v60[136];
  _BYTE v61[8];
  _BYTE v62[8];
  const void *v63;
  const void *v64;

  filter = *(void **)(a2 + 16);
  outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v62, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v62, (uint64_t)&v63, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v17 = v63;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v58);
  outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)v59);
  outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v60);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v60);
  BNNS.Shape.stride.getter();
  in_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v50, v51, v52, v53, v54, v55, v56, v57, v50, v51, v52, v53, v54, v55, v56, v57);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v50);
  outlined init with take of BNNS.Shape((uint64_t)&v50, (uint64_t)v60);
  outlined init with take of BNNS.Shape((uint64_t)v60, (uint64_t)v49);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v60, (uint64_t)v49);
  BNNS.Shape.stride.getter();
  v14 = specialized static BNNS.calculateBatchStride(size:stride:)(v41, v42, v43, v44, v45, v46, v47, v48, v41, v42, v43, v44, v45, v46, v47, v48);
  outlined init with take of BNNSNDArrayDescriptor?(a7 + 136, (uint64_t)v61, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v61, (uint64_t)&v64, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v13 = v64;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v40);
  outlined init with take of BNNS.Shape((uint64_t)v40, (uint64_t)&v41);
  outlined init with take of BNNS.Shape((uint64_t)&v41, (uint64_t)v49);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)&v41, (uint64_t)v49);
  BNNS.Shape.stride.getter();
  v10 = specialized static BNNS.calculateBatchStride(size:stride:)(v32, v33, v34, v35, v36, v37, v38, v39, v32, v33, v34, v35, v36, v37, v38, v39);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v32);
  outlined init with take of BNNS.Shape((uint64_t)&v32, (uint64_t)v49);
  outlined init with take of BNNS.Shape((uint64_t)v49, (uint64_t)v31);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v49, (uint64_t)v31);
  BNNS.Shape.stride.getter();
  v11 = specialized static BNNS.calculateBatchStride(size:stride:)(v23, v24, v25, v26, v27, v28, v29, v30, v23, v24, v25, v26, v27, v28, v29, v30);
  return BNNSFilterApplyBackwardBatch(filter, a3, v17, in_stride, a5, v14, v13, v10, a8, v11, weights_delta, a1);
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingWeightsGradient:generatingBiasGradient:)(BNNSNDArrayDescriptor *a1, uint64_t a2, size_t a3, uint64_t a4, BNNSNDArrayDescriptor *a5, uint64_t a6, uint64_t a7, BNNSNDArrayDescriptor *a8, uint64_t a9, uint64_t a10, const BNNSNDArrayDescriptor *out_delta, uint64_t a12, BNNSNDArrayDescriptor *weights_delta)
{
  size_t v13;
  size_t v14;
  const void *out;
  size_t v18;
  size_t v19;
  const void *v20;
  size_t v21;
  size_t v22;
  const void *v23;
  void *filter;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  _BYTE v37[136];
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  _BYTE v46[136];
  unint64_t v47;
  unint64_t v48;
  unint64_t v49;
  unint64_t v50;
  unint64_t v51;
  unint64_t v52;
  unint64_t v53;
  unint64_t v54;
  _BYTE v55[136];
  unint64_t v56;
  unint64_t v57;
  unint64_t v58;
  unint64_t v59;
  unint64_t v60;
  unint64_t v61;
  unint64_t v62;
  unint64_t v63;
  _BYTE v64[136];
  unint64_t v65;
  unint64_t v66;
  unint64_t v67;
  unint64_t v68;
  unint64_t v69;
  unint64_t v70;
  unint64_t v71;
  unint64_t v72;
  _BYTE v73[136];
  unint64_t v74;
  unint64_t v75;
  unint64_t v76;
  unint64_t v77;
  unint64_t v78;
  unint64_t v79;
  unint64_t v80;
  unint64_t v81;
  _BYTE v82[136];
  _BYTE v83[136];
  _BYTE v84[136];
  _BYTE v85[8];
  _BYTE v86[8];
  _BYTE v87[8];
  const void *v88;
  const void *v89;
  const void *v90;

  filter = *(void **)(a2 + 16);
  outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v87, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v87, (uint64_t)&v88, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v23 = v88;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v82);
  outlined init with take of BNNS.Shape((uint64_t)v82, (uint64_t)v83);
  outlined init with take of BNNS.Shape((uint64_t)v83, (uint64_t)v84);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v83, (uint64_t)v84);
  BNNS.Shape.stride.getter();
  v22 = specialized static BNNS.calculateBatchStride(size:stride:)(v74, v75, v76, v77, v78, v79, v80, v81, v74, v75, v76, v77, v78, v79, v80, v81);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v74);
  outlined init with take of BNNS.Shape((uint64_t)&v74, (uint64_t)v84);
  outlined init with take of BNNS.Shape((uint64_t)v84, (uint64_t)v73);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v84, (uint64_t)v73);
  BNNS.Shape.stride.getter();
  v21 = specialized static BNNS.calculateBatchStride(size:stride:)(v65, v66, v67, v68, v69, v70, v71, v72, v65, v66, v67, v68, v69, v70, v71, v72);
  outlined init with take of BNNSNDArrayDescriptor?(a7 + 136, (uint64_t)v86, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v86, (uint64_t)&v89, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v20 = v89;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v64);
  outlined init with take of BNNS.Shape((uint64_t)v64, (uint64_t)&v65);
  outlined init with take of BNNS.Shape((uint64_t)&v65, (uint64_t)v73);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)&v65, (uint64_t)v73);
  BNNS.Shape.stride.getter();
  v18 = specialized static BNNS.calculateBatchStride(size:stride:)(v56, v57, v58, v59, v60, v61, v62, v63, v56, v57, v58, v59, v60, v61, v62, v63);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v56);
  outlined init with take of BNNS.Shape((uint64_t)&v56, (uint64_t)v73);
  outlined init with take of BNNS.Shape((uint64_t)v73, (uint64_t)v55);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v73, (uint64_t)v55);
  BNNS.Shape.stride.getter();
  v19 = specialized static BNNS.calculateBatchStride(size:stride:)(v47, v48, v49, v50, v51, v52, v53, v54, v47, v48, v49, v50, v51, v52, v53, v54);
  outlined init with take of BNNSNDArrayDescriptor?(a10 + 136, (uint64_t)v85, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v85, (uint64_t)&v90, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  out = v90;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v46);
  outlined init with take of BNNS.Shape((uint64_t)v46, (uint64_t)&v47);
  outlined init with take of BNNS.Shape((uint64_t)&v47, (uint64_t)v55);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)&v47, (uint64_t)v55);
  BNNS.Shape.stride.getter();
  v13 = specialized static BNNS.calculateBatchStride(size:stride:)(v38, v39, v40, v41, v42, v43, v44, v45, v38, v39, v40, v41, v42, v43, v44, v45);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v38);
  outlined init with take of BNNS.Shape((uint64_t)&v38, (uint64_t)v55);
  outlined init with take of BNNS.Shape((uint64_t)v55, (uint64_t)v37);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v55, (uint64_t)v37);
  BNNS.Shape.stride.getter();
  v14 = specialized static BNNS.calculateBatchStride(size:stride:)(v29, v30, v31, v32, v33, v34, v35, v36, v29, v30, v31, v32, v33, v34, v35, v36);
  return BNNSFilterApplyBackwardTwoInputBatch(filter, a3, v23, v22, a5, v21, v20, v18, a8, v19, out, v13, out_delta, v14, weights_delta, a1);
}

uint64_t closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingWeightsGradient:)(BNNSNDArrayDescriptor *a1, uint64_t a2, size_t a3, uint64_t a4, uint64_t a5, const BNNSNDArrayDescriptor *a6)
{
  size_t v7;
  size_t v8;
  const void *v10;
  size_t v11;
  const void *v12;
  void *v13;
  unint64_t v17;
  unint64_t v18;
  unint64_t v19;
  unint64_t v20;
  unint64_t v21;
  unint64_t v22;
  unint64_t v23;
  unint64_t v24;
  _BYTE v25[136];
  unint64_t v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  _BYTE v34[136];
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  _BYTE v43[136];
  _BYTE v44[136];
  _BYTE v45[136];
  _BYTE v46[8];
  _BYTE v47[8];
  const void *v48;
  const void *v49;

  v13 = *(void **)(a2 + 16);
  outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v47, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v47, (uint64_t)&v48, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v12 = v48;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v44);
  outlined init with take of BNNS.Shape((uint64_t)v44, (uint64_t)v45);
  outlined init with take of BNNS.Shape((uint64_t)v45, (uint64_t)v43);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v45, (uint64_t)v43);
  BNNS.Shape.stride.getter();
  v11 = specialized static BNNS.calculateBatchStride(size:stride:)(v35, v36, v37, v38, v39, v40, v41, v42, v35, v36, v37, v38, v39, v40, v41, v42);
  outlined init with take of BNNSNDArrayDescriptor?(a5 + 136, (uint64_t)v46, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v46, (uint64_t)&v49, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v10 = v49;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v34);
  outlined init with take of BNNS.Shape((uint64_t)v34, (uint64_t)&v35);
  outlined init with take of BNNS.Shape((uint64_t)&v35, (uint64_t)v43);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)&v35, (uint64_t)v43);
  BNNS.Shape.stride.getter();
  v7 = specialized static BNNS.calculateBatchStride(size:stride:)(v26, v27, v28, v29, v30, v31, v32, v33, v26, v27, v28, v29, v30, v31, v32, v33);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v26);
  outlined init with take of BNNS.Shape((uint64_t)&v26, (uint64_t)v43);
  outlined init with take of BNNS.Shape((uint64_t)v43, (uint64_t)v25);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v43, (uint64_t)v25);
  BNNS.Shape.stride.getter();
  v8 = specialized static BNNS.calculateBatchStride(size:stride:)(v17, v18, v19, v20, v21, v22, v23, v24, v17, v18, v19, v20, v21, v22, v23, v24);
  return BNNSFilterApplyBackwardBatch(v13, a3, v12, v11, 0, 0, v10, v7, a6, v8, a1, 0);
}

uint64_t specialized static BNNS.layerApply(_:batchSize:input:output:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4)
{
  const void *v7;
  size_t v8;
  uint64_t result;
  _BYTE *v10;
  _BYTE *v11;
  unint64_t in_stride;
  void *v13;
  void *v14;
  unint64_t v15;
  unint64_t v16;
  unint64_t v17;
  unint64_t v18;
  unint64_t v19;
  unint64_t v20;
  unint64_t v21;
  unint64_t v22;
  unint64_t v23;
  unint64_t v24;
  unint64_t v25;
  unint64_t v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  _BYTE v31[136];
  _BYTE v32[136];
  _BYTE v33[136];
  _BYTE v34[136];
  _BYTE v35[8];
  _BYTE v36[8];
  const void *v37;
  void *v38;

  outlined init with take of BNNSNDArrayDescriptor?(a3 + 136, (uint64_t)v36, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v36, (uint64_t)&v37, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v7 = v37;
  if (v37
    && (outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v35, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v35, (uint64_t)&v38, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), v38))
  {
    v13 = v38;
    v14 = *(void **)(a1 + 16);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v32);
    outlined init with take of BNNS.Shape((uint64_t)v32, (uint64_t)v33);
    outlined init with take of BNNS.Shape((uint64_t)v33, (uint64_t)v31);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v33, (uint64_t)v31);
    BNNS.Shape.stride.getter();
    in_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v23, v24, v25, v26, v27, v28, v29, v30, v23, v24, v25, v26, v27, v28, v29, v30);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v31);
    outlined init with take of BNNS.Shape((uint64_t)v31, (uint64_t)v34);
    outlined init with take of BNNS.Shape((uint64_t)v34, (uint64_t)&v23);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v34, (uint64_t)&v23);
    BNNS.Shape.stride.getter();
    v8 = specialized static BNNS.calculateBatchStride(size:stride:)(v15, v16, v17, v18, v19, v20, v21, v22, v15, v16, v17, v18, v19, v20, v21, v22);
    result = BNNSFilterApplyBatch(v14, a2, v7, in_stride, v13, v8);
    if (!(_DWORD)result)
      return result;
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v10 = 0;
  }
  else
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v11 = 2;
  }
  return swift_willThrow();
}

uint64_t specialized static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4, _OWORD *a5, _OWORD *a6, uint64_t a7, uint64_t a8)
{
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  uint64_t result;
  _BYTE *v26;
  BNNSNDArrayDescriptor *v27;
  BNNSNDArrayDescriptor v28;
  BNNSNDArrayDescriptor v29;
  BNNSNDArrayDescriptor v30;
  BNNSNDArrayDescriptor v31;
  BNNSNDArrayDescriptor v32;
  BNNSNDArrayDescriptor v33;
  BNNSNDArrayDescriptor v34;
  uint64_t v35;

  v35 = *MEMORY[0x1E0C80C00];
  v15 = a5[9];
  *(_OWORD *)&v31.stride[7] = a5[8];
  *(_OWORD *)&v31.data_type = v15;
  *(_OWORD *)&v31.table_data_type = a5[10];
  v16 = a5[5];
  *(_OWORD *)&v31.size[7] = a5[4];
  *(_OWORD *)&v31.stride[1] = v16;
  v17 = a5[7];
  *(_OWORD *)&v31.stride[3] = a5[6];
  *(_OWORD *)&v31.stride[5] = v17;
  v18 = a5[1];
  *(_OWORD *)&v31.flags = *a5;
  *(_OWORD *)&v31.size[1] = v18;
  v19 = a5[3];
  *(_OWORD *)&v31.size[3] = a5[2];
  *(_OWORD *)&v31.size[5] = v19;
  v20 = a6[9];
  *(_OWORD *)&v30.stride[7] = a6[8];
  *(_OWORD *)&v30.data_type = v20;
  *(_OWORD *)&v30.table_data_type = a6[10];
  v21 = a6[5];
  *(_OWORD *)&v30.size[7] = a6[4];
  *(_OWORD *)&v30.stride[1] = v21;
  v22 = a6[7];
  *(_OWORD *)&v30.stride[3] = a6[6];
  *(_OWORD *)&v30.stride[5] = v22;
  v23 = a6[1];
  *(_OWORD *)&v30.flags = *a6;
  *(_OWORD *)&v30.size[1] = v23;
  v24 = a6[3];
  *(_OWORD *)&v30.size[3] = a6[2];
  *(_OWORD *)&v30.size[5] = v24;
  outlined init with take of BNNSNDArrayDescriptor?(a7, (uint64_t)&v34, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)&v34) != 1)
  {
    v29 = v34;
    outlined init with take of BNNSNDArrayDescriptor?(a8, (uint64_t)&v32, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)&v32) != 1)
    {
      v28 = v32;
      result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(&v28, a1, a2, a3, &v30, (uint64_t)a6, a4, &v31, (uint64_t)a5, &v29);
      goto LABEL_10;
    }
    v27 = &v29;
    goto LABEL_6;
  }
  outlined init with take of BNNSNDArrayDescriptor?(a8, (uint64_t)&v33, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)&v33) == 1)
  {
    v27 = 0;
LABEL_6:
    result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(0, a1, a2, a3, &v30, (uint64_t)a6, a4, &v31, (uint64_t)a5, v27);
    goto LABEL_10;
  }
  v29 = v33;
  result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(&v29, a1, a2, a3, &v30, (uint64_t)a6, a4, &v31, (uint64_t)a5, 0);
LABEL_10:
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v26 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t specialized static BNNS.layerApply(_:batchSize:inputA:inputB:output:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  const void *v9;
  const void *v10;
  size_t v11;
  uint64_t result;
  _BYTE *v13;
  _BYTE *v14;
  unint64_t inB_stride;
  unint64_t inA_stride;
  void *v17;
  unint64_t v18;
  unint64_t v19;
  unint64_t v20;
  unint64_t v21;
  unint64_t v22;
  unint64_t v23;
  unint64_t v24;
  unint64_t v25;
  unint64_t v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  _BYTE v42[136];
  _BYTE v43[136];
  _BYTE v44[136];
  _BYTE v45[136];
  _BYTE v46[136];
  _BYTE v47[8];
  _BYTE v48[8];
  _BYTE v49[8];
  const void *v50;
  const void *v51;
  void *v52;
  void *v53;

  outlined init with take of BNNSNDArrayDescriptor?(a3 + 136, (uint64_t)v49, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v49, (uint64_t)&v50, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v9 = v50;
  if (v50
    && (outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v48, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v48, (uint64_t)&v51, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), (v10 = v51) != 0)&& (outlined init with take of BNNSNDArrayDescriptor?(a5 + 136, (uint64_t)v47, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v47, (uint64_t)&v52, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), v52))
  {
    v17 = *(void **)(a1 + 16);
    v53 = v52;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v43);
    outlined init with take of BNNS.Shape((uint64_t)v43, (uint64_t)v44);
    outlined init with take of BNNS.Shape((uint64_t)v44, (uint64_t)v42);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v44, (uint64_t)v42);
    BNNS.Shape.stride.getter();
    inA_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v34, v35, v36, v37, v38, v39, v40, v41, v34, v35, v36, v37, v38, v39, v40, v41);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v42);
    outlined init with take of BNNS.Shape((uint64_t)v42, (uint64_t)v45);
    outlined init with take of BNNS.Shape((uint64_t)v45, (uint64_t)&v34);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v45, (uint64_t)&v34);
    BNNS.Shape.stride.getter();
    inB_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v26, v27, v28, v29, v30, v31, v32, v33, v26, v27, v28, v29, v30, v31, v32, v33);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v34);
    outlined init with take of BNNS.Shape((uint64_t)&v34, (uint64_t)v46);
    outlined init with take of BNNS.Shape((uint64_t)v46, (uint64_t)&v26);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v46, (uint64_t)&v26);
    BNNS.Shape.stride.getter();
    v11 = specialized static BNNS.calculateBatchStride(size:stride:)(v18, v19, v20, v21, v22, v23, v24, v25, v18, v19, v20, v21, v22, v23, v24, v25);
    result = BNNSFilterApplyTwoInputBatch(v17, a2, v9, inA_stride, v10, inB_stride, v53, v11);
    if (!(_DWORD)result)
      return result;
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v13 = 0;
  }
  else
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v14 = 2;
  }
  return swift_willThrow();
}

uint64_t type metadata accessor for BNNS.Layer()
{
  return objc_opt_self();
}

uint64_t method lookup function for BNNS.Layer()
{
  return swift_lookUpClassMethod();
}

uint64_t type metadata accessor for BNNS.UnaryLayer()
{
  return objc_opt_self();
}

uint64_t method lookup function for BNNS.UnaryLayer()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of BNNS.UnaryLayer.apply(batchSize:input:output:)(uint64_t a1, uint64_t *a2, uint64_t *a3)
{
  uint64_t v3;
  uint64_t v4;
  int v5;
  uint64_t v6;
  int v7;
  uint64_t v8;
  int v9;
  uint64_t v10;
  int v11;
  uint64_t (*v12)(uint64_t, uint64_t *, uint64_t *);
  uint64_t v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  uint64_t v23;
  int v24;
  uint64_t v25;
  int v26;
  uint64_t v27;
  uint64_t v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  uint64_t v37;
  int v38;
  uint64_t v39;
  int v40;
  uint64_t v41;

  v4 = a2[17];
  v5 = *((_DWORD *)a2 + 36);
  v6 = a2[19];
  v7 = *((_DWORD *)a2 + 40);
  v8 = a3[17];
  v9 = *((_DWORD *)a3 + 36);
  v10 = a3[19];
  v11 = *((_DWORD *)a3 + 40);
  v12 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *))(*(_QWORD *)v3 + 96);
  v28 = *a2;
  v29 = *(_OWORD *)(a2 + 1);
  v30 = *(_OWORD *)(a2 + 3);
  v31 = *(_OWORD *)(a2 + 5);
  v32 = *(_OWORD *)(a2 + 7);
  v33 = *(_OWORD *)(a2 + 9);
  v34 = *(_OWORD *)(a2 + 11);
  v35 = *(_OWORD *)(a2 + 13);
  v36 = *(_OWORD *)(a2 + 15);
  v37 = v4;
  v38 = v5;
  v39 = v6;
  v40 = v7;
  v41 = *(uint64_t *)((char *)a2 + 164);
  v14 = *a3;
  v15 = *(_OWORD *)(a3 + 1);
  v16 = *(_OWORD *)(a3 + 3);
  v17 = *(_OWORD *)(a3 + 5);
  v18 = *(_OWORD *)(a3 + 7);
  v19 = *(_OWORD *)(a3 + 9);
  v20 = *(_OWORD *)(a3 + 11);
  v21 = *(_OWORD *)(a3 + 13);
  v22 = *(_OWORD *)(a3 + 15);
  v23 = v8;
  v24 = v9;
  v25 = v10;
  v26 = v11;
  v27 = *(uint64_t *)((char *)a3 + 164);
  return v12(a1, &v28, &v14);
}

uint64_t dispatch thunk of BNNS.UnaryLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5)
{
  uint64_t v5;
  uint64_t v6;
  int v7;
  uint64_t v8;
  int v9;
  uint64_t v10;
  int v11;
  uint64_t v12;
  int v13;
  uint64_t v14;
  int v15;
  uint64_t v16;
  int v17;
  uint64_t v18;
  int v19;
  uint64_t v20;
  int v21;
  uint64_t (*v22)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *);
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  uint64_t v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  uint64_t v40;
  __int128 v41;
  uint64_t v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  uint64_t v52;
  int v53;
  uint64_t v54;
  int v55;
  uint64_t v56;
  uint64_t v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  __int128 v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  __int128 v65;
  uint64_t v66;
  int v67;
  uint64_t v68;
  int v69;
  uint64_t v70;
  uint64_t v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  __int128 v76;
  __int128 v77;
  __int128 v78;
  __int128 v79;
  uint64_t v80;
  int v81;
  uint64_t v82;
  int v83;
  uint64_t v84;
  uint64_t v85;
  __int128 v86;
  __int128 v87;
  __int128 v88;
  __int128 v89;
  __int128 v90;
  __int128 v91;
  __int128 v92;
  __int128 v93;
  uint64_t v94;
  int v95;
  uint64_t v96;
  int v97;
  uint64_t v98;

  v6 = a2[17];
  v7 = *((_DWORD *)a2 + 36);
  v8 = a2[19];
  v9 = *((_DWORD *)a2 + 40);
  v10 = a3[17];
  v11 = *((_DWORD *)a3 + 36);
  v12 = a3[19];
  v13 = *((_DWORD *)a3 + 40);
  v14 = a4[17];
  v15 = *((_DWORD *)a4 + 36);
  v16 = a4[19];
  v17 = *((_DWORD *)a4 + 40);
  v18 = a5[17];
  v19 = *((_DWORD *)a5 + 36);
  v20 = a5[19];
  v21 = *((_DWORD *)a5 + 40);
  v22 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(_QWORD *)v5 + 104);
  v85 = *a2;
  v86 = *(_OWORD *)(a2 + 1);
  v87 = *(_OWORD *)(a2 + 3);
  v88 = *(_OWORD *)(a2 + 5);
  v89 = *(_OWORD *)(a2 + 7);
  v90 = *(_OWORD *)(a2 + 9);
  v91 = *(_OWORD *)(a2 + 11);
  v92 = *(_OWORD *)(a2 + 13);
  v93 = *(_OWORD *)(a2 + 15);
  v94 = v6;
  v95 = v7;
  v96 = v8;
  v97 = v9;
  v98 = *(uint64_t *)((char *)a2 + 164);
  v71 = *a3;
  v72 = *(_OWORD *)(a3 + 1);
  v73 = *(_OWORD *)(a3 + 3);
  v74 = *(_OWORD *)(a3 + 5);
  v75 = *(_OWORD *)(a3 + 7);
  v76 = *(_OWORD *)(a3 + 9);
  v77 = *(_OWORD *)(a3 + 11);
  v23 = *(_OWORD *)(a3 + 15);
  v78 = *(_OWORD *)(a3 + 13);
  v24 = *(_OWORD *)(a4 + 1);
  v25 = *(_OWORD *)(a4 + 3);
  v26 = *(_OWORD *)(a4 + 5);
  v27 = *(_OWORD *)(a4 + 7);
  v28 = *(_OWORD *)(a4 + 9);
  v29 = *(_OWORD *)(a4 + 11);
  v30 = *(_OWORD *)(a4 + 13);
  v31 = *a4;
  v32 = *(_OWORD *)(a4 + 15);
  v33 = *(_OWORD *)(a5 + 1);
  v34 = *(_OWORD *)(a5 + 3);
  v35 = *(_OWORD *)(a5 + 5);
  v36 = *(_OWORD *)(a5 + 7);
  v37 = *(_OWORD *)(a5 + 9);
  v38 = *(_OWORD *)(a5 + 11);
  v39 = *(_OWORD *)(a5 + 13);
  v40 = *a5;
  v41 = *(_OWORD *)(a5 + 15);
  v79 = v23;
  v80 = v10;
  v81 = v11;
  v82 = v12;
  v83 = v13;
  v84 = *(uint64_t *)((char *)a3 + 164);
  *(_QWORD *)&v23 = *(uint64_t *)((char *)a5 + 164);
  v57 = v31;
  v58 = v24;
  *(_QWORD *)&v24 = *(uint64_t *)((char *)a4 + 164);
  v59 = v25;
  v60 = v26;
  v61 = v27;
  v62 = v28;
  v63 = v29;
  v64 = v30;
  v65 = v32;
  v66 = v14;
  v67 = v15;
  v68 = v16;
  v69 = v17;
  v70 = v24;
  v43 = v40;
  v44 = v33;
  v45 = v34;
  v46 = v35;
  v47 = v36;
  v48 = v37;
  v49 = v38;
  v50 = v39;
  v51 = v41;
  v52 = v18;
  v53 = v19;
  v54 = v20;
  v55 = v21;
  v56 = v23;
  return v22(a1, &v85, &v71, &v57, &v43);
}

uint64_t type metadata accessor for BNNS.BinaryLayer()
{
  return objc_opt_self();
}

uint64_t method lookup function for BNNS.BinaryLayer()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of BNNS.BinaryLayer.apply(batchSize:inputA:inputB:output:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4)
{
  uint64_t v4;
  uint64_t v5;
  int v6;
  uint64_t v7;
  int v8;
  uint64_t v9;
  int v10;
  uint64_t v11;
  int v12;
  uint64_t v13;
  int v14;
  uint64_t v15;
  int v16;
  uint64_t (*v17)(uint64_t, uint64_t *, uint64_t *, uint64_t *);
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  uint64_t v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  uint64_t v33;
  int v34;
  uint64_t v35;
  int v36;
  uint64_t v37;
  uint64_t v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  uint64_t v47;
  int v48;
  uint64_t v49;
  int v50;
  uint64_t v51;
  uint64_t v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  uint64_t v61;
  int v62;
  uint64_t v63;
  int v64;
  uint64_t v65;

  v5 = a2[17];
  v6 = *((_DWORD *)a2 + 36);
  v7 = a2[19];
  v8 = *((_DWORD *)a2 + 40);
  v9 = a3[17];
  v10 = *((_DWORD *)a3 + 36);
  v11 = a3[19];
  v12 = *((_DWORD *)a3 + 40);
  v13 = a4[17];
  v14 = *((_DWORD *)a4 + 36);
  v15 = a4[19];
  v16 = *((_DWORD *)a4 + 40);
  v17 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *))(*(_QWORD *)v4 + 96);
  v52 = *a2;
  v53 = *(_OWORD *)(a2 + 1);
  v54 = *(_OWORD *)(a2 + 3);
  v55 = *(_OWORD *)(a2 + 5);
  v56 = *(_OWORD *)(a2 + 7);
  v57 = *(_OWORD *)(a2 + 9);
  v58 = *(_OWORD *)(a2 + 11);
  v59 = *(_OWORD *)(a2 + 13);
  v60 = *(_OWORD *)(a2 + 15);
  v61 = v5;
  v62 = v6;
  v63 = v7;
  v64 = v8;
  v65 = *(uint64_t *)((char *)a2 + 164);
  v38 = *a3;
  v39 = *(_OWORD *)(a3 + 1);
  v40 = *(_OWORD *)(a3 + 3);
  v41 = *(_OWORD *)(a3 + 5);
  v42 = *(_OWORD *)(a3 + 7);
  v43 = *(_OWORD *)(a3 + 9);
  v44 = *(_OWORD *)(a3 + 11);
  v45 = *(_OWORD *)(a3 + 13);
  v46 = *(_OWORD *)(a3 + 15);
  v47 = v9;
  v48 = v10;
  v49 = v11;
  v50 = v12;
  v51 = *(uint64_t *)((char *)a3 + 164);
  v24 = *a4;
  v25 = *(_OWORD *)(a4 + 1);
  v26 = *(_OWORD *)(a4 + 3);
  v18 = *(_OWORD *)(a4 + 7);
  v19 = *(_OWORD *)(a4 + 9);
  v20 = *(_OWORD *)(a4 + 11);
  v21 = *(_OWORD *)(a4 + 13);
  v22 = *(_OWORD *)(a4 + 15);
  v27 = *(_OWORD *)(a4 + 5);
  v28 = v18;
  v29 = v19;
  v30 = v20;
  v31 = v21;
  v32 = v22;
  v33 = v13;
  v34 = v14;
  v35 = v15;
  v36 = v16;
  v37 = *(uint64_t *)((char *)a4 + 164);
  return v17(a1, &v52, &v38, &v24);
}

uint64_t dispatch thunk of BNNS.BinaryLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5, uint64_t *a6, uint64_t *a7)
{
  uint64_t v7;
  uint64_t v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  uint64_t v18;
  int v19;
  uint64_t v20;
  int v21;
  uint64_t v22;
  uint64_t v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  uint64_t v32;
  int v33;
  uint64_t v34;
  int v35;
  uint64_t v36;
  uint64_t v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  uint64_t v46;
  int v47;
  uint64_t v48;
  int v49;
  uint64_t v50;
  uint64_t v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  uint64_t v60;
  int v61;
  uint64_t v62;
  int v63;
  uint64_t v64;
  uint64_t v65;
  __int128 v66;
  __int128 v67;
  __int128 v68;
  __int128 v69;
  __int128 v70;
  __int128 v71;
  __int128 v72;
  __int128 v73;
  uint64_t v74;
  int v75;
  uint64_t v76;
  int v77;
  uint64_t v78;
  uint64_t v79;
  __int128 v80;
  __int128 v81;
  __int128 v82;
  __int128 v83;
  __int128 v84;
  __int128 v85;
  __int128 v86;
  __int128 v87;
  uint64_t v88;
  int v89;
  uint64_t v90;
  int v91;
  uint64_t v92;

  v85 = *(_OWORD *)(a2 + 11);
  v86 = *(_OWORD *)(a2 + 13);
  v87 = *(_OWORD *)(a2 + 15);
  v92 = *(uint64_t *)((char *)a2 + 164);
  v80 = *(_OWORD *)(a2 + 1);
  v81 = *(_OWORD *)(a2 + 3);
  v82 = *(_OWORD *)(a2 + 5);
  v83 = *(_OWORD *)(a2 + 7);
  v84 = *(_OWORD *)(a2 + 9);
  v71 = *(_OWORD *)(a3 + 11);
  v72 = *(_OWORD *)(a3 + 13);
  v73 = *(_OWORD *)(a3 + 15);
  v78 = *(uint64_t *)((char *)a3 + 164);
  v66 = *(_OWORD *)(a3 + 1);
  v67 = *(_OWORD *)(a3 + 3);
  v68 = *(_OWORD *)(a3 + 5);
  v69 = *(_OWORD *)(a3 + 7);
  v70 = *(_OWORD *)(a3 + 9);
  v57 = *(_OWORD *)(a4 + 11);
  v58 = *(_OWORD *)(a4 + 13);
  v59 = *(_OWORD *)(a4 + 15);
  v64 = *(uint64_t *)((char *)a4 + 164);
  v52 = *(_OWORD *)(a4 + 1);
  v53 = *(_OWORD *)(a4 + 3);
  v54 = *(_OWORD *)(a4 + 5);
  v55 = *(_OWORD *)(a4 + 7);
  v56 = *(_OWORD *)(a4 + 9);
  v43 = *(_OWORD *)(a5 + 11);
  v44 = *(_OWORD *)(a5 + 13);
  v45 = *(_OWORD *)(a5 + 15);
  v50 = *(uint64_t *)((char *)a5 + 164);
  v38 = *(_OWORD *)(a5 + 1);
  v39 = *(_OWORD *)(a5 + 3);
  v40 = *(_OWORD *)(a5 + 5);
  v41 = *(_OWORD *)(a5 + 7);
  v42 = *(_OWORD *)(a5 + 9);
  v29 = *(_OWORD *)(a6 + 11);
  v30 = *(_OWORD *)(a6 + 13);
  v31 = *(_OWORD *)(a6 + 15);
  v36 = *(uint64_t *)((char *)a6 + 164);
  v79 = *a2;
  v65 = *a3;
  v51 = *a4;
  v37 = *a5;
  v23 = *a6;
  v24 = *(_OWORD *)(a6 + 1);
  v25 = *(_OWORD *)(a6 + 3);
  v26 = *(_OWORD *)(a6 + 5);
  v27 = *(_OWORD *)(a6 + 7);
  v28 = *(_OWORD *)(a6 + 9);
  v9 = *a7;
  v10 = *(_OWORD *)(a7 + 1);
  v11 = *(_OWORD *)(a7 + 3);
  v12 = *(_OWORD *)(a7 + 5);
  v13 = *(_OWORD *)(a7 + 7);
  v14 = *(_OWORD *)(a7 + 9);
  v15 = *(_OWORD *)(a7 + 11);
  v16 = *(_OWORD *)(a7 + 13);
  v17 = *(_OWORD *)(a7 + 15);
  v22 = *(uint64_t *)((char *)a7 + 164);
  v88 = a2[17];
  v89 = *((_DWORD *)a2 + 36);
  v90 = a2[19];
  v91 = *((_DWORD *)a2 + 40);
  v74 = a3[17];
  v75 = *((_DWORD *)a3 + 36);
  v76 = a3[19];
  v77 = *((_DWORD *)a3 + 40);
  v60 = a4[17];
  v61 = *((_DWORD *)a4 + 36);
  v62 = a4[19];
  v63 = *((_DWORD *)a4 + 40);
  v46 = a5[17];
  v47 = *((_DWORD *)a5 + 36);
  v48 = a5[19];
  v49 = *((_DWORD *)a5 + 40);
  v32 = a6[17];
  v33 = *((_DWORD *)a6 + 36);
  v34 = a6[19];
  v35 = *((_DWORD *)a6 + 40);
  v18 = a7[17];
  v19 = *((_DWORD *)a7 + 36);
  v20 = a7[19];
  v21 = *((_DWORD *)a7 + 40);
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(_QWORD *)v7 + 104))(a1, &v79, &v65, &v51, &v37, &v23, &v9);
}

uint64_t vImage.PixelBuffer<>.convert(to:)(uint64_t a1)
{
  return vImage.PixelBuffer<>.convert(to:)(a1, MEMORY[0x1E0C8CF20]);
}

{
  return vImage.PixelBuffer<>.convert(to:)(a1, MEMORY[0x1E0C8CD28]);
}

{
  return vImage.PixelBuffer<>.convert(to:)(a1, (uint64_t (*)(_QWORD *, _QWORD *, _QWORD))MEMORY[0x1E0C8CF28]);
}

{
  return vImage.PixelBuffer<>.convert(to:)(a1, MEMORY[0x1E0C8CD28]);
}

{
  return vImage.PixelBuffer<>.convert(to:)(a1, MEMORY[0x1E0C8CF20]);
}

{
  return vImage.PixelBuffer<>.convert(to:)(a1, (uint64_t (*)(_QWORD *, _QWORD *, _QWORD))MEMORY[0x1E0C8CF28]);
}

{
  return vImage.PixelBuffer<>.convert(to:)(a1, (uint64_t (*)(_QWORD *, _QWORD *, _QWORD))MEMORY[0x1E0C8CF28]);
}

{
  return vImage.PixelBuffer<>.convert(to:)(a1, MEMORY[0x1E0C8CD28]);
}

{
  return vImage.PixelBuffer<>.convert(to:)(a1, MEMORY[0x1E0C8CF20]);
}

{
  uint64_t v1;
  _QWORD *v2;
  uint64_t v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  vImagePixelCount v9;
  size_t v10;
  uint64_t inited;
  void *v12;
  uint64_t v13;
  vImagePixelCount v14;
  vImagePixelCount v15;
  size_t v16;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v20;
  vImagePixelCount v21;
  vImagePixelCount v22;
  size_t v23;
  uint64_t v24;
  uint64_t v25;

  v25 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_24;
  }
  v3 = v2[6];
  if (v3 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v3)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v4)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (!v6)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v7)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (v3 != v6)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (v4 != v7)
  {
LABEL_34:
    __break(1u);
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v8 = (void *)v2[4];
  if (!v8)
  {
LABEL_42:
    __break(1u);
LABEL_43:
    __break(1u);
  }
  if ((unint64_t)(v3 - 0x2000000000000000) >> 62 != 3)
    goto LABEL_35;
  v9 = 4 * v3;
  v10 = v2[7];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5E430;
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  *(_QWORD *)(inited + 32) = v8;
  *(_QWORD *)(inited + 40) = v4;
  *(_QWORD *)(inited + 48) = v9;
  *(_QWORD *)(inited + 56) = v10;
  *(_QWORD *)(inited + 64) = 0;
  if (!v5[2])
  {
LABEL_37:
    __break(1u);
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v12 = (void *)v5[4];
  if (!v12)
    goto LABEL_43;
  v13 = v5[6];
  if (v13 < 0)
    goto LABEL_38;
  if ((unint64_t)(v13 - 0x2000000000000000) >> 62 != 3)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v14 = v5[5];
  if ((v14 & 0x8000000000000000) != 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v15 = 4 * v13;
  if ((v15 & 0x8000000000000000) != 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v16 = v5[7];
  v20 = v5[4];
  v21 = v14;
  v22 = v15;
  v23 = v16;
  v24 = 0;
  src.data = v8;
  src.height = v4;
  src.width = v9;
  src.rowBytes = v10;
  dest.data = v12;
  dest.height = v14;
  dest.width = v15;
  dest.rowBytes = v16;
  vImageConvert_Planar8toPlanarF(&src, &dest, 1.0, 0.0, 0);
  swift_bridgeObjectRelease();
  return swift_arrayDestroy();
}

{
  uint64_t v1;
  _QWORD *v2;
  uint64_t v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  vImagePixelCount v9;
  size_t v10;
  uint64_t inited;
  void *v12;
  uint64_t v13;
  uint64_t v14;
  int64_t v15;
  vImagePixelCount v16;
  size_t v17;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v21;
  vImagePixelCount v22;
  int64_t v23;
  size_t v24;
  uint64_t v25;
  uint64_t v26;

  v26 = *MEMORY[0x1E0C80C00];
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    __break(1u);
    goto LABEL_24;
  }
  v3 = v2[6];
  if (v3 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v3)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v4)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v5 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (!v6)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v7)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (v3 != v6)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (v4 != v7)
  {
LABEL_34:
    __break(1u);
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v8 = (void *)v2[4];
  if (!v8)
  {
LABEL_42:
    __break(1u);
LABEL_43:
    __break(1u);
  }
  v9 = 3 * v3;
  if ((unsigned __int128)(v3 * (__int128)3) >> 64 != (3 * v3) >> 63)
    goto LABEL_35;
  v10 = v2[7];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5E430;
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  *(_QWORD *)(inited + 32) = v8;
  *(_QWORD *)(inited + 40) = v4;
  *(_QWORD *)(inited + 48) = v9;
  *(_QWORD *)(inited + 56) = v10;
  *(_QWORD *)(inited + 64) = 0;
  if (!v5[2])
  {
LABEL_37:
    __break(1u);
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v12 = (void *)v5[4];
  if (!v12)
    goto LABEL_43;
  v13 = v5[6];
  if (v13 < 0)
    goto LABEL_38;
  v14 = (unsigned __int128)(v13 * (__int128)3) >> 64;
  v15 = 3 * v13;
  if (v14 != v15 >> 63)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v16 = v5[5];
  if ((v16 & 0x8000000000000000) != 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (v15 < 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v17 = v5[7];
  v21 = v5[4];
  v22 = v16;
  v23 = v15;
  v24 = v17;
  v25 = 0;
  src.data = v8;
  src.height = v4;
  src.width = v9;
  src.rowBytes = v10;
  dest.data = v12;
  dest.height = v16;
  dest.width = v15;
  dest.rowBytes = v17;
  vImageConvert_Planar8toPlanarF(&src, &dest, 1.0, 0.0, 0);
  swift_bridgeObjectRelease();
  return swift_arrayDestroy();
}

uint64_t vImage.PixelBuffer<>.convert(to:)(uint64_t a1, uint64_t (*a2)(_QWORD *, _QWORD *, _QWORD))
{
  uint64_t v2;
  _QWORD *v3;
  uint64_t v4;
  uint64_t v5;
  _QWORD *v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  _QWORD v14[4];
  _QWORD v15[5];

  v15[4] = *MEMORY[0x1E0C80C00];
  v3 = *(_QWORD **)v2;
  if (!*(_QWORD *)(*(_QWORD *)v2 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v4 = v3[6];
  if (v4 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v5 = v3[5];
  if (v5 < 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v4)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v5)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v6 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v7 = v6[6];
  if (v7 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v8 = v6[5];
  if (v8 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v7)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v8)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v4 != v7)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v5 != v8)
    goto LABEL_25;
  v9 = v3[4];
  v10 = v6[4];
  v11 = v3[7];
  v12 = v6[7];
  v15[0] = v9;
  v15[1] = v5;
  v15[2] = 2 * v4;
  v15[3] = v11;
  v14[0] = v10;
  v14[1] = v5;
  v14[2] = 2 * v4;
  v14[3] = v12;
  return a2(v15, v14, 0);
}

{
  uint64_t v2;
  _QWORD *v3;
  unint64_t v4;
  uint64_t v5;
  _QWORD *v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  _QWORD v14[4];
  _QWORD v15[5];

  v15[4] = *MEMORY[0x1E0C80C00];
  v3 = *(_QWORD **)v2;
  if (!*(_QWORD *)(*(_QWORD *)v2 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  v4 = v3[6];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v5 = v3[5];
  if (v5 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v5)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = v6[6];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v8 = v6[5];
  if (v8 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v8)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (v4 != v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v5 != v8)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v4 >> 62)
    goto LABEL_27;
  v9 = v3[4];
  v10 = v6[4];
  v11 = v3[7];
  v12 = v6[7];
  v15[0] = v9;
  v15[1] = v5;
  v15[2] = 4 * v4;
  v15[3] = v11;
  v14[0] = v10;
  v14[1] = v5;
  v14[2] = 4 * v4;
  v14[3] = v12;
  return a2(v15, v14, 0);
}

{
  uint64_t v2;
  _QWORD *v3;
  uint64_t v4;
  uint64_t v5;
  _QWORD *v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  _QWORD v14[4];
  _QWORD v15[5];

  v15[4] = *MEMORY[0x1E0C80C00];
  v3 = *(_QWORD **)v2;
  if (!*(_QWORD *)(*(_QWORD *)v2 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v4 = v3[6];
  if (v4 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v5 = v3[5];
  if (v5 < 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v4)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v5)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v6 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v7 = v6[6];
  if (v7 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v8 = v6[5];
  if (v8 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v7)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v8)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v4 != v7)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v5 != v8)
    goto LABEL_25;
  v9 = v3[4];
  v10 = v3[7];
  v15[0] = v9;
  v15[1] = v5;
  v15[2] = v4;
  v15[3] = v10;
  v11 = v6[4];
  v12 = v6[7];
  v14[0] = v11;
  v14[1] = v5;
  v14[2] = v4;
  v14[3] = v12;
  return a2(v15, v14, 0);
}

uint64_t vImage.FloodFillConnectivity.init(rawValue:)@<X0>(uint64_t result@<X0>, char *a2@<X8>)
{
  char v2;

  if ((_DWORD)result == 8)
    v2 = 1;
  else
    v2 = 2;
  if ((_DWORD)result == 4)
    v2 = 0;
  *a2 = v2;
  return result;
}

uint64_t vImage.FloodFillConnectivity.rawValue.getter()
{
  _BYTE *v0;

  if (*v0)
    return 8;
  else
    return 4;
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance vImage.FloodFillConnectivity()
{
  unsigned __int8 *v0;
  int v1;
  Swift::UInt32 v2;

  v1 = *v0;
  Hasher.init(_seed:)();
  if (v1)
    v2 = 8;
  else
    v2 = 4;
  Hasher._combine(_:)(v2);
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance vImage.FloodFillConnectivity()
{
  _BYTE *v0;
  Swift::UInt32 v1;

  if (*v0)
    v1 = 8;
  else
    v1 = 4;
  Hasher._combine(_:)(v1);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance vImage.FloodFillConnectivity()
{
  unsigned __int8 *v0;
  int v1;
  Swift::UInt32 v2;

  v1 = *v0;
  Hasher.init(_seed:)();
  if (v1)
    v2 = 8;
  else
    v2 = 4;
  Hasher._combine(_:)(v2);
  return Hasher._finalize()();
}

_DWORD *protocol witness for RawRepresentable.init(rawValue:) in conformance vImage.FloodFillConnectivity@<X0>(_DWORD *result@<X0>, char *a2@<X8>)
{
  char v2;
  char v3;

  if (*result == 8)
    v2 = 1;
  else
    v2 = 2;
  if (*result == 4)
    v3 = 0;
  else
    v3 = v2;
  *a2 = v3;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance vImage.FloodFillConnectivity(int *a1@<X8>)
{
  _BYTE *v1;
  int v2;

  if (*v1)
    v2 = 8;
  else
    v2 = 4;
  *a1 = v2;
}

vImage_Error vImage.PixelBuffer<>.floodFill(from:newColor:connectivity:)(Pixel_8 a1, _BYTE *a2, double a3, double a4)
{
  uint64_t *v4;
  void *data;
  uint64_t v9;
  void *v10;
  __int128 v11;
  int v12;
  vImage_Buffer seedY;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&seedY, a3) || LOBYTE(seedY.height) == 1)
    goto LABEL_11;
  data = seedY.data;
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&seedY, a4) || LOBYTE(seedY.height) == 1)
LABEL_12:
    __break(1u);
  v9 = *v4;
  if (!*(_QWORD *)(*v4 + 16))
  {
    __break(1u);
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  v10 = seedY.data;
  v11 = *(_OWORD *)(v9 + 48);
  *(_OWORD *)&seedY.data = *(_OWORD *)(v9 + 32);
  *(_OWORD *)&seedY.width = v11;
  if (*a2)
    v12 = 8;
  else
    v12 = 4;
  return vImageFloodFill_Planar8(&seedY, 0, (vImagePixelCount)data, (vImagePixelCount)v10, a1, v12, 0);
}

vImage_Error vImage.PixelBuffer<>.floodFill(from:newColor:connectivity:)(Pixel_16U a1, _BYTE *a2, double a3, double a4)
{
  uint64_t *v4;
  void *data;
  uint64_t v9;
  void *v10;
  __int128 v11;
  int v12;
  vImage_Buffer seedY;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&seedY, a3) || LOBYTE(seedY.height) == 1)
    goto LABEL_11;
  data = seedY.data;
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&seedY, a4) || LOBYTE(seedY.height) == 1)
LABEL_12:
    __break(1u);
  v9 = *v4;
  if (!*(_QWORD *)(*v4 + 16))
  {
    __break(1u);
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  v10 = seedY.data;
  v11 = *(_OWORD *)(v9 + 48);
  *(_OWORD *)&seedY.data = *(_OWORD *)(v9 + 32);
  *(_OWORD *)&seedY.width = v11;
  if (*a2)
    v12 = 8;
  else
    v12 = 4;
  return vImageFloodFill_Planar16U(&seedY, 0, (vImagePixelCount)data, (vImagePixelCount)v10, a1, v12, 0);
}

vImage_Error vImage.PixelBuffer<>.floodFill(from:newColor:connectivity:)(uint8_t a1, uint8_t a2, uint8_t a3, uint8_t a4, _BYTE *a5, double a6, double a7)
{
  uint64_t *v7;
  void *data;
  uint64_t v15;
  void *v16;
  __int128 v17;
  int v18;
  uint8_t v20[8];
  vImage_Buffer srcDest;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&srcDest, a6) || LOBYTE(srcDest.height) == 1)
    goto LABEL_11;
  data = srcDest.data;
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&srcDest, a7) || LOBYTE(srcDest.height) == 1)
LABEL_12:
    __break(1u);
  v15 = *v7;
  if (!*(_QWORD *)(*v7 + 16))
  {
    __break(1u);
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  v16 = srcDest.data;
  v17 = *(_OWORD *)(v15 + 48);
  *(_OWORD *)&srcDest.data = *(_OWORD *)(v15 + 32);
  *(_OWORD *)&srcDest.width = v17;
  v20[0] = a1;
  v20[1] = a2;
  v20[2] = a3;
  v20[3] = a4;
  if (*a5)
    v18 = 8;
  else
    v18 = 4;
  return vImageFloodFill_ARGB8888(&srcDest, 0, (vImagePixelCount)data, (vImagePixelCount)v16, v20, v18, 0);
}

vImage_Error vImage.PixelBuffer<>.floodFill(from:newColor:connectivity:)(uint16_t a1, uint16_t a2, uint16_t a3, uint16_t a4, _BYTE *a5, double a6, double a7)
{
  uint64_t *v7;
  void *data;
  uint64_t v15;
  void *v16;
  __int128 v17;
  int v18;
  uint16_t v20[4];
  vImage_Buffer srcDest;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&srcDest, a6) || LOBYTE(srcDest.height) == 1)
    goto LABEL_11;
  data = srcDest.data;
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&srcDest, a7) || LOBYTE(srcDest.height) == 1)
LABEL_12:
    __break(1u);
  v15 = *v7;
  if (!*(_QWORD *)(*v7 + 16))
  {
    __break(1u);
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  v16 = srcDest.data;
  v17 = *(_OWORD *)(v15 + 48);
  *(_OWORD *)&srcDest.data = *(_OWORD *)(v15 + 32);
  *(_OWORD *)&srcDest.width = v17;
  v20[0] = a1;
  v20[1] = a2;
  v20[2] = a3;
  v20[3] = a4;
  if (*a5)
    v18 = 8;
  else
    v18 = 4;
  return vImageFloodFill_ARGB16U(&srcDest, 0, (vImagePixelCount)data, (vImagePixelCount)v16, v20, v18, 0);
}

BOOL specialized static FixedWidthInteger._convert<A>(from:)(uint64_t a1, double a2)
{
  uint64_t v3;
  unint64_t v4;
  uint64_t v5;
  _BOOL8 result;
  char v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  unint64_t v13;
  uint64_t v14;

  v3 = (*(_QWORD *)&a2 >> 52) & 0x7FFLL;
  v4 = *(_QWORD *)&a2 & 0xFFFFFFFFFFFFFLL;
  v5 = v3 | *(_QWORD *)&a2 & 0xFFFFFFFFFFFFFLL;
  if (v5)
  {
    result = 0;
    v5 = 0;
    v7 = 1;
    if (v3 != 2047 && a2 > -1.0)
    {
      v8 = Double.exponent.getter();
      if (v8 <= 63)
      {
        v9 = v8;
        v10 = Double.significandWidth.getter();
        v11 = v10 + __clz(__rbit64(v4));
        v12 = v9 - v11;
        if (__OFSUB__(v9, v11))
        {
          __break(1u);
        }
        else
        {
          if (v11 > 63)
          {
            if (v12 < -64 || v12 > 64)
              goto LABEL_12;
          }
          else if (v12 < -64 || v12 > 64)
          {
            goto LABEL_12;
          }
          if ((v12 & 0x8000000000000000) == 0)
          {
            if ((unint64_t)v12 < 0x40)
            {
              v13 = v4 << v12;
              if (v9 < 0)
              {
LABEL_17:
                v14 = 0;
                goto LABEL_24;
              }
LABEL_23:
              v14 = 1 << v9;
              goto LABEL_24;
            }
            goto LABEL_12;
          }
        }
        if ((unint64_t)v12 > 0xFFFFFFFFFFFFFFC0)
        {
          v13 = v4 >> (v11 - v9);
          if (v9 < 0)
            goto LABEL_17;
          goto LABEL_23;
        }
LABEL_12:
        v13 = 0;
        v14 = 0;
        if (v9 < 0)
        {
LABEL_24:
          v7 = 0;
          v5 = v13 | v14;
          result = v9 >= v10;
          goto LABEL_25;
        }
        goto LABEL_23;
      }
      v5 = 0;
      result = 0;
      v7 = 1;
    }
  }
  else
  {
    v7 = 0;
    result = 1;
  }
LABEL_25:
  *(_QWORD *)a1 = v5;
  *(_BYTE *)(a1 + 8) = v7;
  return result;
}

{
  uint64_t v4;
  unint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  _BOOL8 result;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  unint64_t v14;

  v4 = (*(_QWORD *)&a2 >> 52) & 0x7FFLL;
  v5 = *(_QWORD *)&a2 & 0xFFFFFFFFFFFFFLL;
  v6 = v4 | *(_QWORD *)&a2 & 0xFFFFFFFFFFFFFLL;
  if (!v6)
  {
    v12 = 0;
    result = 1;
    goto LABEL_25;
  }
  if (v4 == 2047)
    goto LABEL_28;
  v7 = Double.exponent.getter();
  if (v7 > 63)
    goto LABEL_28;
  v8 = v7;
  v9 = Double.significandWidth.getter();
  result = v8 >= v9;
  v11 = v9 + __clz(__rbit64(v5));
  v12 = v8 - v11;
  if (__OFSUB__(v8, v11))
  {
    __break(1u);
LABEL_27:
    if (a2 < 0.0)
    {
LABEL_36:
      LOBYTE(v6) = 0;
      v12 = 0x8000000000000000;
      goto LABEL_25;
    }
LABEL_28:
    v12 = 0;
    result = 0;
    LOBYTE(v6) = 1;
    goto LABEL_25;
  }
  if (v11 > 63)
  {
    if (v12 < -64 || v12 > 64)
      goto LABEL_11;
  }
  else if (v12 < -64 || v12 > 64)
  {
    goto LABEL_11;
  }
  if (v12 < 0)
    goto LABEL_32;
  if ((unint64_t)v12 < 0x40)
  {
    v14 = v5 << v12;
    if (v8 != 63)
      goto LABEL_17;
LABEL_34:
    if (a2 < 0.0 && !v14)
      goto LABEL_36;
    goto LABEL_28;
  }
LABEL_11:
  if (v8 == 63)
    goto LABEL_27;
  v13 = 0;
  v14 = 0;
  if (v8 < 0)
    goto LABEL_20;
LABEL_19:
  v13 = 1 << v8;
LABEL_20:
  while (1)
  {
    v12 = v14 | v13;
    if (a2 >= 0.0)
      break;
    if ((v12 & 0x8000000000000000) == 0)
    {
      LOBYTE(v6) = 0;
      v12 = -v12;
      goto LABEL_25;
    }
    __break(1u);
LABEL_31:
    __break(1u);
LABEL_32:
    if ((unint64_t)v12 <= 0xFFFFFFFFFFFFFFC0)
      goto LABEL_11;
    v14 = v5 >> -(char)v12;
    if (v8 == 63)
      goto LABEL_34;
LABEL_17:
    if ((v8 & 0x8000000000000000) == 0)
      goto LABEL_19;
    v13 = 0;
  }
  if (v12 < 0)
    goto LABEL_31;
  LOBYTE(v6) = 0;
LABEL_25:
  *(_QWORD *)a1 = v12;
  *(_BYTE *)(a1 + 8) = v6;
  return result;
}

unint64_t lazy protocol witness table accessor for type vImage.FloodFillConnectivity and conformance vImage.FloodFillConnectivity()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImage.FloodFillConnectivity and conformance vImage.FloodFillConnectivity;
  if (!lazy protocol witness table cache variable for type vImage.FloodFillConnectivity and conformance vImage.FloodFillConnectivity)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImage.FloodFillConnectivity, &type metadata for vImage.FloodFillConnectivity);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.FloodFillConnectivity and conformance vImage.FloodFillConnectivity);
  }
  return result;
}

uint64_t storeEnumTagSinglePayload for vImage.FloodFillConnectivity(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 1 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 1) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFF)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFE)
    return ((uint64_t (*)(void))((char *)&loc_1CAAEE5A4 + 4 * byte_1CAB60125[v4]))();
  *a1 = a2 + 1;
  return ((uint64_t (*)(void))((char *)sub_1CAAEE5D8 + 4 * byte_1CAB60120[v4]))();
}

uint64_t sub_1CAAEE5D8(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAAEE5E0(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAAEE5E8);
  return result;
}

uint64_t sub_1CAAEE5F4(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAAEE5FCLL);
  *(_BYTE *)result = a2 + 1;
  return result;
}

uint64_t sub_1CAAEE600(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAAEE608(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vImage.FloodFillConnectivity()
{
  return &type metadata for vImage.FloodFillConnectivity;
}

uint64_t protocol witness for BNNSGraph.PointerArgument.baseAddress.getter in conformance <A> UnsafeMutableBufferPointer<A>()
{
  return UnsafeBufferPointer.baseAddress.getter();
}

uint64_t protocol witness for BNNSGraph.PointerArgument.count.getter in conformance <A> UnsafeMutableBufferPointer<A>()
{
  uint64_t v0;

  return *(_QWORD *)(v0 + 8);
}

uint64_t BNNSGraph.CompileOptions.CompileOptionsRef.__deallocating_deinit()
{
  BNNSGraphCompileOptionsDestroy();
  return swift_deallocClassInstance();
}

uint64_t BNNSGraph.CompileOptions.init()@<X0>(uint64_t *a1@<X8>)
{
  uint64_t v2;
  uint64_t result;
  uint64_t v4;

  type metadata accessor for BNNSGraph.CompileOptions.CompileOptionsRef();
  v2 = swift_allocObject();
  result = BNNSGraphCompileOptionsMakeDefault();
  *(_QWORD *)(v2 + 16) = result;
  *(_QWORD *)(v2 + 24) = v4;
  *a1 = v2;
  return result;
}

uint64_t type metadata accessor for BNNSGraph.CompileOptions.CompileOptionsRef()
{
  return objc_opt_self();
}

void static BNNSGraph.CompileOptions.OptimizationPreference.performance.getter(_DWORD *a1@<X8>)
{
  *a1 = 0;
}

uint64_t BNNSGraph.CompileOptions.init(useSingleThread:generateDebugInfo:optimizationPreference:)@<X0>(uint64_t *a1@<X8>)
{
  uint64_t v2;
  uint64_t v3;

  type metadata accessor for BNNSGraph.CompileOptions.CompileOptionsRef();
  v2 = swift_allocObject();
  *(_QWORD *)(v2 + 16) = BNNSGraphCompileOptionsMakeDefault();
  *(_QWORD *)(v2 + 24) = v3;
  *a1 = v2;
  BNNSGraphCompileOptionsSetTargetSingleThread();
  BNNSGraphCompileOptionsSetGenerateDebugInfo();
  return BNNSGraphCompileOptionsSetOptimizationPreference();
}

uint64_t BNNSGraph.CompileOptions.useSingleThread.setter(char a1)
{
  return BNNSGraph.CompileOptions.useSingleThread.setter(a1, (uint64_t (*)(_QWORD, _QWORD, _QWORD))MEMORY[0x1E0C8B7F8]);
}

uint64_t BNNSGraph.CompileOptions.generateDebugInfo.setter(char a1)
{
  return BNNSGraph.CompileOptions.useSingleThread.setter(a1, (uint64_t (*)(_QWORD, _QWORD, _QWORD))MEMORY[0x1E0C8B790]);
}

uint64_t BNNSGraph.CompileOptions.useSingleThread.setter(char a1, uint64_t (*a2)(_QWORD, _QWORD, _QWORD))
{
  uint64_t v2;

  return a2(*(_QWORD *)(*(_QWORD *)v2 + 16), *(_QWORD *)(*(_QWORD *)v2 + 24), a1 & 1);
}

uint64_t BNNSGraph.CompileOptions.optimizationPreference.setter()
{
  return BNNSGraphCompileOptionsSetOptimizationPreference();
}

uint64_t BNNSGraph.CompileOptions.useSingleThread.getter()
{
  return BNNSGraphCompileOptionsGetTargetSingleThread();
}

uint64_t (*BNNSGraph.CompileOptions.useSingleThread.modify(uint64_t a1))(unsigned __int8 *a1, uint64_t a2)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_BYTE *)(a1 + 8) = BNNSGraphCompileOptionsGetTargetSingleThread();
  return BNNSGraph.CompileOptions.useSingleThread.modify;
}

uint64_t BNNSGraph.CompileOptions.useSingleThread.modify(unsigned __int8 *a1, uint64_t a2)
{
  return BNNSGraph.CompileOptions.useSingleThread.modify(a1, a2, (uint64_t (*)(_QWORD, _QWORD, _QWORD))MEMORY[0x1E0C8B7F8]);
}

uint64_t BNNSGraph.CompileOptions.generateDebugInfo.getter()
{
  return BNNSGraphCompileOptionsGetGenerateDebugInfo();
}

uint64_t (*BNNSGraph.CompileOptions.generateDebugInfo.modify(uint64_t a1))(unsigned __int8 *a1, uint64_t a2)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_BYTE *)(a1 + 8) = BNNSGraphCompileOptionsGetGenerateDebugInfo();
  return BNNSGraph.CompileOptions.generateDebugInfo.modify;
}

uint64_t BNNSGraph.CompileOptions.generateDebugInfo.modify(unsigned __int8 *a1, uint64_t a2)
{
  return BNNSGraph.CompileOptions.useSingleThread.modify(a1, a2, (uint64_t (*)(_QWORD, _QWORD, _QWORD))MEMORY[0x1E0C8B790]);
}

uint64_t BNNSGraph.CompileOptions.useSingleThread.modify(unsigned __int8 *a1, uint64_t a2, uint64_t (*a3)(_QWORD, _QWORD, _QWORD))
{
  return a3(*(_QWORD *)(**(_QWORD **)a1 + 16), *(_QWORD *)(**(_QWORD **)a1 + 24), a1[8]);
}

uint64_t BNNSGraph.CompileOptions.OptimizationPreference.value.getter()
{
  unsigned int *v0;

  return *v0;
}

uint64_t BNNSGraph.CompileOptions.OptimizationPreference.value.setter(uint64_t result)
{
  _DWORD *v1;

  *v1 = result;
  return result;
}

uint64_t (*BNNSGraph.CompileOptions.OptimizationPreference.value.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNSGraph.CompileOptions.OptimizationPreference.init(_:)@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = result;
  return result;
}

void static BNNSGraph.CompileOptions.OptimizationPreference.internalRepresentationSize.getter(_DWORD *a1@<X8>)
{
  *a1 = 1;
}

BOOL static BNNSGraph.CompileOptions.OptimizationPreference.== infix(_:_:)(_DWORD *a1, _DWORD *a2)
{
  return *a1 == *a2;
}

uint64_t BNNSGraph.CompileOptions.optimizationPreference.getter@<X0>(_DWORD *a1@<X8>)
{
  uint64_t result;

  result = BNNSGraphCompileOptionsGetOptimizationPreference();
  *a1 = result;
  return result;
}

uint64_t (*BNNSGraph.CompileOptions.optimizationPreference.modify(uint64_t a1))()
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = BNNSGraphCompileOptionsGetOptimizationPreference();
  return BNNSGraph.CompileOptions.optimizationPreference.modify;
}

uint64_t BNNSGraph.CompileOptions.optimizationPreference.modify()
{
  return BNNSGraphCompileOptionsSetOptimizationPreference();
}

uint64_t BNNSGraph.Context.deinit()
{
  uint64_t v0;

  BNNSGraphContextDestroy_v2();
  return v0;
}

uint64_t BNNSGraph.Context.__deallocating_deinit()
{
  BNNSGraphContextDestroy_v2();
  return swift_deallocClassInstance();
}

uint64_t BNNSGraph.Context.__allocating_init(compileFromPath:functionName:options:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t *a5)
{
  uint64_t v5;
  uint64_t v11;
  _QWORD *v12;
  uint64_t v13;

  v11 = swift_allocObject();
  v12 = (_QWORD *)swift_task_alloc();
  *(_QWORD *)(v5 + 16) = v12;
  *v12 = v5;
  v12[1] = BNNSGraph.Context.__allocating_init(compileFromPath:functionName:options:);
  v12[4] = a3;
  v12[5] = a4;
  v12[2] = a1;
  v12[3] = a2;
  v13 = *a5;
  v12[6] = v11;
  v12[7] = v13;
  return swift_task_switch();
}

uint64_t BNNSGraph.Context.__allocating_init(compileFromPath:functionName:options:)(uint64_t a1)
{
  uint64_t v1;
  uint64_t *v2;
  uint64_t v4;
  uint64_t v5;

  v4 = *v2;
  v5 = swift_task_dealloc();
  if (!v1)
    v5 = a1;
  return (*(uint64_t (**)(uint64_t))(v4 + 8))(v5);
}

uint64_t BNNSGraph.Context.init(compileFromPath:functionName:options:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t *a5)
{
  uint64_t v5;
  _QWORD *v6;
  uint64_t v7;

  v6[4] = a3;
  v6[5] = a4;
  v6[2] = a1;
  v6[3] = a2;
  v7 = *a5;
  v6[6] = v5;
  v6[7] = v7;
  return swift_task_switch();
}

uint64_t BNNSGraph.Context.init(compileFromPath:functionName:options:)()
{
  uint64_t v0;
  uint64_t v1;
  _QWORD *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  BOOL v8;
  char v9;
  BOOL v10;
  char *v11;

  v1 = *(_QWORD *)(v0 + 40);
  *(_BYTE *)(*(_QWORD *)(v0 + 48) + 48) = 0;
  swift_release();
  String.utf8CString.getter();
  swift_bridgeObjectRelease();
  if (v1)
  {
    String.utf8CString.getter();
    swift_bridgeObjectRelease();
  }
  v2 = *(_QWORD **)(v0 + 48);
  v3 = BNNSGraphCompileFromFile_v2();
  v5 = v4;
  swift_unknownObjectRelease();
  swift_release();
  v2[2] = v3;
  v2[3] = v5;
  v6 = BNNSGraphContextMake();
  v2[4] = v6;
  v2[5] = v7;
  if (v5)
    v8 = v3 == 0;
  else
    v8 = 1;
  if (v8)
  {
    v9 = 0;
  }
  else
  {
    if (v7)
      v10 = v6 == 0;
    else
      v10 = 1;
    if (!v10)
    {
      BNNSGraphContextSetArgumentType();
      return (*(uint64_t (**)(_QWORD))(v0 + 8))(*(_QWORD *)(v0 + 48));
    }
    v9 = 1;
  }
  swift_release();
  lazy protocol witness table accessor for type BNNSGraph.Error and conformance BNNSGraph.Error();
  swift_allocError();
  *v11 = v9;
  swift_willThrow();
  return (*(uint64_t (**)(void))(v0 + 8))();
}

unint64_t lazy protocol witness table accessor for type BNNSGraph.Error and conformance BNNSGraph.Error()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNSGraph.Error and conformance BNNSGraph.Error;
  if (!lazy protocol witness table cache variable for type BNNSGraph.Error and conformance BNNSGraph.Error)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNSGraph.Error, &type metadata for BNNSGraph.Error);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNSGraph.Error and conformance BNNSGraph.Error);
  }
  return result;
}

{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNSGraph.Error and conformance BNNSGraph.Error;
  if (!lazy protocol witness table cache variable for type BNNSGraph.Error and conformance BNNSGraph.Error)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNSGraph.Error, &type metadata for BNNSGraph.Error);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNSGraph.Error and conformance BNNSGraph.Error);
  }
  return result;
}

uint64_t BNNSGraph.Context.setDynamicShapes(_:forFunction:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  _QWORD *v4;

  v4[4] = a3;
  v4[5] = v3;
  v4[2] = a1;
  v4[3] = a2;
  return swift_task_switch();
}

uint64_t BNNSGraph.Context.setDynamicShapes(_:forFunction:)()
{
  uint64_t v0;
  unint64_t v1;
  int64_t v2;
  unint64_t v3;
  uint64_t v4;
  uint64_t *v5;
  uint64_t v6;
  unint64_t v7;
  uint64_t v8;
  _QWORD *v9;
  uint64_t v10;
  unint64_t v11;
  uint64_t *v12;
  unint64_t v13;
  uint64_t v14;
  void *v15;
  uint64_t v16;
  unint64_t v17;
  unint64_t v18;
  uint64_t v19;
  unint64_t v20;
  int64_t v21;
  uint64_t v22;
  uint64_t v23;
  unint64_t v24;
  __int128 v25;
  unint64_t v26;
  int64_t v27;
  uint64_t *v28;
  uint64_t v29;
  uint64_t *v30;
  uint64_t v31;
  uint64_t v32;
  _QWORD *v33;
  uint64_t v34;
  unint64_t v35;
  uint64_t *v36;
  unint64_t v37;
  uint64_t v38;
  unint64_t v39;
  uint64_t v40;
  void *v41;
  uint64_t v42;
  uint64_t v43;
  unint64_t v44;
  unint64_t v45;
  uint64_t v46;
  unint64_t v47;
  unint64_t v48;
  uint64_t v49;
  unint64_t v51;
  int64_t v52;
  unint64_t __dst;
  void *__dstb[2];
  void *__dsta;
  uint64_t v56;
  uint64_t v57;
  uint64_t v58;
  uint64_t v59;
  unint64_t v60;
  uint64_t v61;

  v1 = *(_QWORD *)(v0 + 16);
  v2 = *(_QWORD *)(v1 + 16);
  v3 = MEMORY[0x1E0DEE9D8];
  if (!v2)
  {
    v21 = *(_QWORD *)(MEMORY[0x1E0DEE9D8] + 16);
    if (v21)
    {
      v19 = MEMORY[0x1E0DEE9D8];
      goto LABEL_29;
    }
    v7 = MEMORY[0x1E0DEE9D8];
    swift_bridgeObjectRelease();
LABEL_34:
    v1 = v51;
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) != 0)
      goto LABEL_35;
    goto LABEL_78;
  }
  v56 = *(_QWORD *)(v1 + 16);
  v57 = MEMORY[0x1E0DEE9D8];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v2, 0);
  v4 = 0;
  __dst = v1 + 32;
  while (1)
  {
    v5 = (uint64_t *)(__dst + 16 * v4);
    v6 = *v5;
    if (*v5)
    {
      v1 = v5[1];
      v59 = MEMORY[0x1E0DEE9D8];
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v6 & ~(v6 >> 63), 0);
      v7 = v6 - 1;
      if (v6 < 1)
        goto LABEL_76;
      v8 = *(_QWORD *)v1;
      v9 = (_QWORD *)MEMORY[0x1E0DEE9D8];
      if ((*(_QWORD *)v1 & 0x8000000000000000) != 0)
      {
LABEL_70:
        __break(1u);
        goto LABEL_71;
      }
      v10 = v59;
      v11 = *(_QWORD *)(v59 + 16);
      v12 = (uint64_t *)(v1 + 8);
      while (1)
      {
        v13 = *(_QWORD *)(v59 + 24);
        v1 = v11 + 1;
        if (v11 >= v13 >> 1)
        {
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v13 > 1), v11 + 1, 1);
          v9 = (_QWORD *)MEMORY[0x1E0DEE9D8];
        }
        *(_QWORD *)(v59 + 16) = v1;
        *(_QWORD *)(v59 + 8 * v11 + 32) = v8;
        if (!v7)
          break;
        v14 = *v12++;
        v8 = v14;
        --v7;
        ++v11;
        if (v14 < 0)
          goto LABEL_70;
      }
      v7 = v11 + 1;
      if ((v11 + 1) >> 60)
        goto LABEL_74;
    }
    else
    {
      v9 = (_QWORD *)MEMORY[0x1E0DEE9D8];
      v7 = *(_QWORD *)(MEMORY[0x1E0DEE9D8] + 16);
      v10 = MEMORY[0x1E0DEE9D8];
      if (v7 >> 60)
        goto LABEL_74;
    }
    v15 = (void *)swift_slowAlloc();
    if (v7)
      break;
LABEL_21:
    swift_bridgeObjectRelease();
    memcpy(v15, v9 + 4, 8 * v7);
    swift_bridgeObjectRelease();
    v19 = v57;
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(_QWORD *)(v57 + 16) + 1, 1);
      v19 = v57;
    }
    v1 = *(_QWORD *)(v19 + 16);
    v20 = *(_QWORD *)(v19 + 24);
    v21 = v1 + 1;
    if (v1 >= v20 >> 1)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v20 > 1), v1 + 1, 1);
      v19 = v57;
    }
    ++v4;
    *(_QWORD *)(v19 + 16) = v21;
    v22 = v19 + 16 * v1;
    *(_QWORD *)(v22 + 32) = v7;
    *(_QWORD *)(v22 + 40) = v15;
    if (v4 == v56)
    {
      v3 = MEMORY[0x1E0DEE9D8];
LABEL_29:
      v60 = v3;
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v21, 0);
      v23 = 0;
      v7 = v60;
      v24 = *(_QWORD *)(v60 + 16);
      do
      {
        v25 = *(_OWORD *)(v19 + 16 * v23 + 32);
        v26 = *(_QWORD *)(v60 + 24);
        if (v24 >= v26 >> 1)
        {
          *(_OWORD *)__dstb = *(_OWORD *)(v19 + 16 * v23 + 32);
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v26 > 1), v24 + 1, 1);
          v25 = *(_OWORD *)__dstb;
        }
        ++v23;
        *(_QWORD *)(v60 + 16) = v24 + 1;
        *(_OWORD *)(v60 + 16 * v24++ + 32) = v25;
      }
      while (v21 != v23);
      swift_bridgeObjectRelease();
      goto LABEL_34;
    }
  }
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v7, 0);
  v1 = 0;
  while (1)
  {
    v16 = *(_QWORD *)(v10 + 8 * v1 + 32);
    if (v16 < 0)
      break;
    v18 = v9[2];
    v17 = v9[3];
    if (v18 >= v17 >> 1)
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v17 > 1), v18 + 1, 1);
    ++v1;
    v9[2] = v18 + 1;
    v9[v18 + 4] = v16;
    if (v7 == v1)
      goto LABEL_21;
  }
LABEL_71:
  __break(1u);
LABEL_72:
  __break(1u);
  while (1)
  {
    __break(1u);
LABEL_74:
    __break(1u);
LABEL_75:
    __break(1u);
LABEL_76:
    __break(1u);
LABEL_77:
    __break(1u);
LABEL_78:
    v7 = (unint64_t)specialized _ArrayBuffer._consumeAndCreateNew()(v7);
LABEL_35:
    if (*(_QWORD *)(v1 + 32))
      String.utf8CString.getter();
    BNNSGraphContextSetDynamicShapes_v2();
    swift_unknownObjectRelease();
    v27 = *(_QWORD *)(v7 + 16);
    if (!v27)
      break;
    v58 = MEMORY[0x1E0DEE9D8];
    swift_bridgeObjectRetain();
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v27, 0);
    v1 = 0;
    v52 = v27;
    while (1)
    {
      v28 = (uint64_t *)(v7 + 32 + 16 * v1);
      v29 = *v28;
      v30 = (uint64_t *)v28[1];
      if (*v28)
      {
        v61 = MEMORY[0x1E0DEE9D8];
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v29 & ~(v29 >> 63), 0);
        v31 = v29 - 1;
        if (v29 < 1)
          goto LABEL_77;
        v32 = *v30;
        v33 = (_QWORD *)MEMORY[0x1E0DEE9D8];
        if (*v30 < 0)
          goto LABEL_72;
        v34 = v61;
        v35 = *(_QWORD *)(v61 + 16);
        v36 = v30 + 1;
        while (1)
        {
          v37 = *(_QWORD *)(v61 + 24);
          if (v35 >= v37 >> 1)
          {
            specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v37 > 1), v35 + 1, 1);
            v33 = (_QWORD *)MEMORY[0x1E0DEE9D8];
          }
          *(_QWORD *)(v61 + 16) = v35 + 1;
          *(_QWORD *)(v61 + 8 * v35 + 32) = v32;
          if (!v31)
            break;
          v38 = *v36++;
          v32 = v38;
          --v31;
          ++v35;
          if (v38 < 0)
            goto LABEL_72;
        }
        v39 = v35 + 1;
        if ((v35 + 1) >> 60)
          goto LABEL_75;
      }
      else
      {
        v33 = (_QWORD *)MEMORY[0x1E0DEE9D8];
        v39 = *(_QWORD *)(MEMORY[0x1E0DEE9D8] + 16);
        v34 = MEMORY[0x1E0DEE9D8];
        if (v39 >> 60)
          goto LABEL_75;
      }
      v40 = swift_slowAlloc();
      v41 = (void *)v40;
      if (v39)
        break;
      swift_bridgeObjectRelease();
LABEL_59:
      memcpy(v41, v33 + 4, 8 * v39);
      swift_bridgeObjectRelease();
      if (v30)
        MEMORY[0x1D1794DA4](v30, -1, -1);
      v46 = v58;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(_QWORD *)(v58 + 16) + 1, 1);
        v46 = v58;
      }
      v48 = *(_QWORD *)(v46 + 16);
      v47 = *(_QWORD *)(v46 + 24);
      if (v48 >= v47 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v47 > 1), v48 + 1, 1);
        v46 = v58;
      }
      ++v1;
      *(_QWORD *)(v46 + 16) = v48 + 1;
      v49 = v46 + 16 * v48;
      *(_QWORD *)(v49 + 32) = v39;
      *(_QWORD *)(v49 + 40) = v41;
      if (v1 == v52)
      {
        swift_bridgeObjectRelease_n();
        v1 = v51;
        return (*(uint64_t (**)(uint64_t))(v1 + 8))(v46);
      }
    }
    __dsta = (void *)v40;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v39, 0);
    v42 = 0;
    while (1)
    {
      v43 = *(_QWORD *)(v34 + 8 * v42 + 32);
      if (v43 < 0)
        break;
      v45 = v33[2];
      v44 = v33[3];
      if (v45 >= v44 >> 1)
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v44 > 1), v45 + 1, 1);
      ++v42;
      v33[2] = v45 + 1;
      v33[v45 + 4] = v43;
      if (v39 == v42)
      {
        swift_bridgeObjectRelease();
        v41 = __dsta;
        goto LABEL_59;
      }
    }
  }
  swift_bridgeObjectRelease();
  v46 = MEMORY[0x1E0DEE9D8];
  return (*(uint64_t (**)(uint64_t))(v1 + 8))(v46);
}

char *BNNSGraph.Shape.dimensions.getter()
{
  uint64_t *v0;
  uint64_t v1;
  char *result;
  uint64_t *v3;
  BOOL v4;
  uint64_t v5;
  uint64_t v6;
  unint64_t v7;
  uint64_t *v8;
  unint64_t v9;
  uint64_t v10;
  uint64_t v11;
  char *v12;

  v1 = *v0;
  result = (char *)MEMORY[0x1E0DEE9D8];
  if (!*v0)
    return result;
  v3 = (uint64_t *)v0[1];
  v11 = MEMORY[0x1E0DEE9D8];
  result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v1 & ~(v1 >> 63), 0);
  v4 = v1 < 1;
  v5 = v1 - 1;
  if (v4)
  {
LABEL_12:
    __break(1u);
    return result;
  }
  v6 = *v3;
  if (*v3 < 0)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  result = (char *)v11;
  v7 = *(_QWORD *)(v11 + 16);
  v8 = v3 + 1;
  while (1)
  {
    v12 = result;
    v9 = *((_QWORD *)result + 3);
    if (v7 >= v9 >> 1)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v9 > 1), v7 + 1, 1);
      result = v12;
    }
    *((_QWORD *)result + 2) = v7 + 1;
    *(_QWORD *)&result[8 * v7 + 32] = v6;
    if (!v5)
      return result;
    v10 = *v8++;
    v6 = v10;
    --v5;
    ++v7;
    if (v10 < 0)
      goto LABEL_11;
  }
}

uint64_t BNNSGraph.Shape.init(_:)@<X0>(uint64_t result@<X0>, unint64_t *a2@<X8>)
{
  unint64_t v3;
  uint64_t v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  unint64_t v9;
  unint64_t v10;
  uint64_t v11;

  v3 = *(_QWORD *)(result + 16);
  if (v3 >> 60)
  {
LABEL_12:
    __break(1u);
    return result;
  }
  v4 = result;
  v5 = (void *)swift_slowAlloc();
  if (v3)
  {
    v11 = MEMORY[0x1E0DEE9D8];
    result = (uint64_t)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v3, 0);
    v6 = 0;
    v7 = v11;
    while (1)
    {
      v8 = *(_QWORD *)(v4 + 8 * v6 + 32);
      if (v8 < 0)
        break;
      v10 = *(_QWORD *)(v11 + 16);
      v9 = *(_QWORD *)(v11 + 24);
      if (v10 >= v9 >> 1)
        result = (uint64_t)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v9 > 1), v10 + 1, 1);
      ++v6;
      *(_QWORD *)(v11 + 16) = v10 + 1;
      *(_QWORD *)(v11 + 8 * v10 + 32) = v8;
      if (v3 == v6)
      {
        swift_bridgeObjectRelease();
        goto LABEL_10;
      }
    }
    __break(1u);
    goto LABEL_12;
  }
  swift_bridgeObjectRelease();
  v7 = MEMORY[0x1E0DEE9D8];
LABEL_10:
  memcpy(v5, (const void *)(v7 + 32), 8 * v3);
  result = swift_bridgeObjectRelease();
  *a2 = v3;
  a2[1] = (unint64_t)v5;
  return result;
}

uint64_t BNNSGraph.Context.setBatchSize(_:forFunction:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  _QWORD *v4;

  v4[4] = a3;
  v4[5] = v3;
  v4[2] = a1;
  v4[3] = a2;
  return swift_task_switch();
}

uint64_t BNNSGraph.Context.setBatchSize(_:forFunction:)()
{
  uint64_t v0;

  if ((*(_QWORD *)(v0 + 16) & 0x8000000000000000) == 0)
  {
    if (*(_QWORD *)(v0 + 32))
      String.utf8CString.getter();
    BNNSGraphContextSetBatchSize_v2();
    swift_unknownObjectRelease();
  }
  return (*(uint64_t (**)(void))(v0 + 8))();
}

uint64_t BNNSGraph.Context.checkForNaNsAndInfinities.setter(char a1)
{
  uint64_t v1;
  uint64_t result;

  result = BNNSGraphContextEnableNanAndInfChecks();
  *(_BYTE *)(v1 + 48) = a1;
  return result;
}

uint64_t BNNSGraph.Context.checkForNaNsAndInfinities.getter()
{
  uint64_t v0;

  return *(unsigned __int8 *)(v0 + 48);
}

uint64_t (*BNNSGraph.Context.checkForNaNsAndInfinities.modify(uint64_t a1))(uint64_t *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_BYTE *)(a1 + 8) = *(_BYTE *)(v1 + 48);
  return BNNSGraph.Context.checkForNaNsAndInfinities.modify;
}

uint64_t BNNSGraph.Context.checkForNaNsAndInfinities.modify(uint64_t *a1)
{
  uint64_t v1;
  char v2;
  uint64_t result;

  v1 = *a1;
  v2 = *((_BYTE *)a1 + 8);
  result = BNNSGraphContextEnableNanAndInfChecks();
  *(_BYTE *)(v1 + 48) = v2;
  return result;
}

uint64_t BNNSGraph.Context.executeFunction(_:arguments:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  _QWORD *v4;

  v4[4] = a3;
  v4[5] = v3;
  v4[2] = a1;
  v4[3] = a2;
  return swift_task_switch();
}

uint64_t BNNSGraph.Context.executeFunction(_:arguments:)()
{
  uint64_t v0;
  char **v1;
  char *v2;
  unint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  int64x2_t v7;
  double *v8;
  int64x2_t v9;
  __int128 v10;
  int64x2_t v11;
  int64x2_t v12;
  uint64_t v13;
  int64x2_t v14;
  double *v15;
  int64x2_t v16;
  uint64_t v17;
  _QWORD *v18;
  unint64_t v19;
  uint64_t v20;
  int v21;
  char **v22;
  _BYTE *v23;
  float64x2x2_t v25;

  v1 = *(char ***)(v0 + 32);
  v2 = *v1;
  v3 = *((_QWORD *)*v1 + 2);
  BNNSGraphContextSetArgumentType();
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
  {
    v2 = specialized _ArrayBuffer._consumeAndCreateNew()((uint64_t)v2);
    if (v3)
      goto LABEL_3;
LABEL_11:
    v5 = MEMORY[0x1E0DEE9D8];
    goto LABEL_12;
  }
  if (!v3)
    goto LABEL_11;
LABEL_3:
  type metadata accessor for bnns_graph_argument_t(0);
  v4 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  v5 = v4;
  v6 = 0;
  *(_QWORD *)(v4 + 16) = v3;
  if (v3 < 4)
    goto LABEL_7;
  v6 = v3 & 0x7FFFFFFFFFFFFFFCLL;
  v7 = (int64x2_t)xmmword_1CAB601F0;
  v8 = (double *)(v4 + 64);
  v9 = vdupq_n_s64((unint64_t)(v2 + 32));
  v10 = 0uLL;
  v11 = vdupq_n_s64(0x140uLL);
  v12 = vdupq_n_s64(4uLL);
  v13 = v3 & 0x7FFFFFFFFFFFFFFCLL;
  do
  {
    v14.i64[0] = 160 * v7.i64[0];
    v14.i64[1] = 160 * v7.i64[1];
    v15 = v8 - 4;
    v16 = vaddq_s64(v9, v14);
    vst2q_f64(v15, *(float64x2x2_t *)(&v10 - 1));
    v25.val[0] = (float64x2_t)vaddq_s64(v16, v11);
    v25.val[1] = 0uLL;
    vst2q_f64(v8, v25);
    v7 = vaddq_s64(v7, v12);
    v8 += 8;
    v13 -= 4;
  }
  while (v13);
  if (v3 != v6)
  {
LABEL_7:
    v17 = (uint64_t)&v2[160 * v6 + 32];
    v18 = (_QWORD *)(v4 + 16 * v6 + 40);
    v19 = v3 - v6;
    do
    {
      *(v18 - 1) = v17;
      *v18 = 0;
      v17 += 160;
      v18 += 2;
      --v19;
    }
    while (v19);
  }
LABEL_12:
  v20 = *(_QWORD *)(v0 + 24);
  *(_QWORD *)(v5 + 16) = v3;
  if (v20)
    String.utf8CString.getter();
  v21 = BNNSGraphContextExecute_v2();
  swift_bridgeObjectRelease();
  swift_unknownObjectRelease();
  v22 = *(char ***)(v0 + 32);
  if (v21)
  {
    lazy protocol witness table accessor for type BNNSGraph.Error and conformance BNNSGraph.Error();
    swift_allocError();
    *v23 = 2;
    swift_willThrow();
  }
  *v22 = v2;
  return (*(uint64_t (**)(void))(v0 + 8))();
}

uint64_t BNNSGraph.Context.executeFunction<A>(_:arguments:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v5;
  uint64_t v6;
  uint64_t v12;
  int v13;
  uint64_t result;
  _BYTE *v15;
  uint64_t v16;
  _QWORD v17[2];
  uint64_t v18;

  v12 = MEMORY[0x1D1794114](a3, a4);
  if (v12 < 0)
  {
    __break(1u);
  }
  else
  {
    v5 = v12;
    if (v12)
    {
      type metadata accessor for bnns_graph_argument_t(0);
      a1 = static Array._allocateBufferUninitialized(minimumCapacity:)();
      *(_QWORD *)(a1 + 16) = v5;
    }
    else
    {
      a1 = MEMORY[0x1E0DEE9D8];
    }
    v18 = 0;
    v17[0] = a1 + 32;
    v17[1] = v5;
    closure #1 in BNNSGraph.Context.executeFunction<A>(_:arguments:)(v17, (uint64_t)&v18, v5, a3, a4, a5);
    if (v6)
      goto LABEL_14;
    if (v5 >= v18)
    {
      *(_QWORD *)(a1 + 16) = v18;
      BNNSGraphContextSetArgumentType();
      if (a2)
        String.utf8CString.getter();
      v13 = BNNSGraphContextExecute_v2();
      swift_unknownObjectRelease();
      result = swift_bridgeObjectRelease();
      if (v13)
      {
        lazy protocol witness table accessor for type BNNSGraph.Error and conformance BNNSGraph.Error();
        swift_allocError();
        *v15 = 2;
        return swift_willThrow();
      }
      return result;
    }
  }
  __break(1u);
LABEL_14:
  v16 = v18;
  if (v5 < v18)
    __break(1u);
  *(_QWORD *)(a1 + 16) = v16;
  result = swift_bridgeObjectRelease();
  __break(1u);
  return result;
}

uint64_t closure #1 in BNNSGraph.Context.executeFunction<A>(_:arguments:)(_QWORD *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t result;
  _QWORD *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t (*v18)(uint64_t, uint64_t);
  uint64_t v19;
  uint64_t *v20;
  char *v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t (*v24)(char *, uint64_t);
  uint64_t v25;
  char *v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  _QWORD *v30;
  _QWORD *v31;
  uint64_t v32;
  uint64_t (*v33)(uint64_t, uint64_t);
  char *v34;
  char *v35;
  uint64_t v36;
  uint64_t v37;

  v37 = a4;
  v8 = a1;
  v9 = *(_QWORD *)(a5 - 8);
  v10 = MEMORY[0x1E0C80A78](a1);
  v34 = (char *)&v30 - ((v11 + 15) & 0xFFFFFFFFFFFFFFF0);
  result = MEMORY[0x1E0C80A78](v10);
  v35 = (char *)&v30 - v15;
  if ((v14 & 0x8000000000000000) == 0)
  {
    v30 = v13;
    if (!v14)
    {
LABEL_8:
      *v30 = v14;
      return result;
    }
    v16 = 0;
    v17 = 0;
    v18 = *(uint64_t (**)(uint64_t, uint64_t))(a6 + 16);
    v32 = v14;
    v33 = v18;
    v31 = v8;
    while (v14 != v17)
    {
      v19 = *v8;
      v36 = v16;
      v20 = (uint64_t *)(v19 + v16);
      *v20 = 0;
      v20[1] = 0;
      v21 = v35;
      Array.subscript.getter();
      v22 = v33(a5, a6);
      v23 = a6;
      v24 = *(uint64_t (**)(char *, uint64_t))(v9 + 8);
      result = v24(v21, a5);
      if (!v22)
        goto LABEL_12;
      *v20 = v22;
      v25 = *v8;
      v26 = v34;
      Array.subscript.getter();
      v27 = (*(uint64_t (**)(uint64_t, uint64_t))(v23 + 24))(a5, v23);
      result = v24(v26, a5);
      v28 = *(_QWORD *)(v9 + 72);
      if ((unsigned __int128)(v27 * (__int128)v28) >> 64 != (v27 * v28) >> 63)
        goto LABEL_10;
      ++v17;
      v29 = v36;
      *(_QWORD *)(v25 + v36 + 8) = v27 * v28;
      v16 = v29 + 16;
      v14 = v32;
      a6 = v23;
      v8 = v31;
      if (v32 == v17)
        goto LABEL_8;
    }
    __break(1u);
LABEL_10:
    __break(1u);
  }
  __break(1u);
LABEL_12:
  __break(1u);
  return result;
}

Swift::Int __swiftcall BNNSGraph.Context.argumentPosition(forFunction:argument:)(Swift::String_optional forFunction, Swift::String argument)
{
  Swift::Int ArgumentPosition;

  if (forFunction.value._object)
    String.utf8CString.getter();
  String.utf8CString.getter();
  ArgumentPosition = BNNSGraphGetArgumentPosition();
  swift_unknownObjectRelease();
  swift_release();
  return ArgumentPosition;
}

uint64_t BNNSGraph.Context.tensor(forFunction:argument:fillKnownDynamicShapes:)@<X0>(uint64_t a1@<X1>, uint64_t a2@<X8>)
{
  int Tensor;
  _OWORD v5[11];
  _BYTE v6[168];
  _BYTE v7[160];
  uint64_t v8;

  v8 = *MEMORY[0x1E0C80C00];
  *(_DWORD *)v7 = 0;
  v7[4] = 0;
  memset(&v7[8], 0, 152);
  if (a1)
    String.utf8CString.getter();
  String.utf8CString.getter();
  Tensor = BNNSGraphContextGetTensor();
  swift_unknownObjectRelease();
  swift_release();
  if (Tensor)
  {
    _sSo10BNNSTensoraSgWOi0_((uint64_t)v5);
  }
  else
  {
    v5[6] = *(_OWORD *)&v7[96];
    v5[7] = *(_OWORD *)&v7[112];
    v5[8] = *(_OWORD *)&v7[128];
    v5[9] = *(_OWORD *)&v7[144];
    v5[2] = *(_OWORD *)&v7[32];
    v5[3] = *(_OWORD *)&v7[48];
    v5[4] = *(_OWORD *)&v7[64];
    v5[5] = *(_OWORD *)&v7[80];
    v5[0] = *(_OWORD *)v7;
    v5[1] = *(_OWORD *)&v7[16];
    _sSo10BNNSTensoraSgWOi_((uint64_t)v5);
  }
  outlined init with take of BNNSTensor?((uint64_t)v5, (uint64_t)v6);
  return outlined init with take of BNNSTensor?((uint64_t)v6, a2);
}

Swift::Int __swiftcall BNNSGraph.Context.argumentCount(forFunction:)(Swift::String_optional forFunction)
{
  Swift::Int ArgumentCount;

  if (forFunction.value._object)
    String.utf8CString.getter();
  ArgumentCount = BNNSGraphGetArgumentCount();
  swift_unknownObjectRelease();
  return ArgumentCount;
}

uint64_t BNNSGraph.Context.functionCount.getter()
{
  return BNNSGraphGetFunctionCount();
}

uint64_t BNNSGraph.Context.argumentNames(forFunction:)(uint64_t a1, uint64_t a2)
{
  uint64_t ArgumentCount;
  uint64_t result;
  uint64_t v5;
  uint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  unint64_t v12;
  unint64_t v13;
  uint64_t v14;

  if (a2)
    String.utf8CString.getter();
  ArgumentCount = BNNSGraphGetArgumentCount();
  result = swift_unknownObjectRelease();
  if (ArgumentCount < 0)
  {
    __break(1u);
LABEL_18:
    __break(1u);
    return result;
  }
  v5 = MEMORY[0x1E0DEE9D8];
  if (ArgumentCount)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafePointer<Int8>?);
    v6 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v6 + 16) = ArgumentCount;
    bzero((void *)(v6 + 32), 8 * ArgumentCount);
    if (!a2)
      goto LABEL_8;
    goto LABEL_6;
  }
  v6 = MEMORY[0x1E0DEE9D8];
  if (a2)
LABEL_6:
    String.utf8CString.getter();
LABEL_8:
  BNNSGraphGetArgumentNames();
  swift_unknownObjectRelease();
  v7 = *(_QWORD *)(v6 + 16);
  if (v7)
  {
    swift_bridgeObjectRetain();
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v7, 0);
    v8 = 0;
    while (1)
    {
      result = *(_QWORD *)(v6 + 8 * v8 + 32);
      if (!result)
        break;
      v9 = String.init(cString:)();
      v11 = v10;
      v13 = *(_QWORD *)(v5 + 16);
      v12 = *(_QWORD *)(v5 + 24);
      if (v13 >= v12 >> 1)
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v12 > 1), v13 + 1, 1);
      ++v8;
      *(_QWORD *)(v5 + 16) = v13 + 1;
      v14 = v5 + 16 * v13;
      *(_QWORD *)(v14 + 32) = v9;
      *(_QWORD *)(v14 + 40) = v11;
      if (v7 == v8)
      {
        swift_bridgeObjectRelease_n();
        return v5;
      }
    }
    goto LABEL_18;
  }
  swift_bridgeObjectRelease();
  return v5;
}

uint64_t BNNSGraph.Context.functionNames.getter()
{
  uint64_t result;
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  int64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  unint64_t v9;
  unint64_t v10;
  uint64_t v11;

  result = BNNSGraphGetFunctionCount();
  if (result < 0)
  {
    __break(1u);
LABEL_15:
    __break(1u);
    return result;
  }
  v1 = result;
  v2 = MEMORY[0x1E0DEE9D8];
  if (result)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafePointer<Int8>?);
    v3 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v3 + 16) = v1;
    bzero((void *)(v3 + 32), 8 * v1);
  }
  else
  {
    v3 = MEMORY[0x1E0DEE9D8];
  }
  BNNSGraphGetFunctionNames();
  v4 = *(_QWORD *)(v3 + 16);
  if (v4)
  {
    swift_bridgeObjectRetain();
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v4, 0);
    v5 = 0;
    while (1)
    {
      result = *(_QWORD *)(v3 + 8 * v5 + 32);
      if (!result)
        break;
      v6 = String.init(cString:)();
      v8 = v7;
      v10 = *(_QWORD *)(v2 + 16);
      v9 = *(_QWORD *)(v2 + 24);
      if (v10 >= v9 >> 1)
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v9 > 1), v10 + 1, 1);
      ++v5;
      *(_QWORD *)(v2 + 16) = v10 + 1;
      v11 = v2 + 16 * v10;
      *(_QWORD *)(v11 + 32) = v6;
      *(_QWORD *)(v11 + 40) = v8;
      if (v4 == v5)
      {
        swift_bridgeObjectRelease_n();
        return v2;
      }
    }
    goto LABEL_15;
  }
  swift_bridgeObjectRelease();
  return v2;
}

double BNNSGraph.Shape.init(arrayLiteral:)@<D0>(uint64_t a1@<X0>, _OWORD *a2@<X8>)
{
  double result;
  __int128 v4;

  BNNSGraph.Shape.init(_:)(a1, (unint64_t *)&v4);
  result = *(double *)&v4;
  *a2 = v4;
  return result;
}

double protocol witness for ExpressibleByArrayLiteral.init(arrayLiteral:) in conformance BNNSGraph.Shape@<D0>(uint64_t a1@<X0>, _OWORD *a2@<X8>)
{
  double result;
  __int128 v4;

  BNNSGraph.Shape.init(_:)(a1, (unint64_t *)&v4);
  result = *(double *)&v4;
  *a2 = v4;
  return result;
}

BOOL static BNNSGraph.Error.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void BNNSGraph.Error.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

Swift::Int BNNSGraph.Error.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

char *specialized _ArrayBuffer._consumeAndCreateNew()(uint64_t a1)
{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(_QWORD *)(a1 + 16), 0, (char *)a1);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(_QWORD *)(a1 + 16), 0, (char *)a1);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(_QWORD *)(a1 + 16), 0, (char *)a1);
}

double _sSo10BNNSTensoraSgWOi0_(uint64_t a1)
{
  double result;

  result = 0.0;
  *(_OWORD *)(a1 + 128) = 0u;
  *(_OWORD *)(a1 + 144) = 0u;
  *(_OWORD *)(a1 + 96) = 0u;
  *(_OWORD *)(a1 + 112) = 0u;
  *(_OWORD *)(a1 + 64) = 0u;
  *(_OWORD *)(a1 + 80) = 0u;
  *(_OWORD *)(a1 + 32) = 0u;
  *(_OWORD *)(a1 + 48) = 0u;
  *(_OWORD *)a1 = 0u;
  *(_OWORD *)(a1 + 16) = 0u;
  *(_BYTE *)(a1 + 160) = 1;
  return result;
}

uint64_t outlined init with take of BNNSTensor?(uint64_t a1, uint64_t a2)
{
  uint64_t v4;

  v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSTensor?);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(_QWORD *)(v4 - 8) + 32))(a2, a1, v4);
  return a2;
}

uint64_t _sSo10BNNSTensoraSgWOi_(uint64_t result)
{
  *(_BYTE *)(result + 160) = 0;
  return result;
}

uint64_t sub_1CAAF043C@<X0>(uint64_t a1@<X0>, _BYTE *a2@<X8>)
{
  return keypath_getTm(a1, (uint64_t (*)(_QWORD, _QWORD))MEMORY[0x1E0C8B760], a2);
}

uint64_t sub_1CAAF0448(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return keypath_setTm(a1, a2, a3, a4, (uint64_t (*)(_QWORD, _QWORD, _QWORD))MEMORY[0x1E0C8B7F8]);
}

uint64_t sub_1CAAF0454@<X0>(uint64_t a1@<X0>, _BYTE *a2@<X8>)
{
  return keypath_getTm(a1, (uint64_t (*)(_QWORD, _QWORD))MEMORY[0x1E0C8B750], a2);
}

uint64_t keypath_getTm@<X0>(uint64_t a1@<X0>, uint64_t (*a2)(_QWORD, _QWORD)@<X3>, _BYTE *a3@<X8>)
{
  uint64_t result;

  result = a2(*(_QWORD *)(*(_QWORD *)a1 + 16), *(_QWORD *)(*(_QWORD *)a1 + 24));
  *a3 = result;
  return result;
}

uint64_t sub_1CAAF048C(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return keypath_setTm(a1, a2, a3, a4, (uint64_t (*)(_QWORD, _QWORD, _QWORD))MEMORY[0x1E0C8B790]);
}

uint64_t keypath_setTm(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(_QWORD, _QWORD, _QWORD))
{
  return a5(*(_QWORD *)(*(_QWORD *)a2 + 16), *(_QWORD *)(*(_QWORD *)a2 + 24), *a1);
}

uint64_t sub_1CAAF04A8@<X0>(_DWORD *a1@<X8>)
{
  uint64_t result;

  result = BNNSGraphCompileOptionsGetOptimizationPreference();
  *a1 = result;
  return result;
}

uint64_t sub_1CAAF04D4()
{
  return BNNSGraphCompileOptionsSetOptimizationPreference();
}

uint64_t sub_1CAAF04E4@<X0>(uint64_t result@<X0>, _BYTE *a2@<X8>)
{
  *a2 = *(_BYTE *)(*(_QWORD *)result + 48);
  return result;
}

uint64_t sub_1CAAF04F4(char *a1, uint64_t *a2)
{
  char v2;
  uint64_t v3;
  uint64_t result;

  v2 = *a1;
  v3 = *a2;
  result = BNNSGraphContextEnableNanAndInfChecks();
  *(_BYTE *)(v3 + 48) = v2;
  return result;
}

ValueMetadata *type metadata accessor for BNNSGraph()
{
  return &type metadata for BNNSGraph;
}

uint64_t dispatch thunk of BNNSGraph.PointerArgument.baseAddress.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

uint64_t dispatch thunk of BNNSGraph.PointerArgument.count.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 24))();
}

ValueMetadata *type metadata accessor for BNNSGraph.CompileOptions()
{
  return &type metadata for BNNSGraph.CompileOptions;
}

ValueMetadata *type metadata accessor for BNNSGraph.CompileOptions.OptimizationPreference()
{
  return &type metadata for BNNSGraph.CompileOptions.OptimizationPreference;
}

uint64_t type metadata accessor for BNNSGraph.Context()
{
  return objc_opt_self();
}

uint64_t method lookup function for BNNSGraph.Context()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of BNNSGraph.Context.__allocating_init(compileFromPath:functionName:options:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v5;
  uint64_t v6;
  _QWORD *v12;
  uint64_t (*v14)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t);

  v14 = (uint64_t (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))(**(int **)(v5 + 104)
                                                                            + *(_QWORD *)(v5 + 104));
  v12 = (_QWORD *)swift_task_alloc();
  *(_QWORD *)(v6 + 16) = v12;
  *v12 = v6;
  v12[1] = dispatch thunk of BNNSGraph.Context.__allocating_init(compileFromPath:functionName:options:);
  return v14(a1, a2, a3, a4, a5);
}

uint64_t dispatch thunk of BNNSGraph.Context.__allocating_init(compileFromPath:functionName:options:)(uint64_t a1)
{
  uint64_t *v1;
  uint64_t v4;

  v4 = *v1;
  swift_task_dealloc();
  return (*(uint64_t (**)(uint64_t))(v4 + 8))(a1);
}

uint64_t dispatch thunk of BNNSGraph.Context.setDynamicShapes(_:forFunction:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  _QWORD *v8;
  uint64_t (*v10)(uint64_t, uint64_t, uint64_t);

  v10 = (uint64_t (*)(uint64_t, uint64_t, uint64_t))(**(int **)(*(_QWORD *)v3 + 112)
                                                          + *(_QWORD *)(*(_QWORD *)v3 + 112));
  v8 = (_QWORD *)swift_task_alloc();
  *(_QWORD *)(v4 + 16) = v8;
  *v8 = v4;
  v8[1] = dispatch thunk of BNNSGraph.Context.setDynamicShapes(_:forFunction:);
  return v10(a1, a2, a3);
}

uint64_t dispatch thunk of BNNSGraph.Context.setBatchSize(_:forFunction:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  _QWORD *v8;
  uint64_t (*v10)(uint64_t, uint64_t, uint64_t);

  v10 = (uint64_t (*)(uint64_t, uint64_t, uint64_t))(**(int **)(*(_QWORD *)v3 + 120)
                                                          + *(_QWORD *)(*(_QWORD *)v3 + 120));
  v8 = (_QWORD *)swift_task_alloc();
  *(_QWORD *)(v4 + 16) = v8;
  *v8 = v4;
  v8[1] = dispatch thunk of BNNSGraph.Context.setBatchSize(_:forFunction:);
  return v10(a1, a2, a3);
}

uint64_t dispatch thunk of BNNSGraph.Context.setBatchSize(_:forFunction:)()
{
  uint64_t *v0;
  uint64_t v2;

  v2 = *v0;
  swift_task_dealloc();
  return (*(uint64_t (**)(void))(v2 + 8))();
}

uint64_t dispatch thunk of BNNSGraph.Context.checkForNaNsAndInfinities.getter()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 128))();
}

uint64_t dispatch thunk of BNNSGraph.Context.checkForNaNsAndInfinities.setter()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 136))();
}

uint64_t dispatch thunk of BNNSGraph.Context.checkForNaNsAndInfinities.modify()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 144))();
}

uint64_t dispatch thunk of BNNSGraph.Context.executeFunction(_:arguments:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  _QWORD *v8;
  uint64_t (*v10)(uint64_t, uint64_t, uint64_t);

  v10 = (uint64_t (*)(uint64_t, uint64_t, uint64_t))(**(int **)(*(_QWORD *)v3 + 176)
                                                          + *(_QWORD *)(*(_QWORD *)v3 + 176));
  v8 = (_QWORD *)swift_task_alloc();
  *(_QWORD *)(v4 + 16) = v8;
  *v8 = v4;
  v8[1] = dispatch thunk of BNNSGraph.Context.setBatchSize(_:forFunction:);
  return v10(a1, a2, a3);
}

uint64_t dispatch thunk of BNNSGraph.Context.executeFunction<A>(_:arguments:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 184))();
}

uint64_t dispatch thunk of BNNSGraph.Context.argumentPosition(forFunction:argument:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 192))();
}

uint64_t dispatch thunk of BNNSGraph.Context.tensor(forFunction:argument:fillKnownDynamicShapes:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 200))();
}

uint64_t dispatch thunk of BNNSGraph.Context.argumentCount(forFunction:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 208))();
}

uint64_t dispatch thunk of BNNSGraph.Context.functionCount.getter()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 216))();
}

uint64_t dispatch thunk of BNNSGraph.Context.argumentNames(forFunction:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 224))();
}

uint64_t dispatch thunk of BNNSGraph.Context.functionNames.getter()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 232))();
}

ValueMetadata *type metadata accessor for BNNSGraph.Shape()
{
  return &type metadata for BNNSGraph.Shape;
}

uint64_t storeEnumTagSinglePayload for BNNSGraph.Error(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 3 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 3) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFD)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFC)
    return ((uint64_t (*)(void))((char *)&loc_1CAAF0910 + 4 * byte_1CAB60205[v4]))();
  *a1 = a2 + 3;
  return ((uint64_t (*)(void))((char *)sub_1CAAF0944 + 4 * byte_1CAB60200[v4]))();
}

uint64_t sub_1CAAF0944(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAAF094C(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAAF0954);
  return result;
}

uint64_t sub_1CAAF0960(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAAF0968);
  *(_BYTE *)result = a2 + 3;
  return result;
}

uint64_t sub_1CAAF096C(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAAF0974(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for BNNSGraph.Error()
{
  return &type metadata for BNNSGraph.Error;
}

_QWORD *specialized _ArrayBuffer._consumeAndCreateNew()(_QWORD *a1)
{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, a1[2], 0, a1);
}

uint64_t static vDSP.powerToDecibels<A>(_:zeroReference:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.powerToDecibels<A>(_:zeroReference:)(a1, a2, a3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:));
}

{
  return static vDSP.powerToDecibels<A>(_:zeroReference:)(a1, a2, a3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:));
}

uint64_t partial apply for closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:)(uint64_t a1, uint64_t *a2)
{
  uint64_t v2;

  return closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:)(a1, a2, *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float))static vDSP.convert<A, B>(power:toDecibels:zeroReference:), *(float *)(v2 + 40));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:)(a1, a2, *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double))static vDSP.convert<A, B>(power:toDecibels:zeroReference:), *(double *)(v2 + 40));
}

uint64_t static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  return static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:));
}

uint64_t static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  return static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:));
}

uint64_t static vDSP.amplitudeToDecibels<A>(_:zeroReference:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.powerToDecibels<A>(_:zeroReference:)(a1, a2, a3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.amplitudeToDecibels<A>(_:zeroReference:));
}

{
  return static vDSP.powerToDecibels<A>(_:zeroReference:)(a1, a2, a3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.amplitudeToDecibels<A>(_:zeroReference:));
}

uint64_t static vDSP.powerToDecibels<A>(_:zeroReference:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(_QWORD *, uint64_t *))
{
  uint64_t v5;

  v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, a4);
}

{
  uint64_t v5;

  v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, a4);
}

uint64_t closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, void (*a6)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float), float a7)
{
  uint64_t v14;
  uint64_t v15;
  uint64_t result;

  v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  a6(a3, a1, a4, v14, a5, v15, a7);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  return static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:));
}

uint64_t static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(uint64_t a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t result;
  _QWORD v22[2];

  v22[0] = a8;
  v14 = *(_QWORD *)(a4 - 8);
  MEMORY[0x1E0C80A78](a1);
  v16 = (char *)v22 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v17 + 8) + 16))(v18);
  (*(void (**)(char *, uint64_t, uint64_t))(v14 + 16))(v16, a1, a4);
  v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  result = (*(uint64_t (**)(char *, uint64_t))(v14 + 8))(v16, a4);
  if (v20 == v19)
  {
    MEMORY[0x1E0C80A78](result);
    v22[-8] = a4;
    v22[-7] = a5;
    v22[-6] = a6;
    v22[-5] = a7;
    v22[-4] = a1;
    *(float *)&v22[-3] = a2;
    v22[-2] = v19;
    return (*(uint64_t (**)(_QWORD))(a7 + 16))(v22[0]);
  }
  else
  {
    __break(1u);
  }
  return result;
}

void closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(const float *a1, int a2, float **a3, vDSP_Length __N, unsigned int __F, float a6)
{
  float __B;
  uint64_t v7;

  v7 = *MEMORY[0x1E0C80C00];
  __B = a6;
  if (!a1)
    goto LABEL_6;
  if (!*a3)
    goto LABEL_7;
  if ((__N & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  vDSP_vdbcon(a1, 1, &__B, *a3, 1, __N, __F);
}

uint64_t closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, void (*a6)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double), double a7)
{
  uint64_t v14;
  uint64_t v15;
  uint64_t result;

  v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  a6(a3, a1, a4, v14, a5, v15, a7);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  return static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:));
}

uint64_t static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(uint64_t a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t result;
  _QWORD v22[2];

  v22[0] = a8;
  v14 = *(_QWORD *)(a4 - 8);
  MEMORY[0x1E0C80A78](a1);
  v16 = (char *)v22 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v17 + 8) + 16))(v18);
  (*(void (**)(char *, uint64_t, uint64_t))(v14 + 16))(v16, a1, a4);
  v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  result = (*(uint64_t (**)(char *, uint64_t))(v14 + 8))(v16, a4);
  if (v20 == v19)
  {
    MEMORY[0x1E0C80A78](result);
    v22[-8] = a4;
    v22[-7] = a5;
    v22[-6] = a6;
    v22[-5] = a7;
    v22[-4] = a1;
    *(double *)&v22[-3] = a2;
    v22[-2] = v19;
    return (*(uint64_t (**)(_QWORD))(a7 + 16))(v22[0]);
  }
  else
  {
    __break(1u);
  }
  return result;
}

void closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(const double *a1, int a2, double **a3, vDSP_Length __N, unsigned int __F, double a6)
{
  double v6[2];

  v6[1] = *(double *)MEMORY[0x1E0C80C00];
  v6[0] = a6;
  if (!a1)
    goto LABEL_6;
  if (!*a3)
    goto LABEL_7;
  if ((__N & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  vDSP_vdbconD(a1, 1, v6, *a3, 1, __N, __F);
}

uint64_t partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:));
}

{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:));
}

uint64_t partial apply for closure #1 in static vDSP.amplitudeToDecibels<A>(_:zeroReference:)(uint64_t a1, uint64_t *a2)
{
  uint64_t v2;

  return closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:)(a1, a2, *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float))static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:), *(float *)(v2 + 40));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:)(a1, a2, *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double))static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:), *(double *)(v2 + 40));
}

uint64_t partial apply for closure #1 in static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:));
}

{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:));
}

uint64_t partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  _DWORD v7[6];
  uint64_t v8;
  uint64_t v9;

  v3 = *(_QWORD *)(v2 + 16);
  v4 = *(_QWORD *)(v2 + 32);
  v5 = *(_QWORD *)(v2 + 64);
  v7[4] = *(_DWORD *)(v2 + 56);
  v8 = a1;
  v9 = v5;
  return (*(uint64_t (**)(uint64_t, _DWORD *, uint64_t, uint64_t))(v4 + 24))(a2, v7, MEMORY[0x1E0DEE9C0] + 8, v3);
}

{
  _QWORD *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  _QWORD v7[6];

  v3 = v2[2];
  v4 = v2[4];
  v5 = v2[8];
  v7[2] = v2[7];
  v7[3] = a1;
  v7[4] = v5;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v4 + 24))(a2, v7, MEMORY[0x1E0DEE9C0] + 8, v3);
}

void partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:)(const double *a1, int a2)
{
  uint64_t v2;

  closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, a2, *(double ***)(v2 + 24), *(_QWORD *)(v2 + 32), 1u, *(double *)(v2 + 16));
}

void partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:)(const float *a1, int a2)
{
  uint64_t v2;

  closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, a2, *(float ***)(v2 + 24), *(_QWORD *)(v2 + 32), 1u, *(float *)(v2 + 16));
}

void partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(const double *a1, int a2)
{
  uint64_t v2;

  closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, a2, *(double ***)(v2 + 24), *(_QWORD *)(v2 + 32), 0, *(double *)(v2 + 16));
}

void partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(const float *a1, int a2)
{
  uint64_t v2;

  closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, a2, *(float ***)(v2 + 24), *(_QWORD *)(v2 + 32), 0, *(float *)(v2 + 16));
}

uint64_t static vDSP.DFTSinglePrecisionSplitComplexFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, unsigned __int8 *a3, char *a4)
{
  return specialized static vDSP.DFTSinglePrecisionSplitComplexFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(a1, a2, a3, a4, MEMORY[0x1E0C8BFA0], MEMORY[0x1E0C8BFB0]);
}

uint64_t protocol witness for static vDSP_DiscreteTransformLifecycleFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:) in conformance vDSP.DFTSinglePrecisionSplitComplexFunctions(uint64_t a1, uint64_t a2, unsigned __int8 *a3, char *a4)
{
  return specialized static vDSP.DFTSinglePrecisionSplitComplexFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(a1, a2, a3, a4, MEMORY[0x1E0C8BFA0], MEMORY[0x1E0C8BFB0]);
}

uint64_t static vDSP.DFTDoublePrecisionSplitComplexFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, unsigned __int8 *a3, char *a4)
{
  return specialized static vDSP.DFTSinglePrecisionSplitComplexFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(a1, a2, a3, a4, MEMORY[0x1E0C8BFA8], MEMORY[0x1E0C8BFB8]);
}

uint64_t protocol witness for static vDSP_DiscreteTransformLifecycleFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:) in conformance vDSP.DFTDoublePrecisionSplitComplexFunctions(uint64_t a1, uint64_t a2, unsigned __int8 *a3, char *a4)
{
  return specialized static vDSP.DFTSinglePrecisionSplitComplexFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(a1, a2, a3, a4, MEMORY[0x1E0C8BFA8], MEMORY[0x1E0C8BFB8]);
}

void static vDSP.DFTSinglePrecisionInterleavedFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  specialized static vDSP.DFTSinglePrecisionInterleavedFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(a1, a2, a3, a4, MEMORY[0x1E0C8BF70]);
}

void protocol witness for static vDSP_DiscreteTransformLifecycleFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:) in conformance vDSP.DFTSinglePrecisionInterleavedFunctions(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  specialized static vDSP.DFTSinglePrecisionInterleavedFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(a1, a2, a3, a4, MEMORY[0x1E0C8BF70]);
}

void static vDSP.DFTDoublePrecisionInterleavedFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  specialized static vDSP.DFTSinglePrecisionInterleavedFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(a1, a2, a3, a4, MEMORY[0x1E0C8BF78]);
}

void protocol witness for static vDSP_DiscreteTransformLifecycleFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:) in conformance vDSP.DFTDoublePrecisionInterleavedFunctions(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  specialized static vDSP.DFTSinglePrecisionInterleavedFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(a1, a2, a3, a4, MEMORY[0x1E0C8BF78]);
}

uint64_t vDSP.DFTError.errorDescription.getter()
{
  uint64_t v0;
  int v1;
  Swift::String v2;
  Swift::String v3;
  uint64_t v4;
  const char *v5;
  Swift::String v6;
  Swift::String v7;
  unint64_t v8;
  Swift::String v9;
  Swift::String v10;

  v1 = *(char *)(v0 + 8);
  if (v1 < 0)
  {
    _StringGuts.grow(_:)(126);
    v6._countAndFlagsBits = 0x2064696C61766E49;
    v6._object = (void *)0xEF2820746E756F63;
    String.append(_:)(v6);
    v7._countAndFlagsBits = dispatch thunk of CustomStringConvertible.description.getter();
    String.append(_:)(v7);
    swift_bridgeObjectRelease();
    v8 = 0x80000001CAB65A70;
    v4 = 0x100000000000006DLL;
  }
  else
  {
    if ((v1 & 1) != 0)
    {
      _StringGuts.grow(_:)(136);
      v9._countAndFlagsBits = 0x2064696C61766E49;
      v9._object = (void *)0xEF2820746E756F63;
      String.append(_:)(v9);
      v10._countAndFlagsBits = dispatch thunk of CustomStringConvertible.description.getter();
      String.append(_:)(v10);
      swift_bridgeObjectRelease();
      v4 = 0x1000000000000077;
    }
    else
    {
      _StringGuts.grow(_:)(140);
      v2._countAndFlagsBits = 0x2064696C61766E49;
      v2._object = (void *)0xEF2820746E756F63;
      String.append(_:)(v2);
      v3._countAndFlagsBits = dispatch thunk of CustomStringConvertible.description.getter();
      String.append(_:)(v3);
      swift_bridgeObjectRelease();
      v4 = 0x100000000000007BLL;
    }
    v8 = (unint64_t)(v5 - 32) | 0x8000000000000000;
  }
  String.append(_:)(*(Swift::String *)&v4);
  return 0;
}

uint64_t vDSP.DiscreteFourierTransform.__allocating_init(previous:count:direction:transformType:ofType:)(uint64_t a1, uint64_t a2, char *a3, char *a4)
{
  uint64_t v8;

  v8 = swift_allocObject();
  vDSP.DiscreteFourierTransform.init(previous:count:direction:transformType:ofType:)(a1, a2, a3, a4);
  return v8;
}

_QWORD *vDSP.DiscreteFourierTransform.init(previous:count:direction:transformType:ofType:)(uint64_t a1, uint64_t a2, char *a3, char *a4)
{
  _QWORD *v4;
  _QWORD *v5;
  uint64_t v6;
  char v7;
  char v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t AssociatedTypeWitness;
  uint64_t AssociatedConformanceWitness;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v19;
  char v20;
  char v21;

  v5 = v4;
  v6 = *v4;
  v7 = *a3;
  v8 = *a4;
  if (a1)
    v9 = *(_QWORD *)(a1 + 16);
  else
    v9 = 0;
  v11 = *(_QWORD *)(v6 + 80);
  v10 = *(_QWORD *)(v6 + 88);
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v21 = v7;
  v20 = v8;
  AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  v14 = (*(uint64_t (**)(uint64_t, uint64_t, char *, char *, uint64_t, uint64_t))(AssociatedConformanceWitness + 8))(v9, a2, &v21, &v20, AssociatedTypeWitness, AssociatedConformanceWitness);
  if (v19)
  {
    swift_release();
    type metadata accessor for vDSP.DiscreteFourierTransform(0, v11, v10, v15);
    swift_deallocPartialClassInstance();
  }
  else
  {
    v16 = v14;
    swift_release();
    v5[2] = v16;
  }
  return v5;
}

uint64_t vDSP.DiscreteFourierTransform.deinit()
{
  uint64_t v0;
  uint64_t AssociatedTypeWitness;
  uint64_t v2;
  uint64_t AssociatedConformanceWitness;

  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v2 = *(_QWORD *)(v0 + 16);
  AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  (*(void (**)(uint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness + 16))(v2, AssociatedTypeWitness, AssociatedConformanceWitness);
  return v0;
}

uint64_t vDSP.DiscreteFourierTransform.__deallocating_deinit()
{
  vDSP.DiscreteFourierTransform.deinit();
  return swift_deallocClassInstance();
}

uint64_t vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  return vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, a3, a4, a5, a6, a7, a8, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

{
  return vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, a3, a4, a5, a6, a7, a8, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t, _QWORD *))
{
  uint64_t v6;
  uint64_t v9;
  uint64_t v14;
  _QWORD v16[9];
  uint64_t v17;

  v9 = v6;
  v17 = MEMORY[0x1E0DEE9D8];
  v14 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  v16[2] = a3;
  v16[3] = a4;
  v16[4] = &v17;
  v16[5] = a2;
  v16[6] = v9;
  v16[7] = a1;
  return a6(v14, a5, v16);
}

uint64_t closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(uint64_t a1, uint64_t *a2, _QWORD *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t (*a10)(uint64_t, uint64_t, char *))
{
  uint64_t (*v17)(uint64_t, uint64_t);
  uint64_t v18;
  uint64_t result;
  char v20[16];
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t *v27;

  v27 = a2;
  v17 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  v18 = v17(a7, a8);
  v21 = a7;
  v22 = a8;
  v23 = a5;
  v24 = a6;
  v25 = a4;
  v26 = a1;
  *a3 = a10(v18, a9, v20);
  swift_bridgeObjectRelease();
  result = v17(a7, a8);
  *v27 = result;
  return result;
}

uint64_t closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, unint64_t *a10, uint64_t a11)
{
  uint64_t result;
  _BYTE v19[16];
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;

  v20 = a7;
  v21 = __swift_instantiateConcreteTypeFromMangledName(a9);
  v22 = a8;
  v23 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a10, a9);
  v24 = a5;
  v25 = a6;
  v26 = a1;
  v27 = a3;
  (*(void (**)(uint64_t, _BYTE *, uint64_t, uint64_t, uint64_t))(a8 + 24))(a11, v19, MEMORY[0x1E0DEE9C0] + 8, v20, a8);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a7, a8);
  *a2 = result;
  return result;
}

uint64_t vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  _QWORD v10[9];

  v10[2] = a5;
  v10[3] = a6;
  v10[4] = a7;
  v10[5] = a8;
  v10[6] = a2;
  v10[7] = a3;
  v10[8] = a4;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t, uint64_t))(a7 + 24))(a9, v10, MEMORY[0x1E0DEE9C0] + 8, a5, a7);
}

_QWORD *closure #1 in closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(_QWORD *result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, _QWORD *a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t (*a12)(_QWORD, uint64_t, uint64_t))
{
  if (!a3)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (!a5)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (!*a7)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (*result)
    return (_QWORD *)a12(*(_QWORD *)(a2 + 16), a3, a5);
LABEL_9:
  __break(1u);
  return result;
}

uint64_t vDSP.DiscreteFourierTransform<>.transform<A>(input:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return vDSP.DiscreteFourierTransform<>.transform<A>(input:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(input:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return vDSP.DiscreteFourierTransform<>.transform<A>(input:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(input:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:));
}

{
  return vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:));
}

uint64_t vDSP.DiscreteFourierTransform<>.transform<A>(input:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(uint64_t, uint64_t, _QWORD *))
{
  uint64_t v5;
  uint64_t v8;
  uint64_t v12;
  _QWORD v14[8];

  v8 = v5;
  v12 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  v14[2] = a2;
  v14[3] = a3;
  v14[4] = v8;
  v14[5] = a1;
  return a5(v12, a4, v14);
}

uint64_t closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(input:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t *a7, unint64_t *a8, uint64_t a9)
{
  uint64_t result;
  _BYTE v17[16];
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;

  v18 = a5;
  v19 = __swift_instantiateConcreteTypeFromMangledName(a7);
  v20 = a6;
  v21 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a8, a7);
  v22 = a1;
  v23 = a3;
  (*(void (**)(uint64_t, _BYTE *, uint64_t, uint64_t, uint64_t))(a6 + 24))(a9, v17, MEMORY[0x1E0DEE9C0] + 8, v18, a6);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  _QWORD v8[7];

  v8[2] = a3;
  v8[3] = a4;
  v8[4] = a5;
  v8[5] = a6;
  v8[6] = a2;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(a5 + 24))(a7, v8, MEMORY[0x1E0DEE9C0] + 8, a3);
}

uint64_t specialized static vDSP.DFTSinglePrecisionSplitComplexFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(uint64_t result, uint64_t a2, unsigned __int8 *a3, char *a4, uint64_t (*a5)(uint64_t, uint64_t, uint64_t), uint64_t (*a6)(uint64_t, uint64_t, uint64_t))
{
  int v7;
  char v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;

  v7 = *a3;
  v8 = *a4;
  if ((*a4 & 1) != 0)
  {
    a5 = a6;
    if ((a2 & 0x8000000000000000) == 0)
      goto LABEL_5;
    __break(1u);
  }
  if (a2 < 0)
  {
    __break(1u);
    return result;
  }
LABEL_5:
  if (v7)
    v9 = 0xFFFFFFFFLL;
  else
    v9 = 1;
  v10 = a5(result, a2, v9);
  if (!v10)
  {
    lazy protocol witness table accessor for type vDSP.DFTError and conformance vDSP.DFTError();
    swift_allocError();
    *(_QWORD *)v11 = a2;
    *(_BYTE *)(v11 + 8) = v8;
    swift_willThrow();
  }
  return v10;
}

void specialized static vDSP.DFTSinglePrecisionInterleavedFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(void))
{
  uint64_t v6;

  if (a2 < 0)
  {
    __break(1u);
  }
  else if (!a5())
  {
    lazy protocol witness table accessor for type vDSP.DFTError and conformance vDSP.DFTError();
    swift_allocError();
    *(_QWORD *)v6 = a2;
    *(_BYTE *)(v6 + 8) = 0x80;
    swift_willThrow();
  }
}

uint64_t type metadata accessor for vDSP.DiscreteFourierTransform(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return __swift_instantiateGenericMetadata(a1, a2, a3, a4, (uint64_t)&nominal type descriptor for vDSP.DiscreteFourierTransform);
}

uint64_t partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

{
  return partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t (*a4)(uint64_t, uint64_t, char *))
{
  uint64_t v4;

  return closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(a1, a2, *(_QWORD **)(v4 + 32), *(_QWORD *)(v4 + 40), *(_QWORD *)(v4 + 48), *(_QWORD *)(v4 + 56), *(_QWORD *)(v4 + 16), *(_QWORD *)(v4 + 24), a3, a4);
}

uint64_t partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  __int128 v5;
  uint64_t v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;

  v4 = *(_QWORD *)(v3 + 72);
  v5 = *(_OWORD *)(v3 + 32);
  v8 = *(_OWORD *)(v3 + 16);
  v9 = v5;
  v10 = *(_OWORD *)(v3 + 56);
  v11 = v4;
  v12 = a1;
  v13 = a2;
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t, _QWORD))(v5 + 24))(a3, &v7, MEMORY[0x1E0DEE9C0] + 8, v8);
}

uint64_t partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(input:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(input:)(a1, a2, v2[4], v2[5], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<DSPComplex>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<DSPComplex> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:));
}

{
  uint64_t *v2;

  return closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(input:)(a1, a2, v2[4], v2[5], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<DSPDoubleComplex>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<DSPDoubleComplex> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:));
}

uint64_t partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:));
}

{
  return partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:));
}

uint64_t partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  _QWORD v7[3];
  __int128 v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;

  v4 = *(_QWORD *)(v3 + 40);
  v5 = *(_QWORD *)(v3 + 56);
  v7[2] = *(_QWORD *)(v3 + 16);
  v8 = *(_OWORD *)(v3 + 24);
  v9 = v4;
  v10 = v5;
  v11 = a1;
  v12 = a2;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, _QWORD))(v4 + 16))(a3, v7, MEMORY[0x1E0DEE9C0] + 8, v8);
}

_UNKNOWN **associated type witness table accessor for vDSP_DiscreteFourierTransformable.DiscreteFourierTransformFunctions : vDSP_DiscreteTransformLifecycleFunctions in Float()
{
  return &protocol witness table for vDSP.DFTSinglePrecisionSplitComplexFunctions;
}

_UNKNOWN **associated type witness table accessor for vDSP_DiscreteFourierTransformable.DiscreteFourierTransformFunctions : vDSP_DiscreteTransformLifecycleFunctions in Double()
{
  return &protocol witness table for vDSP.DFTDoublePrecisionSplitComplexFunctions;
}

_UNKNOWN **associated type witness table accessor for vDSP_DiscreteFourierTransformable.DiscreteFourierTransformFunctions : vDSP_DiscreteTransformLifecycleFunctions in DSPComplex()
{
  return &protocol witness table for vDSP.DFTSinglePrecisionInterleavedFunctions;
}

_UNKNOWN **associated type witness table accessor for vDSP_DiscreteFourierTransformable.DiscreteFourierTransformFunctions : vDSP_DiscreteTransformLifecycleFunctions in DSPDoubleComplex()
{
  return &protocol witness table for vDSP.DFTDoublePrecisionInterleavedFunctions;
}

uint64_t dispatch thunk of static vDSP_DiscreteTransformLifecycleFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return (*(uint64_t (**)(void))(a6 + 8))();
}

uint64_t dispatch thunk of static vDSP_DiscreteTransformLifecycleFunctions.destroySetup(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 16))();
}

ValueMetadata *type metadata accessor for vDSP.DFTSinglePrecisionSplitComplexFunctions()
{
  return &type metadata for vDSP.DFTSinglePrecisionSplitComplexFunctions;
}

ValueMetadata *type metadata accessor for vDSP.DFTDoublePrecisionSplitComplexFunctions()
{
  return &type metadata for vDSP.DFTDoublePrecisionSplitComplexFunctions;
}

ValueMetadata *type metadata accessor for vDSP.DFTSinglePrecisionInterleavedFunctions()
{
  return &type metadata for vDSP.DFTSinglePrecisionInterleavedFunctions;
}

ValueMetadata *type metadata accessor for vDSP.DFTDoublePrecisionInterleavedFunctions()
{
  return &type metadata for vDSP.DFTDoublePrecisionInterleavedFunctions;
}

uint64_t __swift_memcpy9_8(uint64_t result, uint64_t *a2)
{
  uint64_t v2;

  v2 = *a2;
  *(_BYTE *)(result + 8) = *((_BYTE *)a2 + 8);
  *(_QWORD *)result = v2;
  return result;
}

uint64_t getEnumTagSinglePayload for vDSP.DFTError(uint64_t a1, unsigned int a2)
{
  unsigned int v3;

  if (!a2)
    return 0;
  if (a2 >= 0x7F && *(_BYTE *)(a1 + 9))
    return (*(_DWORD *)a1 + 127);
  v3 = (*(_BYTE *)(a1 + 8) & 0x7E | (*(unsigned __int8 *)(a1 + 8) >> 7)) ^ 0x7F;
  if (v3 >= 0x7E)
    v3 = -1;
  return v3 + 1;
}

uint64_t storeEnumTagSinglePayload for vDSP.DFTError(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0x7E)
  {
    *(_BYTE *)(result + 8) = 0;
    *(_QWORD *)result = a2 - 127;
    if (a3 >= 0x7F)
      *(_BYTE *)(result + 9) = 1;
  }
  else
  {
    if (a3 >= 0x7F)
      *(_BYTE *)(result + 9) = 0;
    if (a2)
    {
      *(_QWORD *)result = 0;
      *(_BYTE *)(result + 8) = 2 * (((-a2 >> 1) & 0x3F) - ((_BYTE)a2 << 6));
    }
  }
  return result;
}

uint64_t getEnumTag for vDSP.DFTError(uint64_t a1)
{
  return *(unsigned __int8 *)(a1 + 8) >> 7;
}

uint64_t destructiveProjectEnumData for vDSP.DFTError(uint64_t result)
{
  *(_BYTE *)(result + 8) &= ~0x80u;
  return result;
}

uint64_t destructiveInjectEnumTag for vDSP.DFTError(uint64_t result, char a2)
{
  *(_BYTE *)(result + 8) = *(_BYTE *)(result + 8) & 1 | (a2 << 7);
  return result;
}

ValueMetadata *type metadata accessor for vDSP.DFTError()
{
  return &type metadata for vDSP.DFTError;
}

uint64_t type metadata completion function for vDSP.DiscreteFourierTransform()
{
  return swift_initClassMetadata2();
}

uint64_t method lookup function for vDSP.DiscreteFourierTransform()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of vDSP.DiscreteFourierTransform.__allocating_init(previous:count:direction:transformType:ofType:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(v0 + 128))();
}

_QWORD *partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(_QWORD *a1)
{
  return partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(a1, MEMORY[0x1E0C8BF98]);
}

{
  return partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(a1, MEMORY[0x1E0C8BF90]);
}

_QWORD *partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(_QWORD *result, uint64_t (*a2)(_QWORD))
{
  uint64_t v2;

  if (*(_QWORD *)(v2 + 56))
  {
    if (*result)
      return (_QWORD *)a2(*(_QWORD *)(*(_QWORD *)(v2 + 48) + 16));
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

{
  return partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

_QWORD *partial apply for closure #1 in closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(_QWORD *a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, MEMORY[0x1E0C8BF68]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, MEMORY[0x1E0C8BF60]);
}

uint64_t partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

{
  return partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  __int128 v5;
  _QWORD v7[3];
  __int128 v8;
  uint64_t v9;
  __int128 v10;
  __int128 v11;
  uint64_t v12;
  uint64_t v13;

  v13 = a2;
  v4 = *(_QWORD *)(v3 + 40);
  v7[2] = *(_QWORD *)(v3 + 16);
  v8 = *(_OWORD *)(v3 + 24);
  v9 = v4;
  v5 = *(_OWORD *)(v3 + 72);
  v10 = *(_OWORD *)(v3 + 56);
  v11 = v5;
  v12 = a1;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, _QWORD))(v4 + 16))(a3, v7, MEMORY[0x1E0DEE9C0] + 8, v8);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  __int128 v5;
  _QWORD v7[3];
  __int128 v8;
  uint64_t v9;
  uint64_t v10;
  __int128 v11;
  __int128 v12;
  uint64_t v13;

  v13 = a1;
  v3 = *(_QWORD *)(v2 + 40);
  v4 = *(_QWORD *)(v2 + 56);
  v7[2] = *(_QWORD *)(v2 + 16);
  v5 = *(_OWORD *)(v2 + 80);
  v11 = *(_OWORD *)(v2 + 64);
  v8 = *(_OWORD *)(v2 + 24);
  v9 = v3;
  v10 = v4;
  v12 = v5;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, _QWORD))(v3 + 16))(a2, v7, MEMORY[0x1E0DEE9C0] + 8, v8);
}

_QWORD *partial apply for closure #1 in closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(_QWORD *a1, uint64_t (*a2)(_QWORD, uint64_t, uint64_t))
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, *(_QWORD *)(v2 + 48), *(_QWORD *)(v2 + 56), *(_QWORD *)(v2 + 64), *(_QWORD *)(v2 + 72), *(_QWORD *)(v2 + 80), *(_QWORD **)(v2 + 88), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), a2);
}

uint64_t partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, uint64_t a5)
{
  uint64_t *v5;

  return closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(a1, a2, v5[4], v5[5], v5[6], v5[7], v5[2], v5[3], a3, a4, a5);
}

unint64_t lazy protocol witness table accessor for type vDSP.DFTError and conformance vDSP.DFTError()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vDSP.DFTError and conformance vDSP.DFTError;
  if (!lazy protocol witness table cache variable for type vDSP.DFTError and conformance vDSP.DFTError)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vDSP.DFTError, &type metadata for vDSP.DFTError);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.DFTError and conformance vDSP.DFTError);
  }
  return result;
}

uint64_t __swift_instantiateGenericMetadata(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  _QWORD v6[3];

  v6[0] = a2;
  v6[1] = a3;
  v6[2] = a4;
  return MEMORY[0x1D1794CD8](a1, v6, a5);
}

uint64_t Array.init(unsafeUninitializedCapacity:initializingWith:)()
{
  return Array.init(_unsafeUninitializedCapacity:initializingWith:)();
}

void vImage.PixelBuffer<>.makeArray<A>(of:channelCount:)(uint64_t a1, uint64_t a2)
{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;

  v3 = *v2;
  if (!*(_QWORD *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  v4 = *(_QWORD *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v5 = *(_QWORD *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  v6 = v4 * v5;
  if ((unsigned __int128)(v4 * (__int128)v5) >> 64 != (v4 * v5) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if ((unsigned __int128)(v6 * (__int128)a2) >> 64 == (v6 * a2) >> 63)
  {
    MEMORY[0x1E0C80A78](v6 * a2);
    Array.init(_unsafeUninitializedCapacity:initializingWith:)();
    return;
  }
LABEL_11:
  __break(1u);
}

vImage_Error closure #1 in vImage.PixelBuffer<>.makeArray<A>(of:channelCount:)(uint64_t a1, _QWORD *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v6;
  void *v12;
  _QWORD *v13;
  vImagePixelCount v14;
  int64_t v15;
  uint64_t v16;
  int64_t v17;
  size_t v18;
  void *v19;
  vImage_Error result;
  vImage_Flags v21;
  vImage_Buffer dest;
  uint64_t v23;

  v23 = *MEMORY[0x1E0C80C00];
  v12 = (void *)UnsafeBufferPointer.baseAddress.getter();
  v13 = *(_QWORD **)a3;
  if (!*(_QWORD *)(*(_QWORD *)a3 + 16))
  {
    __break(1u);
    goto LABEL_10;
  }
  v14 = v13[5];
  if ((v14 & 0x8000000000000000) != 0)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  v15 = v13[6];
  if (v15 < 0)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  v16 = *(_QWORD *)(*(_QWORD *)(a6 - 8) + 72);
  v17 = v15 * v16;
  if ((unsigned __int128)(v15 * (__int128)v16) >> 64 != (v15 * v16) >> 63)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  if ((unsigned __int128)(v17 * (__int128)a4) >> 64 != (v17 * a4) >> 63)
  {
LABEL_13:
    __break(1u);
LABEL_14:
    __break(1u);
  }
  dest.data = v12;
  dest.height = v14;
  dest.width = v15;
  dest.rowBytes = v17 * a4;
  if ((unsigned __int128)(v16 * (__int128)a4) >> 64 != (v16 * a4) >> 63)
    goto LABEL_14;
  v18 = v13[7];
  v19 = (void *)v13[4];
  v21 = 0;
  result = vImage_Buffer.copy(destinationBuffer:pixelSize:flags:)(&dest, v16 * a4, &v21, v19, v14, v15, v18);
  if (v6)
  {
    result = swift_unexpectedError();
    __break(1u);
  }
  else
  {
    *a2 = a5;
  }
  return result;
}

vImage_Error partial apply for closure #1 in vImage.PixelBuffer<>.makeArray<A>(of:channelCount:)(uint64_t a1, _QWORD *a2)
{
  uint64_t *v2;

  return closure #1 in vImage.PixelBuffer<>.makeArray<A>(of:channelCount:)(a1, a2, v2[3], v2[4], v2[5], v2[2]);
}

uint64_t BNNS.ArithmeticTernaryFunction.bnnsArithmeticFunction.getter()
{
  return 28;
}

uint64_t static BNNS.ArithmeticTernaryFunction.== infix(_:_:)()
{
  return 1;
}

void BNNS.ArithmeticTernaryFunction.hash(into:)()
{
  Hasher._combine(_:)(0);
}

Swift::Int BNNS.ArithmeticTernaryFunction.hashValue.getter()
{
  Hasher.init(_seed:)();
  Hasher._combine(_:)(0);
  return Hasher._finalize()();
}

uint64_t protocol witness for static Equatable.== infix(_:_:) in conformance BNNS.ArithmeticTernaryFunction()
{
  return 1;
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance BNNS.ArithmeticTernaryFunction()
{
  Hasher.init(_seed:)();
  Hasher._combine(_:)(0);
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance BNNS.ArithmeticTernaryFunction()
{
  Hasher._combine(_:)(0);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance BNNS.ArithmeticTernaryFunction()
{
  Hasher.init(_seed:)();
  Hasher._combine(_:)(0);
  return Hasher._finalize()();
}

uint64_t BNNS.TernaryArithmeticLayer.__allocating_init(inputA:inputADescriptorType:inputB:inputBDescriptorType:inputC:inputCDescriptorType:output:outputDescriptorType:function:activation:filterParameters:)(_OWORD *a1, unsigned __int8 *a2, _OWORD *a3, unsigned __int8 *a4, _OWORD *a5, unsigned __int8 *a6, _OWORD *a7, unsigned __int8 *a8, uint64_t a9, uint64_t *a10, int a11, uint64_t a12, uint64_t a13, uint64_t a14)
{
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  int v34;
  int v35;
  int v36;
  int v37;
  uint64_t v38;
  int *v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  int v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  _DWORD v48[2];
  _OWORD *v49;
  int v50;
  uint64_t v51;
  uint64_t v52;
  int v53;
  __int128 v54;
  uint64_t v55;
  _OWORD v56[11];
  int v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  __int128 v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  __int128 v65;
  __int128 v66;
  __int128 v67;
  __int128 v68;
  int v69;
  int v70;
  __int128 v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  __int128 v76;
  __int128 v77;
  __int128 v78;
  __int128 v79;
  __int128 v80;
  __int128 v81;
  int v82;
  int v83;
  __int128 v84;
  __int128 v85;
  __int128 v86;
  __int128 v87;
  __int128 v88;
  __int128 v89;
  __int128 v90;
  __int128 v91;
  __int128 v92;
  __int128 v93;
  __int128 v94;
  int v95;
  int v96;
  _BYTE v97[180];
  _BYTE v98[180];
  _BYTE v99[180];
  int v100;
  uint64_t v101;
  uint64_t v102;
  int v103;
  __int128 v104;
  uint64_t v105;
  uint64_t v106;

  v106 = *MEMORY[0x1E0C80C00];
  v14 = a1[9];
  v56[8] = a1[8];
  v56[9] = v14;
  v56[10] = a1[10];
  v15 = a1[5];
  v56[4] = a1[4];
  v56[5] = v15;
  v16 = a1[7];
  v56[6] = a1[6];
  v56[7] = v16;
  v17 = a1[1];
  v56[0] = *a1;
  v56[1] = v17;
  v18 = a1[3];
  v56[2] = a1[2];
  v56[3] = v18;
  v19 = a3[6];
  *(_OWORD *)&v99[116] = a3[7];
  v20 = a3[9];
  *(_OWORD *)&v99[132] = a3[8];
  *(_OWORD *)&v99[148] = v20;
  *(_OWORD *)&v99[164] = a3[10];
  v21 = a3[2];
  *(_OWORD *)&v99[52] = a3[3];
  v22 = a3[5];
  *(_OWORD *)&v99[68] = a3[4];
  *(_OWORD *)&v99[84] = v22;
  *(_OWORD *)&v99[100] = v19;
  v23 = a3[1];
  *(_OWORD *)&v99[4] = *a3;
  *(_OWORD *)&v99[20] = v23;
  *(_OWORD *)&v99[36] = v21;
  v24 = a5[6];
  *(_OWORD *)&v98[116] = a5[7];
  v25 = a5[9];
  *(_OWORD *)&v98[132] = a5[8];
  *(_OWORD *)&v98[148] = v25;
  *(_OWORD *)&v98[164] = a5[10];
  v26 = a5[2];
  *(_OWORD *)&v98[52] = a5[3];
  v27 = a5[5];
  *(_OWORD *)&v98[68] = a5[4];
  *(_OWORD *)&v98[84] = v27;
  *(_OWORD *)&v98[100] = v24;
  v28 = a5[1];
  *(_OWORD *)&v98[4] = *a5;
  *(_OWORD *)&v98[20] = v28;
  *(_OWORD *)&v98[36] = v26;
  v29 = a7[6];
  *(_OWORD *)&v97[116] = a7[7];
  v30 = a7[9];
  *(_OWORD *)&v97[132] = a7[8];
  *(_OWORD *)&v97[148] = v30;
  *(_OWORD *)&v97[164] = a7[10];
  v31 = a7[2];
  *(_OWORD *)&v97[52] = a7[3];
  v32 = a7[5];
  *(_OWORD *)&v97[68] = a7[4];
  *(_OWORD *)&v97[84] = v32;
  *(_OWORD *)&v97[100] = v29;
  v33 = a7[1];
  *(_OWORD *)&v97[4] = *a7;
  *(_OWORD *)&v97[20] = v33;
  *(_OWORD *)&v97[36] = v31;
  v67 = *(_OWORD *)&v99[144];
  v68 = *(_OWORD *)&v99[160];
  v63 = *(_OWORD *)&v99[80];
  v64 = *(_OWORD *)&v99[96];
  v34 = *a2;
  v35 = *a4;
  v36 = *a6;
  v37 = *a8;
  v38 = *a10;
  v65 = *(_OWORD *)&v99[112];
  v66 = *(_OWORD *)&v99[128];
  v57 = v34;
  v69 = *(_DWORD *)&v99[176];
  v62 = *(_OWORD *)&v99[64];
  v58 = *(_OWORD *)v99;
  v59 = *(_OWORD *)&v99[16];
  v60 = *(_OWORD *)&v99[32];
  v61 = *(_OWORD *)&v99[48];
  v70 = v35;
  v79 = *(_OWORD *)&v98[128];
  v80 = *(_OWORD *)&v98[144];
  v81 = *(_OWORD *)&v98[160];
  v75 = *(_OWORD *)&v98[64];
  v76 = *(_OWORD *)&v98[80];
  v77 = *(_OWORD *)&v98[96];
  v78 = *(_OWORD *)&v98[112];
  v71 = *(_OWORD *)v98;
  v72 = *(_OWORD *)&v98[16];
  v73 = *(_OWORD *)&v98[32];
  v74 = *(_OWORD *)&v98[48];
  v82 = *(_DWORD *)&v98[176];
  v83 = v36;
  v92 = *(_OWORD *)&v97[128];
  v93 = *(_OWORD *)&v97[144];
  v94 = *(_OWORD *)&v97[160];
  v88 = *(_OWORD *)&v97[64];
  v89 = *(_OWORD *)&v97[80];
  v90 = *(_OWORD *)&v97[96];
  v91 = *(_OWORD *)&v97[112];
  v84 = *(_OWORD *)v97;
  v85 = *(_OWORD *)&v97[16];
  v86 = *(_OWORD *)&v97[32];
  v87 = *(_OWORD *)&v97[48];
  v95 = *(_DWORD *)&v97[176];
  v96 = v37;
  v48[1] = HIDWORD(v38);
  BNNS.ActivationFunction.bnnsActivation.getter();
  v51 = v101;
  v48[0] = 28;
  v49 = v56;
  v50 = v100;
  v52 = v102;
  v53 = v103;
  v54 = v104;
  v55 = v105;
  if (a13 == 1)
  {
    v39 = 0;
  }
  else
  {
    v44 = a11;
    v45 = a12;
    v46 = a13;
    v47 = a14;
    v39 = &v44;
  }
  v40 = MEMORY[0x1D1794594](v48, v39);
  type metadata accessor for BNNS.TernaryArithmeticLayer();
  v41 = swift_allocObject();
  v42 = v41;
  if (v40)
  {
    *(_QWORD *)(v41 + 16) = v40;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v42;
}

uint64_t BNNS.TernaryArithmeticLayer.apply(batchSize:inputA:inputB:inputC:output:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, unint64_t a5)
{
  uint64_t v5;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  unint64_t v13;
  unint64_t v14;
  unint64_t v15;
  unint64_t v16;
  unint64_t v17;
  unint64_t v18;
  unint64_t v19;
  size_t v20;
  int v21;
  uint64_t result;
  _BYTE *v23;
  _BYTE *v24;
  size_t v25;
  void *v26;
  const void **v27;
  void *out;
  uint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  unint64_t v46;
  unint64_t v47;
  unint64_t v48;
  unint64_t v49;
  unint64_t v50;
  unint64_t v51;
  unint64_t v52;
  unint64_t v53;
  _BYTE v54[136];
  _BYTE v55[136];
  _BYTE v56[136];
  _BYTE v57[136];
  _BYTE v58[136];
  _BYTE v59[136];
  _BYTE v60[8];
  _BYTE v61[8];
  _BYTE v62[8];
  _BYTE v63[8];
  uint64_t v64;
  uint64_t v65;
  uint64_t v66;
  void *v67;
  unint64_t v68;

  v68 = a5;
  outlined init with take of UnsafeMutableRawPointer?(a2 + 136, (uint64_t)v63);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v63, (uint64_t)&v64);
  v9 = v64;
  if (!v64)
    goto LABEL_7;
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v62);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v62, (uint64_t)&v65);
  v10 = v65;
  if (!v65)
    goto LABEL_7;
  outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v61);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v61, (uint64_t)&v66);
  v11 = v66;
  if (v66
    && (outlined init with take of UnsafeMutableRawPointer?(v68 + 136, (uint64_t)v60),
        outlined init with take of UnsafeMutableRawPointer?((uint64_t)v60, (uint64_t)&v67),
        v67))
  {
    out = v67;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
    v12 = swift_allocObject();
    *(_OWORD *)(v12 + 16) = xmmword_1CAB5EF70;
    *(_QWORD *)(v12 + 32) = v9;
    v27 = (const void **)(v12 + 32);
    *(_QWORD *)(v12 + 40) = v10;
    *(_QWORD *)(v12 + 48) = v11;
    v26 = *(void **)(v5 + 16);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
    v29 = swift_allocObject();
    *(_OWORD *)(v29 + 16) = xmmword_1CAB5EF70;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v55);
    outlined init with take of BNNS.Shape((uint64_t)v55, (uint64_t)v56);
    outlined init with take of BNNS.Shape((uint64_t)v56, (uint64_t)v54);
    BNNS.Shape.size.getter();
    v25 = a1;
    outlined init with take of BNNS.Shape((uint64_t)v56, (uint64_t)v54);
    BNNS.Shape.stride.getter();
    *(_QWORD *)(v29 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v46, v47, v48, v49, v50, v51, v52, v53, v46, v47, v48, v49, v50, v51, v52, v53);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v54);
    outlined init with take of BNNS.Shape((uint64_t)v54, (uint64_t)v57);
    outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)&v46);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)&v46);
    BNNS.Shape.stride.getter();
    *(_QWORD *)(v29 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v38, v39, v40, v41, v42, v43, v44, v45, v38, v39, v40, v41, v42, v43, v44, v45);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v46);
    outlined init with take of BNNS.Shape((uint64_t)&v46, (uint64_t)v58);
    outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)&v38);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)&v38);
    BNNS.Shape.stride.getter();
    *(_QWORD *)(v29 + 48) = specialized static BNNS.calculateBatchStride(size:stride:)(v30, v31, v32, v33, v34, v35, v36, v37, v30, v31, v32, v33, v34, v35, v36, v37);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v55);
    outlined init with take of BNNS.Shape((uint64_t)v55, (uint64_t)v59);
    outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v54);
    BNNS.Shape.size.getter();
    v13 = v46;
    v14 = v47;
    v15 = v48;
    v16 = v49;
    v17 = v50;
    v18 = v51;
    v19 = v52;
    v68 = v53;
    outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v54);
    BNNS.Shape.stride.getter();
    v20 = specialized static BNNS.calculateBatchStride(size:stride:)(v13, v14, v15, v16, v17, v18, v19, v68, v46, v47, v48, v49, v50, v51, v52, v53);
    v21 = BNNSArithmeticFilterApplyBatch(v26, v25, 3uLL, v27, (const size_t *)(v29 + 32), out, v20);
    swift_bridgeObjectRelease();
    result = swift_bridgeObjectRelease();
    if (!v21)
      return result;
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v23 = 0;
  }
  else
  {
LABEL_7:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v24 = 2;
  }
  return swift_willThrow();
}

uint64_t BNNS.TernaryArithmeticLayer.applyBackward(batchSize:inputA:inputB:inputC:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingInputCGradient:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, _OWORD *a6, __int128 *a7, _OWORD *a8, __int128 *a9)
{
  uint64_t v14;
  unint64_t v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  _BYTE *v42;
  uint64_t v49;
  uint64_t v50;
  unint64_t v51;
  unint64_t v52;
  unint64_t v53;
  unint64_t v54;
  unint64_t v55;
  unint64_t v56;
  unint64_t v57;
  unint64_t v58;
  char *v59;
  BNNSNDArrayDescriptor v60;
  __int128 v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  __int128 v65;
  __int128 v66;
  __int128 v67;
  __int128 v68;
  __int128 v69;
  __int128 v70;
  __int128 v71;
  _OWORD v72[11];
  _OWORD v73[11];
  _BYTE v74[136];
  _BYTE v75[136];
  _BYTE v76[136];
  _BYTE v77[8];
  _BYTE v78[8];
  _BYTE v79[8];
  uint64_t v80;
  uint64_t v81;
  _QWORD v82[4];

  v82[1] = *MEMORY[0x1E0C80C00];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer?>);
  v14 = swift_allocObject();
  *(_OWORD *)(v14 + 16) = xmmword_1CAB5EF70;
  outlined init with take of UnsafeMutableRawPointer?(a2 + 136, (uint64_t)v79);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v79, (uint64_t)&v80);
  *(_QWORD *)(v14 + 32) = v80;
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v78);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v78, (uint64_t)&v81);
  *(_QWORD *)(v14 + 40) = v81;
  outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v77);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v77, (uint64_t)v82);
  *(_QWORD *)(v14 + 48) = v82[0];
  v59 = (char *)v14;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  v50 = swift_allocObject();
  *(_OWORD *)(v50 + 16) = xmmword_1CAB5EF70;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v73);
  outlined init with take of BNNS.Shape((uint64_t)v73, (uint64_t)v74);
  outlined init with take of BNNS.Shape((uint64_t)v74, (uint64_t)v72);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v74, (uint64_t)v72);
  BNNS.Shape.stride.getter();
  *(_QWORD *)(v50 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v61, *((unint64_t *)&v61 + 1), v62, *((unint64_t *)&v62 + 1), v63, *((unint64_t *)&v63 + 1), v64, *((unint64_t *)&v64 + 1), v61, *((unint64_t *)&v61 + 1), v62, *((unint64_t *)&v62 + 1), v63, *((unint64_t *)&v63 + 1), v64, *((unint64_t *)&v64 + 1));
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v72);
  outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)v75);
  outlined init with take of BNNS.Shape((uint64_t)v75, (uint64_t)&v61);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v75, (uint64_t)&v61);
  BNNS.Shape.stride.getter();
  *(_QWORD *)(v50 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(*(unint64_t *)&v60.flags, v60.size[0], v60.size[1], v60.size[2], v60.size[3], v60.size[4], v60.size[5], v60.size[6], *(unint64_t *)&v60.flags, v60.size[0], v60.size[1], v60.size[2], v60.size[3], v60.size[4], v60.size[5], v60.size[6]);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v61);
  outlined init with take of BNNS.Shape((uint64_t)&v61, (uint64_t)v76);
  outlined init with take of BNNS.Shape((uint64_t)v76, (uint64_t)&v60);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v76, (uint64_t)&v60);
  BNNS.Shape.stride.getter();
  v15 = specialized static BNNS.calculateBatchStride(size:stride:)(v51, v52, v53, v54, v55, v56, v57, v58, v51, v52, v53, v54, v55, v56, v57, v58);
  v16 = a7[8];
  v17 = a7[9];
  v18 = a7[6];
  v73[7] = a7[7];
  v73[8] = v16;
  v19 = a7[10];
  v73[9] = v17;
  v73[10] = v19;
  v20 = a7[4];
  v21 = a7[5];
  v22 = a7[2];
  v73[3] = a7[3];
  v73[4] = v20;
  v73[5] = v21;
  v73[6] = v18;
  v23 = *a7;
  v73[1] = a7[1];
  v73[2] = v22;
  v24 = a8[9];
  v72[8] = a8[8];
  v72[9] = v24;
  v72[10] = a8[10];
  v73[0] = v23;
  v25 = a8[5];
  v72[4] = a8[4];
  v72[5] = v25;
  v26 = a8[7];
  v72[6] = a8[6];
  v72[7] = v26;
  v27 = a8[1];
  v72[0] = *a8;
  v72[1] = v27;
  v28 = a8[3];
  v72[2] = a8[2];
  v72[3] = v28;
  v29 = a9[8];
  v30 = a9[9];
  v31 = a9[6];
  v68 = a9[7];
  v69 = v29;
  v32 = a9[10];
  v70 = v30;
  v71 = v32;
  v33 = a9[4];
  v34 = a9[5];
  v35 = a9[2];
  v64 = a9[3];
  v65 = v33;
  *(_QWORD *)(v50 + 48) = v15;
  v66 = v34;
  v67 = v31;
  v36 = *a9;
  v62 = a9[1];
  v63 = v35;
  v37 = a6[9];
  *(_OWORD *)&v60.stride[7] = a6[8];
  *(_OWORD *)&v60.data_type = v37;
  *(_OWORD *)&v60.table_data_type = a6[10];
  v61 = v36;
  v38 = a6[5];
  *(_OWORD *)&v60.size[7] = a6[4];
  *(_OWORD *)&v60.stride[1] = v38;
  v39 = a6[7];
  *(_OWORD *)&v60.stride[3] = a6[6];
  *(_OWORD *)&v60.stride[5] = v39;
  v40 = a6[1];
  *(_OWORD *)&v60.flags = *a6;
  *(_OWORD *)&v60.size[1] = v40;
  v41 = a6[3];
  *(_OWORD *)&v60.size[3] = a6[2];
  *(_OWORD *)&v60.size[5] = v41;
  closure #1 in closure #1 in closure #1 in closure #1 in BNNS.TernaryArithmeticLayer.applyBackward(batchSize:inputA:inputB:inputC:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingInputCGradient:)(&v60, (uint64_t)v73, (uint64_t)v72, (uint64_t)&v61, v49, a1, &v59, (int *)&v51, a3, a4, v50, a5);
  swift_setDeallocating();
  swift_deallocClassInstance();
  if ((_DWORD)v51)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v42 = 0;
    swift_willThrow();
  }
  return swift_bridgeObjectRelease();
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in BNNS.TernaryArithmeticLayer.applyBackward(batchSize:inputA:inputB:inputC:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingInputCGradient:)@<X0>(BNNSNDArrayDescriptor *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, size_t a6@<X5>, char **a7@<X6>, int *a8@<X8>, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  uint64_t v16;
  unint64_t v17;
  unint64_t v18;
  unint64_t v19;
  unint64_t v20;
  unint64_t v21;
  unint64_t v22;
  unint64_t v23;
  unint64_t v24;
  unint64_t v25;
  uint64_t v26;
  uint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  size_t out_delta_stride;
  char *v38;
  char isUniquelyReferenced_nonNull_native;
  int v40;
  uint64_t result;
  void *out;
  size_t out_stride;
  const size_t *v44;
  void *v46;
  BNNSNDArrayDescriptor **v47;
  unint64_t v51;
  unint64_t v52;
  unint64_t v53;
  unint64_t v54;
  unint64_t v55;
  unint64_t v56;
  unint64_t v57;
  unint64_t v58;
  unint64_t v59;
  unint64_t v60;
  unint64_t v61;
  unint64_t v62;
  unint64_t v63;
  unint64_t v64;
  unint64_t v65;
  unint64_t v66;
  unint64_t v67;
  unint64_t v68;
  unint64_t v69;
  unint64_t v70;
  unint64_t v71;
  unint64_t v72;
  unint64_t v73;
  unint64_t v74;
  unint64_t v75;
  unint64_t v76;
  unint64_t v77;
  unint64_t v78;
  unint64_t v79;
  unint64_t v80;
  unint64_t v81;
  unint64_t v82;
  _BYTE v83[136];
  _BYTE v84[33];
  _BYTE v85[8];
  void *v86;
  _BYTE v87[136];
  unint64_t v88;
  unint64_t v89;
  unint64_t v90;
  unint64_t v91;
  unint64_t v92;
  unint64_t v93;
  unint64_t v94;
  unint64_t v95;
  _BYTE v96[144];
  uint64_t v97;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>>);
  v16 = swift_allocObject();
  *(_OWORD *)(v16 + 16) = xmmword_1CAB5EF70;
  *(_QWORD *)(v16 + 32) = a2;
  v47 = (BNNSNDArrayDescriptor **)(v16 + 32);
  *(_QWORD *)(v16 + 40) = a3;
  *(_QWORD *)(v16 + 48) = a4;
  v46 = *(void **)(a5 + 16);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  v97 = swift_allocObject();
  *(_OWORD *)(v97 + 16) = xmmword_1CAB5EF70;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v84);
  outlined init with take of BNNS.Shape((uint64_t)v84, (uint64_t)v87);
  outlined init with take of BNNS.Shape((uint64_t)v87, (uint64_t)v96);
  BNNS.Shape.size.getter();
  v17 = v88;
  v18 = v89;
  v19 = v90;
  v20 = v91;
  v21 = v92;
  v22 = v93;
  v23 = v94;
  v24 = v95;
  outlined init with take of BNNS.Shape((uint64_t)v87, (uint64_t)v96);
  BNNS.Shape.stride.getter();
  v25 = specialized static BNNS.calculateBatchStride(size:stride:)(v17, v18, v19, v20, v21, v22, v23, v24, v88, v89, v90, v91, v92, v93, v94, v95);
  v26 = v97;
  v27 = v97;
  *(_QWORD *)(v97 + 32) = v25;
  v44 = (const size_t *)(v27 + 32);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v83);
  outlined init with take of BNNS.Shape((uint64_t)v83, (uint64_t)&v88);
  outlined init with take of BNNS.Shape((uint64_t)&v88, (uint64_t)v96);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)&v88, (uint64_t)v96);
  BNNS.Shape.stride.getter();
  *(_QWORD *)(v26 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v75, v76, v77, v78, v79, v80, v81, v82, v75, v76, v77, v78, v79, v80, v81, v82);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v75);
  outlined init with take of BNNS.Shape((uint64_t)&v75, (uint64_t)v96);
  outlined init with take of BNNS.Shape((uint64_t)v96, (uint64_t)&v67);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v96, (uint64_t)&v67);
  BNNS.Shape.stride.getter();
  v28 = specialized static BNNS.calculateBatchStride(size:stride:)(v59, v60, v61, v62, v63, v64, v65, v66, v59, v60, v61, v62, v63, v64, v65, v66);
  *(_QWORD *)(v97 + 48) = v28;
  outlined init with take of UnsafeMutableRawPointer?(a12 + 136, (uint64_t)v85);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v85, (uint64_t)&v86);
  out = v86;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v75);
  outlined init with take of BNNS.Shape((uint64_t)&v75, (uint64_t)v83);
  outlined init with take of BNNS.Shape((uint64_t)v83, (uint64_t)v84);
  BNNS.Shape.size.getter();
  v29 = v67;
  v30 = v68;
  v31 = v69;
  v32 = v70;
  v33 = v71;
  v34 = v72;
  v36 = v73;
  v35 = v74;
  outlined init with take of BNNS.Shape((uint64_t)v83, (uint64_t)v84);
  BNNS.Shape.stride.getter();
  out_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v29, v30, v31, v32, v33, v34, v36, v35, v67, v68, v69, v70, v71, v72, v73, v74);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v67);
  outlined init with take of BNNS.Shape((uint64_t)&v67, (uint64_t)v84);
  outlined init with take of BNNS.Shape((uint64_t)v84, (uint64_t)&v59);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v84, (uint64_t)&v59);
  BNNS.Shape.stride.getter();
  out_delta_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v51, v52, v53, v54, v55, v56, v57, v58, v51, v52, v53, v54, v55, v56, v57, v58);
  v38 = *a7;
  isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
  *a7 = v38;
  if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    v38 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v38 + 2), 0, v38);
  *a7 = v38;
  swift_bridgeObjectRetain();
  v40 = BNNSArithmeticFilterApplyBackwardBatch(v46, a6, 3uLL, (const void **)v38 + 4, v44, v47, (const size_t *)(a11 + 32), out, out_stride, a1, out_delta_stride);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  result = swift_bridgeObjectRelease();
  *a8 = v40;
  return result;
}

uint64_t type metadata accessor for BNNS.TernaryArithmeticLayer()
{
  return objc_opt_self();
}

uint64_t BNNS.TernaryArithmeticLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.TernaryArithmeticLayer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

unint64_t lazy protocol witness table accessor for type BNNS.ArithmeticTernaryFunction and conformance BNNS.ArithmeticTernaryFunction()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNS.ArithmeticTernaryFunction and conformance BNNS.ArithmeticTernaryFunction;
  if (!lazy protocol witness table cache variable for type BNNS.ArithmeticTernaryFunction and conformance BNNS.ArithmeticTernaryFunction)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNS.ArithmeticTernaryFunction, &type metadata for BNNS.ArithmeticTernaryFunction);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.ArithmeticTernaryFunction and conformance BNNS.ArithmeticTernaryFunction);
  }
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.ArithmeticTernaryFunction(unsigned int *a1, int a2)
{
  int v2;
  int v3;

  if (!a2)
    return 0;
  if ((a2 + 1) >= 0x10000)
    v2 = 4;
  else
    v2 = 2;
  if ((a2 + 1) < 0x100)
    v3 = 1;
  else
    v3 = v2;
  if (v3 == 4)
    return *a1;
  if (v3 == 2)
    return *(unsigned __int16 *)a1;
  return *(unsigned __int8 *)a1;
}

uint64_t storeEnumTagSinglePayload for BNNS.ArithmeticTernaryFunction(uint64_t a1, int a2, int a3)
{
  int v3;
  uint64_t v4;

  if ((a3 + 1) >= 0x10000)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 1) < 0x100)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3)
    v4 = v4;
  else
    v4 = 0;
  if (a2)
    return ((uint64_t (*)(void))((char *)sub_1CAAF3B64 + 4 * asc_1CAB60670[v4]))();
  else
    return ((uint64_t (*)(void))((char *)sub_1CAAF3B84 + 4 * byte_1CAB60675[v4]))();
}

_BYTE *sub_1CAAF3B64(_BYTE *result, char a2)
{
  *result = a2;
  return result;
}

_BYTE *sub_1CAAF3B84(_BYTE *result)
{
  *result = 0;
  return result;
}

_DWORD *sub_1CAAF3B8C(_DWORD *result, int a2)
{
  *result = a2;
  return result;
}

_WORD *sub_1CAAF3B94(_WORD *result, __int16 a2)
{
  *result = a2;
  return result;
}

_WORD *sub_1CAAF3B9C(_WORD *result)
{
  *result = 0;
  return result;
}

_DWORD *sub_1CAAF3BA4(_DWORD *result)
{
  *result = 0;
  return result;
}

uint64_t getEnumTag for BNNS.ArithmeticTernaryFunction()
{
  return 0;
}

ValueMetadata *type metadata accessor for BNNS.ArithmeticTernaryFunction()
{
  return &type metadata for BNNS.ArithmeticTernaryFunction;
}

uint64_t method lookup function for BNNS.TernaryArithmeticLayer()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of BNNS.TernaryArithmeticLayer.apply(batchSize:inputA:inputB:inputC:output:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5)
{
  uint64_t v5;
  uint64_t v6;
  int v7;
  uint64_t v8;
  int v9;
  uint64_t v10;
  int v11;
  uint64_t v12;
  int v13;
  uint64_t v14;
  int v15;
  uint64_t v16;
  int v17;
  uint64_t v18;
  int v19;
  uint64_t v20;
  int v21;
  uint64_t (*v22)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *);
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  uint64_t v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  uint64_t v40;
  __int128 v41;
  uint64_t v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  uint64_t v52;
  int v53;
  uint64_t v54;
  int v55;
  uint64_t v56;
  uint64_t v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  __int128 v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  __int128 v65;
  uint64_t v66;
  int v67;
  uint64_t v68;
  int v69;
  uint64_t v70;
  uint64_t v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  __int128 v76;
  __int128 v77;
  __int128 v78;
  __int128 v79;
  uint64_t v80;
  int v81;
  uint64_t v82;
  int v83;
  uint64_t v84;
  uint64_t v85;
  __int128 v86;
  __int128 v87;
  __int128 v88;
  __int128 v89;
  __int128 v90;
  __int128 v91;
  __int128 v92;
  __int128 v93;
  uint64_t v94;
  int v95;
  uint64_t v96;
  int v97;
  uint64_t v98;

  v6 = a2[17];
  v7 = *((_DWORD *)a2 + 36);
  v8 = a2[19];
  v9 = *((_DWORD *)a2 + 40);
  v10 = a3[17];
  v11 = *((_DWORD *)a3 + 36);
  v12 = a3[19];
  v13 = *((_DWORD *)a3 + 40);
  v14 = a4[17];
  v15 = *((_DWORD *)a4 + 36);
  v16 = a4[19];
  v17 = *((_DWORD *)a4 + 40);
  v18 = a5[17];
  v19 = *((_DWORD *)a5 + 36);
  v20 = a5[19];
  v21 = *((_DWORD *)a5 + 40);
  v22 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(_QWORD *)v5 + 96);
  v85 = *a2;
  v86 = *(_OWORD *)(a2 + 1);
  v87 = *(_OWORD *)(a2 + 3);
  v88 = *(_OWORD *)(a2 + 5);
  v89 = *(_OWORD *)(a2 + 7);
  v90 = *(_OWORD *)(a2 + 9);
  v91 = *(_OWORD *)(a2 + 11);
  v92 = *(_OWORD *)(a2 + 13);
  v93 = *(_OWORD *)(a2 + 15);
  v94 = v6;
  v95 = v7;
  v96 = v8;
  v97 = v9;
  v98 = *(uint64_t *)((char *)a2 + 164);
  v71 = *a3;
  v72 = *(_OWORD *)(a3 + 1);
  v73 = *(_OWORD *)(a3 + 3);
  v74 = *(_OWORD *)(a3 + 5);
  v75 = *(_OWORD *)(a3 + 7);
  v76 = *(_OWORD *)(a3 + 9);
  v77 = *(_OWORD *)(a3 + 11);
  v23 = *(_OWORD *)(a3 + 15);
  v78 = *(_OWORD *)(a3 + 13);
  v24 = *(_OWORD *)(a4 + 1);
  v25 = *(_OWORD *)(a4 + 3);
  v26 = *(_OWORD *)(a4 + 5);
  v27 = *(_OWORD *)(a4 + 7);
  v28 = *(_OWORD *)(a4 + 9);
  v29 = *(_OWORD *)(a4 + 11);
  v30 = *(_OWORD *)(a4 + 13);
  v31 = *a4;
  v32 = *(_OWORD *)(a4 + 15);
  v33 = *(_OWORD *)(a5 + 1);
  v34 = *(_OWORD *)(a5 + 3);
  v35 = *(_OWORD *)(a5 + 5);
  v36 = *(_OWORD *)(a5 + 7);
  v37 = *(_OWORD *)(a5 + 9);
  v38 = *(_OWORD *)(a5 + 11);
  v39 = *(_OWORD *)(a5 + 13);
  v40 = *a5;
  v41 = *(_OWORD *)(a5 + 15);
  v79 = v23;
  v80 = v10;
  v81 = v11;
  v82 = v12;
  v83 = v13;
  v84 = *(uint64_t *)((char *)a3 + 164);
  *(_QWORD *)&v23 = *(uint64_t *)((char *)a5 + 164);
  v57 = v31;
  v58 = v24;
  *(_QWORD *)&v24 = *(uint64_t *)((char *)a4 + 164);
  v59 = v25;
  v60 = v26;
  v61 = v27;
  v62 = v28;
  v63 = v29;
  v64 = v30;
  v65 = v32;
  v66 = v14;
  v67 = v15;
  v68 = v16;
  v69 = v17;
  v70 = v24;
  v43 = v40;
  v44 = v33;
  v45 = v34;
  v46 = v35;
  v47 = v36;
  v48 = v37;
  v49 = v38;
  v50 = v39;
  v51 = v41;
  v52 = v18;
  v53 = v19;
  v54 = v20;
  v55 = v21;
  v56 = v23;
  return v22(a1, &v85, &v71, &v57, &v43);
}

uint64_t dispatch thunk of BNNS.TernaryArithmeticLayer.applyBackward(batchSize:inputA:inputB:inputC:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingInputCGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5, uint64_t *a6, uint64_t *a7, uint64_t *a8, uint64_t *a9)
{
  uint64_t v9;
  uint64_t v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  uint64_t v20;
  int v21;
  uint64_t v22;
  int v23;
  uint64_t v24;
  uint64_t v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  uint64_t v34;
  int v35;
  uint64_t v36;
  int v37;
  uint64_t v38;
  uint64_t v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  uint64_t v48;
  int v49;
  uint64_t v50;
  int v51;
  uint64_t v52;
  uint64_t v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  __int128 v61;
  uint64_t v62;
  int v63;
  uint64_t v64;
  int v65;
  uint64_t v66;
  uint64_t v67;
  __int128 v68;
  __int128 v69;
  __int128 v70;
  __int128 v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  uint64_t v76;
  int v77;
  uint64_t v78;
  int v79;
  uint64_t v80;
  uint64_t v81;
  __int128 v82;
  __int128 v83;
  __int128 v84;
  __int128 v85;
  __int128 v86;
  __int128 v87;
  __int128 v88;
  __int128 v89;
  uint64_t v90;
  int v91;
  uint64_t v92;
  int v93;
  uint64_t v94;
  uint64_t v95;
  __int128 v96;
  __int128 v97;
  __int128 v98;
  __int128 v99;
  __int128 v100;
  __int128 v101;
  __int128 v102;
  __int128 v103;
  uint64_t v104;
  int v105;
  uint64_t v106;
  int v107;
  uint64_t v108;
  uint64_t v109;
  __int128 v110;
  __int128 v111;
  __int128 v112;
  __int128 v113;
  __int128 v114;
  __int128 v115;
  __int128 v116;
  __int128 v117;
  uint64_t v118;
  int v119;
  uint64_t v120;
  int v121;
  uint64_t v122;

  v115 = *(_OWORD *)(a2 + 11);
  v116 = *(_OWORD *)(a2 + 13);
  v117 = *(_OWORD *)(a2 + 15);
  v122 = *(uint64_t *)((char *)a2 + 164);
  v110 = *(_OWORD *)(a2 + 1);
  v111 = *(_OWORD *)(a2 + 3);
  v112 = *(_OWORD *)(a2 + 5);
  v113 = *(_OWORD *)(a2 + 7);
  v114 = *(_OWORD *)(a2 + 9);
  v101 = *(_OWORD *)(a3 + 11);
  v102 = *(_OWORD *)(a3 + 13);
  v103 = *(_OWORD *)(a3 + 15);
  v108 = *(uint64_t *)((char *)a3 + 164);
  v96 = *(_OWORD *)(a3 + 1);
  v97 = *(_OWORD *)(a3 + 3);
  v98 = *(_OWORD *)(a3 + 5);
  v99 = *(_OWORD *)(a3 + 7);
  v100 = *(_OWORD *)(a3 + 9);
  v87 = *(_OWORD *)(a4 + 11);
  v88 = *(_OWORD *)(a4 + 13);
  v89 = *(_OWORD *)(a4 + 15);
  v94 = *(uint64_t *)((char *)a4 + 164);
  v82 = *(_OWORD *)(a4 + 1);
  v83 = *(_OWORD *)(a4 + 3);
  v84 = *(_OWORD *)(a4 + 5);
  v85 = *(_OWORD *)(a4 + 7);
  v86 = *(_OWORD *)(a4 + 9);
  v73 = *(_OWORD *)(a5 + 11);
  v74 = *(_OWORD *)(a5 + 13);
  v75 = *(_OWORD *)(a5 + 15);
  v80 = *(uint64_t *)((char *)a5 + 164);
  v68 = *(_OWORD *)(a5 + 1);
  v69 = *(_OWORD *)(a5 + 3);
  v70 = *(_OWORD *)(a5 + 5);
  v71 = *(_OWORD *)(a5 + 7);
  v72 = *(_OWORD *)(a5 + 9);
  v59 = *(_OWORD *)(a6 + 11);
  v60 = *(_OWORD *)(a6 + 13);
  v61 = *(_OWORD *)(a6 + 15);
  v66 = *(uint64_t *)((char *)a6 + 164);
  v54 = *(_OWORD *)(a6 + 1);
  v55 = *(_OWORD *)(a6 + 3);
  v56 = *(_OWORD *)(a6 + 5);
  v57 = *(_OWORD *)(a6 + 7);
  v58 = *(_OWORD *)(a6 + 9);
  v45 = *(_OWORD *)(a7 + 11);
  v46 = *(_OWORD *)(a7 + 13);
  v47 = *(_OWORD *)(a7 + 15);
  v52 = *(uint64_t *)((char *)a7 + 164);
  v109 = *a2;
  v95 = *a3;
  v81 = *a4;
  v67 = *a5;
  v53 = *a6;
  v39 = *a7;
  v40 = *(_OWORD *)(a7 + 1);
  v41 = *(_OWORD *)(a7 + 3);
  v42 = *(_OWORD *)(a7 + 5);
  v43 = *(_OWORD *)(a7 + 7);
  v44 = *(_OWORD *)(a7 + 9);
  v25 = *a8;
  v26 = *(_OWORD *)(a8 + 1);
  v27 = *(_OWORD *)(a8 + 3);
  v28 = *(_OWORD *)(a8 + 5);
  v29 = *(_OWORD *)(a8 + 7);
  v30 = *(_OWORD *)(a8 + 9);
  v31 = *(_OWORD *)(a8 + 11);
  v32 = *(_OWORD *)(a8 + 13);
  v33 = *(_OWORD *)(a8 + 15);
  v38 = *(uint64_t *)((char *)a8 + 164);
  v11 = *a9;
  v12 = *(_OWORD *)(a9 + 1);
  v13 = *(_OWORD *)(a9 + 3);
  v14 = *(_OWORD *)(a9 + 5);
  v15 = *(_OWORD *)(a9 + 7);
  v16 = *(_OWORD *)(a9 + 9);
  v17 = *(_OWORD *)(a9 + 11);
  v18 = *(_OWORD *)(a9 + 13);
  v19 = *(_OWORD *)(a9 + 15);
  v24 = *(uint64_t *)((char *)a9 + 164);
  v118 = a2[17];
  v119 = *((_DWORD *)a2 + 36);
  v120 = a2[19];
  v121 = *((_DWORD *)a2 + 40);
  v104 = a3[17];
  v105 = *((_DWORD *)a3 + 36);
  v106 = a3[19];
  v107 = *((_DWORD *)a3 + 40);
  v90 = a4[17];
  v91 = *((_DWORD *)a4 + 36);
  v92 = a4[19];
  v93 = *((_DWORD *)a4 + 40);
  v76 = a5[17];
  v77 = *((_DWORD *)a5 + 36);
  v78 = a5[19];
  v79 = *((_DWORD *)a5 + 40);
  v62 = a6[17];
  v63 = *((_DWORD *)a6 + 36);
  v64 = a6[19];
  v65 = *((_DWORD *)a6 + 40);
  v48 = a7[17];
  v49 = *((_DWORD *)a7 + 36);
  v50 = a7[19];
  v51 = *((_DWORD *)a7 + 40);
  v34 = a8[17];
  v35 = *((_DWORD *)a8 + 36);
  v36 = a8[19];
  v37 = *((_DWORD *)a8 + 40);
  v20 = a9[17];
  v21 = *((_DWORD *)a9 + 36);
  v22 = a9[19];
  v23 = *((_DWORD *)a9 + 40);
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(_QWORD *)v9 + 104))(a1, &v109, &v95, &v81, &v67, &v53, &v39, &v25, &v11);
}

uint64_t BNNS.GramLayer.__allocating_init(input:output:alpha:filterParameters:)(_OWORD *a1, _OWORD *a2, uint32_t a3, size_t a4, int (__cdecl *a5)(void **, size_t, size_t), void (__cdecl *a6)(void *), float a7)
{
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  BNNSFilterParameters *p_filter_params;
  void *v20;
  uint64_t v21;
  uint64_t v22;
  BNNSFilterParameters filter_params;
  BNNSLayerParametersGram layer_params;
  _BYTE v26[180];
  uint64_t v27;

  v27 = *MEMORY[0x1E0C80C00];
  v7 = a2[8];
  v8 = a2[9];
  v9 = a2[6];
  *(_OWORD *)&layer_params.o_desc.stride[5] = a2[7];
  *(_OWORD *)&layer_params.o_desc.stride[7] = v7;
  v10 = a2[10];
  *(_OWORD *)&layer_params.o_desc.data_type = v8;
  *(_OWORD *)&layer_params.o_desc.table_data_type = v10;
  v11 = a2[4];
  *(_OWORD *)&layer_params.o_desc.stride[1] = a2[5];
  *(_OWORD *)&layer_params.o_desc.stride[3] = v9;
  v12 = a2[2];
  *(_OWORD *)&layer_params.o_desc.size[5] = a2[3];
  *(_OWORD *)&layer_params.o_desc.size[7] = v11;
  v13 = a2[1];
  *(_OWORD *)&layer_params.o_desc.flags = *a2;
  *(_OWORD *)&layer_params.o_desc.size[1] = v13;
  *(_OWORD *)&layer_params.o_desc.size[3] = v12;
  v14 = a1[5];
  *(_OWORD *)&v26[68] = a1[4];
  v15 = a1[2];
  *(_OWORD *)&v26[52] = a1[3];
  v16 = a1[6];
  *(_OWORD *)&v26[116] = a1[7];
  v17 = a1[9];
  *(_OWORD *)&v26[132] = a1[8];
  *(_OWORD *)&v26[148] = v17;
  *(_OWORD *)&v26[164] = a1[10];
  *(_OWORD *)&v26[84] = v14;
  *(_OWORD *)&v26[100] = v16;
  v18 = a1[1];
  *(_OWORD *)&v26[4] = *a1;
  *(_OWORD *)&v26[20] = v18;
  *(_OWORD *)&v26[36] = v15;
  layer_params.alpha = a7;
  *(_OWORD *)((char *)&layer_params.i_desc.stride[6] + 4) = *(_OWORD *)&v26[128];
  *(_OWORD *)((char *)&layer_params.i_desc.data + 4) = *(_OWORD *)&v26[144];
  *(_OWORD *)((char *)&layer_params.i_desc.table_data + 4) = *(_OWORD *)&v26[160];
  *((_DWORD *)&layer_params.i_desc.data_bias + 1) = *(_DWORD *)&v26[176];
  *(_OWORD *)((char *)&layer_params.i_desc.size[6] + 4) = *(_OWORD *)&v26[64];
  *(_OWORD *)((char *)layer_params.i_desc.stride + 4) = *(_OWORD *)&v26[80];
  *(_OWORD *)((char *)&layer_params.i_desc.stride[2] + 4) = *(_OWORD *)&v26[96];
  *(_OWORD *)((char *)&layer_params.i_desc.stride[4] + 4) = *(_OWORD *)&v26[112];
  *(_OWORD *)(&layer_params.alpha + 1) = *(_OWORD *)v26;
  *(_OWORD *)((char *)layer_params.i_desc.size + 4) = *(_OWORD *)&v26[16];
  *(_OWORD *)((char *)&layer_params.i_desc.size[2] + 4) = *(_OWORD *)&v26[32];
  *(_OWORD *)((char *)&layer_params.i_desc.size[4] + 4) = *(_OWORD *)&v26[48];
  if (a5 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    p_filter_params = 0;
  }
  else
  {
    filter_params.flags = a3;
    filter_params.n_threads = a4;
    filter_params.alloc_memory = a5;
    filter_params.free_memory = a6;
    p_filter_params = &filter_params;
  }
  v20 = BNNSFilterCreateLayerGram(&layer_params, p_filter_params);
  type metadata accessor for BNNS.GramLayer();
  v21 = swift_allocObject();
  v22 = v21;
  if (v20)
  {
    *(_QWORD *)(v21 + 16) = v20;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v22;
}

uint64_t type metadata accessor for BNNS.GramLayer()
{
  return objc_opt_self();
}

uint64_t BNNS.GramLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.GramLayer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

uint64_t vImage.PixelBuffer<>.applyGamma(_:intermediateBuffer:destination:)(unsigned int *a1, _QWORD *a2, _QWORD *a3)
{
  return vImage.PixelBuffer<>.applyGamma(_:intermediateBuffer:destination:)(a1, a2, a3, 4, (uint64_t (*)(unint64_t, _QWORD, _QWORD, uint64_t, _QWORD))specialized vImage.PixelBuffer<>._applyGamma<A, B>(_:intermediateBuffer:destination:widthMultiplier:pixelFormat:));
}

{
  return vImage.PixelBuffer<>.applyGamma(_:intermediateBuffer:destination:)(a1, a2, a3, 3, (uint64_t (*)(unint64_t, _QWORD, _QWORD, uint64_t, _QWORD))specialized vImage.PixelBuffer<>._applyGamma<A, B>(_:intermediateBuffer:destination:widthMultiplier:pixelFormat:));
}

{
  return vImage.PixelBuffer<>.applyGamma(_:intermediateBuffer:destination:)(a1, a2, a3, 2, (uint64_t (*)(unint64_t, _QWORD, _QWORD, uint64_t, _QWORD))specialized vImage.PixelBuffer<>._applyGamma<A, B>(_:intermediateBuffer:destination:widthMultiplier:pixelFormat:));
}

{
  return vImage.PixelBuffer<>.applyGamma(_:intermediateBuffer:destination:)(a1, a2, a3, 1, (uint64_t (*)(unint64_t, _QWORD, _QWORD, uint64_t, _QWORD))specialized vImage.PixelBuffer<>._applyGamma<A, B>(_:intermediateBuffer:destination:widthMultiplier:pixelFormat:));
}

vImage_Error specialized vImage.PixelBuffer<>._applyGamma<A, B>(_:intermediateBuffer:destination:widthMultiplier:pixelFormat:)(uint64_t a1, uint64_t a2, _QWORD *a3, uint64_t a4, _QWORD *a5)
{
  vImagePixelCount v5;
  vImagePixelCount v6;
  int64_t v8;
  vImagePixelCount v9;
  uint64_t v10;
  uint64_t v11;
  vImagePixelCount v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  uint64_t inited;
  __int128 v20;
  uint64_t v21;
  uint64_t v22;
  _QWORD *v23;
  int v24;
  float v25;
  vImagePixelCount v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  _QWORD *v31;
  GammaFunction GammaFunction;
  void *v33;
  size_t v34;
  vImagePixelCount v36;
  __int128 v37;
  void *v38;
  size_t v39;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v44;

  v44 = *MEMORY[0x1E0C80C00];
  if (!a5[2])
  {
    __break(1u);
    goto LABEL_40;
  }
  v8 = a5[6];
  if (v8 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v9 = a5[5];
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v8)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v9)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!a3[2])
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  v10 = a3[6];
  if (v10 < 0)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v11 = a3[5];
  if (v11 < 0)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (!v10)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (!v11)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (v8 != v10)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (v9 != v11)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  v13 = (vImagePixelCount)&loc_1CAB5E000;
  if (a2)
  {
    v14 = a2;
    if (*(_QWORD *)(a2 + 16))
      goto LABEL_15;
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v39 = 0;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v14 = swift_allocObject();
  *(_OWORD *)(v14 + 16) = xmmword_1CAB5E430;
  v6 = (vImagePixelCount)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v8, v9, 0x80u);
  v5 = v26;
  v28 = v27;
  v30 = v29;
  type metadata accessor for vImage.BufferReference();
  v31 = (_QWORD *)swift_allocObject();
  v31[2] = v6;
  v31[3] = v5;
  v31[4] = v28;
  v31[5] = v30;
  *(_QWORD *)(v14 + 32) = v6;
  *(_QWORD *)(v14 + 40) = v5;
  *(_QWORD *)(v14 + 48) = v28;
  *(_QWORD *)(v14 + 56) = v30;
  *(_QWORD *)(v14 + 64) = v31;
  v13 = 0x1CAB5E000;
  if (!*(_QWORD *)(v14 + 16))
    goto LABEL_34;
LABEL_15:
  v15 = *(_QWORD *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  v16 = *(_QWORD *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v15)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (!v16)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v8 != v15)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (v9 != v16)
  {
LABEL_56:
    __break(1u);
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  v17 = (void *)a5[4];
  if (!v17)
  {
LABEL_64:
    swift_bridgeObjectRetain();
    swift_bridgeObjectRelease();
    __break(1u);
    goto LABEL_65;
  }
  v5 = v8 * a4;
  if ((unsigned __int128)(v8 * (__int128)a4) >> 64 != (v8 * a4) >> 63)
    goto LABEL_57;
  v18 = a5[7];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  inited = swift_initStackObject();
  v20 = *(_OWORD *)(v13 + 1072);
  *(_OWORD *)(inited + 16) = v20;
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  *(_QWORD *)(inited + 32) = v17;
  *(_QWORD *)(inited + 40) = v9;
  *(_QWORD *)(inited + 48) = v5;
  *(_QWORD *)(inited + 56) = v18;
  *(_QWORD *)(inited + 64) = 0;
  if (!*(_QWORD *)(v14 + 16))
  {
LABEL_59:
    __break(1u);
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  v39 = v18;
  v8 = *(_QWORD *)(v14 + 32);
  if (!v8)
  {
LABEL_65:
    swift_setDeallocating();
    swift_bridgeObjectRetain();
    swift_arrayDestroy();
    swift_bridgeObjectRelease();
    __break(1u);
LABEL_66:
    swift_setDeallocating();
    swift_arrayDestroy();
    __break(1u);
  }
  v21 = *(_QWORD *)(v14 + 48);
  if (v21 < 0)
    goto LABEL_60;
  v13 = v21 * a4;
  if ((unsigned __int128)(v21 * (__int128)a4) >> 64 != (v21 * a4) >> 63)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  v38 = v17;
  v6 = *(_QWORD *)(v14 + 40);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  v36 = v9;
  v9 = *(_QWORD *)(v14 + 56);
  v37 = v20;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRelease();
  v22 = swift_initStackObject();
  *(_OWORD *)(v22 + 16) = v37;
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  LODWORD(v11) = a1;
  *(_QWORD *)(v22 + 32) = v8;
  *(_QWORD *)(v22 + 40) = v6;
  *(_QWORD *)(v22 + 48) = v13;
  *(_QWORD *)(v22 + 56) = v9;
  *(_QWORD *)(v22 + 64) = 0;
  if (BYTE4(a1) == 1)
  {
LABEL_35:
    v25 = *(float *)&v11;
    v24 = 1;
    v23 = a3;
    goto LABEL_37;
  }
  v23 = a3;
  if (BYTE4(a1))
  {
    v24 = a1 + 2;
    v25 = 0.0;
  }
  else
  {
    v24 = 0;
    v25 = *(float *)&a1;
  }
LABEL_37:
  GammaFunction = vImageCreateGammaFunction(v25, v24, 0);
  swift_release();
  src.data = v38;
  src.height = v36;
  src.width = v5;
  src.rowBytes = v39;
  dest.data = (void *)v8;
  dest.height = v6;
  dest.width = v13;
  dest.rowBytes = v9;
  vImageGamma_Planar8toPlanarF(&src, &dest, GammaFunction, 0);
  v33 = (void *)v23[4];
  if (!v33)
    goto LABEL_66;
  v34 = v23[7];
  swift_release();
  src.data = (void *)v8;
  src.height = v6;
  src.width = v13;
  src.rowBytes = v9;
  dest.data = v33;
  dest.height = v36;
  dest.width = v5;
  dest.rowBytes = v34;
  return vImageConvert_PlanarFtoPlanar8(&src, &dest, 1.0, 0.0, 0);
}

{
  vImagePixelCount v5;
  vImagePixelCount v6;
  int64_t v8;
  vImagePixelCount v9;
  uint64_t v10;
  uint64_t v11;
  vImagePixelCount v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  uint64_t inited;
  __int128 v20;
  uint64_t v21;
  uint64_t v22;
  _QWORD *v23;
  int v24;
  float v25;
  vImagePixelCount v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  _QWORD *v31;
  GammaFunction GammaFunction;
  void *v33;
  size_t v34;
  vImagePixelCount v36;
  __int128 v37;
  void *v38;
  size_t v39;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v44;

  v44 = *MEMORY[0x1E0C80C00];
  if (!a5[2])
  {
    __break(1u);
    goto LABEL_40;
  }
  v8 = a5[6];
  if (v8 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v9 = a5[5];
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v8)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v9)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!a3[2])
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  v10 = a3[6];
  if (v10 < 0)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v11 = a3[5];
  if (v11 < 0)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (!v10)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (!v11)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (v8 != v10)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (v9 != v11)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  v13 = (vImagePixelCount)&loc_1CAB5E000;
  if (a2)
  {
    v14 = a2;
    if (*(_QWORD *)(a2 + 16))
      goto LABEL_15;
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v39 = 0;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v14 = swift_allocObject();
  *(_OWORD *)(v14 + 16) = xmmword_1CAB5E430;
  v6 = (vImagePixelCount)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v8, v9, 0x60u);
  v5 = v26;
  v28 = v27;
  v30 = v29;
  type metadata accessor for vImage.BufferReference();
  v31 = (_QWORD *)swift_allocObject();
  v31[2] = v6;
  v31[3] = v5;
  v31[4] = v28;
  v31[5] = v30;
  *(_QWORD *)(v14 + 32) = v6;
  *(_QWORD *)(v14 + 40) = v5;
  *(_QWORD *)(v14 + 48) = v28;
  *(_QWORD *)(v14 + 56) = v30;
  *(_QWORD *)(v14 + 64) = v31;
  v13 = 0x1CAB5E000;
  if (!*(_QWORD *)(v14 + 16))
    goto LABEL_34;
LABEL_15:
  v15 = *(_QWORD *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  v16 = *(_QWORD *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v15)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (!v16)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v8 != v15)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (v9 != v16)
  {
LABEL_56:
    __break(1u);
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  v17 = (void *)a5[4];
  if (!v17)
  {
LABEL_64:
    swift_bridgeObjectRetain();
    swift_bridgeObjectRelease();
    __break(1u);
    goto LABEL_65;
  }
  v5 = v8 * a4;
  if ((unsigned __int128)(v8 * (__int128)a4) >> 64 != (v8 * a4) >> 63)
    goto LABEL_57;
  v18 = a5[7];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  inited = swift_initStackObject();
  v20 = *(_OWORD *)(v13 + 1072);
  *(_OWORD *)(inited + 16) = v20;
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  *(_QWORD *)(inited + 32) = v17;
  *(_QWORD *)(inited + 40) = v9;
  *(_QWORD *)(inited + 48) = v5;
  *(_QWORD *)(inited + 56) = v18;
  *(_QWORD *)(inited + 64) = 0;
  if (!*(_QWORD *)(v14 + 16))
  {
LABEL_59:
    __break(1u);
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  v39 = v18;
  v8 = *(_QWORD *)(v14 + 32);
  if (!v8)
  {
LABEL_65:
    swift_setDeallocating();
    swift_bridgeObjectRetain();
    swift_arrayDestroy();
    swift_bridgeObjectRelease();
    __break(1u);
LABEL_66:
    swift_setDeallocating();
    swift_arrayDestroy();
    __break(1u);
  }
  v21 = *(_QWORD *)(v14 + 48);
  if (v21 < 0)
    goto LABEL_60;
  v13 = v21 * a4;
  if ((unsigned __int128)(v21 * (__int128)a4) >> 64 != (v21 * a4) >> 63)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  v38 = v17;
  v6 = *(_QWORD *)(v14 + 40);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  v36 = v9;
  v9 = *(_QWORD *)(v14 + 56);
  v37 = v20;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRelease();
  v22 = swift_initStackObject();
  *(_OWORD *)(v22 + 16) = v37;
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  LODWORD(v11) = a1;
  *(_QWORD *)(v22 + 32) = v8;
  *(_QWORD *)(v22 + 40) = v6;
  *(_QWORD *)(v22 + 48) = v13;
  *(_QWORD *)(v22 + 56) = v9;
  *(_QWORD *)(v22 + 64) = 0;
  if (BYTE4(a1) == 1)
  {
LABEL_35:
    v25 = *(float *)&v11;
    v24 = 1;
    v23 = a3;
    goto LABEL_37;
  }
  v23 = a3;
  if (BYTE4(a1))
  {
    v24 = a1 + 2;
    v25 = 0.0;
  }
  else
  {
    v24 = 0;
    v25 = *(float *)&a1;
  }
LABEL_37:
  GammaFunction = vImageCreateGammaFunction(v25, v24, 0);
  swift_release();
  src.data = v38;
  src.height = v36;
  src.width = v5;
  src.rowBytes = v39;
  dest.data = (void *)v8;
  dest.height = v6;
  dest.width = v13;
  dest.rowBytes = v9;
  vImageGamma_Planar8toPlanarF(&src, &dest, GammaFunction, 0);
  v33 = (void *)v23[4];
  if (!v33)
    goto LABEL_66;
  v34 = v23[7];
  swift_release();
  src.data = (void *)v8;
  src.height = v6;
  src.width = v13;
  src.rowBytes = v9;
  dest.data = v33;
  dest.height = v36;
  dest.width = v5;
  dest.rowBytes = v34;
  return vImageConvert_PlanarFtoPlanar8(&src, &dest, 1.0, 0.0, 0);
}

{
  vImagePixelCount v5;
  vImagePixelCount v6;
  int64_t v8;
  vImagePixelCount v9;
  uint64_t v10;
  uint64_t v11;
  vImagePixelCount v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  uint64_t inited;
  __int128 v20;
  uint64_t v21;
  uint64_t v22;
  _QWORD *v23;
  int v24;
  float v25;
  vImagePixelCount v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  _QWORD *v31;
  GammaFunction GammaFunction;
  void *v33;
  size_t v34;
  vImagePixelCount v36;
  __int128 v37;
  void *v38;
  size_t v39;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v44;

  v44 = *MEMORY[0x1E0C80C00];
  if (!a5[2])
  {
    __break(1u);
    goto LABEL_40;
  }
  v8 = a5[6];
  if (v8 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v9 = a5[5];
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v8)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v9)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!a3[2])
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  v10 = a3[6];
  if (v10 < 0)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v11 = a3[5];
  if (v11 < 0)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (!v10)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (!v11)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (v8 != v10)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (v9 != v11)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  v13 = (vImagePixelCount)&loc_1CAB5E000;
  if (a2)
  {
    v14 = a2;
    if (*(_QWORD *)(a2 + 16))
      goto LABEL_15;
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v39 = 0;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v14 = swift_allocObject();
  *(_OWORD *)(v14 + 16) = xmmword_1CAB5E430;
  v6 = (vImagePixelCount)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v8, v9, 0x40u);
  v5 = v26;
  v28 = v27;
  v30 = v29;
  type metadata accessor for vImage.BufferReference();
  v31 = (_QWORD *)swift_allocObject();
  v31[2] = v6;
  v31[3] = v5;
  v31[4] = v28;
  v31[5] = v30;
  *(_QWORD *)(v14 + 32) = v6;
  *(_QWORD *)(v14 + 40) = v5;
  *(_QWORD *)(v14 + 48) = v28;
  *(_QWORD *)(v14 + 56) = v30;
  *(_QWORD *)(v14 + 64) = v31;
  v13 = 0x1CAB5E000;
  if (!*(_QWORD *)(v14 + 16))
    goto LABEL_34;
LABEL_15:
  v15 = *(_QWORD *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  v16 = *(_QWORD *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v15)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (!v16)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v8 != v15)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (v9 != v16)
  {
LABEL_56:
    __break(1u);
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  v17 = (void *)a5[4];
  if (!v17)
  {
LABEL_64:
    swift_bridgeObjectRetain();
    swift_bridgeObjectRelease();
    __break(1u);
    goto LABEL_65;
  }
  v5 = v8 * a4;
  if ((unsigned __int128)(v8 * (__int128)a4) >> 64 != (v8 * a4) >> 63)
    goto LABEL_57;
  v18 = a5[7];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  inited = swift_initStackObject();
  v20 = *(_OWORD *)(v13 + 1072);
  *(_OWORD *)(inited + 16) = v20;
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  *(_QWORD *)(inited + 32) = v17;
  *(_QWORD *)(inited + 40) = v9;
  *(_QWORD *)(inited + 48) = v5;
  *(_QWORD *)(inited + 56) = v18;
  *(_QWORD *)(inited + 64) = 0;
  if (!*(_QWORD *)(v14 + 16))
  {
LABEL_59:
    __break(1u);
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  v39 = v18;
  v8 = *(_QWORD *)(v14 + 32);
  if (!v8)
  {
LABEL_65:
    swift_setDeallocating();
    swift_bridgeObjectRetain();
    swift_arrayDestroy();
    swift_bridgeObjectRelease();
    __break(1u);
LABEL_66:
    swift_setDeallocating();
    swift_arrayDestroy();
    __break(1u);
  }
  v21 = *(_QWORD *)(v14 + 48);
  if (v21 < 0)
    goto LABEL_60;
  v13 = v21 * a4;
  if ((unsigned __int128)(v21 * (__int128)a4) >> 64 != (v21 * a4) >> 63)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  v38 = v17;
  v6 = *(_QWORD *)(v14 + 40);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  v36 = v9;
  v9 = *(_QWORD *)(v14 + 56);
  v37 = v20;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRelease();
  v22 = swift_initStackObject();
  *(_OWORD *)(v22 + 16) = v37;
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  LODWORD(v11) = a1;
  *(_QWORD *)(v22 + 32) = v8;
  *(_QWORD *)(v22 + 40) = v6;
  *(_QWORD *)(v22 + 48) = v13;
  *(_QWORD *)(v22 + 56) = v9;
  *(_QWORD *)(v22 + 64) = 0;
  if (BYTE4(a1) == 1)
  {
LABEL_35:
    v25 = *(float *)&v11;
    v24 = 1;
    v23 = a3;
    goto LABEL_37;
  }
  v23 = a3;
  if (BYTE4(a1))
  {
    v24 = a1 + 2;
    v25 = 0.0;
  }
  else
  {
    v24 = 0;
    v25 = *(float *)&a1;
  }
LABEL_37:
  GammaFunction = vImageCreateGammaFunction(v25, v24, 0);
  swift_release();
  src.data = v38;
  src.height = v36;
  src.width = v5;
  src.rowBytes = v39;
  dest.data = (void *)v8;
  dest.height = v6;
  dest.width = v13;
  dest.rowBytes = v9;
  vImageGamma_Planar8toPlanarF(&src, &dest, GammaFunction, 0);
  v33 = (void *)v23[4];
  if (!v33)
    goto LABEL_66;
  v34 = v23[7];
  swift_release();
  src.data = (void *)v8;
  src.height = v6;
  src.width = v13;
  src.rowBytes = v9;
  dest.data = v33;
  dest.height = v36;
  dest.width = v5;
  dest.rowBytes = v34;
  return vImageConvert_PlanarFtoPlanar8(&src, &dest, 1.0, 0.0, 0);
}

{
  vImagePixelCount v5;
  vImagePixelCount v6;
  int64_t v8;
  vImagePixelCount v9;
  uint64_t v10;
  uint64_t v11;
  vImagePixelCount v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  uint64_t inited;
  __int128 v20;
  uint64_t v21;
  uint64_t v22;
  _QWORD *v23;
  int v24;
  float v25;
  vImagePixelCount v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  _QWORD *v31;
  GammaFunction GammaFunction;
  void *v33;
  size_t v34;
  vImagePixelCount v36;
  __int128 v37;
  void *v38;
  size_t v39;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v44;

  v44 = *MEMORY[0x1E0C80C00];
  if (!a5[2])
  {
    __break(1u);
    goto LABEL_40;
  }
  v8 = a5[6];
  if (v8 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v9 = a5[5];
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v8)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v9)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!a3[2])
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  v10 = a3[6];
  if (v10 < 0)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v11 = a3[5];
  if (v11 < 0)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (!v10)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (!v11)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (v8 != v10)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (v9 != v11)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  v13 = (vImagePixelCount)&loc_1CAB5E000;
  if (a2)
  {
    v14 = a2;
    if (*(_QWORD *)(a2 + 16))
      goto LABEL_15;
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v39 = 0;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v14 = swift_allocObject();
  *(_OWORD *)(v14 + 16) = xmmword_1CAB5E430;
  v6 = (vImagePixelCount)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v8, v9, 0x20u);
  v5 = v26;
  v28 = v27;
  v30 = v29;
  type metadata accessor for vImage.BufferReference();
  v31 = (_QWORD *)swift_allocObject();
  v31[2] = v6;
  v31[3] = v5;
  v31[4] = v28;
  v31[5] = v30;
  *(_QWORD *)(v14 + 32) = v6;
  *(_QWORD *)(v14 + 40) = v5;
  *(_QWORD *)(v14 + 48) = v28;
  *(_QWORD *)(v14 + 56) = v30;
  *(_QWORD *)(v14 + 64) = v31;
  v13 = 0x1CAB5E000;
  if (!*(_QWORD *)(v14 + 16))
    goto LABEL_34;
LABEL_15:
  v15 = *(_QWORD *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  v16 = *(_QWORD *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v15)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (!v16)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v8 != v15)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (v9 != v16)
  {
LABEL_56:
    __break(1u);
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  v17 = (void *)a5[4];
  if (!v17)
  {
LABEL_64:
    swift_bridgeObjectRetain();
    swift_bridgeObjectRelease();
    __break(1u);
    goto LABEL_65;
  }
  v5 = v8 * a4;
  if ((unsigned __int128)(v8 * (__int128)a4) >> 64 != (v8 * a4) >> 63)
    goto LABEL_57;
  v18 = a5[7];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  inited = swift_initStackObject();
  v20 = *(_OWORD *)(v13 + 1072);
  *(_OWORD *)(inited + 16) = v20;
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  *(_QWORD *)(inited + 32) = v17;
  *(_QWORD *)(inited + 40) = v9;
  *(_QWORD *)(inited + 48) = v5;
  *(_QWORD *)(inited + 56) = v18;
  *(_QWORD *)(inited + 64) = 0;
  if (!*(_QWORD *)(v14 + 16))
  {
LABEL_59:
    __break(1u);
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  v39 = v18;
  v8 = *(_QWORD *)(v14 + 32);
  if (!v8)
  {
LABEL_65:
    swift_setDeallocating();
    swift_bridgeObjectRetain();
    swift_arrayDestroy();
    swift_bridgeObjectRelease();
    __break(1u);
LABEL_66:
    swift_setDeallocating();
    swift_arrayDestroy();
    __break(1u);
  }
  v21 = *(_QWORD *)(v14 + 48);
  if (v21 < 0)
    goto LABEL_60;
  v13 = v21 * a4;
  if ((unsigned __int128)(v21 * (__int128)a4) >> 64 != (v21 * a4) >> 63)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  v38 = v17;
  v6 = *(_QWORD *)(v14 + 40);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  v36 = v9;
  v9 = *(_QWORD *)(v14 + 56);
  v37 = v20;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRelease();
  v22 = swift_initStackObject();
  *(_OWORD *)(v22 + 16) = v37;
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  LODWORD(v11) = a1;
  *(_QWORD *)(v22 + 32) = v8;
  *(_QWORD *)(v22 + 40) = v6;
  *(_QWORD *)(v22 + 48) = v13;
  *(_QWORD *)(v22 + 56) = v9;
  *(_QWORD *)(v22 + 64) = 0;
  if (BYTE4(a1) == 1)
  {
LABEL_35:
    v25 = *(float *)&v11;
    v24 = 1;
    v23 = a3;
    goto LABEL_37;
  }
  v23 = a3;
  if (BYTE4(a1))
  {
    v24 = a1 + 2;
    v25 = 0.0;
  }
  else
  {
    v24 = 0;
    v25 = *(float *)&a1;
  }
LABEL_37:
  GammaFunction = vImageCreateGammaFunction(v25, v24, 0);
  swift_release();
  src.data = v38;
  src.height = v36;
  src.width = v5;
  src.rowBytes = v39;
  dest.data = (void *)v8;
  dest.height = v6;
  dest.width = v13;
  dest.rowBytes = v9;
  vImageGamma_Planar8toPlanarF(&src, &dest, GammaFunction, 0);
  v33 = (void *)v23[4];
  if (!v33)
    goto LABEL_66;
  v34 = v23[7];
  swift_release();
  src.data = (void *)v8;
  src.height = v6;
  src.width = v13;
  src.rowBytes = v9;
  dest.data = v33;
  dest.height = v36;
  dest.width = v5;
  dest.rowBytes = v34;
  return vImageConvert_PlanarFtoPlanar8(&src, &dest, 1.0, 0.0, 0);
}

vImage_Error vImage.PixelBuffer<>._applyGamma<A, B>(_:intermediateBuffer:destination:widthMultiplier:pixelFormat:)(uint64_t a1, void **a2, uint64_t *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  void **v11;
  void *v14;
  uint64_t v15;
  void *v16;
  vImagePixelCount height;
  void *data;
  uint64_t v19;
  void *v20;
  vImagePixelCount v21;
  void *v22;
  uint64_t v23;
  uint64_t v24;
  void *v25;
  uint64_t v26;
  vImagePixelCount v27;
  uint64_t v28;
  size_t v29;
  size_t v30;
  uint64_t inited;
  uint64_t v32;
  void *v33;
  uint64_t v34;
  vImagePixelCount v35;
  vImagePixelCount v36;
  size_t v37;
  size_t v38;
  uint64_t v39;
  int v40;
  float v41;
  GammaFunction GammaFunction;
  uint64_t v43;
  void *v44;
  uint64_t v45;
  vImagePixelCount v46;
  vImagePixelCount v47;
  size_t v48;
  size_t v49;
  void *v51;
  size_t v52;
  vImagePixelCount v53;
  float v54;
  int v55;
  void *v56;
  void *v58;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v61;

  v61 = *MEMORY[0x1E0C80C00];
  v54 = *(float *)a1;
  v55 = *(unsigned __int8 *)(a1 + 4);
  v14 = *a2;
  v15 = *a3;
  v16 = *v11;
  dest.data = *v11;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&src);
  data = src.data;
  height = src.height;
  v58 = (void *)v15;
  type metadata accessor for vImage.PixelBuffer(0, a7, *(_QWORD *)(*(_QWORD *)(a10 + 8) + 8), v19);
  vImage.PixelBuffer.size.getter(&dest);
  v56 = (void *)v15;
  swift_bridgeObjectRelease();
  if (data != dest.data || height != dest.height)
  {
    __break(1u);
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v14)
  {
    v20 = v14;
  }
  else
  {
    dest.data = v16;
    vImage.PixelBuffer.size.getter(&src);
    *(_OWORD *)&dest.data = *(_OWORD *)&src.data;
    vImage.PixelBuffer<>.init(size:pixelFormat:)((uint64_t)&dest, a8, a11, (uint64_t *)&v58);
    v20 = v58;
  }
  dest.data = v16;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&src);
  v22 = src.data;
  v21 = src.height;
  v58 = v20;
  type metadata accessor for vImage.PixelBuffer(0, a8, *(_QWORD *)(*(_QWORD *)(a11 + 8) + 8), v23);
  vImage.PixelBuffer.size.getter(&dest);
  if (v22 != dest.data || v21 != dest.height)
    goto LABEL_24;
  src.data = v16;
  v24 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v24)
  {
LABEL_31:
    swift_bridgeObjectRelease();
    __break(1u);
    goto LABEL_32;
  }
  v25 = (void *)v24;
  src.data = v16;
  v26 = vImage.PixelBuffer.width.getter();
  v27 = v26 * a4;
  if ((unsigned __int128)(v26 * (__int128)a4) >> 64 != (v26 * a4) >> 63)
    goto LABEL_25;
  src.data = v16;
  v28 = vImage.PixelBuffer.height.getter();
  src.data = v16;
  vImage.PixelBuffer<>.vImageBuffer.getter();
  v30 = v29;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5E430;
  if (((v28 | v27) & 0x8000000000000000) != 0)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  *(_QWORD *)(inited + 32) = v25;
  *(_QWORD *)(inited + 40) = v28;
  v52 = v30;
  v53 = v28;
  *(_QWORD *)(inited + 48) = v27;
  *(_QWORD *)(inited + 56) = v30;
  *(_QWORD *)(inited + 64) = 0;
  src.data = v20;
  v32 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v32)
  {
LABEL_32:
    swift_setDeallocating();
    swift_arrayDestroy();
    swift_bridgeObjectRelease();
    __break(1u);
    goto LABEL_33;
  }
  v33 = (void *)v32;
  src.data = v20;
  v34 = vImage.PixelBuffer.width.getter();
  v35 = v34 * a4;
  if ((unsigned __int128)(v34 * (__int128)a4) >> 64 != (v34 * a4) >> 63)
    goto LABEL_27;
  v51 = v25;
  src.data = v20;
  v36 = vImage.PixelBuffer.height.getter();
  src.data = v20;
  vImage.PixelBuffer<>.vImageBuffer.getter();
  v38 = v37;
  swift_bridgeObjectRelease();
  v39 = swift_initStackObject();
  *(_OWORD *)(v39 + 16) = xmmword_1CAB5E430;
  if (((v36 | v35) & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  *(_QWORD *)(v39 + 32) = v33;
  *(_QWORD *)(v39 + 40) = v36;
  *(_QWORD *)(v39 + 48) = v35;
  *(_QWORD *)(v39 + 56) = v38;
  *(_QWORD *)(v39 + 64) = 0;
  if (v55 == 1)
  {
    v41 = v54;
    v40 = 1;
  }
  else if (v55)
  {
    v40 = LODWORD(v54) + 2;
    v41 = 0.0;
  }
  else
  {
    v40 = 0;
    v41 = v54;
  }
  GammaFunction = vImageCreateGammaFunction(v41, v40, 0);
  swift_release();
  src.data = v51;
  src.height = v53;
  src.width = v27;
  src.rowBytes = v52;
  dest.data = v33;
  dest.height = v36;
  dest.width = v35;
  dest.rowBytes = v38;
  vImageGamma_Planar8toPlanarF(&src, &dest, GammaFunction, 0);
  src.data = v56;
  v43 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v43)
  {
LABEL_33:
    swift_setDeallocating();
    swift_arrayDestroy();
    __break(1u);
  }
  v44 = (void *)v43;
  src.data = v56;
  v45 = vImage.PixelBuffer.width.getter();
  v46 = v45 * a4;
  if ((unsigned __int128)(v45 * (__int128)a4) >> 64 != (v45 * a4) >> 63)
    goto LABEL_29;
  src.data = v56;
  v47 = vImage.PixelBuffer.height.getter();
  src.data = v56;
  vImage.PixelBuffer<>.vImageBuffer.getter();
  v49 = v48;
  swift_release();
  if (((v47 | v46) & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  src.data = v33;
  src.height = v36;
  src.width = v35;
  src.rowBytes = v38;
  dest.data = v44;
  dest.height = v47;
  dest.width = v46;
  dest.rowBytes = v49;
  return vImageConvert_PlanarFtoPlanar8(&src, &dest, 1.0, 0.0, 0);
}

uint64_t vImage.PixelBuffer<>.applyGamma(_:intermediateBuffer:destination:)(unsigned int *a1, _QWORD *a2, _QWORD *a3, uint64_t a4, uint64_t (*a5)(unint64_t, _QWORD, _QWORD, uint64_t, _QWORD))
{
  _QWORD *v5;

  return a5(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a2, *a3, a4, *v5);
}

vImage_Error vImage.PixelBuffer<>.applyGamma(_:destination:)(uint64_t a1, void **a2, uint64_t a3, uint64_t a4)
{
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;

  v8 = *(_QWORD *)(a3 + 16);
  v9 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 24))(v8, a4);
  return vImage.PixelBuffer<>._applyGamma<A>(_:destination:widthMultiplier:pixelFormat:)(a1, a2, v9, v10, a3, v8, a4, a4);
}

vImage_Error vImage.PixelBuffer<>._applyGamma<A>(_:destination:widthMultiplier:pixelFormat:)(uint64_t a1, void **a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  void **v8;
  void *v12;
  void *v13;
  vImagePixelCount height;
  void *data;
  uint64_t v16;
  uint64_t v17;
  vImagePixelCount v18;
  vImagePixelCount v19;
  size_t v20;
  size_t v21;
  uint64_t inited;
  uint64_t v23;
  void *v24;
  uint64_t v25;
  vImagePixelCount v26;
  vImagePixelCount v27;
  size_t v28;
  size_t v29;
  int v30;
  float v31;
  GammaFunction GammaFunction;
  size_t v34;
  float v35;
  int v36;
  void *v37;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v40;

  v40 = *MEMORY[0x1E0C80C00];
  v35 = *(float *)a1;
  v36 = *(unsigned __int8 *)(a1 + 4);
  v12 = *a2;
  v13 = *v8;
  dest.data = *v8;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&src);
  data = src.data;
  height = src.height;
  type metadata accessor for vImage.PixelBuffer(0, a6, *(_QWORD *)(*(_QWORD *)(a8 + 8) + 8), v16);
  vImage.PixelBuffer.size.getter(&dest);
  swift_bridgeObjectRelease();
  if (data != dest.data || height != dest.height)
  {
    __break(1u);
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  src.data = v13;
  v37 = (void *)vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v37)
  {
LABEL_20:
    __break(1u);
LABEL_21:
    swift_setDeallocating();
    swift_arrayDestroy();
    __break(1u);
  }
  src.data = v13;
  v17 = vImage.PixelBuffer.width.getter();
  v18 = v17 * a3;
  if ((unsigned __int128)(v17 * (__int128)a3) >> 64 != (v17 * a3) >> 63)
    goto LABEL_16;
  src.data = v13;
  v19 = vImage.PixelBuffer.height.getter();
  src.data = v13;
  vImage.PixelBuffer<>.vImageBuffer.getter();
  v21 = v20;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5E430;
  if (((v19 | v18) & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  *(_QWORD *)(inited + 32) = v37;
  *(_QWORD *)(inited + 40) = v19;
  *(_QWORD *)(inited + 48) = v18;
  *(_QWORD *)(inited + 56) = v21;
  v34 = v21;
  *(_QWORD *)(inited + 64) = 0;
  src.data = v12;
  v23 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v23)
    goto LABEL_21;
  v24 = (void *)v23;
  src.data = v12;
  v25 = vImage.PixelBuffer.width.getter();
  v26 = v25 * a3;
  if ((unsigned __int128)(v25 * (__int128)a3) >> 64 != (v25 * a3) >> 63)
    goto LABEL_18;
  src.data = v12;
  v27 = vImage.PixelBuffer.height.getter();
  src.data = v12;
  vImage.PixelBuffer<>.vImageBuffer.getter();
  if (((v27 | v26) & 0x8000000000000000) != 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v29 = v28;
  if (v36 == 1)
  {
    v31 = v35;
    v30 = 1;
  }
  else if (v36)
  {
    v30 = LODWORD(v35) + 2;
    v31 = 0.0;
  }
  else
  {
    v30 = 0;
    v31 = v35;
  }
  GammaFunction = vImageCreateGammaFunction(v31, v30, 0);
  swift_release();
  src.data = v37;
  src.height = v19;
  src.width = v18;
  src.rowBytes = v34;
  dest.data = v24;
  dest.height = v27;
  dest.width = v26;
  dest.rowBytes = v29;
  return vImageGamma_PlanarF(&src, &dest, GammaFunction, 0);
}

uint64_t vImage.PixelBuffer<>.applyGamma(linearParameters:exponentialParameters:boundary:destination:)(float a1, float a2, float a3, float a4, float a5, float a6, float a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t (*v23)(uint64_t, uint64_t);
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t inited;
  uint64_t v30;
  uint64_t v31;
  void *v32;
  uint64_t v33;
  uint64_t v34;
  vImagePixelCount v35;
  vImagePixelCount v36;
  size_t v37;
  __int128 v38;
  uint64_t v40;
  vImage_Buffer dest;
  vImage_Buffer src;
  void *v43;
  vImagePixelCount v44;
  vImagePixelCount v45;
  size_t v46;
  uint64_t v47;
  float v48[2];
  float v49[4];
  uint64_t v50;

  v50 = *MEMORY[0x1E0C80C00];
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&src);
  vImage.PixelBuffer.size.getter(&dest);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  if (src.data != dest.data || src.height != dest.height)
  {
    __break(1u);
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  v19 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v19)
  {
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
  }
  v20 = v19;
  v21 = vImage.PixelBuffer.width.getter();
  v22 = *(_QWORD *)(a9 + 16);
  v23 = *(uint64_t (**)(uint64_t, uint64_t))(a10 + 24);
  v40 = a10;
  v24 = v23(v22, a10);
  v25 = v21 * v24;
  if ((unsigned __int128)(v21 * (__int128)v24) >> 64 != (v21 * v24) >> 63)
    goto LABEL_11;
  v26 = vImage.PixelBuffer.height.getter();
  vImage.PixelBuffer<>.vImageBuffer.getter();
  v28 = v27;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5E430;
  if ((v26 | v25) < 0)
  {
LABEL_12:
    __break(1u);
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  v30 = inited;
  *(_QWORD *)(inited + 32) = v20;
  *(_QWORD *)(inited + 40) = v26;
  *(_QWORD *)(inited + 48) = v25;
  *(_QWORD *)(inited + 56) = v28;
  *(_QWORD *)(inited + 64) = 0;
  v31 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v31)
    goto LABEL_16;
  v32 = (void *)v31;
  v33 = vImage.PixelBuffer.width.getter();
  v34 = v23(v22, v40);
  v35 = v33 * v34;
  if ((unsigned __int128)(v33 * (__int128)v34) >> 64 != (v33 * v34) >> 63)
    goto LABEL_13;
  v36 = vImage.PixelBuffer.height.getter();
  vImage.PixelBuffer<>.vImageBuffer.getter();
  if (((v36 | v35) & 0x8000000000000000) != 0)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  v43 = v32;
  v44 = v36;
  v45 = v35;
  v46 = v37;
  v47 = 0;
  v38 = *(_OWORD *)(v30 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v30 + 32);
  *(_OWORD *)&src.width = v38;
  dest.data = v32;
  dest.height = v36;
  dest.width = v35;
  dest.rowBytes = v37;
  v49[0] = a3;
  v49[1] = a4;
  v49[2] = a6;
  v48[0] = a1;
  v48[1] = a2;
  vImagePiecewiseGamma_PlanarF(&src, &dest, v49, a5, v48, a7, 0);
  swift_bridgeObjectRelease();
  return swift_arrayDestroy();
}

uint64_t vImage.PixelBuffer<>.applyGamma(linearParameters:exponentialParameters:boundary:destination:)(Pixel_8 a1, float a2, float a3, float a4, float a5, float a6, float a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t (*v23)(uint64_t, uint64_t);
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t inited;
  uint64_t v30;
  uint64_t v31;
  void *v32;
  uint64_t v33;
  uint64_t v34;
  vImagePixelCount v35;
  vImagePixelCount v36;
  size_t v37;
  __int128 v38;
  Pixel_8 v40;
  uint64_t v41;
  vImage_Buffer dest;
  vImage_Buffer src;
  void *v44;
  vImagePixelCount v45;
  vImagePixelCount v46;
  size_t v47;
  uint64_t v48;
  float v49[2];
  float v50[4];
  uint64_t v51;

  v51 = *MEMORY[0x1E0C80C00];
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&src);
  vImage.PixelBuffer.size.getter(&dest);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  if (src.data != dest.data || src.height != dest.height)
  {
    __break(1u);
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  v40 = a1;
  v19 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v19)
  {
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
  }
  v20 = v19;
  v21 = vImage.PixelBuffer.width.getter();
  v22 = *(_QWORD *)(a9 + 16);
  v23 = *(uint64_t (**)(uint64_t, uint64_t))(a10 + 24);
  v41 = a10;
  v24 = v23(v22, a10);
  v25 = v21 * v24;
  if ((unsigned __int128)(v21 * (__int128)v24) >> 64 != (v21 * v24) >> 63)
    goto LABEL_11;
  v26 = vImage.PixelBuffer.height.getter();
  vImage.PixelBuffer<>.vImageBuffer.getter();
  v28 = v27;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5E430;
  if ((v26 | v25) < 0)
  {
LABEL_12:
    __break(1u);
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  v30 = inited;
  *(_QWORD *)(inited + 32) = v20;
  *(_QWORD *)(inited + 40) = v26;
  *(_QWORD *)(inited + 48) = v25;
  *(_QWORD *)(inited + 56) = v28;
  *(_QWORD *)(inited + 64) = 0;
  v31 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v31)
    goto LABEL_16;
  v32 = (void *)v31;
  v33 = vImage.PixelBuffer.width.getter();
  v34 = v23(v22, v41);
  v35 = v33 * v34;
  if ((unsigned __int128)(v33 * (__int128)v34) >> 64 != (v33 * v34) >> 63)
    goto LABEL_13;
  v36 = vImage.PixelBuffer.height.getter();
  vImage.PixelBuffer<>.vImageBuffer.getter();
  if (((v36 | v35) & 0x8000000000000000) != 0)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  v44 = v32;
  v45 = v36;
  v46 = v35;
  v47 = v37;
  v48 = 0;
  v38 = *(_OWORD *)(v30 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v30 + 32);
  *(_OWORD *)&src.width = v38;
  dest.data = v32;
  dest.height = v36;
  dest.width = v35;
  dest.rowBytes = v37;
  v50[0] = a4;
  v50[1] = a5;
  v50[2] = a7;
  v49[0] = a2;
  v49[1] = a3;
  vImagePiecewiseGamma_Planar8(&src, &dest, v50, a6, v49, v40, 0);
  swift_bridgeObjectRelease();
  return swift_arrayDestroy();
}

uint64_t __swift_memcpy5_4(uint64_t result, int *a2)
{
  int v2;

  v2 = *a2;
  *(_BYTE *)(result + 4) = *((_BYTE *)a2 + 4);
  *(_DWORD *)result = v2;
  return result;
}

uint64_t getEnumTagSinglePayload for vImage.Gamma(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  int v4;

  if (!a2)
    return 0;
  if (a2 >= 0xFE && *(_BYTE *)(a1 + 5))
    return (*(_DWORD *)a1 + 254);
  v3 = *(unsigned __int8 *)(a1 + 4);
  if (v3 <= 2)
    v4 = -1;
  else
    v4 = v3 ^ 0xFF;
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for vImage.Gamma(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFD)
  {
    *(_BYTE *)(result + 4) = 0;
    *(_DWORD *)result = a2 - 254;
    if (a3 >= 0xFE)
      *(_BYTE *)(result + 5) = 1;
  }
  else
  {
    if (a3 >= 0xFE)
      *(_BYTE *)(result + 5) = 0;
    if (a2)
      *(_BYTE *)(result + 4) = -(char)a2;
  }
  return result;
}

uint64_t getEnumTag for vImage.Gamma(uint64_t a1)
{
  if (*(unsigned __int8 *)(a1 + 4) <= 1u)
    return *(unsigned __int8 *)(a1 + 4);
  else
    return (*(_DWORD *)a1 + 2);
}

uint64_t destructiveInjectEnumTag for vImage.Gamma(uint64_t result, unsigned int a2)
{
  if (a2 >= 2)
  {
    *(_DWORD *)result = a2 - 2;
    LOBYTE(a2) = 2;
  }
  *(_BYTE *)(result + 4) = a2;
  return result;
}

ValueMetadata *type metadata accessor for vImage.Gamma()
{
  return &type metadata for vImage.Gamma;
}

void *BNNS.FusedFullyConnectedParameters.init(weights:bias:)@<X0>(_OWORD *a1@<X0>, uint64_t a2@<X1>, void *a3@<X8>)
{
  __int128 v5;
  __int128 v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  _BYTE v11[184];
  _BYTE v12[184];
  _OWORD __src[23];

  outlined init with take of BNNSNDArrayDescriptor?(a2, (uint64_t)v11);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v11, (uint64_t)v12);
  v5 = a1[9];
  __src[8] = a1[8];
  __src[9] = v5;
  __src[10] = a1[10];
  v6 = a1[5];
  __src[4] = a1[4];
  __src[5] = v6;
  v7 = a1[7];
  __src[6] = a1[6];
  __src[7] = v7;
  v8 = a1[1];
  __src[0] = *a1;
  __src[1] = v8;
  v9 = a1[3];
  __src[2] = a1[2];
  __src[3] = v9;
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v12, (uint64_t)&__src[11]);
  return memcpy(a3, __src, 0x161uLL);
}

__n128 BNNS.FusedFullyConnectedParameters.weights.getter@<Q0>(uint64_t a1@<X8>)
{
  uint64_t v1;
  __int128 v2;
  __int128 v3;
  __int128 v4;
  __int128 v5;
  __n128 result;

  v2 = *(_OWORD *)(v1 + 144);
  *(_OWORD *)(a1 + 128) = *(_OWORD *)(v1 + 128);
  *(_OWORD *)(a1 + 144) = v2;
  *(_OWORD *)(a1 + 160) = *(_OWORD *)(v1 + 160);
  v3 = *(_OWORD *)(v1 + 80);
  *(_OWORD *)(a1 + 64) = *(_OWORD *)(v1 + 64);
  *(_OWORD *)(a1 + 80) = v3;
  v4 = *(_OWORD *)(v1 + 112);
  *(_OWORD *)(a1 + 96) = *(_OWORD *)(v1 + 96);
  *(_OWORD *)(a1 + 112) = v4;
  v5 = *(_OWORD *)(v1 + 16);
  *(_OWORD *)a1 = *(_OWORD *)v1;
  *(_OWORD *)(a1 + 16) = v5;
  result = *(__n128 *)(v1 + 48);
  *(_OWORD *)(a1 + 32) = *(_OWORD *)(v1 + 32);
  *(__n128 *)(a1 + 48) = result;
  return result;
}

__n128 BNNS.FusedFullyConnectedParameters.weights.setter(uint64_t a1)
{
  uint64_t v1;
  __int128 v2;
  __int128 v3;
  __int128 v4;
  __int128 v5;
  __n128 result;

  v2 = *(_OWORD *)(a1 + 144);
  *(_OWORD *)(v1 + 128) = *(_OWORD *)(a1 + 128);
  *(_OWORD *)(v1 + 144) = v2;
  *(_OWORD *)(v1 + 160) = *(_OWORD *)(a1 + 160);
  v3 = *(_OWORD *)(a1 + 80);
  *(_OWORD *)(v1 + 64) = *(_OWORD *)(a1 + 64);
  *(_OWORD *)(v1 + 80) = v3;
  v4 = *(_OWORD *)(a1 + 112);
  *(_OWORD *)(v1 + 96) = *(_OWORD *)(a1 + 96);
  *(_OWORD *)(v1 + 112) = v4;
  v5 = *(_OWORD *)(a1 + 16);
  *(_OWORD *)v1 = *(_OWORD *)a1;
  *(_OWORD *)(v1 + 16) = v5;
  result = *(__n128 *)(a1 + 48);
  *(_OWORD *)(v1 + 32) = *(_OWORD *)(a1 + 32);
  *(__n128 *)(v1 + 48) = result;
  return result;
}

uint64_t (*BNNS.FusedFullyConnectedParameters.weights.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedFullyConnectedParameters.bias.getter@<X0>(uint64_t a1@<X8>)
{
  uint64_t v1;
  _BYTE v4[184];

  outlined init with take of BNNSNDArrayDescriptor?(v1 + 176, (uint64_t)v4);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v4, a1);
}

uint64_t BNNS.FusedFullyConnectedParameters.bias.setter(uint64_t a1)
{
  uint64_t v1;

  return outlined init with take of BNNSNDArrayDescriptor?(a1, v1 + 176);
}

uint64_t (*BNNS.FusedFullyConnectedParameters.bias.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

double protocol witness for FusableLayerParametersWrapper.layerParameters(input:output:) in conformance BNNS.FusedFullyConnectedParameters@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, uint64_t *a3@<X8>)
{
  __int128 *v3;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  int v17;
  uint64_t v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  uint64_t v30;
  uint64_t v31;
  double result;
  int v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  _OWORD __src[33];
  __int128 v50;
  __int128 v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  _BYTE v61[93];
  _QWORD v62[20];
  int v63;
  uint64_t v64;
  int v65;

  v7 = v3[9];
  v58 = v3[8];
  v59 = v7;
  v60 = v3[10];
  v8 = v3[5];
  v54 = v3[4];
  v55 = v8;
  v9 = v3[7];
  v56 = v3[6];
  v57 = v9;
  v10 = v3[1];
  v50 = *v3;
  v51 = v10;
  v11 = v3[3];
  v52 = v3[2];
  v53 = v11;
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)(v3 + 11), (uint64_t)v61);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v61, (uint64_t)v62);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v62) == 1)
  {
    v47 = 0;
    v48 = 0;
    v45 = 0;
    v46 = 0;
    v43 = 0;
    v44 = 0;
    v41 = 0;
    v42 = 0;
    v39 = 0;
    v40 = 0;
    v37 = 0;
    v38 = 0;
    v36 = 0;
    v12 = 0;
    v13 = 0;
    v14 = 0;
    v15 = 0;
    v16 = 0;
    v17 = 0;
    v18 = 0;
    v34 = 0;
    v35 = 0;
  }
  else
  {
    v35 = v62[0];
    v47 = v62[2];
    v48 = v62[1];
    v45 = v62[4];
    v46 = v62[3];
    v43 = v62[6];
    v44 = v62[5];
    v41 = v62[8];
    v42 = v62[7];
    v39 = v62[10];
    v40 = v62[9];
    v37 = v62[12];
    v38 = v62[11];
    v36 = v62[13];
    v12 = v62[14];
    v13 = v62[15];
    v14 = v62[16];
    v15 = v62[17];
    v34 = v62[18];
    v16 = v62[19];
    v17 = v63;
    v18 = v64;
    v33 = v65;
  }
  v19 = a1[9];
  __src[8] = a1[8];
  __src[9] = v19;
  v20 = a1[5];
  __src[4] = a1[4];
  __src[5] = v20;
  v21 = a1[7];
  __src[6] = a1[6];
  __src[7] = v21;
  v22 = a1[1];
  __src[0] = *a1;
  __src[1] = v22;
  v23 = a1[3];
  __src[2] = a1[2];
  __src[3] = v23;
  __src[18] = v57;
  __src[19] = v58;
  __src[20] = v59;
  __src[21] = v60;
  __src[14] = v53;
  __src[15] = v54;
  v24 = a1[10];
  __src[16] = v55;
  __src[17] = v56;
  __src[10] = v24;
  __src[11] = v50;
  __src[12] = v51;
  __src[13] = v52;
  v25 = a2[9];
  __src[30] = a2[8];
  __src[31] = v25;
  __src[32] = a2[10];
  v26 = a2[5];
  __src[26] = a2[4];
  __src[27] = v26;
  v27 = a2[7];
  __src[28] = a2[6];
  __src[29] = v27;
  v28 = a2[1];
  __src[22] = *a2;
  __src[23] = v28;
  v29 = a2[3];
  __src[24] = a2[2];
  __src[25] = v29;
  type metadata accessor for BNNSLayerParametersFullyConnected(0);
  a3[3] = v30;
  a3[4] = (uint64_t)&protocol witness table for BNNSLayerParametersFullyConnected;
  v31 = swift_allocObject();
  *a3 = v31;
  memcpy((void *)(v31 + 16), __src, 0x210uLL);
  *(_QWORD *)(v31 + 544) = v35;
  *(_QWORD *)(v31 + 552) = v48;
  *(_QWORD *)(v31 + 560) = v47;
  *(_QWORD *)(v31 + 568) = v46;
  *(_QWORD *)(v31 + 576) = v45;
  *(_QWORD *)(v31 + 584) = v44;
  *(_QWORD *)(v31 + 592) = v43;
  *(_QWORD *)(v31 + 600) = v42;
  *(_QWORD *)(v31 + 608) = v41;
  *(_QWORD *)(v31 + 616) = v40;
  *(_QWORD *)(v31 + 624) = v39;
  *(_QWORD *)(v31 + 632) = v38;
  *(_QWORD *)(v31 + 640) = v37;
  *(_QWORD *)(v31 + 648) = v36;
  *(_QWORD *)(v31 + 656) = v12;
  *(_QWORD *)(v31 + 664) = v13;
  *(_QWORD *)(v31 + 672) = v14;
  *(_QWORD *)(v31 + 680) = v15;
  *(_QWORD *)(v31 + 688) = v34;
  *(_QWORD *)(v31 + 696) = v16;
  *(_DWORD *)(v31 + 704) = v17;
  *(_QWORD *)(v31 + 708) = v18;
  *(_DWORD *)(v31 + 716) = v33;
  *(_QWORD *)(v31 + 720) = 0x7FC0000000000000;
  *(_QWORD *)(v31 + 728) = 0x17FC00000;
  result = 0.0;
  *(_OWORD *)(v31 + 736) = 0u;
  *(_OWORD *)(v31 + 752) = 0u;
  return result;
}

void *__swift_memcpy353_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x161uLL);
}

uint64_t getEnumTagSinglePayload for BNNS.FusedFullyConnectedParameters(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 353))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for BNNS.FusedFullyConnectedParameters(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_QWORD *)(result + 344) = 0;
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_BYTE *)(result + 352) = 0;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(_QWORD *)result = (a2 - 1);
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 353) = v3;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.FusedFullyConnectedParameters()
{
  return &type metadata for BNNS.FusedFullyConnectedParameters;
}

uint64_t sub_1CAAF6E58()
{
  return swift_deallocObject();
}

uint64_t vImage.BufferReference.__deallocating_deinit()
{
  uint64_t v0;
  uint64_t result;

  result = *(_QWORD *)(v0 + 16);
  if (result)
  {
    MEMORY[0x1D1794DA4](result, -1, -1);
    return swift_deallocClassInstance();
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t type metadata accessor for vImage.BufferReference()
{
  return objc_opt_self();
}

uint64_t destroy for vImage.BufferWrapper()
{
  return swift_release();
}

uint64_t initializeWithCopy for vImage.BufferWrapper(uint64_t a1, uint64_t a2)
{
  __int128 v3;

  v3 = *(_OWORD *)(a2 + 16);
  *(_OWORD *)a1 = *(_OWORD *)a2;
  *(_OWORD *)(a1 + 16) = v3;
  *(_QWORD *)(a1 + 32) = *(_QWORD *)(a2 + 32);
  swift_retain();
  return a1;
}

_QWORD *assignWithCopy for vImage.BufferWrapper(_QWORD *a1, _QWORD *a2)
{
  *a1 = *a2;
  a1[1] = a2[1];
  a1[2] = a2[2];
  a1[3] = a2[3];
  a1[4] = a2[4];
  swift_retain();
  swift_release();
  return a1;
}

uint64_t assignWithTake for vImage.BufferWrapper(uint64_t a1, uint64_t a2)
{
  __int128 v3;

  v3 = *(_OWORD *)(a2 + 16);
  *(_OWORD *)a1 = *(_OWORD *)a2;
  *(_OWORD *)(a1 + 16) = v3;
  *(_QWORD *)(a1 + 32) = *(_QWORD *)(a2 + 32);
  swift_release();
  return a1;
}

uint64_t getEnumTagSinglePayload for vImage.BufferWrapper(uint64_t a1, unsigned int a2)
{
  unint64_t v3;
  int v4;

  if (!a2)
    return 0;
  if (a2 >= 0x7FFFFFFF && *(_BYTE *)(a1 + 40))
    return (*(_DWORD *)a1 + 0x7FFFFFFF);
  v3 = *(_QWORD *)(a1 + 32);
  if (v3 >= 0xFFFFFFFF)
    LODWORD(v3) = -1;
  v4 = v3 - 1;
  if (v4 < 0)
    v4 = -1;
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for vImage.BufferWrapper(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0x7FFFFFFE)
  {
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_QWORD *)result = a2 - 0x7FFFFFFF;
    if (a3 >= 0x7FFFFFFF)
      *(_BYTE *)(result + 40) = 1;
  }
  else
  {
    if (a3 >= 0x7FFFFFFF)
      *(_BYTE *)(result + 40) = 0;
    if (a2)
      *(_QWORD *)(result + 32) = a2;
  }
  return result;
}

ValueMetadata *type metadata accessor for vImage.BufferWrapper()
{
  return &type metadata for vImage.BufferWrapper;
}

_QWORD *specialized vImage.BufferWrapper.init(copying:bitsPerPixel:)@<X0>(void *a1@<X0>, vImagePixelCount a2@<X1>, vImagePixelCount a3@<X2>, size_t a4@<X3>, unint64_t a5@<X4>, _QWORD *a6@<X8>)
{
  CGSize Size;
  vImagePixelCount v13;
  vImagePixelCount v14;
  size_t v15;
  void *data;
  vImagePixelCount height;
  vImagePixelCount width;
  size_t rowBytes;
  _QWORD *result;
  vImage_Flags v21[3];
  vImage_Buffer buf;
  uint64_t v23;

  v23 = *MEMORY[0x1E0C80C00];
  buf.data = a1;
  buf.height = a2;
  buf.width = a3;
  buf.rowBytes = a4;
  Size = vImageBuffer_GetSize(&buf);
  if ((a5 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  if (HIDWORD(a5))
    goto LABEL_5;
  buf.data = specialized vImage_Buffer.init(size:bitsPerPixel:)(a5, Size.width, Size.height);
  buf.height = v13;
  buf.width = v14;
  buf.rowBytes = v15;
  v21[0] = 0;
  vImage_Buffer.copy(destinationBuffer:pixelSize:flags:)(&buf, a5 >> 3, v21, a1, a2, a3, a4);
  data = buf.data;
  height = buf.height;
  width = buf.width;
  rowBytes = buf.rowBytes;
  type metadata accessor for vImage.BufferReference();
  result = (_QWORD *)swift_allocObject();
  result[2] = data;
  result[3] = height;
  result[4] = width;
  result[5] = rowBytes;
  *a6 = data;
  a6[1] = height;
  a6[2] = width;
  a6[3] = rowBytes;
  a6[4] = result;
  return result;
}

_QWORD *specialized vImage.BufferWrapper.init(cgImage:format:)@<X0>(CGImageRef image@<X0>, vImage_CGImageFormat *a2@<X1>, _QWORD *a3@<X8>)
{
  vImage_Error v4;
  uint64_t v5;
  char *v6;
  char *v7;
  char v8;
  _QWORD *result;
  void *data;
  vImagePixelCount height;
  vImagePixelCount width;
  size_t rowBytes;
  char v14;
  vImage_Buffer buf;
  uint64_t v16;

  v16 = *MEMORY[0x1E0C80C00];
  memset(&buf, 0, sizeof(buf));
  v4 = vImageBuffer_InitWithCGImage(&buf, a2, 0, image, 0);
  if (v4)
  {
    v5 = v4;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v7 = v6;
    vImage.Error.init(rawValue:)(v5, &v14);
    v8 = v14;
    if (v14 == 20)
      v8 = 11;
    *v7 = v8;
    return (_QWORD *)swift_willThrow();
  }
  else
  {
    data = buf.data;
    height = buf.height;
    width = buf.width;
    rowBytes = buf.rowBytes;
    type metadata accessor for vImage.BufferReference();
    result = (_QWORD *)swift_allocObject();
    result[2] = data;
    result[3] = height;
    result[4] = width;
    result[5] = rowBytes;
    *a3 = data;
    a3[1] = height;
    a3[2] = width;
    a3[3] = rowBytes;
    a3[4] = result;
  }
  return result;
}

_QWORD *specialized vImage.BufferWrapper.init(cvPixelBuffer:cvImageFormat:cgImageFormat:)@<X0>(CVPixelBufferRef cvPixelBuffer@<X0>, vImageCVImageFormatRef cvImageFormat@<X1>, vImage_CGImageFormat *desiredFormat@<X2>, _QWORD *a4@<X8>)
{
  vImage_Error v5;
  uint64_t v6;
  char *v7;
  char *v8;
  char v9;
  _QWORD *result;
  void *data;
  vImagePixelCount height;
  vImagePixelCount width;
  size_t rowBytes;
  char v15;
  vImage_Buffer buffer;
  uint64_t v17;

  v17 = *MEMORY[0x1E0C80C00];
  memset(&buffer, 0, sizeof(buffer));
  v5 = vImageBuffer_InitWithCVPixelBuffer(&buffer, desiredFormat, cvPixelBuffer, cvImageFormat, 0, 0);
  if (v5)
  {
    v6 = v5;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v8 = v7;
    vImage.Error.init(rawValue:)(v6, &v15);
    v9 = v15;
    if (v15 == 20)
      v9 = 11;
    *v8 = v9;
    return (_QWORD *)swift_willThrow();
  }
  else
  {
    data = buffer.data;
    height = buffer.height;
    width = buffer.width;
    rowBytes = buffer.rowBytes;
    type metadata accessor for vImage.BufferReference();
    result = (_QWORD *)swift_allocObject();
    result[2] = data;
    result[3] = height;
    result[4] = width;
    result[5] = rowBytes;
    *a4 = data;
    a4[1] = height;
    a4[2] = width;
    a4[3] = rowBytes;
    a4[4] = result;
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error;
  if (!lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImage.Error, &type metadata for vImage.Error);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error);
  }
  return result;
}

{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error;
  if (!lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImage.Error, &type metadata for vImage.Error);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error);
  }
  return result;
}

{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error;
  if (!lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImage.Error, &type metadata for vImage.Error);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error);
  }
  return result;
}

uint64_t vImage.PixelBuffer<>.init(interleavedBuffer:)@<X0>(uint64_t *a1@<X0>, uint64_t *a2@<X8>)
{
  vImagePixelCount v2;
  uint64_t v4;
  vImagePixelCount v5;
  uint64_t v6;
  void *v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  _QWORD *v14;
  uint64_t result;
  _QWORD v16[3];

  v4 = *a1;
  if (!*(_QWORD *)(*a1 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  v5 = *(_QWORD *)(v4 + 48);
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v2 = *(_QWORD *)(v4 + 40);
  if ((v2 & 0x8000000000000000) != 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (!v5)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (v2)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    v6 = swift_allocObject();
    *(_OWORD *)(v6 + 16) = xmmword_1CAB5E430;
    v7 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v2, 0x80u);
    v9 = v8;
    v11 = v10;
    v13 = v12;
    type metadata accessor for vImage.BufferReference();
    v14 = (_QWORD *)swift_allocObject();
    v14[2] = v7;
    v14[3] = v9;
    v14[4] = v11;
    v14[5] = v13;
    *(_QWORD *)(v6 + 32) = v7;
    *(_QWORD *)(v6 + 40) = v9;
    *(_QWORD *)(v6 + 48) = v11;
    *(_QWORD *)(v6 + 56) = v13;
    *(_QWORD *)(v6 + 64) = v14;
    v16[0] = v6;
    v16[1] = v4;
    vImage.PixelBuffer<>.convert(to:)((uint64_t)v16);
    result = swift_bridgeObjectRelease();
    *a2 = v6;
    return result;
  }
LABEL_11:
  __break(1u);

  result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

vImage_Error vImage.PixelBuffer<>.convert(to:channelOrdering:)(uint64_t a1, _BYTE *a2)
{
  uint64_t v2;
  _QWORD *v3;
  vImagePixelCount v4;
  vImagePixelCount v5;
  _QWORD *v6;
  uint64_t v7;
  uint64_t v8;
  void *v9;
  size_t v10;
  void *v11;
  size_t v12;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v16;

  v16 = *MEMORY[0x1E0C80C00];
  v3 = *(_QWORD **)v2;
  if (!*(_QWORD *)(*(_QWORD *)v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  v4 = v3[6];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = v3[5];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v6 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v7 = v6[6];
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v8 = v6[5];
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
LABEL_28:
    __break(1u);
  }
  if (v5 != v8)
    goto LABEL_28;
  v9 = (void *)v3[4];
  v10 = v3[7];
  src.data = v9;
  src.height = v5;
  src.width = v4;
  src.rowBytes = v10;
  v11 = (void *)v6[4];
  v12 = v6[7];
  dest.data = v11;
  dest.height = v5;
  dest.width = v4;
  dest.rowBytes = v12;
  if ((*a2 & 1) != 0)
    return vImageConvert_RGBAFFFFtoRGBFFF(&src, &dest, 0);
  else
    return vImageConvert_ARGBFFFFtoRGBFFF(&src, &dest, 0);
}

{
  uint64_t v2;
  _QWORD *v3;
  vImagePixelCount v4;
  vImagePixelCount v5;
  _QWORD *v6;
  uint64_t v7;
  uint64_t v8;
  void *v9;
  size_t v10;
  void *v11;
  size_t v12;
  vImage_Buffer v14;
  vImage_Buffer v15;
  uint64_t v16;

  v16 = *MEMORY[0x1E0C80C00];
  v3 = *(_QWORD **)v2;
  if (!*(_QWORD *)(*(_QWORD *)v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  v4 = v3[6];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = v3[5];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v6 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v7 = v6[6];
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v8 = v6[5];
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
LABEL_28:
    __break(1u);
  }
  if (v5 != v8)
    goto LABEL_28;
  v9 = (void *)v3[4];
  v10 = v3[7];
  v15.data = v9;
  v15.height = v5;
  v15.width = v4;
  v15.rowBytes = v10;
  v11 = (void *)v6[4];
  v12 = v6[7];
  v14.data = v11;
  v14.height = v5;
  v14.width = v4;
  v14.rowBytes = v12;
  if ((*a2 & 1) != 0)
    return vImageConvert_RGBA8888toRGB888(&v15, &v14, 0);
  else
    return vImageConvert_ARGB8888toRGB888(&v15, &v14, 0);
}

vImage_Error vImage.PixelBuffer<>.interleave(planarSourceBuffers:)(_QWORD *a1)
{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  void *v19;
  size_t v20;
  size_t v21;
  size_t v22;
  void *v23;
  size_t v24;
  vImage_Buffer dest;
  vImage_Buffer srcB;
  vImage_Buffer srcG;
  vImage_Buffer srcR;
  vImage_Buffer srcA;
  uint64_t v31;

  v31 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_37;
  }
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v3)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v4)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v5 = (_QWORD *)a1[4];
  if (!v5[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v6)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v6)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v7)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  v8 = (_QWORD *)a1[5];
  if (!v8[2])
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  v9 = v8[6];
  if (v9 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  v10 = v8[5];
  if (v10 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v9)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v10)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v9)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v4 != v10)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  v11 = a1[6];
  if (!*(_QWORD *)(v11 + 16))
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  v12 = *(_QWORD *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  v13 = *(_QWORD *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v12)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (!v13)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v3 != v12)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  if (v4 != v13)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  v14 = a1[7];
  if (!*(_QWORD *)(v14 + 16))
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  v15 = *(_QWORD *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  v16 = *(_QWORD *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v15)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (!v16)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v3 != v15)
  {
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
  }
  if (v4 != v16)
    goto LABEL_69;
  v17 = (void *)v5[4];
  v18 = v5[7];
  srcA.data = v17;
  srcA.height = v4;
  srcA.width = v3;
  srcA.rowBytes = v18;
  v19 = (void *)v8[4];
  v20 = v8[7];
  srcR.data = v19;
  srcR.height = v4;
  srcR.width = v3;
  srcR.rowBytes = v20;
  v21 = *(_QWORD *)(v11 + 56);
  srcG.data = *(void **)(v11 + 32);
  srcG.height = v4;
  srcG.width = v3;
  srcG.rowBytes = v21;
  v22 = *(_QWORD *)(v14 + 56);
  srcB.data = *(void **)(v14 + 32);
  srcB.height = v4;
  srcB.width = v3;
  srcB.rowBytes = v22;
  v23 = (void *)v2[4];
  v24 = v2[7];
  dest.data = v23;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v24;
  return vImageConvert_PlanarFtoARGBFFFF(&srcA, &srcR, &srcG, &srcB, &dest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  void *v19;
  size_t v20;
  size_t v21;
  size_t v22;
  void *v23;
  size_t v24;
  vImage_Buffer argbDest;
  vImage_Buffer bSrc;
  vImage_Buffer gSrc;
  vImage_Buffer rSrc;
  vImage_Buffer aSrc;
  uint64_t v31;

  v31 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_37;
  }
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v3)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v4)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v5 = (_QWORD *)a1[4];
  if (!v5[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v6)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v6)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v7)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  v8 = (_QWORD *)a1[5];
  if (!v8[2])
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  v9 = v8[6];
  if (v9 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  v10 = v8[5];
  if (v10 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v9)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v10)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v9)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v4 != v10)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  v11 = a1[6];
  if (!*(_QWORD *)(v11 + 16))
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  v12 = *(_QWORD *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  v13 = *(_QWORD *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v12)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (!v13)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v3 != v12)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  if (v4 != v13)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  v14 = a1[7];
  if (!*(_QWORD *)(v14 + 16))
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  v15 = *(_QWORD *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  v16 = *(_QWORD *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v15)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (!v16)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v3 != v15)
  {
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
  }
  if (v4 != v16)
    goto LABEL_69;
  v17 = (void *)v5[4];
  v18 = v5[7];
  aSrc.data = v17;
  aSrc.height = v4;
  aSrc.width = v3;
  aSrc.rowBytes = v18;
  v19 = (void *)v8[4];
  v20 = v8[7];
  rSrc.data = v19;
  rSrc.height = v4;
  rSrc.width = v3;
  rSrc.rowBytes = v20;
  v21 = *(_QWORD *)(v11 + 56);
  gSrc.data = *(void **)(v11 + 32);
  gSrc.height = v4;
  gSrc.width = v3;
  gSrc.rowBytes = v21;
  v22 = *(_QWORD *)(v14 + 56);
  bSrc.data = *(void **)(v14 + 32);
  bSrc.height = v4;
  bSrc.width = v3;
  bSrc.rowBytes = v22;
  v23 = (void *)v2[4];
  v24 = v2[7];
  argbDest.data = v23;
  argbDest.height = v4;
  argbDest.width = v3;
  argbDest.rowBytes = v24;
  return vImageConvert_Planar16UtoARGB16U(&aSrc, &rSrc, &gSrc, &bSrc, &argbDest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  void *v19;
  size_t v20;
  size_t v21;
  size_t v22;
  void *v23;
  size_t v24;
  vImage_Buffer dest;
  vImage_Buffer srcB;
  vImage_Buffer srcG;
  vImage_Buffer srcR;
  vImage_Buffer srcA;
  uint64_t v31;

  v31 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_37;
  }
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v3)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v4)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v5 = (_QWORD *)a1[4];
  if (!v5[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v6)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v6)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v7)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  v8 = (_QWORD *)a1[5];
  if (!v8[2])
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  v9 = v8[6];
  if (v9 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  v10 = v8[5];
  if (v10 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v9)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v10)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v9)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v4 != v10)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  v11 = a1[6];
  if (!*(_QWORD *)(v11 + 16))
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  v12 = *(_QWORD *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  v13 = *(_QWORD *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v12)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (!v13)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v3 != v12)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  if (v4 != v13)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  v14 = a1[7];
  if (!*(_QWORD *)(v14 + 16))
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  v15 = *(_QWORD *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  v16 = *(_QWORD *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v15)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (!v16)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v3 != v15)
  {
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
  }
  if (v4 != v16)
    goto LABEL_69;
  v17 = (void *)v5[4];
  v18 = v5[7];
  srcA.data = v17;
  srcA.height = v4;
  srcA.width = v3;
  srcA.rowBytes = v18;
  v19 = (void *)v8[4];
  v20 = v8[7];
  srcR.data = v19;
  srcR.height = v4;
  srcR.width = v3;
  srcR.rowBytes = v20;
  v21 = *(_QWORD *)(v11 + 56);
  srcG.data = *(void **)(v11 + 32);
  srcG.height = v4;
  srcG.width = v3;
  srcG.rowBytes = v21;
  v22 = *(_QWORD *)(v14 + 56);
  srcB.data = *(void **)(v14 + 32);
  srcB.height = v4;
  srcB.width = v3;
  srcB.rowBytes = v22;
  v23 = (void *)v2[4];
  v24 = v2[7];
  dest.data = v23;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v24;
  return vImageConvert_Planar8toARGB8888(&srcA, &srcR, &srcG, &srcB, &dest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  void *v14;
  size_t v15;
  void *v16;
  size_t v17;
  size_t v18;
  void *v19;
  size_t v20;
  vImage_Buffer rgbDest;
  vImage_Buffer planarBlue;
  vImage_Buffer planarGreen;
  vImage_Buffer planarRed;
  uint64_t v26;

  v26 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 3)
  {
    __break(1u);
    goto LABEL_30;
  }
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v3)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v4)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v5 = (_QWORD *)a1[4];
  if (!v5[2])
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v7)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v3 != v6)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (v4 != v7)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v8 = (_QWORD *)a1[5];
  if (!v8[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v9 = v8[6];
  if (v9 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  v10 = v8[5];
  if (v10 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v10)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v9)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v10)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  v11 = a1[6];
  if (!*(_QWORD *)(v11 + 16))
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  v12 = *(_QWORD *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  v13 = *(_QWORD *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v13)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v12)
  {
LABEL_54:
    __break(1u);
LABEL_55:
    __break(1u);
  }
  if (v4 != v13)
    goto LABEL_55;
  v14 = (void *)v5[4];
  v15 = v5[7];
  planarRed.data = v14;
  planarRed.height = v4;
  planarRed.width = v3;
  planarRed.rowBytes = v15;
  v16 = (void *)v8[4];
  v17 = v8[7];
  planarGreen.data = v16;
  planarGreen.height = v4;
  planarGreen.width = v3;
  planarGreen.rowBytes = v17;
  v18 = *(_QWORD *)(v11 + 56);
  planarBlue.data = *(void **)(v11 + 32);
  planarBlue.height = v4;
  planarBlue.width = v3;
  planarBlue.rowBytes = v18;
  v19 = (void *)v2[4];
  v20 = v2[7];
  rgbDest.data = v19;
  rgbDest.height = v4;
  rgbDest.width = v3;
  rgbDest.rowBytes = v20;
  return vImageConvert_Planar8toRGB888(&planarRed, &planarGreen, &planarBlue, &rgbDest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  void *v14;
  size_t v15;
  void *v16;
  size_t v17;
  size_t v18;
  void *v19;
  size_t v20;
  vImage_Buffer rgbDest;
  vImage_Buffer planarBlue;
  vImage_Buffer planarGreen;
  vImage_Buffer planarRed;
  uint64_t v26;

  v26 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 3)
  {
    __break(1u);
    goto LABEL_30;
  }
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v3)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v4)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v5 = (_QWORD *)a1[4];
  if (!v5[2])
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v7)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v3 != v6)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (v4 != v7)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v8 = (_QWORD *)a1[5];
  if (!v8[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v9 = v8[6];
  if (v9 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  v10 = v8[5];
  if (v10 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v10)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v9)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v10)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  v11 = a1[6];
  if (!*(_QWORD *)(v11 + 16))
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  v12 = *(_QWORD *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  v13 = *(_QWORD *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v13)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v12)
  {
LABEL_54:
    __break(1u);
LABEL_55:
    __break(1u);
  }
  if (v4 != v13)
    goto LABEL_55;
  v14 = (void *)v5[4];
  v15 = v5[7];
  planarRed.data = v14;
  planarRed.height = v4;
  planarRed.width = v3;
  planarRed.rowBytes = v15;
  v16 = (void *)v8[4];
  v17 = v8[7];
  planarGreen.data = v16;
  planarGreen.height = v4;
  planarGreen.width = v3;
  planarGreen.rowBytes = v17;
  v18 = *(_QWORD *)(v11 + 56);
  planarBlue.data = *(void **)(v11 + 32);
  planarBlue.height = v4;
  planarBlue.width = v3;
  planarBlue.rowBytes = v18;
  v19 = (void *)v2[4];
  v20 = v2[7];
  rgbDest.data = v19;
  rgbDest.height = v4;
  rgbDest.width = v3;
  rgbDest.rowBytes = v20;
  return vImageConvert_PlanarFtoRGBFFF(&planarRed, &planarGreen, &planarBlue, &rgbDest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  void *v19;
  size_t v20;
  size_t v21;
  size_t v22;
  void *v23;
  size_t v24;
  vImage_Buffer argbDest;
  vImage_Buffer bSrc;
  vImage_Buffer gSrc;
  vImage_Buffer rSrc;
  vImage_Buffer aSrc;
  uint64_t v31;

  v31 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_37;
  }
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v3)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v4)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v5 = (_QWORD *)a1[4];
  if (!v5[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v6)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v6)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v7)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  v8 = (_QWORD *)a1[5];
  if (!v8[2])
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  v9 = v8[6];
  if (v9 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  v10 = v8[5];
  if (v10 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v9)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v10)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v9)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v4 != v10)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  v11 = a1[6];
  if (!*(_QWORD *)(v11 + 16))
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  v12 = *(_QWORD *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  v13 = *(_QWORD *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v12)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (!v13)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v3 != v12)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  if (v4 != v13)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  v14 = a1[7];
  if (!*(_QWORD *)(v14 + 16))
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  v15 = *(_QWORD *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  v16 = *(_QWORD *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v15)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (!v16)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v3 != v15)
  {
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
  }
  if (v4 != v16)
    goto LABEL_69;
  v17 = (void *)v5[4];
  v18 = v5[7];
  aSrc.data = v17;
  aSrc.height = v4;
  aSrc.width = v3;
  aSrc.rowBytes = v18;
  v19 = (void *)v8[4];
  v20 = v8[7];
  rSrc.data = v19;
  rSrc.height = v4;
  rSrc.width = v3;
  rSrc.rowBytes = v20;
  v21 = *(_QWORD *)(v11 + 56);
  gSrc.data = *(void **)(v11 + 32);
  gSrc.height = v4;
  gSrc.width = v3;
  gSrc.rowBytes = v21;
  v22 = *(_QWORD *)(v14 + 56);
  bSrc.data = *(void **)(v14 + 32);
  bSrc.height = v4;
  bSrc.width = v3;
  bSrc.rowBytes = v22;
  v23 = (void *)v2[4];
  v24 = v2[7];
  argbDest.data = v23;
  argbDest.height = v4;
  argbDest.width = v3;
  argbDest.rowBytes = v24;
  return vImageConvert_Planar16UtoARGB16U(&aSrc, &rSrc, &gSrc, &bSrc, &argbDest, 0);
}

vImage_Error vImage.PixelBuffer<>.deinterleave(planarDestinationBuffers:)(_QWORD *a1)
{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  void *v19;
  size_t v20;
  size_t v21;
  size_t v22;
  void *v23;
  size_t v24;
  vImage_Buffer srcARGB;
  vImage_Buffer destB;
  vImage_Buffer destG;
  vImage_Buffer destR;
  vImage_Buffer destA;
  uint64_t v31;

  v31 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_37;
  }
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v3)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v4)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v5 = (_QWORD *)a1[4];
  if (!v5[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v6)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v6)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v7)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  v8 = (_QWORD *)a1[5];
  if (!v8[2])
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  v9 = v8[6];
  if (v9 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  v10 = v8[5];
  if (v10 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v9)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v10)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v9)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v4 != v10)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  v11 = a1[6];
  if (!*(_QWORD *)(v11 + 16))
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  v12 = *(_QWORD *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  v13 = *(_QWORD *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v12)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (!v13)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v3 != v12)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  if (v4 != v13)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  v14 = a1[7];
  if (!*(_QWORD *)(v14 + 16))
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  v15 = *(_QWORD *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  v16 = *(_QWORD *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v15)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (!v16)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v3 != v15)
  {
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
  }
  if (v4 != v16)
    goto LABEL_69;
  v17 = (void *)v5[4];
  v18 = v5[7];
  destA.data = v17;
  destA.height = v4;
  destA.width = v3;
  destA.rowBytes = v18;
  v19 = (void *)v8[4];
  v20 = v8[7];
  destR.data = v19;
  destR.height = v4;
  destR.width = v3;
  destR.rowBytes = v20;
  v21 = *(_QWORD *)(v11 + 56);
  destG.data = *(void **)(v11 + 32);
  destG.height = v4;
  destG.width = v3;
  destG.rowBytes = v21;
  v22 = *(_QWORD *)(v14 + 56);
  destB.data = *(void **)(v14 + 32);
  destB.height = v4;
  destB.width = v3;
  destB.rowBytes = v22;
  v23 = (void *)v2[4];
  v24 = v2[7];
  srcARGB.data = v23;
  srcARGB.height = v4;
  srcARGB.width = v3;
  srcARGB.rowBytes = v24;
  return vImageConvert_ARGBFFFFtoPlanarF(&srcARGB, &destA, &destR, &destG, &destB, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  void *v19;
  size_t v20;
  size_t v21;
  size_t v22;
  void *v23;
  size_t v24;
  vImage_Buffer argbSrc;
  vImage_Buffer bDest;
  vImage_Buffer gDest;
  vImage_Buffer rDest;
  vImage_Buffer aDest;
  uint64_t v31;

  v31 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_37;
  }
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v3)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v4)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v5 = (_QWORD *)a1[4];
  if (!v5[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v6)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v6)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v7)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  v8 = (_QWORD *)a1[5];
  if (!v8[2])
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  v9 = v8[6];
  if (v9 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  v10 = v8[5];
  if (v10 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v9)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v10)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v9)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v4 != v10)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  v11 = a1[6];
  if (!*(_QWORD *)(v11 + 16))
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  v12 = *(_QWORD *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  v13 = *(_QWORD *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v12)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (!v13)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v3 != v12)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  if (v4 != v13)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  v14 = a1[7];
  if (!*(_QWORD *)(v14 + 16))
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  v15 = *(_QWORD *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  v16 = *(_QWORD *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v15)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (!v16)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v3 != v15)
  {
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
  }
  if (v4 != v16)
    goto LABEL_69;
  v17 = (void *)v5[4];
  v18 = v5[7];
  aDest.data = v17;
  aDest.height = v4;
  aDest.width = v3;
  aDest.rowBytes = v18;
  v19 = (void *)v8[4];
  v20 = v8[7];
  rDest.data = v19;
  rDest.height = v4;
  rDest.width = v3;
  rDest.rowBytes = v20;
  v21 = *(_QWORD *)(v11 + 56);
  gDest.data = *(void **)(v11 + 32);
  gDest.height = v4;
  gDest.width = v3;
  gDest.rowBytes = v21;
  v22 = *(_QWORD *)(v14 + 56);
  bDest.data = *(void **)(v14 + 32);
  bDest.height = v4;
  bDest.width = v3;
  bDest.rowBytes = v22;
  v23 = (void *)v2[4];
  v24 = v2[7];
  argbSrc.data = v23;
  argbSrc.height = v4;
  argbSrc.width = v3;
  argbSrc.rowBytes = v24;
  return vImageConvert_ARGB16UtoPlanar16U(&argbSrc, &aDest, &rDest, &gDest, &bDest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  void *v19;
  size_t v20;
  size_t v21;
  size_t v22;
  void *v23;
  size_t v24;
  vImage_Buffer srcARGB;
  vImage_Buffer destB;
  vImage_Buffer destG;
  vImage_Buffer destR;
  vImage_Buffer destA;
  uint64_t v31;

  v31 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_37;
  }
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v3)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v4)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v5 = (_QWORD *)a1[4];
  if (!v5[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v6)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v6)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v7)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  v8 = (_QWORD *)a1[5];
  if (!v8[2])
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  v9 = v8[6];
  if (v9 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  v10 = v8[5];
  if (v10 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v9)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v10)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v9)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v4 != v10)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  v11 = a1[6];
  if (!*(_QWORD *)(v11 + 16))
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  v12 = *(_QWORD *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  v13 = *(_QWORD *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v12)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (!v13)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v3 != v12)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  if (v4 != v13)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  v14 = a1[7];
  if (!*(_QWORD *)(v14 + 16))
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  v15 = *(_QWORD *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  v16 = *(_QWORD *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v15)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (!v16)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v3 != v15)
  {
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
  }
  if (v4 != v16)
    goto LABEL_69;
  v17 = (void *)v5[4];
  v18 = v5[7];
  destA.data = v17;
  destA.height = v4;
  destA.width = v3;
  destA.rowBytes = v18;
  v19 = (void *)v8[4];
  v20 = v8[7];
  destR.data = v19;
  destR.height = v4;
  destR.width = v3;
  destR.rowBytes = v20;
  v21 = *(_QWORD *)(v11 + 56);
  destG.data = *(void **)(v11 + 32);
  destG.height = v4;
  destG.width = v3;
  destG.rowBytes = v21;
  v22 = *(_QWORD *)(v14 + 56);
  destB.data = *(void **)(v14 + 32);
  destB.height = v4;
  destB.width = v3;
  destB.rowBytes = v22;
  v23 = (void *)v2[4];
  v24 = v2[7];
  srcARGB.data = v23;
  srcARGB.height = v4;
  srcARGB.width = v3;
  srcARGB.rowBytes = v24;
  return vImageConvert_ARGB8888toPlanar8(&srcARGB, &destA, &destR, &destG, &destB, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  void *v14;
  size_t v15;
  void *v16;
  size_t v17;
  size_t v18;
  void *v19;
  size_t v20;
  vImage_Buffer rgbSrc;
  vImage_Buffer blueDest;
  vImage_Buffer greenDest;
  vImage_Buffer redDest;
  uint64_t v26;

  v26 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 3)
  {
    __break(1u);
    goto LABEL_30;
  }
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v3)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v4)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v5 = (_QWORD *)a1[4];
  if (!v5[2])
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v7)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v3 != v6)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (v4 != v7)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v8 = (_QWORD *)a1[5];
  if (!v8[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v9 = v8[6];
  if (v9 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  v10 = v8[5];
  if (v10 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v10)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v9)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v10)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  v11 = a1[6];
  if (!*(_QWORD *)(v11 + 16))
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  v12 = *(_QWORD *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  v13 = *(_QWORD *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v13)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v12)
  {
LABEL_54:
    __break(1u);
LABEL_55:
    __break(1u);
  }
  if (v4 != v13)
    goto LABEL_55;
  v14 = (void *)v5[4];
  v15 = v5[7];
  redDest.data = v14;
  redDest.height = v4;
  redDest.width = v3;
  redDest.rowBytes = v15;
  v16 = (void *)v8[4];
  v17 = v8[7];
  greenDest.data = v16;
  greenDest.height = v4;
  greenDest.width = v3;
  greenDest.rowBytes = v17;
  v18 = *(_QWORD *)(v11 + 56);
  blueDest.data = *(void **)(v11 + 32);
  blueDest.height = v4;
  blueDest.width = v3;
  blueDest.rowBytes = v18;
  v19 = (void *)v2[4];
  v20 = v2[7];
  rgbSrc.data = v19;
  rgbSrc.height = v4;
  rgbSrc.width = v3;
  rgbSrc.rowBytes = v20;
  return vImageConvert_RGB888toPlanar8(&rgbSrc, &redDest, &greenDest, &blueDest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  void *v14;
  size_t v15;
  void *v16;
  size_t v17;
  size_t v18;
  void *v19;
  size_t v20;
  vImage_Buffer rgbSrc;
  vImage_Buffer blueDest;
  vImage_Buffer greenDest;
  vImage_Buffer redDest;
  uint64_t v26;

  v26 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 3)
  {
    __break(1u);
    goto LABEL_30;
  }
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v3)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v4)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v5 = (_QWORD *)a1[4];
  if (!v5[2])
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v7)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v3 != v6)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (v4 != v7)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v8 = (_QWORD *)a1[5];
  if (!v8[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v9 = v8[6];
  if (v9 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  v10 = v8[5];
  if (v10 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v10)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v9)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v10)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  v11 = a1[6];
  if (!*(_QWORD *)(v11 + 16))
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  v12 = *(_QWORD *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  v13 = *(_QWORD *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v13)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v12)
  {
LABEL_54:
    __break(1u);
LABEL_55:
    __break(1u);
  }
  if (v4 != v13)
    goto LABEL_55;
  v14 = (void *)v5[4];
  v15 = v5[7];
  redDest.data = v14;
  redDest.height = v4;
  redDest.width = v3;
  redDest.rowBytes = v15;
  v16 = (void *)v8[4];
  v17 = v8[7];
  greenDest.data = v16;
  greenDest.height = v4;
  greenDest.width = v3;
  greenDest.rowBytes = v17;
  v18 = *(_QWORD *)(v11 + 56);
  blueDest.data = *(void **)(v11 + 32);
  blueDest.height = v4;
  blueDest.width = v3;
  blueDest.rowBytes = v18;
  v19 = (void *)v2[4];
  v20 = v2[7];
  rgbSrc.data = v19;
  rgbSrc.height = v4;
  rgbSrc.width = v3;
  rgbSrc.rowBytes = v20;
  return vImageConvert_RGBFFFtoPlanarF(&rgbSrc, &redDest, &greenDest, &blueDest, 0);
}

{
  uint64_t v1;
  _QWORD *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  _QWORD *v5;
  uint64_t v6;
  uint64_t v7;
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  void *v19;
  size_t v20;
  size_t v21;
  size_t v22;
  void *v23;
  size_t v24;
  vImage_Buffer argbSrc;
  vImage_Buffer bDest;
  vImage_Buffer gDest;
  vImage_Buffer rDest;
  vImage_Buffer aDest;
  uint64_t v31;

  v31 = *MEMORY[0x1E0C80C00];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_37;
  }
  v2 = *(_QWORD **)v1;
  if (!*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v3)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v4)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v5 = (_QWORD *)a1[4];
  if (!v5[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v6 = v5[6];
  if (v6 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  v7 = v5[5];
  if (v7 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v6)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v6)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v7)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  v8 = (_QWORD *)a1[5];
  if (!v8[2])
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  v9 = v8[6];
  if (v9 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  v10 = v8[5];
  if (v10 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v9)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v10)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v9)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v4 != v10)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  v11 = a1[6];
  if (!*(_QWORD *)(v11 + 16))
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  v12 = *(_QWORD *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  v13 = *(_QWORD *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v12)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (!v13)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v3 != v12)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  if (v4 != v13)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  v14 = a1[7];
  if (!*(_QWORD *)(v14 + 16))
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  v15 = *(_QWORD *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  v16 = *(_QWORD *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v15)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (!v16)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v3 != v15)
  {
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
  }
  if (v4 != v16)
    goto LABEL_69;
  v17 = (void *)v5[4];
  v18 = v5[7];
  aDest.data = v17;
  aDest.height = v4;
  aDest.width = v3;
  aDest.rowBytes = v18;
  v19 = (void *)v8[4];
  v20 = v8[7];
  rDest.data = v19;
  rDest.height = v4;
  rDest.width = v3;
  rDest.rowBytes = v20;
  v21 = *(_QWORD *)(v11 + 56);
  gDest.data = *(void **)(v11 + 32);
  gDest.height = v4;
  gDest.width = v3;
  gDest.rowBytes = v21;
  v22 = *(_QWORD *)(v14 + 56);
  bDest.data = *(void **)(v14 + 32);
  bDest.height = v4;
  bDest.width = v3;
  bDest.rowBytes = v22;
  v23 = (void *)v2[4];
  v24 = v2[7];
  argbSrc.data = v23;
  argbSrc.height = v4;
  argbSrc.width = v3;
  argbSrc.rowBytes = v24;
  return vImageConvert_ARGB16UtoPlanar16U(&argbSrc, &aDest, &rDest, &gDest, &bDest, 0);
}

uint64_t vImage.PixelBuffer<>.deinterleave(destination:)(uint64_t *a1)
{
  _QWORD *v1;

  v1 = (_QWORD *)specialized vImage.PixelBuffer<>.pixelBuffers.getter(*a1);
  vImage.PixelBuffer<>.deinterleave(planarDestinationBuffers:)(v1);
  return swift_bridgeObjectRelease();
}

{
  _QWORD *v1;

  v1 = (_QWORD *)specialized vImage.PixelBuffer<>.pixelBuffers.getter(*a1);
  vImage.PixelBuffer<>.deinterleave(planarDestinationBuffers:)(v1);
  return swift_bridgeObjectRelease();
}

{
  _QWORD *v1;

  v1 = (_QWORD *)specialized vImage.PixelBuffer<>.pixelBuffers.getter(*a1);
  vImage.PixelBuffer<>.deinterleave(planarDestinationBuffers:)(v1);
  return swift_bridgeObjectRelease();
}

{
  _QWORD *v1;

  v1 = (_QWORD *)specialized vImage.PixelBuffer<>.pixelBuffers.getter(*a1);
  vImage.PixelBuffer<>.deinterleave(planarDestinationBuffers:)(v1);
  return swift_bridgeObjectRelease();
}

vImage_Error vImage.PixelBuffer<>.overwriteChannels(withScalar:)(Pixel_8 a1)
{
  uint64_t *v1;
  uint64_t v2;
  __int128 v3;
  vImage_Buffer v5;
  uint64_t v6;

  v6 = *MEMORY[0x1E0C80C00];
  v2 = *v1;
  if (!*(_QWORD *)(*v1 + 16))
    __break(1u);
  v3 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&v5.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&v5.width = v3;
  return vImageOverwriteChannelsWithScalar_Planar8(a1, &v5, 0);
}

vImage_Error vImage.PixelBuffer<>.overwriteChannels(_:withScalar:destination:)(uint64_t a1, Pixel_8 a2, uint64_t *a3)
{
  uint64_t *v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  Pixel_8 v10;
  int64_t v12;
  int8x16_t *data;
  unsigned __int8 *v14;
  int v15;
  char v16;
  unint64_t v17;
  unint64_t v18;
  unint64_t v19;
  unint64_t v20;
  uint8_t v21;
  int8x16_t *v22;
  int8x16_t v23;
  unint64_t v24;
  int8x16_t v25;
  int8x16_t v26;
  unint64_t v27;
  unint64_t v28;
  int8x8_t v29;
  int8x8_t *v30;
  unint64_t v31;
  int8x8_t v32;
  uint64_t v33;
  unint64_t v34;
  __int8 *v35;
  char v36;
  __int128 v37;
  __int128 v38;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v43;

  v43 = *MEMORY[0x1E0C80C00];
  v4 = *v3;
  if (!*(_QWORD *)(*v3 + 16))
    goto LABEL_42;
  v5 = *(_QWORD *)(v4 + 48);
  if (v5 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  v6 = *(_QWORD *)(v4 + 40);
  if (v6 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v5)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v6)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  v7 = *a3;
  if (!*(_QWORD *)(*a3 + 16))
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  v8 = *(_QWORD *)(v7 + 48);
  if (v8 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  v9 = *(_QWORD *)(v7 + 40);
  if (v9 < 0)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v8)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (!v9)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v5 != v8)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (v6 != v9)
  {
LABEL_53:
    __break(1u);
LABEL_54:
    __break(1u);
  }
  v10 = a2;
  v12 = *(_QWORD *)(a1 + 16);
  if (v12)
  {
    src.data = (void *)MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v12, 0);
    data = (int8x16_t *)src.data;
    v14 = (unsigned __int8 *)(a1 + 32);
    do
    {
      v15 = *v14++;
      v16 = 3 - v15;
      if (((3 - v15) & 0xFFFFFF00) != 0)
        goto LABEL_40;
      src.data = data;
      v18 = data[1].u64[0];
      v17 = data[1].u64[1];
      v19 = v18 + 1;
      if (v18 >= v17 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v17 > 1), v18 + 1, 1);
        data = (int8x16_t *)src.data;
      }
      data[1].i64[0] = v19;
      data[2].i8[v18] = 1 << (v16 & 7);
      --v12;
    }
    while (v12);
    v10 = a2;
LABEL_21:
    if (v19 < 8)
    {
      v20 = 0;
      v21 = 0;
LABEL_32:
      v34 = v19 - v20;
      v35 = &data[2].i8[v20];
      do
      {
        v36 = *v35++;
        v21 |= v36;
        --v34;
      }
      while (v34);
LABEL_34:
      swift_bridgeObjectRelease();
      if (v21 > 0xFu)
      {
LABEL_41:
        __break(1u);
LABEL_42:
        __break(1u);
        goto LABEL_43;
      }
      if (*(_QWORD *)(v4 + 16))
        goto LABEL_36;
LABEL_39:
      __break(1u);
LABEL_40:
      __break(1u);
      goto LABEL_41;
    }
    if (v19 >= 0x20)
    {
      v20 = v19 & 0xFFFFFFFFFFFFFFE0;
      v22 = data + 3;
      v23 = 0uLL;
      v24 = v19 & 0xFFFFFFFFFFFFFFE0;
      v25 = 0uLL;
      do
      {
        v23 = vorrq_s8(v22[-1], v23);
        v25 = vorrq_s8(*v22, v25);
        v22 += 2;
        v24 -= 32;
      }
      while (v24);
      v26 = vorrq_s8(v25, v23);
      *(int8x8_t *)v26.i8 = vorr_s8(*(int8x8_t *)v26.i8, (int8x8_t)*(_OWORD *)&vextq_s8(v26, v26, 8uLL));
      v27 = v26.i64[0] | HIDWORD(v26.i64[0]) | ((unint64_t)(v26.i64[0] | HIDWORD(v26.i64[0])) >> 16);
      v21 = v27 | BYTE1(v27);
      if (v19 == v20)
        goto LABEL_34;
      if ((v19 & 0x18) == 0)
        goto LABEL_32;
    }
    else
    {
      v21 = 0;
      v20 = 0;
    }
    v28 = v20;
    v20 = v19 & 0xFFFFFFFFFFFFFFF8;
    v29 = (int8x8_t)v21;
    v30 = (int8x8_t *)&data[2].i8[v28];
    v31 = v28 - (v19 & 0xFFFFFFFFFFFFFFF8);
    do
    {
      v32 = *v30++;
      v29 = vorr_s8(v32, v29);
      v31 += 8;
    }
    while (v31);
    v33 = *(_QWORD *)&v29 | HIDWORD(*(_QWORD *)&v29) | ((*(_QWORD *)&v29 | HIDWORD(*(_QWORD *)&v29)) >> 16);
    v21 = v33 | BYTE1(v33);
    if (v19 == v20)
      goto LABEL_34;
    goto LABEL_32;
  }
  data = (int8x16_t *)MEMORY[0x1E0DEE9D8];
  v19 = *(_QWORD *)(MEMORY[0x1E0DEE9D8] + 16);
  if (v19)
    goto LABEL_21;
  swift_bridgeObjectRelease();
  v21 = 0;
  if (!*(_QWORD *)(v4 + 16))
    goto LABEL_39;
LABEL_36:
  v37 = *(_OWORD *)(v4 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v4 + 32);
  *(_OWORD *)&src.width = v37;
  if (!*(_QWORD *)(v7 + 16))
    goto LABEL_54;
  v38 = *(_OWORD *)(v7 + 48);
  *(_OWORD *)&dest.data = *(_OWORD *)(v7 + 32);
  *(_OWORD *)&dest.width = v38;
  return vImageOverwriteChannelsWithScalar_ARGB8888(v10, &src, &dest, v21, 0);
}

vImage_Error vImage.PixelBuffer<>.overwriteChannels(_:withPixel:destination:)(uint64_t a1, uint8_t a2, uint8_t a3, uint8_t a4, uint8_t a5, uint64_t *a6)
{
  uint64_t *v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint8_t v13;
  uint8_t v14;
  uint8_t v15;
  uint8_t v16;
  int64_t v18;
  int8x16_t *data;
  unsigned __int8 *v20;
  int v21;
  char v22;
  unint64_t v23;
  unint64_t v24;
  unint64_t v25;
  unint64_t v26;
  uint8_t v27;
  int8x16_t *v28;
  int8x16_t v29;
  unint64_t v30;
  int8x16_t v31;
  int8x16_t v32;
  unint64_t v33;
  unint64_t v34;
  int8x8_t v35;
  int8x8_t *v36;
  unint64_t v37;
  int8x8_t v38;
  uint64_t v39;
  unint64_t v40;
  __int8 *v41;
  char v42;
  __int128 v43;
  __int128 v44;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint8_t v52[8];
  uint64_t v53;

  v53 = *MEMORY[0x1E0C80C00];
  v7 = *v6;
  if (!*(_QWORD *)(*v6 + 16))
    goto LABEL_41;
  v8 = *(_QWORD *)(v7 + 48);
  if (v8 < 0)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v9 = *(_QWORD *)(v7 + 40);
  if (v9 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!v8)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v10 = *a6;
  if (!*(_QWORD *)(*a6 + 16))
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  v11 = *(_QWORD *)(v10 + 48);
  if (v11 < 0)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  v12 = *(_QWORD *)(v10 + 40);
  if (v12 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (!v11)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v12)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (v8 != v11)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v9 != v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  v13 = a5;
  v14 = a4;
  v15 = a3;
  v16 = a2;
  v18 = *(_QWORD *)(a1 + 16);
  if (v18)
  {
    src.data = (void *)MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v18, 0);
    data = (int8x16_t *)src.data;
    v20 = (unsigned __int8 *)(a1 + 32);
    while (1)
    {
      v21 = *v20++;
      v22 = 3 - v21;
      if (((3 - v21) & 0xFFFFFF00) != 0)
        break;
      src.data = data;
      v24 = data[1].u64[0];
      v23 = data[1].u64[1];
      v25 = v24 + 1;
      if (v24 >= v23 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v23 > 1), v24 + 1, 1);
        data = (int8x16_t *)src.data;
      }
      data[1].i64[0] = v25;
      data[2].i8[v24] = 1 << (v22 & 7);
      if (!--v18)
      {
        v14 = a4;
        v13 = a5;
        v16 = a2;
        v15 = a3;
        goto LABEL_21;
      }
    }
    __break(1u);
    goto LABEL_40;
  }
  data = (int8x16_t *)MEMORY[0x1E0DEE9D8];
  v25 = *(_QWORD *)(MEMORY[0x1E0DEE9D8] + 16);
  if (!v25)
  {
    swift_bridgeObjectRelease();
    v27 = 0;
    goto LABEL_35;
  }
LABEL_21:
  if (v25 < 8)
  {
    v26 = 0;
    v27 = 0;
    goto LABEL_32;
  }
  if (v25 >= 0x20)
  {
    v26 = v25 & 0xFFFFFFFFFFFFFFE0;
    v28 = data + 3;
    v29 = 0uLL;
    v30 = v25 & 0xFFFFFFFFFFFFFFE0;
    v31 = 0uLL;
    do
    {
      v29 = vorrq_s8(v28[-1], v29);
      v31 = vorrq_s8(*v28, v31);
      v28 += 2;
      v30 -= 32;
    }
    while (v30);
    v32 = vorrq_s8(v31, v29);
    *(int8x8_t *)v32.i8 = vorr_s8(*(int8x8_t *)v32.i8, (int8x8_t)*(_OWORD *)&vextq_s8(v32, v32, 8uLL));
    v33 = v32.i64[0] | HIDWORD(v32.i64[0]) | ((unint64_t)(v32.i64[0] | HIDWORD(v32.i64[0])) >> 16);
    v27 = v33 | BYTE1(v33);
    if (v25 == v26)
      goto LABEL_34;
    if ((v25 & 0x18) == 0)
    {
LABEL_32:
      v40 = v25 - v26;
      v41 = &data[2].i8[v26];
      do
      {
        v42 = *v41++;
        v27 |= v42;
        --v40;
      }
      while (v40);
      goto LABEL_34;
    }
  }
  else
  {
    v27 = 0;
    v26 = 0;
  }
  v34 = v26;
  v26 = v25 & 0xFFFFFFFFFFFFFFF8;
  v35 = (int8x8_t)v27;
  v36 = (int8x8_t *)&data[2].i8[v34];
  v37 = v34 - (v25 & 0xFFFFFFFFFFFFFFF8);
  do
  {
    v38 = *v36++;
    v35 = vorr_s8(v38, v35);
    v37 += 8;
  }
  while (v37);
  v39 = *(_QWORD *)&v35 | HIDWORD(*(_QWORD *)&v35) | ((*(_QWORD *)&v35 | HIDWORD(*(_QWORD *)&v35)) >> 16);
  v27 = v39 | BYTE1(v39);
  if (v25 != v26)
    goto LABEL_32;
LABEL_34:
  swift_bridgeObjectRelease();
  if (v27 > 0xFu)
  {
LABEL_40:
    __break(1u);
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
LABEL_35:
  v52[0] = v16;
  v52[1] = v15;
  v52[2] = v14;
  v52[3] = v13;
  if (!*(_QWORD *)(v7 + 16))
  {
LABEL_53:
    __break(1u);
LABEL_54:
    __break(1u);
  }
  v43 = *(_OWORD *)(v7 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v7 + 32);
  *(_OWORD *)&src.width = v43;
  if (!*(_QWORD *)(v10 + 16))
    goto LABEL_54;
  v44 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&dest.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&dest.width = v44;
  return vImageOverwriteChannelsWithPixel_ARGB8888(v52, &src, &dest, v27, 0);
}

uint64_t vImage.PixelBuffer<>.overwriteChannels(_:withInterleavedBuffer:destination:)(uint64_t a1, uint64_t *a2, uint64_t *a3)
{
  return vImage.PixelBuffer<>.overwriteChannels(_:withInterleavedBuffer:destination:)(a1, a2, a3, MEMORY[0x1E0C8D750]);
}

{
  return vImage.PixelBuffer<>.overwriteChannels(_:withInterleavedBuffer:destination:)(a1, a2, a3, MEMORY[0x1E0C8D758]);
}

uint64_t vImage.PixelBuffer<>.overwriteChannels(_:withPlanarBuffer:destination:)(uint64_t a1, uint64_t *a2, uint64_t *a3)
{
  return vImage.PixelBuffer<>.overwriteChannels(_:withInterleavedBuffer:destination:)(a1, a2, a3, MEMORY[0x1E0C8D4E8]);
}

{
  return vImage.PixelBuffer<>.overwriteChannels(_:withInterleavedBuffer:destination:)(a1, a2, a3, MEMORY[0x1E0C8D4F0]);
}

vImage_Error vImage.PixelBuffer<>.overwriteChannels(withScalar:)(Pixel_16F a1)
{
  uint64_t *v1;
  uint64_t v2;
  __int128 v3;
  vImage_Buffer v5;
  uint64_t v6;

  v6 = *MEMORY[0x1E0C80C00];
  v2 = *v1;
  if (!*(_QWORD *)(*v1 + 16))
    __break(1u);
  v3 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&v5.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&v5.width = v3;
  return vImageOverwriteChannelsWithScalar_Planar16F(a1, &v5, 0);
}

vImage_Error vImage.PixelBuffer<>.overwriteChannels(_:withPixel:destination:)(uint64_t a1, uint16_t a2, uint16_t a3, uint16_t a4, uint16_t a5, uint64_t *a6)
{
  uint64_t *v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint16_t v13;
  uint16_t v14;
  uint16_t v15;
  uint16_t v16;
  int64_t v18;
  int8x16_t *data;
  unsigned __int8 *v20;
  int v21;
  char v22;
  unint64_t v23;
  unint64_t v24;
  unint64_t v25;
  unint64_t v26;
  uint8_t v27;
  int8x16_t *v28;
  int8x16_t v29;
  unint64_t v30;
  int8x16_t v31;
  int8x16_t v32;
  unint64_t v33;
  unint64_t v34;
  int8x8_t v35;
  int8x8_t *v36;
  unint64_t v37;
  int8x8_t v38;
  uint64_t v39;
  unint64_t v40;
  __int8 *v41;
  char v42;
  __int128 v43;
  __int128 v44;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint16_t v52[4];
  uint64_t v53;

  v53 = *MEMORY[0x1E0C80C00];
  v7 = *v6;
  if (!*(_QWORD *)(*v6 + 16))
    goto LABEL_41;
  v8 = *(_QWORD *)(v7 + 48);
  if (v8 < 0)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v9 = *(_QWORD *)(v7 + 40);
  if (v9 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!v8)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v10 = *a6;
  if (!*(_QWORD *)(*a6 + 16))
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  v11 = *(_QWORD *)(v10 + 48);
  if (v11 < 0)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  v12 = *(_QWORD *)(v10 + 40);
  if (v12 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (!v11)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v12)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (v8 != v11)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v9 != v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  v13 = a5;
  v14 = a4;
  v15 = a3;
  v16 = a2;
  v18 = *(_QWORD *)(a1 + 16);
  if (v18)
  {
    src.data = (void *)MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v18, 0);
    data = (int8x16_t *)src.data;
    v20 = (unsigned __int8 *)(a1 + 32);
    while (1)
    {
      v21 = *v20++;
      v22 = 3 - v21;
      if (((3 - v21) & 0xFFFFFF00) != 0)
        break;
      src.data = data;
      v24 = data[1].u64[0];
      v23 = data[1].u64[1];
      v25 = v24 + 1;
      if (v24 >= v23 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v23 > 1), v24 + 1, 1);
        data = (int8x16_t *)src.data;
      }
      data[1].i64[0] = v25;
      data[2].i8[v24] = 1 << (v22 & 7);
      if (!--v18)
      {
        v14 = a4;
        v13 = a5;
        v16 = a2;
        v15 = a3;
        goto LABEL_21;
      }
    }
    __break(1u);
    goto LABEL_40;
  }
  data = (int8x16_t *)MEMORY[0x1E0DEE9D8];
  v25 = *(_QWORD *)(MEMORY[0x1E0DEE9D8] + 16);
  if (!v25)
  {
    swift_bridgeObjectRelease();
    v27 = 0;
    goto LABEL_35;
  }
LABEL_21:
  if (v25 < 8)
  {
    v26 = 0;
    v27 = 0;
    goto LABEL_32;
  }
  if (v25 >= 0x20)
  {
    v26 = v25 & 0xFFFFFFFFFFFFFFE0;
    v28 = data + 3;
    v29 = 0uLL;
    v30 = v25 & 0xFFFFFFFFFFFFFFE0;
    v31 = 0uLL;
    do
    {
      v29 = vorrq_s8(v28[-1], v29);
      v31 = vorrq_s8(*v28, v31);
      v28 += 2;
      v30 -= 32;
    }
    while (v30);
    v32 = vorrq_s8(v31, v29);
    *(int8x8_t *)v32.i8 = vorr_s8(*(int8x8_t *)v32.i8, (int8x8_t)*(_OWORD *)&vextq_s8(v32, v32, 8uLL));
    v33 = v32.i64[0] | HIDWORD(v32.i64[0]) | ((unint64_t)(v32.i64[0] | HIDWORD(v32.i64[0])) >> 16);
    v27 = v33 | BYTE1(v33);
    if (v25 == v26)
      goto LABEL_34;
    if ((v25 & 0x18) == 0)
    {
LABEL_32:
      v40 = v25 - v26;
      v41 = &data[2].i8[v26];
      do
      {
        v42 = *v41++;
        v27 |= v42;
        --v40;
      }
      while (v40);
      goto LABEL_34;
    }
  }
  else
  {
    v27 = 0;
    v26 = 0;
  }
  v34 = v26;
  v26 = v25 & 0xFFFFFFFFFFFFFFF8;
  v35 = (int8x8_t)v27;
  v36 = (int8x8_t *)&data[2].i8[v34];
  v37 = v34 - (v25 & 0xFFFFFFFFFFFFFFF8);
  do
  {
    v38 = *v36++;
    v35 = vorr_s8(v38, v35);
    v37 += 8;
  }
  while (v37);
  v39 = *(_QWORD *)&v35 | HIDWORD(*(_QWORD *)&v35) | ((*(_QWORD *)&v35 | HIDWORD(*(_QWORD *)&v35)) >> 16);
  v27 = v39 | BYTE1(v39);
  if (v25 != v26)
    goto LABEL_32;
LABEL_34:
  swift_bridgeObjectRelease();
  if (v27 > 0xFu)
  {
LABEL_40:
    __break(1u);
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
LABEL_35:
  v52[0] = v16;
  v52[1] = v15;
  v52[2] = v14;
  v52[3] = v13;
  if (!*(_QWORD *)(v7 + 16))
  {
LABEL_53:
    __break(1u);
LABEL_54:
    __break(1u);
  }
  v43 = *(_OWORD *)(v7 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v7 + 32);
  *(_OWORD *)&src.width = v43;
  if (!*(_QWORD *)(v10 + 16))
    goto LABEL_54;
  v44 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&dest.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&dest.width = v44;
  return vImageOverwriteChannelsWithPixel_ARGB16U(v52, &src, &dest, v27, 0);
}

vImage_Error vImage.PixelBuffer<>.overwriteChannels(withScalar:)(Pixel_F a1)
{
  uint64_t *v1;
  uint64_t v2;
  __int128 v3;
  vImage_Buffer v5;
  uint64_t v6;

  v6 = *MEMORY[0x1E0C80C00];
  v2 = *v1;
  if (!*(_QWORD *)(*v1 + 16))
    __break(1u);
  v3 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&v5.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&v5.width = v3;
  return vImageOverwriteChannelsWithScalar_PlanarF(a1, &v5, 0);
}

vImage_Error vImage.PixelBuffer<>.overwriteChannels(_:withScalar:destination:)(uint64_t a1, uint64_t *a2, Pixel_F a3)
{
  uint64_t *v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  int64_t v12;
  int8x16_t *data;
  unsigned __int8 *v14;
  int v15;
  char v16;
  unint64_t v17;
  unint64_t v18;
  unint64_t v19;
  unint64_t v20;
  uint8_t v21;
  int8x16_t *v22;
  int8x16_t v23;
  unint64_t v24;
  int8x16_t v25;
  int8x16_t v26;
  unint64_t v27;
  unint64_t v28;
  int8x8_t v29;
  int8x8_t *v30;
  unint64_t v31;
  int8x8_t v32;
  uint64_t v33;
  unint64_t v34;
  __int8 *v35;
  char v36;
  __int128 v37;
  __int128 v38;
  vImage_Buffer v40;
  vImage_Buffer src;
  uint64_t v42;

  v42 = *MEMORY[0x1E0C80C00];
  v4 = *v3;
  if (!*(_QWORD *)(*v3 + 16))
    goto LABEL_42;
  v5 = *(_QWORD *)(v4 + 48);
  if (v5 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  v6 = *(_QWORD *)(v4 + 40);
  if (v6 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v5)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v6)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  v7 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  v8 = *(_QWORD *)(v7 + 48);
  if (v8 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  v9 = *(_QWORD *)(v7 + 40);
  if (v9 < 0)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v8)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (!v9)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v5 != v8)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (v6 != v9)
  {
LABEL_53:
    __break(1u);
LABEL_54:
    __break(1u);
  }
  v12 = *(_QWORD *)(a1 + 16);
  if (v12)
  {
    src.data = (void *)MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v12, 0);
    data = (int8x16_t *)src.data;
    v14 = (unsigned __int8 *)(a1 + 32);
    while (1)
    {
      v15 = *v14++;
      v16 = 3 - v15;
      if (((3 - v15) & 0xFFFFFF00) != 0)
        goto LABEL_40;
      src.data = data;
      v18 = data[1].u64[0];
      v17 = data[1].u64[1];
      v19 = v18 + 1;
      if (v18 >= v17 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v17 > 1), v18 + 1, 1);
        data = (int8x16_t *)src.data;
      }
      data[1].i64[0] = v19;
      data[2].i8[v18] = 1 << (v16 & 7);
      if (!--v12)
        goto LABEL_21;
    }
  }
  data = (int8x16_t *)MEMORY[0x1E0DEE9D8];
  v19 = *(_QWORD *)(MEMORY[0x1E0DEE9D8] + 16);
  if (!v19)
  {
    swift_bridgeObjectRelease();
    v21 = 0;
    if (*(_QWORD *)(v4 + 16))
      goto LABEL_36;
LABEL_39:
    __break(1u);
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
LABEL_21:
  if (v19 < 8)
  {
    v20 = 0;
    v21 = 0;
    goto LABEL_32;
  }
  if (v19 >= 0x20)
  {
    v20 = v19 & 0xFFFFFFFFFFFFFFE0;
    v22 = data + 3;
    v23 = 0uLL;
    v24 = v19 & 0xFFFFFFFFFFFFFFE0;
    v25 = 0uLL;
    do
    {
      v23 = vorrq_s8(v22[-1], v23);
      v25 = vorrq_s8(*v22, v25);
      v22 += 2;
      v24 -= 32;
    }
    while (v24);
    v26 = vorrq_s8(v25, v23);
    *(int8x8_t *)v26.i8 = vorr_s8(*(int8x8_t *)v26.i8, (int8x8_t)*(_OWORD *)&vextq_s8(v26, v26, 8uLL));
    v27 = v26.i64[0] | HIDWORD(v26.i64[0]) | ((unint64_t)(v26.i64[0] | HIDWORD(v26.i64[0])) >> 16);
    v21 = v27 | BYTE1(v27);
    if (v19 == v20)
      goto LABEL_34;
    if ((v19 & 0x18) == 0)
    {
LABEL_32:
      v34 = v19 - v20;
      v35 = &data[2].i8[v20];
      do
      {
        v36 = *v35++;
        v21 |= v36;
        --v34;
      }
      while (v34);
      goto LABEL_34;
    }
  }
  else
  {
    v21 = 0;
    v20 = 0;
  }
  v28 = v20;
  v20 = v19 & 0xFFFFFFFFFFFFFFF8;
  v29 = (int8x8_t)v21;
  v30 = (int8x8_t *)&data[2].i8[v28];
  v31 = v28 - (v19 & 0xFFFFFFFFFFFFFFF8);
  do
  {
    v32 = *v30++;
    v29 = vorr_s8(v32, v29);
    v31 += 8;
  }
  while (v31);
  v33 = *(_QWORD *)&v29 | HIDWORD(*(_QWORD *)&v29) | ((*(_QWORD *)&v29 | HIDWORD(*(_QWORD *)&v29)) >> 16);
  v21 = v33 | BYTE1(v33);
  if (v19 != v20)
    goto LABEL_32;
LABEL_34:
  swift_bridgeObjectRelease();
  if (v21 > 0xFu)
  {
LABEL_41:
    __break(1u);
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!*(_QWORD *)(v4 + 16))
    goto LABEL_39;
LABEL_36:
  v37 = *(_OWORD *)(v4 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v4 + 32);
  *(_OWORD *)&src.width = v37;
  if (!*(_QWORD *)(v7 + 16))
    goto LABEL_54;
  v38 = *(_OWORD *)(v7 + 48);
  *(_OWORD *)&v40.data = *(_OWORD *)(v7 + 32);
  *(_OWORD *)&v40.width = v38;
  return vImageOverwriteChannelsWithScalar_ARGBFFFF(a3, &src, &v40, v21, 0);
}

vImage_Error vImage.PixelBuffer<>.overwriteChannels(_:withPixel:destination:)(uint64_t a1, uint64_t *a2, float a3, float a4, float a5, float a6)
{
  uint64_t *v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  int64_t v18;
  int8x16_t *data;
  unsigned __int8 *v20;
  int v21;
  char v22;
  unint64_t v23;
  unint64_t v24;
  unint64_t v25;
  unint64_t v26;
  uint8_t v27;
  int8x16_t *v28;
  int8x16_t v29;
  unint64_t v30;
  int8x16_t v31;
  int8x16_t v32;
  unint64_t v33;
  unint64_t v34;
  int8x8_t v35;
  int8x8_t *v36;
  unint64_t v37;
  int8x8_t v38;
  uint64_t v39;
  unint64_t v40;
  __int8 *v41;
  char v42;
  __int128 v43;
  __int128 v44;
  vImage_Buffer v46;
  vImage_Buffer src;
  float v48[4];
  uint64_t v49;

  v49 = *MEMORY[0x1E0C80C00];
  v7 = *v6;
  if (!*(_QWORD *)(*v6 + 16))
    goto LABEL_41;
  v8 = *(_QWORD *)(v7 + 48);
  if (v8 < 0)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v9 = *(_QWORD *)(v7 + 40);
  if (v9 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!v8)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v10 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  v11 = *(_QWORD *)(v10 + 48);
  if (v11 < 0)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  v12 = *(_QWORD *)(v10 + 40);
  if (v12 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (!v11)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v12)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (v8 != v11)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v9 != v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  v18 = *(_QWORD *)(a1 + 16);
  if (v18)
  {
    src.data = (void *)MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v18, 0);
    data = (int8x16_t *)src.data;
    v20 = (unsigned __int8 *)(a1 + 32);
    while (1)
    {
      v21 = *v20++;
      v22 = 3 - v21;
      if (((3 - v21) & 0xFFFFFF00) != 0)
        break;
      src.data = data;
      v24 = data[1].u64[0];
      v23 = data[1].u64[1];
      v25 = v24 + 1;
      if (v24 >= v23 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v23 > 1), v24 + 1, 1);
        data = (int8x16_t *)src.data;
      }
      data[1].i64[0] = v25;
      data[2].i8[v24] = 1 << (v22 & 7);
      if (!--v18)
        goto LABEL_21;
    }
    __break(1u);
    goto LABEL_40;
  }
  data = (int8x16_t *)MEMORY[0x1E0DEE9D8];
  v25 = *(_QWORD *)(MEMORY[0x1E0DEE9D8] + 16);
  if (!v25)
  {
    swift_bridgeObjectRelease();
    v27 = 0;
    goto LABEL_35;
  }
LABEL_21:
  if (v25 < 8)
  {
    v26 = 0;
    v27 = 0;
    goto LABEL_32;
  }
  if (v25 >= 0x20)
  {
    v26 = v25 & 0xFFFFFFFFFFFFFFE0;
    v28 = data + 3;
    v29 = 0uLL;
    v30 = v25 & 0xFFFFFFFFFFFFFFE0;
    v31 = 0uLL;
    do
    {
      v29 = vorrq_s8(v28[-1], v29);
      v31 = vorrq_s8(*v28, v31);
      v28 += 2;
      v30 -= 32;
    }
    while (v30);
    v32 = vorrq_s8(v31, v29);
    *(int8x8_t *)v32.i8 = vorr_s8(*(int8x8_t *)v32.i8, (int8x8_t)*(_OWORD *)&vextq_s8(v32, v32, 8uLL));
    v33 = v32.i64[0] | HIDWORD(v32.i64[0]) | ((unint64_t)(v32.i64[0] | HIDWORD(v32.i64[0])) >> 16);
    v27 = v33 | BYTE1(v33);
    if (v25 == v26)
      goto LABEL_34;
    if ((v25 & 0x18) == 0)
    {
LABEL_32:
      v40 = v25 - v26;
      v41 = &data[2].i8[v26];
      do
      {
        v42 = *v41++;
        v27 |= v42;
        --v40;
      }
      while (v40);
      goto LABEL_34;
    }
  }
  else
  {
    v27 = 0;
    v26 = 0;
  }
  v34 = v26;
  v26 = v25 & 0xFFFFFFFFFFFFFFF8;
  v35 = (int8x8_t)v27;
  v36 = (int8x8_t *)&data[2].i8[v34];
  v37 = v34 - (v25 & 0xFFFFFFFFFFFFFFF8);
  do
  {
    v38 = *v36++;
    v35 = vorr_s8(v38, v35);
    v37 += 8;
  }
  while (v37);
  v39 = *(_QWORD *)&v35 | HIDWORD(*(_QWORD *)&v35) | ((*(_QWORD *)&v35 | HIDWORD(*(_QWORD *)&v35)) >> 16);
  v27 = v39 | BYTE1(v39);
  if (v25 != v26)
    goto LABEL_32;
LABEL_34:
  swift_bridgeObjectRelease();
  if (v27 > 0xFu)
  {
LABEL_40:
    __break(1u);
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
LABEL_35:
  v48[0] = a3;
  v48[1] = a4;
  v48[2] = a5;
  v48[3] = a6;
  if (!*(_QWORD *)(v7 + 16))
  {
LABEL_53:
    __break(1u);
LABEL_54:
    __break(1u);
  }
  v43 = *(_OWORD *)(v7 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v7 + 32);
  *(_OWORD *)&src.width = v43;
  if (!*(_QWORD *)(v10 + 16))
    goto LABEL_54;
  v44 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&v46.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&v46.width = v44;
  return vImageOverwriteChannelsWithPixel_ARGBFFFF(v48, &src, &v46, v27, 0);
}

uint64_t vImage.PixelBuffer<>.overwriteChannels(_:withInterleavedBuffer:destination:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t (*a4)(_OWORD *, _OWORD *, _OWORD *, _QWORD, _QWORD))
{
  uint64_t *v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t (*v14)(_OWORD *, _OWORD *, _OWORD *, _QWORD, _QWORD);
  int64_t v16;
  int8x16_t *v17;
  unsigned __int8 *v18;
  int v19;
  char v20;
  unint64_t v21;
  unint64_t v22;
  unint64_t v23;
  unint64_t v24;
  unsigned __int8 v25;
  int8x16_t *v26;
  int8x16_t v27;
  unint64_t v28;
  int8x16_t v29;
  int8x16_t v30;
  unint64_t v31;
  unint64_t v32;
  int8x8_t v33;
  int8x8_t *v34;
  unint64_t v35;
  int8x8_t v36;
  uint64_t v37;
  unint64_t v38;
  __int8 *v39;
  char v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  uint64_t v45;
  _OWORD v47[2];
  _OWORD v48[2];
  _OWORD v49[2];
  uint64_t v50;

  v50 = *MEMORY[0x1E0C80C00];
  v5 = *v4;
  if (!*(_QWORD *)(*v4 + 16))
    goto LABEL_50;
  v6 = *(_QWORD *)(v5 + 48);
  if (v6 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  v7 = *(_QWORD *)(v5 + 40);
  if (v7 < 0)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v6)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (!v7)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  v8 = *a3;
  if (!*(_QWORD *)(*a3 + 16))
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  v9 = *(_QWORD *)(v8 + 48);
  if (v9 < 0)
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  v10 = *(_QWORD *)(v8 + 40);
  if (v10 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  if (!v9)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v10)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (v6 != v9)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v7 != v10)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  v11 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  v12 = *(_QWORD *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  v13 = *(_QWORD *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  if (!v12)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v13)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (v6 != v12)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v7 != v13)
  {
LABEL_68:
    __break(1u);
    goto LABEL_69;
  }
  v14 = a4;
  v16 = *(_QWORD *)(a1 + 16);
  if (v16)
  {
    v45 = *v4;
    *(_QWORD *)&v49[0] = MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v16, 0);
    v17 = *(int8x16_t **)&v49[0];
    v18 = (unsigned __int8 *)(a1 + 32);
    do
    {
      v19 = *v18++;
      v20 = 3 - v19;
      if (((3 - v19) & 0xFFFFFF00) != 0)
        goto LABEL_48;
      *(_QWORD *)&v49[0] = v17;
      v22 = v17[1].u64[0];
      v21 = v17[1].u64[1];
      v23 = v22 + 1;
      if (v22 >= v21 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v21 > 1), v22 + 1, 1);
        v17 = *(int8x16_t **)&v49[0];
      }
      v17[1].i64[0] = v23;
      v17[2].i8[v22] = 1 << (v20 & 7);
      --v16;
    }
    while (v16);
    v5 = v45;
    v14 = a4;
LABEL_28:
    if (v23 < 8)
    {
      v24 = 0;
      v25 = 0;
LABEL_39:
      v38 = v23 - v24;
      v39 = &v17[2].i8[v24];
      do
      {
        v40 = *v39++;
        v25 |= v40;
        --v38;
      }
      while (v38);
LABEL_41:
      swift_bridgeObjectRelease();
      if (v25 > 0xFu)
      {
LABEL_49:
        __break(1u);
LABEL_50:
        __break(1u);
        goto LABEL_51;
      }
      if (*(_QWORD *)(v11 + 16))
        goto LABEL_43;
LABEL_47:
      __break(1u);
LABEL_48:
      __break(1u);
      goto LABEL_49;
    }
    if (v23 >= 0x20)
    {
      v24 = v23 & 0xFFFFFFFFFFFFFFE0;
      v26 = v17 + 3;
      v27 = 0uLL;
      v28 = v23 & 0xFFFFFFFFFFFFFFE0;
      v29 = 0uLL;
      do
      {
        v27 = vorrq_s8(v26[-1], v27);
        v29 = vorrq_s8(*v26, v29);
        v26 += 2;
        v28 -= 32;
      }
      while (v28);
      v30 = vorrq_s8(v29, v27);
      *(int8x8_t *)v30.i8 = vorr_s8(*(int8x8_t *)v30.i8, (int8x8_t)*(_OWORD *)&vextq_s8(v30, v30, 8uLL));
      v31 = v30.i64[0] | HIDWORD(v30.i64[0]) | ((unint64_t)(v30.i64[0] | HIDWORD(v30.i64[0])) >> 16);
      v25 = v31 | BYTE1(v31);
      if (v23 == v24)
        goto LABEL_41;
      if ((v23 & 0x18) == 0)
        goto LABEL_39;
    }
    else
    {
      v25 = 0;
      v24 = 0;
    }
    v32 = v24;
    v24 = v23 & 0xFFFFFFFFFFFFFFF8;
    v33 = (int8x8_t)v25;
    v34 = (int8x8_t *)&v17[2].i8[v32];
    v35 = v32 - (v23 & 0xFFFFFFFFFFFFFFF8);
    do
    {
      v36 = *v34++;
      v33 = vorr_s8(v36, v33);
      v35 += 8;
    }
    while (v35);
    v37 = *(_QWORD *)&v33 | HIDWORD(*(_QWORD *)&v33) | ((*(_QWORD *)&v33 | HIDWORD(*(_QWORD *)&v33)) >> 16);
    v25 = v37 | BYTE1(v37);
    if (v23 == v24)
      goto LABEL_41;
    goto LABEL_39;
  }
  v17 = (int8x16_t *)MEMORY[0x1E0DEE9D8];
  v23 = *(_QWORD *)(MEMORY[0x1E0DEE9D8] + 16);
  if (v23)
    goto LABEL_28;
  swift_bridgeObjectRelease();
  v25 = 0;
  if (!*(_QWORD *)(v11 + 16))
    goto LABEL_47;
LABEL_43:
  v41 = *(_OWORD *)(v11 + 48);
  v49[0] = *(_OWORD *)(v11 + 32);
  v49[1] = v41;
  if (!*(_QWORD *)(v5 + 16))
  {
LABEL_69:
    __break(1u);
LABEL_70:
    __break(1u);
  }
  v42 = *(_OWORD *)(v5 + 48);
  v48[0] = *(_OWORD *)(v5 + 32);
  v48[1] = v42;
  if (!*(_QWORD *)(v8 + 16))
    goto LABEL_70;
  v43 = *(_OWORD *)(v8 + 48);
  v47[0] = *(_OWORD *)(v8 + 32);
  v47[1] = v43;
  return v14(v49, v48, v47, v25, 0);
}

Swift::Void __swiftcall __spoils<CF,ZF,NF,VF,X0,X1,X2,X3,X4,X5,X6,X7,X8,X9,X10,X11,X12,X13,X14,X15,X16,X17,X21,Q0,Q1,Q2,Q3,Q4,Q5,Q6,Q7,Q16,Q17,Q18,Q19,Q20,Q21,Q22,Q23,Q24,Q25,Q26,Q27,Q28,Q29,Q30,Q31> BNNSOptimizer.step(parameters:gradients:accumulators:filterParameters:)(Swift::OpaquePointer parameters, Swift::OpaquePointer gradients, Swift::OpaquePointer accumulators, BNNSFilterParameters_optional *filterParameters)
{
  BNNSOptimizer.step(parameters:gradients:accumulators:filterParameters:)(parameters, gradients, accumulators, filterParameters);
}

{
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t *boxed_opaque_existential_1;
  uint64_t v11[3];
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;

  v6 = v4;
  v12 = v4;
  v13 = v14;
  boxed_opaque_existential_1 = __swift_allocate_boxed_opaque_existential_1(v11);
  (*(void (**)(uint64_t *, uint64_t, uint64_t))(*(_QWORD *)(v6 - 8) + 16))(boxed_opaque_existential_1, v5, v6);
  specialized static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)((uint64_t)v11, (uint64_t)parameters._rawValue, (uint64_t)gradients._rawValue, (uint64_t)accumulators._rawValue);
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v11);
}

uint64_t *__swift_allocate_boxed_opaque_existential_1(uint64_t *a1)
{
  uint64_t *v1;
  uint64_t v2;

  v1 = a1;
  if ((*(_BYTE *)(*(_QWORD *)(a1[3] - 8) + 82) & 2) != 0)
  {
    *a1 = swift_allocBox();
    return (uint64_t *)v2;
  }
  return v1;
}

uint64_t closure #1 in closure #4 in static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)(uint64_t a1, _DWORD *a2, _QWORD *a3, uint64_t a4, char **a5, char **a6, char **a7, uint64_t a8)
{
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  char isUniquelyReferenced_nonNull_native;
  char *v20;
  char v21;
  char *v22;
  char v23;
  int v24;
  uint64_t result;

  v15 = a3[3];
  v16 = a3[4];
  __swift_project_boxed_opaque_existential_1(a3, v15);
  v17 = (*(uint64_t (**)(uint64_t, uint64_t))(v16 + 16))(v15, v16);
  v18 = *a5;
  isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
  *a5 = v18;
  if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    v18 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v18 + 2), 0, v18);
  *a5 = v18;
  v20 = *a6;
  swift_bridgeObjectRetain();
  v21 = swift_isUniquelyReferenced_nonNull_native();
  *a6 = v20;
  if ((v21 & 1) == 0)
    v20 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v20 + 2), 0, v20);
  *a6 = v20;
  v22 = *a7;
  swift_bridgeObjectRetain();
  v23 = swift_isUniquelyReferenced_nonNull_native();
  *a7 = v22;
  if ((v23 & 1) == 0)
    v22 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v22 + 2), 0, v22);
  *a7 = v22;
  v24 = MEMORY[0x1D179481C](v17, a1, a4, v18 + 32, v20 + 32, v22 + 32, a8);
  swift_bridgeObjectRelease();
  result = swift_bridgeObjectRelease();
  *a2 = v24;
  return result;
}

uint64_t BNNS.AdamOptimizer.accumulatorCountMultiplier.getter()
{
  uint64_t v0;

  if (*(_BYTE *)(v0 + 53))
    return 3;
  else
    return 2;
}

float BNNS.AdamOptimizer.learningRate.getter()
{
  uint64_t v0;

  return *(float *)v0;
}

void BNNS.AdamOptimizer.learningRate.setter(float a1)
{
  unint64_t *v1;
  unint64_t v2;
  unint64_t v3;
  unint64_t v4;
  int v5;
  unint64_t v6;

  v2 = *v1;
  v3 = v1[3];
  v4 = v1[5];
  if (*((_BYTE *)v1 + 52) == 1)
  {
    v5 = *((_DWORD *)v1 + 12);
    v6 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
  }
  else
  {
    v5 = 0;
    v6 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    v3 &= 0x1FFFFFFFFuLL;
    v4 = v1[5];
  }
  *v1 = v6;
  v1[3] = v3;
  v1[5] = v4;
  *((_DWORD *)v1 + 12) = v5;
}

uint64_t **(*BNNS.AdamOptimizer.learningRate.modify(uint64_t a1))(uint64_t **a1)
{
  _QWORD *v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *v1;
  return BNNS.AdamOptimizer.learningRate.modify;
}

uint64_t **BNNS.AdamOptimizer.learningRate.modify(uint64_t **a1)
{
  unint64_t *v1;
  uint64_t **result;
  unsigned int v3;
  unsigned int v4;
  unint64_t v5;
  unint64_t v6;
  unint64_t v7;
  int v8;
  unint64_t v9;

  v1 = (unint64_t *)*a1;
  v3 = *((_DWORD *)a1 + 2);
  result = a1 + 1;
  v4 = v3;
  v5 = *v1;
  v6 = v1[3];
  v7 = v1[5];
  v8 = *((unsigned __int8 *)v1 + 52);
  if (*((_BYTE *)v1 + 52))
  {
    v8 = *((_DWORD *)v1 + 12);
    v9 = v5 & 0xFFFFFFFF00000000 | v4;
  }
  else
  {
    v9 = v5 & 0xFFFFFFFF00000000 | v4;
    v6 &= 0x1FFFFFFFFuLL;
    v7 = v1[5];
  }
  *v1 = v9;
  v1[3] = v6;
  v1[5] = v7;
  *((_DWORD *)v1 + 12) = v8;
  return result;
}

float BNNS.AdamOptimizer.beta1.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 4);
}

void BNNS.AdamOptimizer.beta1.setter(float a1)
{
  unsigned int *v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  unsigned int v5;
  unint64_t v6;

  v2 = *v1;
  v3 = *((_QWORD *)v1 + 3);
  v4 = *((_QWORD *)v1 + 5);
  if (*((_BYTE *)v1 + 52) == 1)
  {
    v5 = v1[12];
    v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
  }
  else
  {
    v5 = 0;
    v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
    v3 &= 0x1FFFFFFFFuLL;
    v4 = *((_QWORD *)v1 + 5);
  }
  *(_QWORD *)v1 = v6;
  *((_QWORD *)v1 + 3) = v3;
  *((_QWORD *)v1 + 5) = v4;
  v1[12] = v5;
}

unsigned int **(*BNNS.AdamOptimizer.beta1.modify(uint64_t a1))(unsigned int **a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 4);
  return BNNS.AdamOptimizer.beta1.modify;
}

unsigned int **BNNS.AdamOptimizer.beta1.modify(unsigned int **a1)
{
  unsigned int *v1;
  unsigned int **result;
  unsigned int v3;
  unsigned int v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  unsigned int v8;
  unint64_t v9;

  v1 = *a1;
  v3 = *((_DWORD *)a1 + 2);
  result = a1 + 1;
  v4 = v3;
  v5 = *v1;
  v6 = *((_QWORD *)v1 + 3);
  v7 = *((_QWORD *)v1 + 5);
  v8 = *((unsigned __int8 *)v1 + 52);
  if (*((_BYTE *)v1 + 52))
  {
    v8 = v1[12];
    v9 = v5 | ((unint64_t)v4 << 32);
  }
  else
  {
    v9 = v5 | ((unint64_t)v4 << 32);
    v6 &= 0x1FFFFFFFFuLL;
    v7 = *((_QWORD *)v1 + 5);
  }
  *(_QWORD *)v1 = v9;
  *((_QWORD *)v1 + 3) = v6;
  *((_QWORD *)v1 + 5) = v7;
  v1[12] = v8;
  return result;
}

float BNNS.AdamOptimizer.beta2.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 8);
}

void BNNS.AdamOptimizer.beta2.setter(float a1)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  int v5;
  unint64_t v6;

  v2 = *(_QWORD *)(v1 + 8);
  v3 = *(_QWORD *)(v1 + 24);
  v4 = *(_QWORD *)(v1 + 40);
  if (*(_BYTE *)(v1 + 52) == 1)
  {
    v5 = *(_DWORD *)(v1 + 48);
    v6 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
  }
  else
  {
    v5 = 0;
    v6 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    v3 &= 0x1FFFFFFFFuLL;
    v4 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 8) = v6;
  *(_QWORD *)(v1 + 24) = v3;
  *(_QWORD *)(v1 + 40) = v4;
  *(_DWORD *)(v1 + 48) = v5;
}

uint64_t *(*BNNS.AdamOptimizer.beta2.modify(uint64_t a1))(uint64_t *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_QWORD *)(v1 + 8);
  return BNNS.AdamOptimizer.beta2.modify;
}

uint64_t *BNNS.AdamOptimizer.beta2.modify(uint64_t *a1)
{
  uint64_t v1;
  uint64_t *result;
  unsigned int v3;
  unsigned int v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  int v8;
  unint64_t v9;

  v1 = *a1;
  v3 = *((_DWORD *)a1 + 2);
  result = a1 + 1;
  v4 = v3;
  v5 = *(_QWORD *)(v1 + 8);
  v6 = *(_QWORD *)(v1 + 24);
  v7 = *(_QWORD *)(v1 + 40);
  v8 = *(unsigned __int8 *)(v1 + 52);
  if (*(_BYTE *)(v1 + 52))
  {
    v8 = *(_DWORD *)(v1 + 48);
    v9 = v5 & 0xFFFFFFFF00000000 | v4;
  }
  else
  {
    v9 = v5 & 0xFFFFFFFF00000000 | v4;
    v6 &= 0x1FFFFFFFFuLL;
    v7 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 8) = v9;
  *(_QWORD *)(v1 + 24) = v6;
  *(_QWORD *)(v1 + 40) = v7;
  *(_DWORD *)(v1 + 48) = v8;
  return result;
}

float BNNS.AdamOptimizer.timeStep.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 12);
}

void BNNS.AdamOptimizer.timeStep.setter(float a1)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  int v5;
  unint64_t v6;

  v2 = *(unsigned int *)(v1 + 8);
  v3 = *(_QWORD *)(v1 + 24);
  v4 = *(_QWORD *)(v1 + 40);
  if (*(_BYTE *)(v1 + 52) == 1)
  {
    v5 = *(_DWORD *)(v1 + 48);
    v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
  }
  else
  {
    v5 = 0;
    v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
    v3 &= 0x1FFFFFFFFuLL;
    v4 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 8) = v6;
  *(_QWORD *)(v1 + 24) = v3;
  *(_QWORD *)(v1 + 40) = v4;
  *(_DWORD *)(v1 + 48) = v5;
}

uint64_t *(*BNNS.AdamOptimizer.timeStep.modify(uint64_t a1))(uint64_t *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 12);
  return BNNS.AdamOptimizer.timeStep.modify;
}

uint64_t *BNNS.AdamOptimizer.timeStep.modify(uint64_t *a1)
{
  uint64_t v1;
  uint64_t *result;
  unsigned int v3;
  unsigned int v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  int v8;
  unint64_t v9;

  v1 = *a1;
  v3 = *((_DWORD *)a1 + 2);
  result = a1 + 1;
  v4 = v3;
  v5 = *(unsigned int *)(v1 + 8);
  v6 = *(_QWORD *)(v1 + 24);
  v7 = *(_QWORD *)(v1 + 40);
  v8 = *(unsigned __int8 *)(v1 + 52);
  if (*(_BYTE *)(v1 + 52))
  {
    v8 = *(_DWORD *)(v1 + 48);
    v9 = v5 | ((unint64_t)v4 << 32);
  }
  else
  {
    v9 = v5 | ((unint64_t)v4 << 32);
    v6 &= 0x1FFFFFFFFuLL;
    v7 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 8) = v9;
  *(_QWORD *)(v1 + 24) = v6;
  *(_QWORD *)(v1 + 40) = v7;
  *(_DWORD *)(v1 + 48) = v8;
  return result;
}

float BNNS.AdamOptimizer.epsilon.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 16);
}

void BNNS.AdamOptimizer.epsilon.setter(float a1)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  int v5;
  unint64_t v6;

  v3 = *(_QWORD *)(v1 + 16);
  v2 = *(_QWORD *)(v1 + 24);
  v4 = *(_QWORD *)(v1 + 40);
  if (*(_BYTE *)(v1 + 52) == 1)
  {
    v5 = *(_DWORD *)(v1 + 48);
    v6 = v3 & 0xFFFFFFFF00000000 | LODWORD(a1);
  }
  else
  {
    v5 = 0;
    v6 = v3 & 0xFFFFFFFF00000000 | LODWORD(a1);
    v2 &= 0x1FFFFFFFFuLL;
    v4 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 16) = v6;
  *(_QWORD *)(v1 + 24) = v2;
  *(_QWORD *)(v1 + 40) = v4;
  *(_DWORD *)(v1 + 48) = v5;
}

uint64_t *(*BNNS.AdamOptimizer.epsilon.modify(uint64_t a1))(uint64_t *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_QWORD *)(v1 + 16);
  return BNNS.AdamOptimizer.epsilon.modify;
}

uint64_t *BNNS.AdamOptimizer.epsilon.modify(uint64_t *a1)
{
  uint64_t v1;
  uint64_t *result;
  unsigned int v3;
  unsigned int v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  int v8;
  unint64_t v9;

  v1 = *a1;
  v3 = *((_DWORD *)a1 + 2);
  result = a1 + 1;
  v4 = v3;
  v6 = *(_QWORD *)(v1 + 16);
  v5 = *(_QWORD *)(v1 + 24);
  v7 = *(_QWORD *)(v1 + 40);
  v8 = *(unsigned __int8 *)(v1 + 52);
  if (*(_BYTE *)(v1 + 52))
  {
    v8 = *(_DWORD *)(v1 + 48);
    v9 = v6 & 0xFFFFFFFF00000000 | v4;
  }
  else
  {
    v9 = v6 & 0xFFFFFFFF00000000 | v4;
    v5 &= 0x1FFFFFFFFuLL;
    v7 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 16) = v9;
  *(_QWORD *)(v1 + 24) = v5;
  *(_QWORD *)(v1 + 40) = v7;
  *(_DWORD *)(v1 + 48) = v8;
  return result;
}

float BNNS.AdamOptimizer.gradientScale.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 20);
}

void BNNS.AdamOptimizer.gradientScale.setter(float a1)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  int v5;
  unint64_t v6;

  v2 = *(unsigned int *)(v1 + 16);
  v3 = *(_QWORD *)(v1 + 24);
  v4 = *(_QWORD *)(v1 + 40);
  if (*(_BYTE *)(v1 + 52) == 1)
  {
    v5 = *(_DWORD *)(v1 + 48);
    v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
  }
  else
  {
    v5 = 0;
    v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
    v3 &= 0x1FFFFFFFFuLL;
    v4 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 16) = v6;
  *(_QWORD *)(v1 + 24) = v3;
  *(_QWORD *)(v1 + 40) = v4;
  *(_DWORD *)(v1 + 48) = v5;
}

uint64_t *(*BNNS.AdamOptimizer.gradientScale.modify(uint64_t a1))(uint64_t *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 20);
  return BNNS.AdamOptimizer.gradientScale.modify;
}

uint64_t *BNNS.AdamOptimizer.gradientScale.modify(uint64_t *a1)
{
  uint64_t v1;
  uint64_t *result;
  unsigned int v3;
  unsigned int v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  int v8;
  unint64_t v9;

  v1 = *a1;
  v3 = *((_DWORD *)a1 + 2);
  result = a1 + 1;
  v4 = v3;
  v5 = *(unsigned int *)(v1 + 16);
  v6 = *(_QWORD *)(v1 + 24);
  v7 = *(_QWORD *)(v1 + 40);
  v8 = *(unsigned __int8 *)(v1 + 52);
  if (*(_BYTE *)(v1 + 52))
  {
    v8 = *(_DWORD *)(v1 + 48);
    v9 = v5 | ((unint64_t)v4 << 32);
  }
  else
  {
    v9 = v5 | ((unint64_t)v4 << 32);
    v6 &= 0x1FFFFFFFFuLL;
    v7 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 16) = v9;
  *(_QWORD *)(v1 + 24) = v6;
  *(_QWORD *)(v1 + 40) = v7;
  *(_DWORD *)(v1 + 48) = v8;
  return result;
}

float BNNS.AdamOptimizer.regularizationScale.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 24);
}

void BNNS.AdamOptimizer.regularizationScale.setter(float a1)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  int v4;
  unint64_t v5;

  v2 = *(_QWORD *)(v1 + 24);
  v3 = *(_QWORD *)(v1 + 40);
  if (*(_BYTE *)(v1 + 52) == 1)
  {
    v4 = *(_DWORD *)(v1 + 48);
    v5 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
  }
  else
  {
    v4 = 0;
    v5 = v2 & 0x100000000 | LODWORD(a1);
    v3 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 24) = v5;
  *(_QWORD *)(v1 + 40) = v3;
  *(_DWORD *)(v1 + 48) = v4;
}

uint64_t *(*BNNS.AdamOptimizer.regularizationScale.modify(uint64_t a1))(uint64_t *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_QWORD *)(v1 + 24);
  return BNNS.AdamOptimizer.regularizationScale.modify;
}

uint64_t *BNNS.AdamOptimizer.regularizationScale.modify(uint64_t *a1)
{
  uint64_t v1;
  uint64_t *result;
  unsigned int v3;
  unsigned int v4;
  uint64_t v5;
  uint64_t v6;
  int v7;
  unint64_t v8;

  v1 = *a1;
  v3 = *((_DWORD *)a1 + 2);
  result = a1 + 1;
  v4 = v3;
  v5 = *(_QWORD *)(v1 + 24);
  v6 = *(_QWORD *)(v1 + 40);
  v7 = *(unsigned __int8 *)(v1 + 52);
  if (*(_BYTE *)(v1 + 52))
  {
    v7 = *(_DWORD *)(v1 + 48);
    v8 = v5 & 0xFFFFFFFF00000000 | v4;
  }
  else
  {
    v8 = v5 & 0x100000000 | v4;
    v6 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 24) = v8;
  *(_QWORD *)(v1 + 40) = v6;
  *(_DWORD *)(v1 + 48) = v7;
  return result;
}

uint64_t BNNS.AdamOptimizer.gradientBounds.getter()
{
  uint64_t v0;
  uint64_t result;

  result = *(_QWORD *)(v0 + 32);
  if (*(_BYTE *)(v0 + 52) == 1)
  {
    if ((_DWORD)result == 1)
    {
      if (*((float *)&result + 1) <= COERCE_FLOAT(*(_QWORD *)(v0 + 40)))
        return *(__int128 *)(v0 + 32) >> 32;
      __break(1u);
      goto LABEL_10;
    }
    return 0;
  }
  if ((*(_BYTE *)(v0 + 28) & 1) == 0)
    return 0;
  if (*(float *)&result > *((float *)&result + 1))
LABEL_10:
    __break(1u);
  return result;
}

uint64_t key path getter for BNNS.AdamOptimizer.gradientBounds : BNNS.AdamOptimizer@<X0>(uint64_t a1@<X8>)
{
  uint64_t result;
  char v3;

  result = BNNS.AdamOptimizer.gradientBounds.getter();
  *(_QWORD *)a1 = result;
  *(_BYTE *)(a1 + 8) = v3 & 1;
  return result;
}

uint64_t key path setter for BNNS.AdamOptimizer.gradientBounds : BNNS.AdamOptimizer(uint64_t result, uint64_t a2)
{
  unint64_t v2;
  int v3;
  unint64_t v4;
  int v5;
  unint64_t v6;
  BOOL v7;
  unint64_t v8;
  unint64_t v9;
  uint64_t v10;

  v2 = *(_QWORD *)result;
  v3 = *(unsigned __int8 *)(result + 8);
  v4 = *(_QWORD *)(a2 + 24);
  if (*(_BYTE *)(a2 + 52) == 1)
  {
    v5 = *(_DWORD *)(a2 + 48);
    v6 = HIDWORD(v2);
    v2 = (v2 << 32) | 1;
    v7 = v3 == 0;
    if (*(_BYTE *)(result + 8))
      v8 = 0;
    else
      v8 = v6;
    if (!v7)
      v2 = 0;
    v9 = *(_QWORD *)(a2 + 40) & 0xFFFFFFFF00000000 | v8;
  }
  else
  {
    v5 = 0;
    v7 = v3 == 0;
    v10 = 0x100000000;
    if (!v7)
    {
      v10 = 0;
      v2 = 0;
    }
    v9 = *(_QWORD *)(a2 + 40);
    v4 = v10 & 0xFFFFFFFF00000000 | *(_QWORD *)(a2 + 24);
  }
  *(_QWORD *)(a2 + 24) = v4;
  *(_QWORD *)(a2 + 32) = v2;
  *(_QWORD *)(a2 + 40) = v9;
  *(_DWORD *)(a2 + 48) = v5;
  return result;
}

unint64_t BNNS.AdamOptimizer.gradientBounds.setter(unint64_t result, char a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  int v5;
  BOOL v6;
  unint64_t v7;
  uint64_t v8;
  unint64_t v9;

  v3 = a2 & 1;
  v4 = *(_QWORD *)(v2 + 24);
  if (*(_BYTE *)(v2 + 52) == 1)
  {
    v5 = *(_DWORD *)(v2 + 48);
    v6 = (_DWORD)v3 == 0;
    if ((a2 & 1) != 0)
      v7 = 0;
    else
      v7 = HIDWORD(result);
    if (v6)
      v8 = (result << 32) | 1;
    else
      v8 = 0;
    v9 = *(_QWORD *)(v2 + 40) & 0xFFFFFFFF00000000 | v7;
  }
  else
  {
    v5 = 0;
    if ((a2 & 1) != 0)
      v8 = 0;
    else
      v8 = result;
    v4 = (*(_QWORD *)(v2 + 24) | (unint64_t)(v3 << 32)) ^ 0x100000000;
    v9 = *(_QWORD *)(v2 + 40);
  }
  *(_QWORD *)(v2 + 24) = v4;
  *(_QWORD *)(v2 + 32) = v8;
  *(_QWORD *)(v2 + 40) = v9;
  *(_DWORD *)(v2 + 48) = v5;
  return result;
}

uint64_t (*BNNS.AdamOptimizer.gradientBounds.modify(uint64_t (*result)(uint64_t result)))(uint64_t result)
{
  uint64_t v1;
  unint64_t v2;
  unint64_t v3;
  char v4;

  *((_QWORD *)result + 2) = v1;
  v2 = *(_QWORD *)(v1 + 32);
  if (*(_BYTE *)(v1 + 52) != 1)
  {
    if ((*(_BYTE *)(v1 + 28) & 1) != 0)
    {
      if (*(float *)&v2 <= *((float *)&v2 + 1))
      {
        v4 = 0;
        goto LABEL_9;
      }
      goto LABEL_11;
    }
LABEL_6:
    v2 = 0;
    v4 = 1;
    goto LABEL_9;
  }
  if ((_DWORD)v2 != 1)
    goto LABEL_6;
  v3 = HIDWORD(v2);
  if (*(float *)&v3 <= COERCE_FLOAT(*(_QWORD *)(v1 + 40)))
  {
    v4 = 0;
    v2 = v3 | (*(_QWORD *)(v1 + 40) << 32);
LABEL_9:
    *(_QWORD *)result = v2;
    *((_BYTE *)result + 8) = v4;
    return BNNS.AdamOptimizer.gradientBounds.modify;
  }
  __break(1u);
LABEL_11:
  __break(1u);
  return result;
}

uint64_t BNNS.AdamOptimizer.gradientBounds.modify(uint64_t result)
{
  uint64_t v1;
  unint64_t v2;
  int v3;
  unint64_t v4;
  int v5;
  unint64_t v6;
  BOOL v7;
  unint64_t v8;
  unint64_t v9;
  uint64_t v10;

  v1 = *(_QWORD *)(result + 16);
  v2 = *(_QWORD *)result;
  v3 = *(unsigned __int8 *)(result + 8);
  v4 = *(_QWORD *)(v1 + 24);
  v5 = *(unsigned __int8 *)(v1 + 52);
  if (*(_BYTE *)(v1 + 52))
  {
    v5 = *(_DWORD *)(v1 + 48);
    v6 = HIDWORD(v2);
    v2 = (v2 << 32) | 1;
    v7 = v3 == 0;
    if (*(_BYTE *)(result + 8))
      v8 = 0;
    else
      v8 = v6;
    if (!v7)
      v2 = 0;
    v9 = *(_QWORD *)(v1 + 40) & 0xFFFFFFFF00000000 | v8;
  }
  else
  {
    v7 = v3 == 0;
    v10 = 0x100000000;
    if (!v7)
    {
      v10 = 0;
      v2 = 0;
    }
    v9 = *(_QWORD *)(v1 + 40);
    v4 = v10 & 0xFFFFFFFF00000000 | *(_QWORD *)(v1 + 24);
  }
  *(_QWORD *)(v1 + 24) = v4;
  *(_QWORD *)(v1 + 32) = v2;
  *(_QWORD *)(v1 + 40) = v9;
  *(_DWORD *)(v1 + 48) = v5;
  return result;
}

uint64_t BNNS.AdamOptimizer.regularizationFunction.getter()
{
  uint64_t v0;

  if (*(_BYTE *)(v0 + 52) == 1)
    return *(unsigned int *)(v0 + 28);
  else
    return *(_QWORD *)(v0 + 40);
}

uint64_t BNNS.AdamOptimizer.regularizationFunction.setter(uint64_t result)
{
  uint64_t v1;
  int v2;
  uint64_t v3;
  unint64_t v4;

  if (*(_BYTE *)(v1 + 52) == 1)
  {
    v2 = *(_DWORD *)(v1 + 48);
    v3 = *(_QWORD *)(v1 + 40);
    v4 = *(_QWORD *)(v1 + 24) | ((unint64_t)result << 32);
  }
  else
  {
    v2 = 0;
    v4 = *(_QWORD *)(v1 + 24) & 0x1FFFFFFFFLL;
    v3 = result;
  }
  *(_QWORD *)(v1 + 24) = v4;
  *(_QWORD *)(v1 + 40) = v3;
  *(_DWORD *)(v1 + 48) = v2;
  return result;
}

uint64_t *(*BNNS.AdamOptimizer.regularizationFunction.modify(uint64_t a1))(uint64_t *a1)
{
  uint64_t v1;
  uint64_t v2;

  *(_QWORD *)a1 = v1;
  if (*(_BYTE *)(v1 + 52) == 1)
    LODWORD(v2) = *(_DWORD *)(v1 + 28);
  else
    v2 = *(_QWORD *)(v1 + 40);
  *(_DWORD *)(a1 + 8) = v2;
  return BNNS.AdamOptimizer.regularizationFunction.modify;
}

uint64_t *BNNS.AdamOptimizer.regularizationFunction.modify(uint64_t *a1)
{
  uint64_t v1;
  uint64_t *result;
  uint64_t v3;
  unsigned int v4;
  int v5;
  unint64_t v6;

  v1 = *a1;
  v4 = *((_DWORD *)a1 + 2);
  result = a1 + 1;
  v3 = v4;
  v5 = *(unsigned __int8 *)(v1 + 52);
  if (*(_BYTE *)(v1 + 52))
  {
    v5 = *(_DWORD *)(v1 + 48);
    v6 = *(_QWORD *)(v1 + 24) | ((unint64_t)v3 << 32);
    v3 = *(_QWORD *)(v1 + 40);
  }
  else
  {
    v6 = *(_QWORD *)(v1 + 24) & 0x1FFFFFFFFLL;
  }
  *(_QWORD *)(v1 + 24) = v6;
  *(_QWORD *)(v1 + 40) = v3;
  *(_DWORD *)(v1 + 48) = v5;
  return result;
}

unint64_t BNNS.AdamOptimizer.init(learningRate:beta1:beta2:timeStep:epsilon:gradientScale:regularizationScale:clipsGradientsTo:regularizationFunction:)@<X0>(unint64_t result@<X0>, char a2@<W1>, uint64_t a3@<X2>, int8x16_t *a4@<X8>, unsigned int a5@<S0>, int32x2_t a6@<D1>, unsigned int a7@<S2>, __int32 a8@<S3>, unsigned int a9@<S4>, unsigned int a10@<S5>, unsigned int a11@<S6>)
{
  uint64_t v11;
  int8x16_t v12;
  uint64_t v13;

  v11 = HIDWORD(result);
  a6.i32[1] = a8;
  v12.i64[0] = a5;
  v12.i64[1] = a7;
  *a4 = vorrq_s8((int8x16_t)vshll_n_s32(a6, 0x20uLL), v12);
  a4[1].i64[0] = a9 | ((unint64_t)a10 << 32);
  a4[1].i64[1] = a11 | (unint64_t)(a3 << 32);
  if ((a2 & 1) != 0)
  {
    v11 = 0;
    v13 = 0;
  }
  else
  {
    v13 = (result << 32) | 1;
  }
  a4[2].i64[0] = v13;
  a4[2].i64[1] = v11;
  a4[3].i32[0] = 0;
  a4[3].i16[2] = 1;
  return result;
}

uint64_t BNNS.AdamOptimizer.bnnsOptimizerFunction.getter()
{
  uint64_t v0;
  unsigned int v1;

  if (*(_BYTE *)(v0 + 53))
    v1 = 11;
  else
    v1 = 8;
  if (*(_BYTE *)(v0 + 52))
    return v1;
  else
    return 2;
}

uint64_t protocol witness for BNNSOptimizer.step(parameters:gradients:accumulators:filterParameters:) in conformance BNNS.AdamOptimizer(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  __int128 *v9;
  uint64_t v13;
  uint64_t v14;
  int v15;
  char v16;
  char v17;
  uint64_t v18;
  __int128 v20;
  __int128 v21;
  _QWORD v22[5];

  v20 = v9[1];
  v21 = *v9;
  v13 = *((_QWORD *)v9 + 4);
  v14 = *((_QWORD *)v9 + 5);
  v15 = *((_DWORD *)v9 + 12);
  v16 = *((_BYTE *)v9 + 52);
  v17 = *((_BYTE *)v9 + 53);
  v22[3] = a8;
  v22[4] = a9;
  v18 = swift_allocObject();
  v22[0] = v18;
  *(_OWORD *)(v18 + 16) = v21;
  *(_OWORD *)(v18 + 32) = v20;
  *(_QWORD *)(v18 + 48) = v13;
  *(_QWORD *)(v18 + 56) = v14;
  *(_DWORD *)(v18 + 64) = v15;
  *(_BYTE *)(v18 + 68) = v16;
  *(_BYTE *)(v18 + 69) = v17;
  specialized static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)((uint64_t)v22, a1, a2, a3);
  return __swift_destroy_boxed_opaque_existential_1((uint64_t)v22);
}

uint64_t protocol witness for BNNSOptimizer.bnnsOptimizerFunction.getter in conformance BNNS.AdamOptimizer()
{
  uint64_t v0;
  unsigned int v1;

  if (*(_BYTE *)(v0 + 53))
    v1 = 11;
  else
    v1 = 8;
  if (*(_BYTE *)(v0 + 52))
    return v1;
  else
    return 2;
}

uint64_t protocol witness for BNNSOptimizer.accumulatorCountMultiplier.getter in conformance BNNS.AdamOptimizer()
{
  uint64_t v0;

  if (*(_BYTE *)(v0 + 53))
    return 3;
  else
    return 2;
}

uint64_t protocol witness for WithOptimizerAlgFields.withOptimizerAlgFields(body:) in conformance BNNS.AdamOptimizer(uint64_t (*a1)(_OWORD *))
{
  uint64_t v1;
  int v2;
  char v3;
  __int128 v4;
  _OWORD v6[3];
  int v7;
  char v8;
  uint64_t v9;

  v9 = *MEMORY[0x1E0C80C00];
  v2 = *(_DWORD *)(v1 + 48);
  v3 = *(_BYTE *)(v1 + 52);
  v4 = *(_OWORD *)(v1 + 16);
  v6[0] = *(_OWORD *)v1;
  v6[1] = v4;
  v6[2] = *(_OWORD *)(v1 + 32);
  v7 = v2;
  v8 = v3;
  return a1(v6);
}

uint64_t BNNS.RMSPropOptimizer.accumulatorCountMultiplier.getter()
{
  uint64_t v0;
  uint64_t v1;

  v1 = 1;
  if ((*(_DWORD *)(v0 + 16) & 0x7FFFFFFF) != 0)
    v1 = 2;
  return v1 + (HIDWORD(*(_QWORD *)(v0 + 8)) & 1);
}

float BNNS.RMSPropOptimizer.momentum.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 16);
}

uint64_t BNNS.RMSPropOptimizer.centered.getter()
{
  uint64_t v0;

  return *(_BYTE *)(v0 + 12) & 1;
}

float BNNS.RMSPropOptimizer.learningRate.getter()
{
  uint64_t v0;

  return *(float *)v0;
}

unsigned int *key path setter for BNNS.RMSPropOptimizer.learningRate : BNNS.RMSPropOptimizer(unsigned int *result, unint64_t *a2)
{
  unsigned int v2;
  unint64_t v3;
  unint64_t v4;
  unint64_t v5;
  unint64_t v6;
  int v7;
  unint64_t v8;
  unint64_t v9;

  v2 = *result;
  v3 = *a2;
  v4 = a2[1];
  v5 = a2[3];
  v6 = a2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
    v7 = *((_DWORD *)a2 + 12);
    v8 = v3 & 0xFFFFFFFF00000000 | v2;
    v9 = v4 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v7 = 0;
    v8 = v3 & 0xFFFFFFFF00000000 | v2;
    v9 = v4 & 0x1FFFFFFFFLL;
    v5 &= 0x1FFFFFFFFuLL;
    v6 = a2[5];
  }
  *a2 = v8;
  a2[1] = v9;
  a2[3] = v5;
  a2[5] = v6;
  *((_DWORD *)a2 + 12) = v7;
  return result;
}

void BNNS.RMSPropOptimizer.learningRate.setter(float a1)
{
  unint64_t *v1;
  unint64_t v2;
  unint64_t v3;
  unint64_t v4;
  unint64_t v5;
  int v6;
  unint64_t v7;
  unint64_t v8;

  v2 = *v1;
  v3 = v1[1];
  v4 = v1[3];
  v5 = v1[5];
  if ((v3 & 0x8000000000000000) != 0)
  {
    v6 = *((_DWORD *)v1 + 12);
    v7 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    v8 = v3 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v6 = 0;
    v7 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    v8 = v3 & 0x1FFFFFFFFLL;
    v4 &= 0x1FFFFFFFFuLL;
    v5 = v1[5];
  }
  *v1 = v7;
  v1[1] = v8;
  v1[3] = v4;
  v1[5] = v5;
  *((_DWORD *)v1 + 12) = v6;
}

uint64_t **(*BNNS.RMSPropOptimizer.learningRate.modify(uint64_t a1))(uint64_t **a1)
{
  _QWORD *v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *v1;
  return BNNS.RMSPropOptimizer.learningRate.modify;
}

uint64_t **BNNS.RMSPropOptimizer.learningRate.modify(uint64_t **a1)
{
  unint64_t *v1;
  uint64_t **result;
  unsigned int v3;
  unsigned int v4;
  unint64_t v5;
  unint64_t v6;
  unint64_t v7;
  unint64_t v8;
  int v9;
  unint64_t v10;
  unint64_t v11;

  v1 = (unint64_t *)*a1;
  v3 = *((_DWORD *)a1 + 2);
  result = a1 + 1;
  v4 = v3;
  v5 = *v1;
  v6 = v1[1];
  v7 = v1[3];
  v8 = v1[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
    v9 = *((_DWORD *)v1 + 12);
    v10 = v5 & 0xFFFFFFFF00000000 | v4;
    v11 = v6 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v9 = 0;
    v10 = v5 & 0xFFFFFFFF00000000 | v4;
    v11 = v6 & 0x1FFFFFFFFLL;
    v7 &= 0x1FFFFFFFFuLL;
    v8 = v1[5];
  }
  *v1 = v10;
  v1[1] = v11;
  v1[3] = v7;
  v1[5] = v8;
  *((_DWORD *)v1 + 12) = v9;
  return result;
}

float BNNS.RMSPropOptimizer.alpha.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 4);
}

unsigned int *key path setter for BNNS.RMSPropOptimizer.alpha : BNNS.RMSPropOptimizer(unsigned int *result, unsigned int *a2)
{
  unsigned int v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  unsigned int v7;
  unint64_t v8;
  unint64_t v9;

  v2 = *result;
  v3 = *a2;
  v4 = *((_QWORD *)a2 + 1);
  v5 = *((_QWORD *)a2 + 3);
  v6 = *((_QWORD *)a2 + 5);
  if (v4 < 0)
  {
    v7 = a2[12];
    v8 = v3 | ((unint64_t)v2 << 32);
    v9 = v4 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v7 = 0;
    v8 = v3 | ((unint64_t)v2 << 32);
    v9 = v4 & 0x1FFFFFFFFLL;
    v5 &= 0x1FFFFFFFFuLL;
    v6 = *((_QWORD *)a2 + 5);
  }
  *(_QWORD *)a2 = v8;
  *((_QWORD *)a2 + 1) = v9;
  *((_QWORD *)a2 + 3) = v5;
  *((_QWORD *)a2 + 5) = v6;
  a2[12] = v7;
  return result;
}

void BNNS.RMSPropOptimizer.alpha.setter(float a1)
{
  unsigned int *v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  unsigned int v6;
  unint64_t v7;
  unint64_t v8;

  v2 = *v1;
  v3 = *((_QWORD *)v1 + 1);
  v4 = *((_QWORD *)v1 + 3);
  v5 = *((_QWORD *)v1 + 5);
  if (v3 < 0)
  {
    v6 = v1[12];
    v7 = v2 | ((unint64_t)LODWORD(a1) << 32);
    v8 = v3 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v6 = 0;
    v7 = v2 | ((unint64_t)LODWORD(a1) << 32);
    v8 = v3 & 0x1FFFFFFFFLL;
    v4 &= 0x1FFFFFFFFuLL;
    v5 = *((_QWORD *)v1 + 5);
  }
  *(_QWORD *)v1 = v7;
  *((_QWORD *)v1 + 1) = v8;
  *((_QWORD *)v1 + 3) = v4;
  *((_QWORD *)v1 + 5) = v5;
  v1[12] = v6;
}

unsigned int **(*BNNS.RMSPropOptimizer.alpha.modify(uint64_t a1))(unsigned int **a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 4);
  return BNNS.RMSPropOptimizer.alpha.modify;
}

unsigned int **BNNS.RMSPropOptimizer.alpha.modify(unsigned int **a1)
{
  unsigned int *v1;
  unsigned int **result;
  unsigned int v3;
  unsigned int v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  unsigned int v9;
  unint64_t v10;
  unint64_t v11;

  v1 = *a1;
  v3 = *((_DWORD *)a1 + 2);
  result = a1 + 1;
  v4 = v3;
  v5 = *v1;
  v6 = *((_QWORD *)v1 + 1);
  v7 = *((_QWORD *)v1 + 3);
  v8 = *((_QWORD *)v1 + 5);
  if (v6 < 0)
  {
    v9 = v1[12];
    v10 = v5 | ((unint64_t)v4 << 32);
    v11 = v6 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v9 = 0;
    v10 = v5 | ((unint64_t)v4 << 32);
    v11 = v6 & 0x1FFFFFFFFLL;
    v7 &= 0x1FFFFFFFFuLL;
    v8 = *((_QWORD *)v1 + 5);
  }
  *(_QWORD *)v1 = v10;
  *((_QWORD *)v1 + 1) = v11;
  *((_QWORD *)v1 + 3) = v7;
  *((_QWORD *)v1 + 5) = v8;
  v1[12] = v9;
  return result;
}

float BNNS.RMSPropOptimizer.epsilon.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 8);
}

void BNNS.RMSPropOptimizer.epsilon.setter(float a1)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  int v5;
  unint64_t v6;

  v2 = *(_QWORD *)(v1 + 8);
  v3 = *(_QWORD *)(v1 + 24);
  v4 = *(_QWORD *)(v1 + 40);
  if (v2 < 0)
  {
    v5 = *(_DWORD *)(v1 + 48);
    v6 = v2 & 0x100000000 | LODWORD(a1) | 0x8000000000000000;
  }
  else
  {
    v5 = 0;
    v6 = v2 & 0x100000000 | LODWORD(a1);
    v3 &= 0x1FFFFFFFFuLL;
    v4 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 8) = v6;
  *(_QWORD *)(v1 + 24) = v3;
  *(_QWORD *)(v1 + 40) = v4;
  *(_DWORD *)(v1 + 48) = v5;
}

uint64_t *(*BNNS.RMSPropOptimizer.epsilon.modify(uint64_t a1))(uint64_t *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_QWORD *)(v1 + 8);
  return BNNS.RMSPropOptimizer.epsilon.modify;
}

uint64_t *BNNS.RMSPropOptimizer.epsilon.modify(uint64_t *a1)
{
  uint64_t v1;
  uint64_t *result;
  unsigned int v3;
  unsigned int v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  int v8;
  unint64_t v9;

  v1 = *a1;
  v3 = *((_DWORD *)a1 + 2);
  result = a1 + 1;
  v4 = v3;
  v5 = *(_QWORD *)(v1 + 8);
  v6 = *(_QWORD *)(v1 + 24);
  v7 = *(_QWORD *)(v1 + 40);
  if (v5 < 0)
  {
    v8 = *(_DWORD *)(v1 + 48);
    v9 = v5 & 0x100000000 | v4 | 0x8000000000000000;
  }
  else
  {
    v8 = 0;
    v9 = v5 & 0x100000000 | v4;
    v6 &= 0x1FFFFFFFFuLL;
    v7 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 8) = v9;
  *(_QWORD *)(v1 + 24) = v6;
  *(_QWORD *)(v1 + 40) = v7;
  *(_DWORD *)(v1 + 48) = v8;
  return result;
}

uint64_t BNNS.RMSPropOptimizer.centered.setter(uint64_t result)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  int v4;
  uint64_t v5;
  unint64_t v6;
  unint64_t v7;

  v2 = *(_QWORD *)(v1 + 24);
  v3 = *(_QWORD *)(v1 + 40);
  if ((*(_QWORD *)(v1 + 8) & 0x8000000000000000) != 0)
  {
    v4 = *(_DWORD *)(v1 + 48);
    v7 = 0x8000000000000000;
    if ((result & 1) != 0)
      v7 = 0x8000000100000000;
    v6 = v7 & 0xFFFFFFFF00000000 | *(_QWORD *)(v1 + 8);
  }
  else
  {
    v4 = 0;
    v5 = 0x100000000;
    if ((result & 1) == 0)
      v5 = 0;
    v6 = v5 & 0xFFFFFFFF00000000 | *(_QWORD *)(v1 + 8);
    v2 &= 0x1FFFFFFFFuLL;
    v3 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 8) = v6;
  *(_QWORD *)(v1 + 24) = v2;
  *(_QWORD *)(v1 + 40) = v3;
  *(_DWORD *)(v1 + 48) = v4;
  return result;
}

unsigned __int8 *(*BNNS.RMSPropOptimizer.centered.modify(uint64_t a1))(unsigned __int8 *result)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_BYTE *)(a1 + 8) = *(_BYTE *)(v1 + 12) & 1;
  return BNNS.RMSPropOptimizer.centered.modify;
}

unsigned __int8 *BNNS.RMSPropOptimizer.centered.modify(unsigned __int8 *result)
{
  uint64_t v1;
  int v2;
  uint64_t v3;
  uint64_t v4;
  int v5;
  BOOL v6;
  uint64_t v7;
  unint64_t v8;
  unint64_t v9;

  v1 = *(_QWORD *)result;
  v2 = result[8];
  v3 = *(_QWORD *)(*(_QWORD *)result + 24);
  v4 = *(_QWORD *)(*(_QWORD *)result + 40);
  if ((*(_QWORD *)(*(_QWORD *)result + 8) & 0x8000000000000000) != 0)
  {
    v5 = *(_DWORD *)(v1 + 48);
    v6 = v2 == 0;
    v9 = 0x8000000000000000;
    if (!v6)
      v9 = 0x8000000100000000;
    v8 = v9 & 0xFFFFFFFF00000000 | *(_QWORD *)(*(_QWORD *)result + 8);
  }
  else
  {
    v5 = 0;
    v6 = v2 == 0;
    v7 = 0x100000000;
    if (v6)
      v7 = 0;
    v8 = v7 & 0xFFFFFFFF00000000 | *(_QWORD *)(*(_QWORD *)result + 8);
    v3 &= 0x1FFFFFFFFuLL;
    v4 = *(_QWORD *)(*(_QWORD *)result + 40);
  }
  *(_QWORD *)(v1 + 8) = v8;
  *(_QWORD *)(v1 + 24) = v3;
  *(_QWORD *)(v1 + 40) = v4;
  *(_DWORD *)(v1 + 48) = v5;
  return result;
}

unsigned int *key path setter for BNNS.RMSPropOptimizer.momentum : BNNS.RMSPropOptimizer(unsigned int *result, uint64_t a2)
{
  unsigned int v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  int v7;
  unint64_t v8;
  unint64_t v9;
  unint64_t v10;

  v2 = *result;
  v4 = *(_QWORD *)(a2 + 8);
  v3 = *(_QWORD *)(a2 + 16);
  v5 = *(_QWORD *)(a2 + 24);
  v6 = *(_QWORD *)(a2 + 40);
  if (v4 < 0)
  {
    v7 = *(_DWORD *)(a2 + 48);
    v10 = v3 & 0xFFFFFFFF00000000 | v2;
    v9 = v4 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v7 = 0;
    v8 = v3 & 0xFFFFFFFF00000000;
    v9 = v4 & 0x1FFFFFFFFLL;
    v10 = v8 | v2;
    v5 &= 0x1FFFFFFFFuLL;
    v6 = *(_QWORD *)(a2 + 40);
  }
  *(_QWORD *)(a2 + 8) = v9;
  *(_QWORD *)(a2 + 16) = v10;
  *(_QWORD *)(a2 + 24) = v5;
  *(_QWORD *)(a2 + 40) = v6;
  *(_DWORD *)(a2 + 48) = v7;
  return result;
}

void BNNS.RMSPropOptimizer.momentum.setter(float a1)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  int v6;
  unint64_t v7;
  unint64_t v8;
  unint64_t v9;

  v3 = *(_QWORD *)(v1 + 8);
  v2 = *(_QWORD *)(v1 + 16);
  v4 = *(_QWORD *)(v1 + 24);
  v5 = *(_QWORD *)(v1 + 40);
  if (v3 < 0)
  {
    v6 = *(_DWORD *)(v1 + 48);
    v9 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    v8 = v3 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v6 = 0;
    v7 = v2 & 0xFFFFFFFF00000000;
    v8 = v3 & 0x1FFFFFFFFLL;
    v9 = v7 | LODWORD(a1);
    v4 &= 0x1FFFFFFFFuLL;
    v5 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 8) = v8;
  *(_QWORD *)(v1 + 16) = v9;
  *(_QWORD *)(v1 + 24) = v4;
  *(_QWORD *)(v1 + 40) = v5;
  *(_DWORD *)(v1 + 48) = v6;
}

uint64_t *(*BNNS.RMSPropOptimizer.momentum.modify(uint64_t a1))(uint64_t *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_QWORD *)(v1 + 16);
  return BNNS.RMSPropOptimizer.momentum.modify;
}

uint64_t *BNNS.RMSPropOptimizer.momentum.modify(uint64_t *a1)
{
  uint64_t v1;
  uint64_t *result;
  unsigned int v3;
  unsigned int v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  int v9;
  unint64_t v10;
  unint64_t v11;
  unint64_t v12;

  v1 = *a1;
  v3 = *((_DWORD *)a1 + 2);
  result = a1 + 1;
  v4 = v3;
  v6 = *(_QWORD *)(v1 + 8);
  v5 = *(_QWORD *)(v1 + 16);
  v7 = *(_QWORD *)(v1 + 24);
  v8 = *(_QWORD *)(v1 + 40);
  if (v6 < 0)
  {
    v9 = *(_DWORD *)(v1 + 48);
    v12 = v5 & 0xFFFFFFFF00000000 | v4;
    v11 = v6 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v9 = 0;
    v10 = v5 & 0xFFFFFFFF00000000;
    v11 = v6 & 0x1FFFFFFFFLL;
    v12 = v10 | v4;
    v7 &= 0x1FFFFFFFFuLL;
    v8 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 8) = v11;
  *(_QWORD *)(v1 + 16) = v12;
  *(_QWORD *)(v1 + 24) = v7;
  *(_QWORD *)(v1 + 40) = v8;
  *(_DWORD *)(v1 + 48) = v9;
  return result;
}

float BNNS.RMSPropOptimizer.gradientScale.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 20);
}

unsigned int *key path setter for BNNS.RMSPropOptimizer.gradientScale : BNNS.RMSPropOptimizer(unsigned int *result, uint64_t a2)
{
  unsigned int v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  int v7;
  unint64_t v8;
  unint64_t v9;

  v2 = *result;
  v3 = *(_QWORD *)(a2 + 8);
  v4 = *(unsigned int *)(a2 + 16);
  v5 = *(_QWORD *)(a2 + 24);
  v6 = *(_QWORD *)(a2 + 40);
  if (v3 < 0)
  {
    v7 = *(_DWORD *)(a2 + 48);
    v9 = v4 | ((unint64_t)v2 << 32);
    v8 = v3 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v7 = 0;
    v8 = v3 & 0x1FFFFFFFFLL;
    v9 = v4 | ((unint64_t)v2 << 32);
    v5 &= 0x1FFFFFFFFuLL;
    v6 = *(_QWORD *)(a2 + 40);
  }
  *(_QWORD *)(a2 + 8) = v8;
  *(_QWORD *)(a2 + 16) = v9;
  *(_QWORD *)(a2 + 24) = v5;
  *(_QWORD *)(a2 + 40) = v6;
  *(_DWORD *)(a2 + 48) = v7;
  return result;
}

void BNNS.RMSPropOptimizer.gradientScale.setter(float a1)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  int v6;
  unint64_t v7;
  unint64_t v8;

  v2 = *(_QWORD *)(v1 + 8);
  v3 = *(unsigned int *)(v1 + 16);
  v4 = *(_QWORD *)(v1 + 24);
  v5 = *(_QWORD *)(v1 + 40);
  if (v2 < 0)
  {
    v6 = *(_DWORD *)(v1 + 48);
    v8 = v3 | ((unint64_t)LODWORD(a1) << 32);
    v7 = v2 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v6 = 0;
    v7 = v2 & 0x1FFFFFFFFLL;
    v8 = v3 | ((unint64_t)LODWORD(a1) << 32);
    v4 &= 0x1FFFFFFFFuLL;
    v5 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 8) = v7;
  *(_QWORD *)(v1 + 16) = v8;
  *(_QWORD *)(v1 + 24) = v4;
  *(_QWORD *)(v1 + 40) = v5;
  *(_DWORD *)(v1 + 48) = v6;
}

uint64_t *(*BNNS.RMSPropOptimizer.gradientScale.modify(uint64_t a1))(uint64_t *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 20);
  return BNNS.RMSPropOptimizer.gradientScale.modify;
}

uint64_t *BNNS.RMSPropOptimizer.gradientScale.modify(uint64_t *a1)
{
  uint64_t v1;
  uint64_t *result;
  unsigned int v3;
  unsigned int v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  int v9;
  unint64_t v10;
  unint64_t v11;

  v1 = *a1;
  v3 = *((_DWORD *)a1 + 2);
  result = a1 + 1;
  v4 = v3;
  v5 = *(_QWORD *)(v1 + 8);
  v6 = *(unsigned int *)(v1 + 16);
  v7 = *(_QWORD *)(v1 + 24);
  v8 = *(_QWORD *)(v1 + 40);
  if (v5 < 0)
  {
    v9 = *(_DWORD *)(v1 + 48);
    v11 = v6 | ((unint64_t)v4 << 32);
    v10 = v5 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v9 = 0;
    v10 = v5 & 0x1FFFFFFFFLL;
    v11 = v6 | ((unint64_t)v4 << 32);
    v7 &= 0x1FFFFFFFFuLL;
    v8 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 8) = v10;
  *(_QWORD *)(v1 + 16) = v11;
  *(_QWORD *)(v1 + 24) = v7;
  *(_QWORD *)(v1 + 40) = v8;
  *(_DWORD *)(v1 + 48) = v9;
  return result;
}

float BNNS.RMSPropOptimizer.regularizationScale.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 24);
}

void BNNS.RMSPropOptimizer.regularizationScale.setter(float a1)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  int v5;
  unint64_t v6;
  unint64_t v7;

  v2 = *(_QWORD *)(v1 + 8);
  v3 = *(_QWORD *)(v1 + 24);
  v4 = *(_QWORD *)(v1 + 40);
  if (v2 < 0)
  {
    v5 = *(_DWORD *)(v1 + 48);
    v7 = v3 & 0xFFFFFFFF00000000 | LODWORD(a1);
    v6 = v2 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v5 = 0;
    v6 = v2 & 0x1FFFFFFFFLL;
    v7 = v3 & 0x100000000 | LODWORD(a1);
    v4 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 8) = v6;
  *(_QWORD *)(v1 + 24) = v7;
  *(_QWORD *)(v1 + 40) = v4;
  *(_DWORD *)(v1 + 48) = v5;
}

uint64_t *(*BNNS.RMSPropOptimizer.regularizationScale.modify(uint64_t a1))(uint64_t *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_QWORD *)(v1 + 24);
  return BNNS.RMSPropOptimizer.regularizationScale.modify;
}

uint64_t *BNNS.RMSPropOptimizer.regularizationScale.modify(uint64_t *a1)
{
  uint64_t v1;
  uint64_t *result;
  unsigned int v3;
  unsigned int v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  int v8;
  unint64_t v9;
  unint64_t v10;

  v1 = *a1;
  v3 = *((_DWORD *)a1 + 2);
  result = a1 + 1;
  v4 = v3;
  v5 = *(_QWORD *)(v1 + 8);
  v6 = *(_QWORD *)(v1 + 24);
  v7 = *(_QWORD *)(v1 + 40);
  if (v5 < 0)
  {
    v8 = *(_DWORD *)(v1 + 48);
    v10 = v6 & 0xFFFFFFFF00000000 | v4;
    v9 = v5 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v8 = 0;
    v9 = v5 & 0x1FFFFFFFFLL;
    v10 = v6 & 0x100000000 | v4;
    v7 = *(_QWORD *)(v1 + 40);
  }
  *(_QWORD *)(v1 + 8) = v9;
  *(_QWORD *)(v1 + 24) = v10;
  *(_QWORD *)(v1 + 40) = v7;
  *(_DWORD *)(v1 + 48) = v8;
  return result;
}

uint64_t BNNS.RMSPropOptimizer.gradientBounds.getter()
{
  uint64_t v0;
  uint64_t result;

  result = *(_QWORD *)(v0 + 32);
  if ((*(_QWORD *)(v0 + 8) & 0x8000000000000000) != 0)
  {
    if ((_DWORD)result == 1)
    {
      if (*((float *)&result + 1) <= COERCE_FLOAT(*(_QWORD *)(v0 + 40)))
        return *(__int128 *)(v0 + 32) >> 32;
      goto LABEL_10;
    }
    return 0;
  }
  if ((*(_BYTE *)(v0 + 28) & 1) == 0)
    return 0;
  if (*(float *)&result > *((float *)&result + 1))
  {
    __break(1u);
LABEL_10:
    __break(1u);
  }
  return result;
}

uint64_t key path getter for BNNS.RMSPropOptimizer.gradientBounds : BNNS.RMSPropOptimizer@<X0>(uint64_t a1@<X8>)
{
  uint64_t result;
  char v3;

  result = BNNS.RMSPropOptimizer.gradientBounds.getter();
  *(_QWORD *)a1 = result;
  *(_BYTE *)(a1 + 8) = v3 & 1;
  return result;
}

uint64_t key path setter for BNNS.RMSPropOptimizer.gradientBounds : BNNS.RMSPropOptimizer(uint64_t result, uint64_t a2)
{
  unint64_t v2;
  uint64_t v3;
  unint64_t v4;
  int v5;
  uint64_t v6;
  unint64_t v7;
  unint64_t v8;
  unint64_t v9;
  unint64_t v10;

  v2 = *(_QWORD *)result;
  v3 = *(_QWORD *)(a2 + 8);
  v4 = *(_QWORD *)(a2 + 24);
  if (v3 < 0)
  {
    v5 = *(_DWORD *)(a2 + 48);
    v9 = HIDWORD(v2);
    v2 = (v2 << 32) | 1;
    if (*(_BYTE *)(result + 8))
      v10 = 0;
    else
      v10 = v9;
    if (*(_BYTE *)(result + 8))
      v2 = 0;
    v8 = *(_QWORD *)(a2 + 40) & 0xFFFFFFFF00000000 | v10;
    v7 = v3 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v5 = 0;
    if (*(_BYTE *)(result + 8))
      v6 = 0;
    else
      v6 = 0x100000000;
    if (*(_BYTE *)(result + 8))
      v2 = 0;
    v7 = v3 & 0x1FFFFFFFFLL;
    v8 = *(_QWORD *)(a2 + 40);
    v4 = v6 & 0xFFFFFFFF00000000 | *(_QWORD *)(a2 + 24);
  }
  *(_QWORD *)(a2 + 8) = v7;
  *(_QWORD *)(a2 + 24) = v4;
  *(_QWORD *)(a2 + 32) = v2;
  *(_QWORD *)(a2 + 40) = v8;
  *(_DWORD *)(a2 + 48) = v5;
  return result;
}

unint64_t BNNS.RMSPropOptimizer.gradientBounds.setter(unint64_t result, char a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  int v6;
  uint64_t v7;
  unint64_t v8;
  unint64_t v9;
  BOOL v10;
  unint64_t v11;

  v3 = a2 & 1;
  v4 = *(_QWORD *)(v2 + 8);
  v5 = *(_QWORD *)(v2 + 24);
  if (v4 < 0)
  {
    v6 = *(_DWORD *)(v2 + 48);
    v10 = (_DWORD)v3 == 0;
    if ((a2 & 1) != 0)
      v11 = 0;
    else
      v11 = HIDWORD(result);
    if (v10)
      v7 = (result << 32) | 1;
    else
      v7 = 0;
    v9 = *(_QWORD *)(v2 + 40) & 0xFFFFFFFF00000000 | v11;
    v8 = v4 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v6 = 0;
    if ((a2 & 1) != 0)
      v7 = 0;
    else
      v7 = result;
    v8 = v4 & 0x1FFFFFFFFLL;
    v5 = (*(_QWORD *)(v2 + 24) | (unint64_t)(v3 << 32)) ^ 0x100000000;
    v9 = *(_QWORD *)(v2 + 40);
  }
  *(_QWORD *)(v2 + 8) = v8;
  *(_QWORD *)(v2 + 24) = v5;
  *(_QWORD *)(v2 + 32) = v7;
  *(_QWORD *)(v2 + 40) = v9;
  *(_DWORD *)(v2 + 48) = v6;
  return result;
}

uint64_t (*BNNS.RMSPropOptimizer.gradientBounds.modify(uint64_t (*result)(uint64_t result)))(uint64_t result)
{
  uint64_t v1;
  unint64_t v2;
  char v3;
  unint64_t v4;

  *((_QWORD *)result + 2) = v1;
  v2 = *(_QWORD *)(v1 + 32);
  if ((*(_QWORD *)(v1 + 8) & 0x8000000000000000) == 0)
  {
    if ((*(_BYTE *)(v1 + 28) & 1) != 0)
    {
      if (*(float *)&v2 <= *((float *)&v2 + 1))
      {
        v3 = 0;
LABEL_9:
        *(_QWORD *)result = v2;
        *((_BYTE *)result + 8) = v3;
        return BNNS.RMSPropOptimizer.gradientBounds.modify;
      }
      __break(1u);
      goto LABEL_11;
    }
LABEL_8:
    v2 = 0;
    v3 = 1;
    goto LABEL_9;
  }
  if ((_DWORD)v2 != 1)
    goto LABEL_8;
  v4 = HIDWORD(v2);
  if (*(float *)&v4 <= COERCE_FLOAT(*(_QWORD *)(v1 + 40)))
  {
    v3 = 0;
    v2 = v4 | (*(_QWORD *)(v1 + 40) << 32);
    goto LABEL_9;
  }
LABEL_11:
  __break(1u);
  return result;
}

uint64_t BNNS.RMSPropOptimizer.gradientBounds.modify(uint64_t result)
{
  uint64_t v1;
  unint64_t v2;
  uint64_t v3;
  unint64_t v4;
  int v5;
  uint64_t v6;
  unint64_t v7;
  unint64_t v8;
  unint64_t v9;
  unint64_t v10;

  v1 = *(_QWORD *)(result + 16);
  v2 = *(_QWORD *)result;
  v3 = *(_QWORD *)(v1 + 8);
  v4 = *(_QWORD *)(v1 + 24);
  if (v3 < 0)
  {
    v5 = *(_DWORD *)(v1 + 48);
    v9 = HIDWORD(v2);
    v2 = (v2 << 32) | 1;
    if (*(_BYTE *)(result + 8))
      v10 = 0;
    else
      v10 = v9;
    if (*(_BYTE *)(result + 8))
      v2 = 0;
    v8 = *(_QWORD *)(v1 + 40) & 0xFFFFFFFF00000000 | v10;
    v7 = v3 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v5 = 0;
    if (*(_BYTE *)(result + 8))
      v6 = 0;
    else
      v6 = 0x100000000;
    if (*(_BYTE *)(result + 8))
      v2 = 0;
    v7 = v3 & 0x1FFFFFFFFLL;
    v8 = *(_QWORD *)(v1 + 40);
    v4 = v6 & 0xFFFFFFFF00000000 | *(_QWORD *)(v1 + 24);
  }
  *(_QWORD *)(v1 + 8) = v7;
  *(_QWORD *)(v1 + 24) = v4;
  *(_QWORD *)(v1 + 32) = v2;
  *(_QWORD *)(v1 + 40) = v8;
  *(_DWORD *)(v1 + 48) = v5;
  return result;
}

uint64_t BNNS.RMSPropOptimizer.regularizationFunction.getter()
{
  uint64_t v0;

  if ((*(_QWORD *)(v0 + 8) & 0x8000000000000000) != 0)
    return *(unsigned int *)(v0 + 28);
  else
    return *(_QWORD *)(v0 + 40);
}

uint64_t BNNS.RMSPropOptimizer.regularizationFunction.setter(uint64_t result)
{
  uint64_t v1;
  uint64_t v2;
  int v3;
  unint64_t v4;
  unint64_t v5;
  uint64_t v6;

  v2 = *(_QWORD *)(v1 + 8);
  if (v2 < 0)
  {
    v3 = *(_DWORD *)(v1 + 48);
    v6 = *(_QWORD *)(v1 + 40);
    v5 = *(_QWORD *)(v1 + 24) | ((unint64_t)result << 32);
    v4 = v2 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v3 = 0;
    v4 = v2 & 0x1FFFFFFFFLL;
    v5 = *(_QWORD *)(v1 + 24) & 0x1FFFFFFFFLL;
    v6 = result;
  }
  *(_QWORD *)(v1 + 8) = v4;
  *(_QWORD *)(v1 + 24) = v5;
  *(_QWORD *)(v1 + 40) = v6;
  *(_DWORD *)(v1 + 48) = v3;
  return result;
}

uint64_t *(*BNNS.RMSPropOptimizer.regularizationFunction.modify(uint64_t a1))(uint64_t *a1)
{
  uint64_t v1;
  uint64_t v2;

  *(_QWORD *)a1 = v1;
  if ((*(_QWORD *)(v1 + 8) & 0x8000000000000000) != 0)
    LODWORD(v2) = *(_DWORD *)(v1 + 28);
  else
    v2 = *(_QWORD *)(v1 + 40);
  *(_DWORD *)(a1 + 8) = v2;
  return BNNS.RMSPropOptimizer.regularizationFunction.modify;
}

uint64_t *BNNS.RMSPropOptimizer.regularizationFunction.modify(uint64_t *a1)
{
  uint64_t v1;
  uint64_t *result;
  uint64_t v3;
  unsigned int v4;
  uint64_t v5;
  int v6;
  unint64_t v7;
  unint64_t v8;

  v1 = *a1;
  v4 = *((_DWORD *)a1 + 2);
  result = a1 + 1;
  v3 = v4;
  v5 = *(_QWORD *)(v1 + 8);
  if (v5 < 0)
  {
    v6 = *(_DWORD *)(v1 + 48);
    v8 = *(_QWORD *)(v1 + 24) | ((unint64_t)v3 << 32);
    v3 = *(_QWORD *)(v1 + 40);
    v7 = v5 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v6 = 0;
    v7 = v5 & 0x1FFFFFFFFLL;
    v8 = *(_QWORD *)(v1 + 24) & 0x1FFFFFFFFLL;
  }
  *(_QWORD *)(v1 + 8) = v7;
  *(_QWORD *)(v1 + 24) = v8;
  *(_QWORD *)(v1 + 40) = v3;
  *(_DWORD *)(v1 + 48) = v6;
  return result;
}

uint64_t BNNS.RMSPropOptimizer.init(learningRate:alpha:epsilon:centered:momentum:gradientScale:regularizationScale:clipsGradientsTo:regularizationFunction:)@<X0>(uint64_t result@<X0>, unint64_t a2@<X1>, char a3@<W2>, uint64_t a4@<X3>, uint64_t a5@<X8>, unsigned int a6@<S0>, unsigned int a7@<S1>, unsigned int a8@<S2>, unsigned int a9@<S3>, unsigned int a10@<S4>, unsigned int a11@<S5>)
{
  unint64_t v11;
  uint64_t v12;
  uint64_t v13;

  v11 = HIDWORD(a2);
  v12 = (a2 << 32) | 1;
  if ((a3 & 1) != 0)
  {
    v11 = 0;
    v12 = 0;
  }
  v13 = 0x100000000;
  if ((result & 1) == 0)
    v13 = 0;
  *(_QWORD *)a5 = a6 | ((unint64_t)a7 << 32);
  *(_QWORD *)(a5 + 8) = v13 | a8 | 0x8000000000000000;
  *(_QWORD *)(a5 + 16) = a9 | ((unint64_t)a10 << 32);
  *(_QWORD *)(a5 + 24) = a11 | (unint64_t)(a4 << 32);
  *(_QWORD *)(a5 + 32) = v12;
  *(_QWORD *)(a5 + 40) = v11;
  *(_DWORD *)(a5 + 48) = 0;
  return result;
}

uint64_t BNNS.RMSPropOptimizer.bnnsOptimizerFunction.getter()
{
  uint64_t v0;

  if (*(uint64_t *)(v0 + 8) < 0)
    return 9;
  else
    return 3;
}

uint64_t protocol witness for BNNSOptimizer.step(parameters:gradients:accumulators:filterParameters:) in conformance BNNS.RMSPropOptimizer(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  __int128 *v9;
  uint64_t v13;
  uint64_t v14;
  int v15;
  uint64_t v16;
  __int128 v18;
  __int128 v19;
  _QWORD v20[5];

  v18 = v9[1];
  v19 = *v9;
  v13 = *((_QWORD *)v9 + 4);
  v14 = *((_QWORD *)v9 + 5);
  v15 = *((_DWORD *)v9 + 12);
  v20[3] = a8;
  v20[4] = a9;
  v16 = swift_allocObject();
  v20[0] = v16;
  *(_OWORD *)(v16 + 16) = v19;
  *(_OWORD *)(v16 + 32) = v18;
  *(_QWORD *)(v16 + 48) = v13;
  *(_QWORD *)(v16 + 56) = v14;
  *(_DWORD *)(v16 + 64) = v15;
  specialized static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)((uint64_t)v20, a1, a2, a3);
  return __swift_destroy_boxed_opaque_existential_1((uint64_t)v20);
}

uint64_t protocol witness for BNNSOptimizer.bnnsOptimizerFunction.getter in conformance BNNS.RMSPropOptimizer()
{
  uint64_t v0;

  if (*(uint64_t *)(v0 + 8) < 0)
    return 9;
  else
    return 3;
}

uint64_t protocol witness for BNNSOptimizer.accumulatorCountMultiplier.getter in conformance BNNS.RMSPropOptimizer()
{
  uint64_t v0;
  uint64_t v1;

  v1 = 1;
  if ((*(_DWORD *)(v0 + 16) & 0x7FFFFFFF) != 0)
    v1 = 2;
  return v1 + (HIDWORD(*(_QWORD *)(v0 + 8)) & 1);
}

uint64_t protocol witness for WithOptimizerAlgFields.withOptimizerAlgFields(body:) in conformance BNNS.RMSPropOptimizer(uint64_t (*a1)(_OWORD *))
{
  uint64_t v1;
  int v2;
  __int128 v3;
  _OWORD v5[3];
  int v6;
  uint64_t v7;

  v7 = *MEMORY[0x1E0C80C00];
  v2 = *(_DWORD *)(v1 + 48);
  v3 = *(_OWORD *)(v1 + 16);
  v5[0] = *(_OWORD *)v1;
  v5[1] = v3;
  v5[2] = *(_OWORD *)(v1 + 32);
  v6 = v2;
  return a1(v5);
}

BOOL BNNS.SGDMomentumOptimizer.accumulatorCountMultiplier.getter()
{
  uint64_t v0;

  return (*(_DWORD *)(v0 + 4) & 0x7FFFFFFF) != 0;
}

float BNNS.SGDMomentumOptimizer.momentum.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 4);
}

float BNNS.SGDMomentumOptimizer.learningRate.getter()
{
  uint64_t v0;

  return *(float *)v0;
}

unsigned int *key path setter for BNNS.SGDMomentumOptimizer.learningRate : BNNS.SGDMomentumOptimizer(unsigned int *result, unint64_t *a2)
{
  unsigned int v2;
  unint64_t v3;
  unint64_t v4;
  unint64_t v5;
  unint64_t v6;
  unint64_t v7;
  unint64_t v8;

  v2 = *result;
  v3 = *a2;
  v5 = a2[2];
  v4 = a2[3];
  if ((v5 & 0x80000000) != 0)
  {
    v6 = a2[5];
    v7 = v3 & 0xFFFFFFFF00000000 | v2;
    v8 = v5 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v6 = 0;
    v7 = v3 & 0xFFFFFFFF00000000 | v2;
    v8 = v5 & 0xFFFFFFFF00000001;
    v4 &= 0x1FFFFFFFFuLL;
  }
  *a2 = v7;
  a2[2] = v8;
  a2[3] = v4;
  a2[5] = v6;
  return result;
}

void BNNS.SGDMomentumOptimizer.learningRate.setter(float a1)
{
  unint64_t *v1;
  unint64_t v2;
  unint64_t v3;
  unint64_t v4;
  unint64_t v5;
  unint64_t v6;
  unint64_t v7;

  v2 = *v1;
  v4 = v1[2];
  v3 = v1[3];
  if ((v4 & 0x80000000) != 0)
  {
    v5 = v1[5];
    v6 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    v7 = v4 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v5 = 0;
    v6 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    v7 = v4 & 0xFFFFFFFF00000001;
    v3 &= 0x1FFFFFFFFuLL;
  }
  *v1 = v6;
  v1[2] = v7;
  v1[3] = v3;
  v1[5] = v5;
}

uint64_t (*BNNS.SGDMomentumOptimizer.learningRate.modify(uint64_t a1))(uint64_t a1)
{
  _QWORD *v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *v1;
  return BNNS.SGDMomentumOptimizer.learningRate.modify;
}

uint64_t BNNS.SGDMomentumOptimizer.learningRate.modify(uint64_t a1)
{
  unint64_t *v1;
  uint64_t result;
  unsigned int v3;
  unsigned int v4;
  unint64_t v5;
  unint64_t v6;
  unint64_t v7;
  unint64_t v8;
  unint64_t v9;
  unint64_t v10;

  v1 = *(unint64_t **)a1;
  v3 = *(_DWORD *)(a1 + 8);
  result = a1 + 8;
  v4 = v3;
  v5 = *v1;
  v7 = v1[2];
  v6 = v1[3];
  if ((v7 & 0x80000000) != 0)
  {
    v8 = v1[5];
    v9 = v5 & 0xFFFFFFFF00000000 | v4;
    v10 = v7 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v8 = 0;
    v9 = v5 & 0xFFFFFFFF00000000 | v4;
    v10 = v7 & 0xFFFFFFFF00000001;
    v6 &= 0x1FFFFFFFFuLL;
  }
  *v1 = v9;
  v1[2] = v10;
  v1[3] = v6;
  v1[5] = v8;
  return result;
}

unsigned int *key path setter for BNNS.SGDMomentumOptimizer.momentum : BNNS.SGDMomentumOptimizer(unsigned int *result, unint64_t *a2)
{
  unsigned int v2;
  uint64_t v3;
  unint64_t v4;
  unint64_t v5;
  unint64_t v6;
  unint64_t v7;
  unint64_t v8;

  v2 = *result;
  v3 = *(unsigned int *)a2;
  v5 = a2[2];
  v4 = a2[3];
  if ((v5 & 0x80000000) != 0)
  {
    v6 = a2[5];
    v7 = v3 | ((unint64_t)v2 << 32);
    v8 = v5 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v6 = 0;
    v7 = v3 | ((unint64_t)v2 << 32);
    v8 = v5 & 0xFFFFFFFF00000001;
    v4 &= 0x1FFFFFFFFuLL;
  }
  *a2 = v7;
  a2[2] = v8;
  a2[3] = v4;
  a2[5] = v6;
  return result;
}

void BNNS.SGDMomentumOptimizer.momentum.setter(float a1)
{
  unint64_t *v1;
  uint64_t v2;
  unint64_t v3;
  unint64_t v4;
  unint64_t v5;
  unint64_t v6;
  unint64_t v7;

  v2 = *(unsigned int *)v1;
  v4 = v1[2];
  v3 = v1[3];
  if ((v4 & 0x80000000) != 0)
  {
    v5 = v1[5];
    v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
    v7 = v4 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v5 = 0;
    v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
    v7 = v4 & 0xFFFFFFFF00000001;
    v3 &= 0x1FFFFFFFFuLL;
  }
  *v1 = v6;
  v1[2] = v7;
  v1[3] = v3;
  v1[5] = v5;
}

uint64_t (*BNNS.SGDMomentumOptimizer.momentum.modify(uint64_t a1))(uint64_t a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 4);
  return BNNS.SGDMomentumOptimizer.momentum.modify;
}

uint64_t BNNS.SGDMomentumOptimizer.momentum.modify(uint64_t a1)
{
  unint64_t *v1;
  uint64_t result;
  unsigned int v3;
  unsigned int v4;
  uint64_t v5;
  unint64_t v6;
  unint64_t v7;
  unint64_t v8;
  unint64_t v9;
  unint64_t v10;

  v1 = *(unint64_t **)a1;
  v3 = *(_DWORD *)(a1 + 8);
  result = a1 + 8;
  v4 = v3;
  v5 = *(unsigned int *)v1;
  v7 = v1[2];
  v6 = v1[3];
  if ((v7 & 0x80000000) != 0)
  {
    v8 = v1[5];
    v9 = v5 | ((unint64_t)v4 << 32);
    v10 = v7 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v8 = 0;
    v9 = v5 | ((unint64_t)v4 << 32);
    v10 = v7 & 0xFFFFFFFF00000001;
    v6 &= 0x1FFFFFFFFuLL;
  }
  *v1 = v9;
  v1[2] = v10;
  v1[3] = v6;
  v1[5] = v8;
  return result;
}

float BNNS.SGDMomentumOptimizer.gradientScale.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 8);
}

unsigned int *key path setter for BNNS.SGDMomentumOptimizer.gradientScale : BNNS.SGDMomentumOptimizer(unsigned int *result, _QWORD *a2)
{
  unsigned int v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  unint64_t v7;
  unint64_t v8;

  v2 = *result;
  v3 = a2[1];
  v4 = a2[2];
  v5 = a2[3];
  if ((v4 & 0x80000000) != 0)
  {
    v6 = a2[5];
    v7 = v3 & 0xFFFFFFFF00000000 | v2;
    v8 = v4 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v6 = 0;
    v7 = v3 & 0xFFFFFFFF00000000 | v2;
    v8 = v4 & 0xFFFFFFFF00000001;
    v5 &= 0x1FFFFFFFFuLL;
  }
  a2[1] = v7;
  a2[2] = v8;
  a2[3] = v5;
  a2[5] = v6;
  return result;
}

void BNNS.SGDMomentumOptimizer.gradientScale.setter(float a1)
{
  _QWORD *v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  unint64_t v6;
  unint64_t v7;

  v2 = v1[1];
  v3 = v1[2];
  v4 = v1[3];
  if ((v3 & 0x80000000) != 0)
  {
    v5 = v1[5];
    v6 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    v7 = v3 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v5 = 0;
    v6 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    v7 = v3 & 0xFFFFFFFF00000001;
    v4 &= 0x1FFFFFFFFuLL;
  }
  v1[1] = v6;
  v1[2] = v7;
  v1[3] = v4;
  v1[5] = v5;
}

uint64_t (*BNNS.SGDMomentumOptimizer.gradientScale.modify(uint64_t a1))(uint64_t a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_QWORD *)(v1 + 8);
  return BNNS.SGDMomentumOptimizer.gradientScale.modify;
}

uint64_t BNNS.SGDMomentumOptimizer.gradientScale.modify(uint64_t a1)
{
  _QWORD *v1;
  uint64_t result;
  unsigned int v3;
  unsigned int v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  unint64_t v9;
  unint64_t v10;

  v1 = *(_QWORD **)a1;
  v3 = *(_DWORD *)(a1 + 8);
  result = a1 + 8;
  v4 = v3;
  v5 = v1[1];
  v6 = v1[2];
  v7 = v1[3];
  if ((v6 & 0x80000000) != 0)
  {
    v8 = v1[5];
    v9 = v5 & 0xFFFFFFFF00000000 | v4;
    v10 = v6 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v8 = 0;
    v9 = v5 & 0xFFFFFFFF00000000 | v4;
    v10 = v6 & 0xFFFFFFFF00000001;
    v7 &= 0x1FFFFFFFFuLL;
  }
  v1[1] = v9;
  v1[2] = v10;
  v1[3] = v7;
  v1[5] = v8;
  return result;
}

float BNNS.SGDMomentumOptimizer.regularizationScale.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 12);
}

unsigned int *key path setter for BNNS.SGDMomentumOptimizer.regularizationScale : BNNS.SGDMomentumOptimizer(unsigned int *result, uint64_t a2)
{
  unsigned int v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  unint64_t v7;
  unint64_t v8;

  v2 = *result;
  v3 = *(unsigned int *)(a2 + 8);
  v5 = *(_QWORD *)(a2 + 16);
  v4 = *(_QWORD *)(a2 + 24);
  if ((v5 & 0x80000000) != 0)
  {
    v6 = *(_QWORD *)(a2 + 40);
    v7 = v3 | ((unint64_t)v2 << 32);
    v8 = v5 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v6 = 0;
    v7 = v3 | ((unint64_t)v2 << 32);
    v8 = v5 & 0xFFFFFFFF00000001;
    v4 &= 0x1FFFFFFFFuLL;
  }
  *(_QWORD *)(a2 + 8) = v7;
  *(_QWORD *)(a2 + 16) = v8;
  *(_QWORD *)(a2 + 24) = v4;
  *(_QWORD *)(a2 + 40) = v6;
  return result;
}

void BNNS.SGDMomentumOptimizer.regularizationScale.setter(float a1)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  unint64_t v6;
  unint64_t v7;

  v2 = *(unsigned int *)(v1 + 8);
  v4 = *(_QWORD *)(v1 + 16);
  v3 = *(_QWORD *)(v1 + 24);
  if ((v4 & 0x80000000) != 0)
  {
    v5 = *(_QWORD *)(v1 + 40);
    v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
    v7 = v4 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v5 = 0;
    v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
    v7 = v4 & 0xFFFFFFFF00000001;
    v3 &= 0x1FFFFFFFFuLL;
  }
  *(_QWORD *)(v1 + 8) = v6;
  *(_QWORD *)(v1 + 16) = v7;
  *(_QWORD *)(v1 + 24) = v3;
  *(_QWORD *)(v1 + 40) = v5;
}

uint64_t *(*BNNS.SGDMomentumOptimizer.regularizationScale.modify(uint64_t a1))(uint64_t *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 12);
  return BNNS.SGDMomentumOptimizer.regularizationScale.modify;
}

uint64_t *BNNS.SGDMomentumOptimizer.regularizationScale.modify(uint64_t *a1)
{
  uint64_t v1;
  uint64_t *result;
  unsigned int v3;
  unsigned int v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  unint64_t v9;
  unint64_t v10;

  v1 = *a1;
  v3 = *((_DWORD *)a1 + 2);
  result = a1 + 1;
  v4 = v3;
  v5 = *(unsigned int *)(v1 + 8);
  v7 = *(_QWORD *)(v1 + 16);
  v6 = *(_QWORD *)(v1 + 24);
  if ((v7 & 0x80000000) != 0)
  {
    v8 = *(_QWORD *)(v1 + 40);
    v9 = v5 | ((unint64_t)v4 << 32);
    v10 = v7 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v8 = 0;
    v9 = v5 | ((unint64_t)v4 << 32);
    v10 = v7 & 0xFFFFFFFF00000001;
    v6 &= 0x1FFFFFFFFuLL;
  }
  *(_QWORD *)(v1 + 8) = v9;
  *(_QWORD *)(v1 + 16) = v10;
  *(_QWORD *)(v1 + 24) = v6;
  *(_QWORD *)(v1 + 40) = v8;
  return result;
}

uint64_t BNNS.SGDMomentumOptimizer.gradientBounds.getter()
{
  _QWORD *v0;
  unint64_t v1;
  unint64_t v2;
  unint64_t v3;
  uint64_t result;

  v2 = v0[2];
  v1 = v0[3];
  if ((v2 & 0x80000000) == 0)
  {
    if ((v2 & 1) != 0)
    {
      v3 = HIDWORD(v2);
      if (*(float *)&v3 <= *(float *)&v1)
        return v3 | (v1 << 32);
      __break(1u);
      goto LABEL_10;
    }
    return 0;
  }
  if (HIDWORD(v1) != 1)
    return 0;
  result = v0[4];
  if (*(float *)&result > *((float *)&result + 1))
LABEL_10:
    __break(1u);
  return result;
}

unint64_t *key path setter for BNNS.SGDMomentumOptimizer.gradientBounds : BNNS.SGDMomentumOptimizer(unint64_t *result, _QWORD *a2)
{
  unint64_t v2;
  int v3;
  uint64_t v4;
  uint64_t v5;
  unint64_t v6;
  unint64_t v7;
  uint64_t v8;
  unint64_t v9;
  unint64_t v10;
  BOOL v11;
  uint64_t v12;

  v2 = *result;
  v3 = *((unsigned __int8 *)result + 8);
  v4 = a2[2];
  if ((v4 & 0x80000000) != 0)
  {
    v5 = a2[5];
    v11 = v3 == 0;
    if (*((_BYTE *)result + 8))
      v12 = 0;
    else
      v12 = 0x100000000;
    if (v11)
      v6 = *result;
    else
      v6 = 0;
    v10 = v12 & 0xFFFFFFFF00000000 | a2[3];
    v9 = v4 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v5 = 0;
    v6 = a2[4];
    v7 = HIDWORD(v2);
    v8 = v2 << 32;
    if (*((_BYTE *)result + 8))
    {
      v8 = 0;
      v7 = 0;
    }
    v9 = v8 | v3 ^ 1u;
    v10 = a2[3] & 0x100000000 | v7;
  }
  a2[2] = v9;
  a2[3] = v10;
  a2[4] = v6;
  a2[5] = v5;
  return result;
}

unint64_t BNNS.SGDMomentumOptimizer.gradientBounds.setter(unint64_t result, char a2)
{
  _QWORD *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  BOOL v6;
  _BOOL8 v7;
  unint64_t v8;
  unint64_t v9;
  uint64_t v10;
  unint64_t v11;
  uint64_t v12;

  v3 = a2 & 1;
  v4 = v2[2];
  if ((v4 & 0x80000000) != 0)
  {
    v5 = v2[5];
    if ((a2 & 1) != 0)
      v8 = 0;
    else
      v8 = result;
    v12 = (v2[3] | (unint64_t)(v3 << 32)) ^ 0x100000000;
    v11 = v4 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v5 = 0;
    v6 = (_DWORD)v3 == 0;
    v7 = (_DWORD)v3 == 0;
    v8 = v2[4];
    v9 = HIDWORD(result);
    v10 = result << 32;
    if (!v6)
    {
      v10 = 0;
      v9 = 0;
    }
    v11 = v10 | v7;
    v12 = v2[3] & 0x100000000 | v9;
  }
  v2[2] = v11;
  v2[3] = v12;
  v2[4] = v8;
  v2[5] = v5;
  return result;
}

unint64_t *(*BNNS.SGDMomentumOptimizer.gradientBounds.modify(unint64_t *(*result)(unint64_t *result)))(unint64_t *result)
{
  _QWORD *v1;
  unint64_t v2;
  unint64_t v3;
  unint64_t v4;
  char v5;
  uint64_t v6;

  *((_QWORD *)result + 2) = v1;
  v3 = v1[2];
  v2 = v1[3];
  if ((v3 & 0x80000000) == 0)
  {
    if ((v3 & 1) != 0)
    {
      v4 = HIDWORD(v3);
      if (*((float *)&v3 + 1) <= *(float *)&v2)
      {
        v5 = 0;
        v6 = v4 | (v2 << 32);
LABEL_9:
        *(_QWORD *)result = v6;
        *((_BYTE *)result + 8) = v5;
        return BNNS.SGDMomentumOptimizer.gradientBounds.modify;
      }
      __break(1u);
      goto LABEL_11;
    }
LABEL_8:
    v6 = 0;
    v5 = 1;
    goto LABEL_9;
  }
  if (HIDWORD(v2) != 1)
    goto LABEL_8;
  v6 = v1[4];
  if (*(float *)&v6 <= *((float *)&v6 + 1))
  {
    v5 = 0;
    goto LABEL_9;
  }
LABEL_11:
  __break(1u);
  return result;
}

unint64_t *BNNS.SGDMomentumOptimizer.gradientBounds.modify(unint64_t *result)
{
  _QWORD *v1;
  unint64_t v2;
  int v3;
  uint64_t v4;
  uint64_t v5;
  unint64_t v6;
  unint64_t v7;
  uint64_t v8;
  unint64_t v9;
  unint64_t v10;
  BOOL v11;
  uint64_t v12;

  v1 = (_QWORD *)result[2];
  v2 = *result;
  v3 = *((unsigned __int8 *)result + 8);
  v4 = v1[2];
  if ((v4 & 0x80000000) != 0)
  {
    v5 = v1[5];
    v11 = v3 == 0;
    if (*((_BYTE *)result + 8))
      v12 = 0;
    else
      v12 = 0x100000000;
    if (v11)
      v6 = *result;
    else
      v6 = 0;
    v10 = v12 & 0xFFFFFFFF00000000 | v1[3];
    v9 = v4 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v5 = 0;
    v6 = v1[4];
    v7 = HIDWORD(v2);
    v8 = v2 << 32;
    if (*((_BYTE *)result + 8))
    {
      v8 = 0;
      v7 = 0;
    }
    v9 = v8 | v3 ^ 1u;
    v10 = v1[3] & 0x100000000 | v7;
  }
  v1[2] = v9;
  v1[3] = v10;
  v1[4] = v6;
  v1[5] = v5;
  return result;
}

uint64_t (*BNNS.SGDMomentumOptimizer.usesNestrovMomentum.modify(uint64_t a1))()
{
  uint64_t v1;
  uint64_t v2;

  *(_QWORD *)a1 = v1;
  if ((*(_QWORD *)(v1 + 16) & 0x80000000) != 0)
    v2 = *(_QWORD *)(v1 + 16) & 1;
  else
    v2 = *(_QWORD *)(v1 + 24) & 0x100000000;
  *(_BYTE *)(a1 + 8) = v2 != 0;
  return BNNS.SGDMomentumOptimizer.usesNestrovMomentum.modify;
}

BOOL BNNS.SGDMomentumOptimizer.usesNestrovMomentum.getter()
{
  uint64_t v0;
  uint64_t v1;

  if ((*(_QWORD *)(v0 + 16) & 0x80000000) != 0)
    v1 = *(_QWORD *)(v0 + 16) & 1;
  else
    v1 = *(_QWORD *)(v0 + 24) & 0x100000000;
  return v1 != 0;
}

uint64_t BNNS.SGDMomentumOptimizer.usesNestrovMomentum.setter(uint64_t result)
{
  _QWORD *v1;
  unint64_t v2;
  uint64_t v3;
  uint64_t v4;
  unint64_t v5;
  uint64_t v6;

  v3 = v1[2];
  v2 = v1[3];
  if ((v3 & 0x80000000) != 0)
  {
    v4 = v1[5];
    v5 = v3 & 0xFFFFFFFF00000000 | result & 1 | 0x80000000;
  }
  else
  {
    v4 = 0;
    v5 = v3 & 0xFFFFFFFF00000001;
    v6 = 0x100000000;
    if ((result & 1) == 0)
      v6 = 0;
    v2 = v6 & 0xFFFFFFFF00000000 | v1[3];
  }
  v1[2] = v5;
  v1[3] = v2;
  v1[5] = v4;
  return result;
}

uint64_t (*BNNS.SGDMomentumOptimizer.usesNesterovMomentum.modify(uint64_t a1))()
{
  uint64_t v1;
  uint64_t v2;

  *(_QWORD *)a1 = v1;
  if ((*(_QWORD *)(v1 + 16) & 0x80000000) != 0)
    v2 = *(_QWORD *)(v1 + 16) & 1;
  else
    v2 = *(_QWORD *)(v1 + 24) & 0x100000000;
  *(_BYTE *)(a1 + 8) = v2 != 0;
  return BNNS.SGDMomentumOptimizer.usesNestrovMomentum.modify;
}

unsigned __int8 *BNNS.SGDMomentumOptimizer.usesNestrovMomentum.modify(unsigned __int8 *result)
{
  _QWORD *v1;
  uint64_t v2;
  unint64_t v3;
  uint64_t v4;
  uint64_t v5;
  unint64_t v6;
  BOOL v7;
  uint64_t v8;

  v1 = *(_QWORD **)result;
  v2 = result[8];
  v4 = *(_QWORD *)(*(_QWORD *)result + 16);
  v3 = *(_QWORD *)(*(_QWORD *)result + 24);
  if ((v4 & 0x80000000) != 0)
  {
    v5 = v1[5];
    v6 = v4 & 0xFFFFFFFF00000000 | v2 | 0x80000000;
  }
  else
  {
    v5 = 0;
    v6 = v4 & 0xFFFFFFFF00000001;
    v7 = (_DWORD)v2 == 0;
    v8 = 0x100000000;
    if (v7)
      v8 = 0;
    v3 = v8 & 0xFFFFFFFF00000000 | *(_QWORD *)(*(_QWORD *)result + 24);
  }
  v1[2] = v6;
  v1[3] = v3;
  v1[5] = v5;
  return result;
}

unint64_t BNNS.SGDMomentumOptimizer.regularizationFunction.getter()
{
  uint64_t v0;
  unint64_t v1;

  v1 = *(_QWORD *)(v0 + 16);
  if ((v1 & 0x80000000) != 0)
    return HIDWORD(v1);
  else
    return *(_QWORD *)(v0 + 32);
}

unsigned int *key path setter for BNNS.SGDMomentumOptimizer.regularizationFunction : BNNS.SGDMomentumOptimizer(unsigned int *result, _QWORD *a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  unint64_t v5;
  uint64_t v6;
  unint64_t v7;

  v2 = *result;
  v4 = a2[2];
  v3 = a2[3];
  v5 = a2[4];
  if ((v4 & 0x80000000) != 0)
  {
    v6 = a2[5];
    v7 = a2[2] & 1 | (v2 << 32) | 0x80000000;
  }
  else
  {
    v6 = 0;
    v7 = v4 & 0xFFFFFFFF00000001;
    v3 &= 0x1FFFFFFFFuLL;
    v5 = v5 & 0xFFFFFFFF00000000 | v2;
  }
  a2[2] = v7;
  a2[3] = v3;
  a2[4] = v5;
  a2[5] = v6;
  return result;
}

uint64_t BNNS.SGDMomentumOptimizer.regularizationFunction.setter(uint64_t result)
{
  _QWORD *v1;
  uint64_t v2;
  uint64_t v3;
  unint64_t v4;
  uint64_t v5;
  unint64_t v6;

  v3 = v1[2];
  v2 = v1[3];
  v4 = v1[4];
  if ((v3 & 0x80000000) != 0)
  {
    v5 = v1[5];
    v6 = v1[2] & 1 | (result << 32) | 0x80000000;
  }
  else
  {
    v5 = 0;
    v6 = v3 & 0xFFFFFFFF00000001;
    v2 &= 0x1FFFFFFFFuLL;
    v4 = v4 & 0xFFFFFFFF00000000 | result;
  }
  v1[2] = v6;
  v1[3] = v2;
  v1[4] = v4;
  v1[5] = v5;
  return result;
}

uint64_t (*BNNS.SGDMomentumOptimizer.regularizationFunction.modify(uint64_t a1))(uint64_t a1)
{
  uint64_t v1;
  unint64_t v2;
  unint64_t v3;

  *(_QWORD *)a1 = v1;
  v2 = *(_QWORD *)(v1 + 16);
  if ((v2 & 0x80000000) != 0)
    v3 = HIDWORD(v2);
  else
    v3 = *(_QWORD *)(v1 + 32);
  *(_DWORD *)(a1 + 8) = v3;
  return BNNS.SGDMomentumOptimizer.regularizationFunction.modify;
}

uint64_t BNNS.SGDMomentumOptimizer.regularizationFunction.modify(uint64_t a1)
{
  _QWORD *v1;
  uint64_t result;
  uint64_t v3;
  unsigned int v4;
  uint64_t v5;
  uint64_t v6;
  unint64_t v7;
  uint64_t v8;
  unint64_t v9;

  v1 = *(_QWORD **)a1;
  v4 = *(_DWORD *)(a1 + 8);
  result = a1 + 8;
  v3 = v4;
  v6 = v1[2];
  v5 = v1[3];
  v7 = v1[4];
  if ((v6 & 0x80000000) != 0)
  {
    v8 = v1[5];
    v9 = v1[2] & 1 | (v3 << 32) | 0x80000000;
  }
  else
  {
    v8 = 0;
    v9 = v6 & 0xFFFFFFFF00000001;
    v5 &= 0x1FFFFFFFFuLL;
    v7 = v7 & 0xFFFFFFFF00000000 | v3;
  }
  v1[2] = v9;
  v1[3] = v5;
  v1[4] = v7;
  v1[5] = v8;
  return result;
}

uint64_t BNNS.SGDMomentumOptimizer.sgdMomentumVariant.getter()
{
  uint64_t v0;

  if ((*(_BYTE *)(v0 + 19) & 0x80) != 0)
    return *(_QWORD *)(v0 + 24);
  else
    return *(unsigned int *)(v0 + 36);
}

unsigned int *key path setter for BNNS.SGDMomentumOptimizer.sgdMomentumVariant : BNNS.SGDMomentumOptimizer(unsigned int *result, int8x16_t *a2)
{
  int8x16_t v2;
  unint64_t v3;
  uint64_t v4;
  int8x16_t v5;
  int8x16_t v6;

  v2 = a2[1];
  v3 = a2[2].u64[0];
  if ((a2[1].i64[0] & 0x80000000) != 0)
  {
    v4 = a2[2].i64[1];
    v6.i64[0] = vdupq_n_s64(0x80000000uLL).u64[0];
    v6.i64[1] = *result;
    v5 = vorrq_s8(vandq_s8(v2, (int8x16_t)xmmword_1CAB60810), v6);
  }
  else
  {
    v4 = 0;
    v5 = vandq_s8(v2, (int8x16_t)xmmword_1CAB60820);
    v3 = a2[2].i64[0] | ((unint64_t)*result << 32);
  }
  a2[1] = v5;
  a2[2].i64[0] = v3;
  a2[2].i64[1] = v4;
  return result;
}

uint64_t BNNS.SGDMomentumOptimizer.sgdMomentumVariant.setter(uint64_t result)
{
  int8x16_t *v1;
  int8x16_t v2;
  unint64_t v3;
  uint64_t v4;
  int8x16_t v5;
  int8x16_t v6;

  v2 = v1[1];
  v3 = v1[2].u64[0];
  if ((v1[1].i64[0] & 0x80000000) != 0)
  {
    v4 = v1[2].i64[1];
    v6.i64[0] = vdupq_n_s64(0x80000000uLL).u64[0];
    v6.i64[1] = result;
    v5 = vorrq_s8(vandq_s8(v2, (int8x16_t)xmmword_1CAB60810), v6);
  }
  else
  {
    v4 = 0;
    v5 = vandq_s8(v2, (int8x16_t)xmmword_1CAB60820);
    v3 = v1[2].i64[0] | ((unint64_t)result << 32);
  }
  v1[1] = v5;
  v1[2].i64[0] = v3;
  v1[2].i64[1] = v4;
  return result;
}

uint64_t (*BNNS.SGDMomentumOptimizer.sgdMomentumVariant.modify(uint64_t a1))(uint64_t a1)
{
  uint64_t v1;
  uint64_t v2;

  *(_QWORD *)a1 = v1;
  if ((*(_BYTE *)(v1 + 19) & 0x80) != 0)
    v2 = *(_QWORD *)(v1 + 24);
  else
    LODWORD(v2) = *(_DWORD *)(v1 + 36);
  *(_DWORD *)(a1 + 8) = v2;
  return BNNS.SGDMomentumOptimizer.sgdMomentumVariant.modify;
}

uint64_t BNNS.SGDMomentumOptimizer.sgdMomentumVariant.modify(uint64_t a1)
{
  int8x16_t *v1;
  uint64_t result;
  uint64_t v3;
  unsigned int v4;
  int8x16_t v5;
  unint64_t v6;
  uint64_t v7;
  int8x16_t v8;
  int8x16_t v9;

  v1 = *(int8x16_t **)a1;
  v4 = *(_DWORD *)(a1 + 8);
  result = a1 + 8;
  v3 = v4;
  v5 = v1[1];
  v6 = v1[2].u64[0];
  if ((v1[1].i64[0] & 0x80000000) != 0)
  {
    v7 = v1[2].i64[1];
    v9.i64[0] = vdupq_n_s64(0x80000000uLL).u64[0];
    v9.i64[1] = v3;
    v8 = vorrq_s8(vandq_s8(v5, (int8x16_t)xmmword_1CAB60810), v9);
  }
  else
  {
    v7 = 0;
    v8 = vandq_s8(v5, (int8x16_t)xmmword_1CAB60820);
    v6 = v1[2].i64[0] | ((unint64_t)v3 << 32);
  }
  v1[1] = v8;
  v1[2].i64[0] = v6;
  v1[2].i64[1] = v7;
  return result;
}

uint64_t BNNS.SGDMomentumOptimizer.init(learningRate:momentum:gradientScale:regularizationScale:clipsGradientsTo:usesNestrovMomentum:regularizationFunction:sgdMomentumVariant:)@<X0>(uint64_t result@<X0>, char a2@<W1>, char a3@<W2>, uint64_t a4@<X3>, unsigned int a5@<W4>, int8x16_t *a6@<X8>, unsigned int a7@<S0>, int32x2_t a8@<D1>, unsigned int a9@<S2>, __int32 a10@<S3>)
{
  int8x16_t v10;
  uint64_t v11;

  a8.i32[1] = a10;
  v10.i64[0] = a7;
  v10.i64[1] = a9;
  *a6 = vorrq_s8((int8x16_t)vshll_n_s32(a8, 0x20uLL), v10);
  a6[1].i64[0] = a3 & 1 | (unint64_t)(a4 << 32) | 0x80000000;
  a6[1].i64[1] = (a5 | ((unint64_t)(a2 & 1) << 32)) ^ 0x100000000;
  if ((a2 & 1) != 0)
    v11 = 0;
  else
    v11 = result;
  a6[2].i64[0] = v11;
  a6[2].i64[1] = 0;
  return result;
}

uint64_t BNNS.SGDMomentumOptimizer.bnnsOptimizerFunction.getter()
{
  uint64_t v0;

  if ((*(_BYTE *)(v0 + 19) & 0x80) != 0)
    return 7;
  else
    return 1;
}

uint64_t protocol witness for BNNSOptimizer.step(parameters:gradients:accumulators:filterParameters:) in conformance BNNS.SGDMomentumOptimizer(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  __int128 *v9;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  __int128 v17;
  __int128 v18;
  _QWORD v19[5];

  v17 = v9[1];
  v18 = *v9;
  v13 = *((_QWORD *)v9 + 4);
  v14 = *((_QWORD *)v9 + 5);
  v19[3] = a8;
  v19[4] = a9;
  v15 = swift_allocObject();
  v19[0] = v15;
  *(_OWORD *)(v15 + 16) = v18;
  *(_OWORD *)(v15 + 32) = v17;
  *(_QWORD *)(v15 + 48) = v13;
  *(_QWORD *)(v15 + 56) = v14;
  specialized static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)((uint64_t)v19, a1, a2, a3);
  return __swift_destroy_boxed_opaque_existential_1((uint64_t)v19);
}

uint64_t protocol witness for BNNSOptimizer.bnnsOptimizerFunction.getter in conformance BNNS.SGDMomentumOptimizer()
{
  uint64_t v0;

  if ((*(_BYTE *)(v0 + 19) & 0x80) != 0)
    return 7;
  else
    return 1;
}

BOOL protocol witness for BNNSOptimizer.accumulatorCountMultiplier.getter in conformance BNNS.SGDMomentumOptimizer()
{
  uint64_t v0;

  return (*(_DWORD *)(v0 + 4) & 0x7FFFFFFF) != 0;
}

uint64_t protocol witness for WithOptimizerAlgFields.withOptimizerAlgFields(body:) in conformance BNNS.SGDMomentumOptimizer(uint64_t (*a1)(_OWORD *))
{
  _OWORD *v1;
  __int128 v2;
  _OWORD v4[3];
  uint64_t v5;

  v5 = *MEMORY[0x1E0C80C00];
  v2 = v1[1];
  v4[0] = *v1;
  v4[1] = v2;
  v4[2] = v1[2];
  return a1(v4);
}

uint64_t protocol witness for BNNSOptimizer.step(parameters:gradients:accumulators:filterParameters:) in conformance BNNS.AdamWOptimizer(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  uint64_t v9;
  int v13;
  char v14;
  uint64_t v15;
  _QWORD v17[5];

  v13 = *(_DWORD *)(v9 + 24);
  v14 = *(_BYTE *)(v9 + 52);
  v17[3] = a8;
  v17[4] = a9;
  v15 = swift_allocObject();
  v17[0] = v15;
  *(_OWORD *)(v15 + 16) = *(_OWORD *)v9;
  *(_QWORD *)(v15 + 32) = *(_QWORD *)(v9 + 16);
  *(_DWORD *)(v15 + 40) = v13;
  *(_QWORD *)(v15 + 44) = *(_QWORD *)(v9 + 28);
  *(_OWORD *)(v15 + 52) = *(_OWORD *)(v9 + 36);
  *(_BYTE *)(v15 + 68) = v14;
  specialized static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)((uint64_t)v17, a1, a2, a3);
  return __swift_destroy_boxed_opaque_existential_1((uint64_t)v17);
}

uint64_t specialized static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  int64_t v6;
  uint64_t v7;
  int64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  int64_t v12;
  uint64_t v13;
  int64_t v14;
  __int128 *v15;
  uint64_t v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  _OWORD *v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  unint64_t v28;
  unint64_t v29;
  _BYTE *v30;
  int64_t v32;
  __int128 *v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  _OWORD *v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  unint64_t v45;
  unint64_t v46;
  uint64_t v47;
  uint64_t v48;
  __int128 *v49;
  __int128 v50;
  __int128 v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  _OWORD *v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  int64_t v61;
  unint64_t v62;
  unint64_t v63;
  uint64_t v64;
  _QWORD *v65;
  uint64_t v66;
  uint64_t i;
  uint64_t v68;
  uint64_t v69;
  uint64_t j;
  uint64_t v71;
  uint64_t v72;
  uint64_t k;
  uint64_t v74;
  uint64_t v75;
  int64_t v76;
  int64_t v77;
  uint64_t v78;
  uint64_t v81;
  _QWORD v82[5];
  __int128 v83;
  __int128 v84;
  __int128 v85;
  __int128 v86;
  __int128 v87;
  __int128 v88;
  __int128 v89;
  __int128 v90;
  __int128 v91;
  __int128 v92;
  __int128 v93;
  _QWORD v94[3];
  uint64_t v95;
  uint64_t v96;
  uint64_t v97;

  v97 = *MEMORY[0x1E0C80C00];
  v6 = *(_QWORD *)(a2 + 16);
  v7 = *(_QWORD *)(a3 + 16);
  outlined init with copy of BNNSOptimizer(a1, (uint64_t)v94);
  if (v6 != v7)
  {
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v94);
LABEL_13:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v30 = 3;
    return swift_willThrow();
  }
  v78 = a1;
  v8 = *(_QWORD *)(a4 + 16);
  v9 = v95;
  v10 = v96;
  __swift_project_boxed_opaque_existential_1(v94, v95);
  v11 = (*(uint64_t (**)(uint64_t, uint64_t))(v10 + 24))(v9, v10);
  v12 = v6 * v11;
  if ((unsigned __int128)(v6 * (__int128)v11) >> 64 != (v6 * v11) >> 63)
    __break(1u);
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v94);
  if (v8 != v12)
    goto LABEL_13;
  v13 = MEMORY[0x1E0DEE9D8];
  v77 = v6;
  if (v6)
  {
    v75 = a4;
    v76 = v8;
    v82[0] = MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v6, 0);
    v14 = v6;
    v15 = (__int128 *)(a2 + 32);
    v16 = v82[0];
    do
    {
      v17 = v15[9];
      v91 = v15[8];
      v92 = v17;
      v93 = v15[10];
      v18 = v15[5];
      v87 = v15[4];
      v88 = v18;
      v19 = v15[7];
      v89 = v15[6];
      v90 = v19;
      v20 = v15[1];
      v83 = *v15;
      v84 = v20;
      v21 = v15[3];
      v85 = v15[2];
      v86 = v21;
      v22 = (_OWORD *)swift_slowAlloc();
      v23 = v92;
      v22[8] = v91;
      v22[9] = v23;
      v22[10] = v93;
      v24 = v88;
      v22[4] = v87;
      v22[5] = v24;
      v25 = v90;
      v22[6] = v89;
      v22[7] = v25;
      v26 = v84;
      *v22 = v83;
      v22[1] = v26;
      v27 = v86;
      v22[2] = v85;
      v22[3] = v27;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(_QWORD *)(v16 + 16) + 1, 1);
        v16 = v82[0];
      }
      v29 = *(_QWORD *)(v16 + 16);
      v28 = *(_QWORD *)(v16 + 24);
      if (v29 >= v28 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v28 > 1), v29 + 1, 1);
        v16 = v82[0];
      }
      *(_QWORD *)(v16 + 16) = v29 + 1;
      *(_QWORD *)(v16 + 8 * v29 + 32) = v22;
      v15 += 11;
      --v14;
    }
    while (v14);
    v81 = v16;
    v82[0] = v13;
    v32 = v77;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v77, 0);
    v33 = (__int128 *)(a3 + 32);
    do
    {
      v34 = v33[9];
      v91 = v33[8];
      v92 = v34;
      v93 = v33[10];
      v35 = v33[5];
      v87 = v33[4];
      v88 = v35;
      v36 = v33[7];
      v89 = v33[6];
      v90 = v36;
      v37 = v33[1];
      v83 = *v33;
      v84 = v37;
      v38 = v33[3];
      v85 = v33[2];
      v86 = v38;
      v39 = (_OWORD *)swift_slowAlloc();
      v40 = v92;
      v39[8] = v91;
      v39[9] = v40;
      v39[10] = v93;
      v41 = v88;
      v39[4] = v87;
      v39[5] = v41;
      v42 = v90;
      v39[6] = v89;
      v39[7] = v42;
      v43 = v84;
      *v39 = v83;
      v39[1] = v43;
      v44 = v86;
      v39[2] = v85;
      v39[3] = v44;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(_QWORD *)(v13 + 16) + 1, 1);
        v13 = v82[0];
      }
      v46 = *(_QWORD *)(v13 + 16);
      v45 = *(_QWORD *)(v13 + 24);
      if (v46 >= v45 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v45 > 1), v46 + 1, 1);
        v13 = v82[0];
      }
      *(_QWORD *)(v13 + 16) = v46 + 1;
      *(_QWORD *)(v13 + 8 * v46 + 32) = v39;
      v33 += 11;
      --v32;
    }
    while (v32);
    a4 = v75;
    v8 = v76;
  }
  else
  {
    v81 = MEMORY[0x1E0DEE9D8];
  }
  v47 = MEMORY[0x1E0DEE9D8];
  v48 = v78;
  if (v8)
  {
    v82[0] = MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v8, 0);
    v49 = (__int128 *)(a4 + 32);
    v47 = v82[0];
    do
    {
      v50 = v49[9];
      v91 = v49[8];
      v92 = v50;
      v93 = v49[10];
      v51 = v49[5];
      v87 = v49[4];
      v88 = v51;
      v52 = v49[7];
      v89 = v49[6];
      v90 = v52;
      v53 = v49[1];
      v83 = *v49;
      v84 = v53;
      v54 = v49[3];
      v85 = v49[2];
      v86 = v54;
      v55 = (_OWORD *)swift_slowAlloc();
      v56 = v92;
      v55[8] = v91;
      v55[9] = v56;
      v55[10] = v93;
      v57 = v88;
      v55[4] = v87;
      v55[5] = v57;
      v58 = v90;
      v55[6] = v89;
      v55[7] = v58;
      v59 = v84;
      *v55 = v83;
      v55[1] = v59;
      v60 = v86;
      v55[2] = v85;
      v55[3] = v60;
      v61 = v8;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(_QWORD *)(v47 + 16) + 1, 1);
        v47 = v82[0];
      }
      v63 = *(_QWORD *)(v47 + 16);
      v62 = *(_QWORD *)(v47 + 24);
      if (v63 >= v62 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v62 > 1), v63 + 1, 1);
        v47 = v82[0];
      }
      *(_QWORD *)(v47 + 16) = v63 + 1;
      *(_QWORD *)(v47 + 8 * v63 + 32) = v55;
      v49 += 11;
      v8 = v61 - 1;
    }
    while (v61 != 1);
    v48 = v78;
  }
  outlined init with copy of BNNSOptimizer(v48, (uint64_t)&v83);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSOptimizer);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for WithOptimizerAlgFields);
  swift_dynamicCast();
  v64 = v82[4];
  v65 = __swift_project_boxed_opaque_existential_1(v82, v82[3]);
  MEMORY[0x1E0C80A78](v65);
  (*(void (**)(uint64_t (*)()))(v64 + 8))(partial apply for closure #1 in closure #4 in static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:));
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v82);
  v66 = *(_QWORD *)(v81 + 16);
  if (v66)
  {
    swift_bridgeObjectRetain();
    for (i = 0; i != v66; ++i)
    {
      v68 = *(_QWORD *)(v81 + 8 * i + 32);
      MEMORY[0x1D1794DA4](v68, -1, -1);
    }
    swift_bridgeObjectRelease();
  }
  v69 = *(_QWORD *)(v13 + 16);
  if (v69)
  {
    swift_bridgeObjectRetain();
    for (j = 0; j != v69; ++j)
    {
      v71 = *(_QWORD *)(v13 + 8 * j + 32);
      MEMORY[0x1D1794DA4](v71, -1, -1);
    }
    swift_bridgeObjectRelease();
  }
  v72 = *(_QWORD *)(v47 + 16);
  if (v72)
  {
    swift_bridgeObjectRetain();
    for (k = 0; k != v72; ++k)
    {
      v74 = *(_QWORD *)(v47 + 8 * k + 32);
      if (v74)
        MEMORY[0x1D1794DA4](v74, -1, -1);
    }
    swift_bridgeObjectRelease();
  }
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

uint64_t __swift_destroy_boxed_opaque_existential_1(uint64_t a1)
{
  uint64_t v1;

  v1 = *(_QWORD *)(*(_QWORD *)(a1 + 24) - 8);
  if ((*(_BYTE *)(v1 + 82) & 2) != 0)
    return swift_release();
  else
    return (*(uint64_t (**)(void))(v1 + 8))();
}

_QWORD *sub_1CAAFD120@<X0>(_QWORD *result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *result;
  return result;
}

unsigned int *sub_1CAAFD12C(unsigned int *result, unint64_t *a2)
{
  unsigned int v2;
  unint64_t v3;
  unint64_t v4;
  unint64_t v5;
  int v6;
  unint64_t v7;

  v2 = *result;
  v3 = *a2;
  v4 = a2[3];
  v5 = a2[5];
  if (*((_BYTE *)a2 + 52) == 1)
  {
    v6 = *((_DWORD *)a2 + 12);
    v7 = v3 & 0xFFFFFFFF00000000 | v2;
  }
  else
  {
    v6 = 0;
    v7 = v3 & 0xFFFFFFFF00000000 | v2;
    v4 &= 0x1FFFFFFFFuLL;
    v5 = a2[5];
  }
  *a2 = v7;
  a2[3] = v4;
  a2[5] = v5;
  *((_DWORD *)a2 + 12) = v6;
  return result;
}

uint64_t sub_1CAAFD188@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_DWORD *)(result + 4);
  return result;
}

unsigned int *sub_1CAAFD194(unsigned int *result, unsigned int *a2)
{
  unsigned int v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  unsigned int v6;
  unint64_t v7;

  v2 = *result;
  v3 = *a2;
  v4 = *((_QWORD *)a2 + 3);
  v5 = *((_QWORD *)a2 + 5);
  if (*((_BYTE *)a2 + 52) == 1)
  {
    v6 = a2[12];
    v7 = v3 | ((unint64_t)v2 << 32);
  }
  else
  {
    v6 = 0;
    v7 = v3 | ((unint64_t)v2 << 32);
    v4 &= 0x1FFFFFFFFuLL;
    v5 = *((_QWORD *)a2 + 5);
  }
  *(_QWORD *)a2 = v7;
  *((_QWORD *)a2 + 3) = v4;
  *((_QWORD *)a2 + 5) = v5;
  a2[12] = v6;
  return result;
}

uint64_t sub_1CAAFD1E8@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_QWORD *)(result + 8);
  return result;
}

unsigned int *sub_1CAAFD1F4(unsigned int *result, uint64_t a2)
{
  unsigned int v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  int v6;
  unint64_t v7;

  v2 = *result;
  v3 = *(_QWORD *)(a2 + 8);
  v4 = *(_QWORD *)(a2 + 24);
  v5 = *(_QWORD *)(a2 + 40);
  if (*(_BYTE *)(a2 + 52) == 1)
  {
    v6 = *(_DWORD *)(a2 + 48);
    v7 = v3 & 0xFFFFFFFF00000000 | v2;
  }
  else
  {
    v6 = 0;
    v7 = v3 & 0xFFFFFFFF00000000 | v2;
    v4 &= 0x1FFFFFFFFuLL;
    v5 = *(_QWORD *)(a2 + 40);
  }
  *(_QWORD *)(a2 + 8) = v7;
  *(_QWORD *)(a2 + 24) = v4;
  *(_QWORD *)(a2 + 40) = v5;
  *(_DWORD *)(a2 + 48) = v6;
  return result;
}

uint64_t sub_1CAAFD250@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_DWORD *)(result + 12);
  return result;
}

unsigned int *sub_1CAAFD25C(unsigned int *result, uint64_t a2)
{
  unsigned int v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  int v6;
  unint64_t v7;

  v2 = *result;
  v3 = *(unsigned int *)(a2 + 8);
  v4 = *(_QWORD *)(a2 + 24);
  v5 = *(_QWORD *)(a2 + 40);
  if (*(_BYTE *)(a2 + 52) == 1)
  {
    v6 = *(_DWORD *)(a2 + 48);
    v7 = v3 | ((unint64_t)v2 << 32);
  }
  else
  {
    v6 = 0;
    v7 = v3 | ((unint64_t)v2 << 32);
    v4 &= 0x1FFFFFFFFuLL;
    v5 = *(_QWORD *)(a2 + 40);
  }
  *(_QWORD *)(a2 + 8) = v7;
  *(_QWORD *)(a2 + 24) = v4;
  *(_QWORD *)(a2 + 40) = v5;
  *(_DWORD *)(a2 + 48) = v6;
  return result;
}

uint64_t sub_1CAAFD2B0@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_QWORD *)(result + 16);
  return result;
}

unsigned int *sub_1CAAFD2BC(unsigned int *result, uint64_t a2)
{
  unsigned int v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  int v6;
  unint64_t v7;

  v2 = *result;
  v4 = *(_QWORD *)(a2 + 16);
  v3 = *(_QWORD *)(a2 + 24);
  v5 = *(_QWORD *)(a2 + 40);
  if (*(_BYTE *)(a2 + 52) == 1)
  {
    v6 = *(_DWORD *)(a2 + 48);
    v7 = v4 & 0xFFFFFFFF00000000 | v2;
  }
  else
  {
    v6 = 0;
    v7 = v4 & 0xFFFFFFFF00000000 | v2;
    v3 &= 0x1FFFFFFFFuLL;
    v5 = *(_QWORD *)(a2 + 40);
  }
  *(_QWORD *)(a2 + 16) = v7;
  *(_QWORD *)(a2 + 24) = v3;
  *(_QWORD *)(a2 + 40) = v5;
  *(_DWORD *)(a2 + 48) = v6;
  return result;
}

uint64_t sub_1CAAFD310@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_DWORD *)(result + 20);
  return result;
}

unsigned int *sub_1CAAFD31C(unsigned int *result, uint64_t a2)
{
  unsigned int v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  int v6;
  unint64_t v7;

  v2 = *result;
  v3 = *(unsigned int *)(a2 + 16);
  v4 = *(_QWORD *)(a2 + 24);
  v5 = *(_QWORD *)(a2 + 40);
  if (*(_BYTE *)(a2 + 52) == 1)
  {
    v6 = *(_DWORD *)(a2 + 48);
    v7 = v3 | ((unint64_t)v2 << 32);
  }
  else
  {
    v6 = 0;
    v7 = v3 | ((unint64_t)v2 << 32);
    v4 &= 0x1FFFFFFFFuLL;
    v5 = *(_QWORD *)(a2 + 40);
  }
  *(_QWORD *)(a2 + 16) = v7;
  *(_QWORD *)(a2 + 24) = v4;
  *(_QWORD *)(a2 + 40) = v5;
  *(_DWORD *)(a2 + 48) = v6;
  return result;
}

uint64_t sub_1CAAFD36C@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_QWORD *)(result + 24);
  return result;
}

unsigned int *sub_1CAAFD378(unsigned int *result, uint64_t a2)
{
  unsigned int v2;
  uint64_t v3;
  uint64_t v4;
  int v5;
  unint64_t v6;

  v2 = *result;
  v3 = *(_QWORD *)(a2 + 24);
  v4 = *(_QWORD *)(a2 + 40);
  if (*(_BYTE *)(a2 + 52) == 1)
  {
    v5 = *(_DWORD *)(a2 + 48);
    v6 = v3 & 0xFFFFFFFF00000000 | v2;
  }
  else
  {
    v5 = 0;
    v6 = v3 & 0x100000000 | v2;
    v4 = *(_QWORD *)(a2 + 40);
  }
  *(_QWORD *)(a2 + 24) = v6;
  *(_QWORD *)(a2 + 40) = v4;
  *(_DWORD *)(a2 + 48) = v5;
  return result;
}

uint64_t sub_1CAAFD3D0@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  if (*(_BYTE *)(result + 52) == 1)
    *a2 = *(_DWORD *)(result + 28);
  else
    *a2 = *(_QWORD *)(result + 40);
  return result;
}

unsigned int *sub_1CAAFD3F4(unsigned int *result, uint64_t a2)
{
  int v2;
  uint64_t v3;
  unint64_t v4;

  if (*(_BYTE *)(a2 + 52) == 1)
  {
    v2 = *(_DWORD *)(a2 + 48);
    v3 = *(_QWORD *)(a2 + 40);
    v4 = *(_QWORD *)(a2 + 24) | ((unint64_t)*result << 32);
  }
  else
  {
    v2 = 0;
    v4 = *(_QWORD *)(a2 + 24) & 0x1FFFFFFFFLL;
    v3 = *result;
  }
  *(_QWORD *)(a2 + 24) = v4;
  *(_QWORD *)(a2 + 40) = v3;
  *(_DWORD *)(a2 + 48) = v2;
  return result;
}

_QWORD *sub_1CAAFD434@<X0>(_QWORD *result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *result;
  return result;
}

uint64_t sub_1CAAFD444@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_DWORD *)(result + 4);
  return result;
}

uint64_t sub_1CAAFD454@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_QWORD *)(result + 8);
  return result;
}

unsigned int *sub_1CAAFD460(unsigned int *result, uint64_t a2)
{
  unsigned int v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  int v6;
  unint64_t v7;

  v2 = *result;
  v3 = *(_QWORD *)(a2 + 8);
  v4 = *(_QWORD *)(a2 + 24);
  v5 = *(_QWORD *)(a2 + 40);
  if (v3 < 0)
  {
    v6 = *(_DWORD *)(a2 + 48);
    v7 = v3 & 0x100000000 | v2 | 0x8000000000000000;
  }
  else
  {
    v6 = 0;
    v7 = v3 & 0x100000000 | v2;
    v4 &= 0x1FFFFFFFFuLL;
    v5 = *(_QWORD *)(a2 + 40);
  }
  *(_QWORD *)(a2 + 8) = v7;
  *(_QWORD *)(a2 + 24) = v4;
  *(_QWORD *)(a2 + 40) = v5;
  *(_DWORD *)(a2 + 48) = v6;
  return result;
}

uint64_t sub_1CAAFD4B8@<X0>(uint64_t result@<X0>, _BYTE *a2@<X8>)
{
  *a2 = *(_BYTE *)(result + 12) & 1;
  return result;
}

unsigned __int8 *sub_1CAAFD4C8(unsigned __int8 *result, uint64_t a2)
{
  int v2;
  uint64_t v3;
  uint64_t v4;
  int v5;
  BOOL v6;
  uint64_t v7;
  unint64_t v8;
  unint64_t v9;

  v2 = *result;
  v3 = *(_QWORD *)(a2 + 24);
  v4 = *(_QWORD *)(a2 + 40);
  if ((*(_QWORD *)(a2 + 8) & 0x8000000000000000) != 0)
  {
    v5 = *(_DWORD *)(a2 + 48);
    v6 = v2 == 0;
    v9 = 0x8000000000000000;
    if (!v6)
      v9 = 0x8000000100000000;
    v8 = v9 & 0xFFFFFFFF00000000 | *(_QWORD *)(a2 + 8);
  }
  else
  {
    v5 = 0;
    v6 = v2 == 0;
    v7 = 0x100000000;
    if (v6)
      v7 = 0;
    v8 = v7 & 0xFFFFFFFF00000000 | *(_QWORD *)(a2 + 8);
    v3 &= 0x1FFFFFFFFuLL;
    v4 = *(_QWORD *)(a2 + 40);
  }
  *(_QWORD *)(a2 + 8) = v8;
  *(_QWORD *)(a2 + 24) = v3;
  *(_QWORD *)(a2 + 40) = v4;
  *(_DWORD *)(a2 + 48) = v5;
  return result;
}

uint64_t sub_1CAAFD52C@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_QWORD *)(result + 16);
  return result;
}

uint64_t sub_1CAAFD53C@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_DWORD *)(result + 20);
  return result;
}

uint64_t sub_1CAAFD54C@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_QWORD *)(result + 24);
  return result;
}

unsigned int *sub_1CAAFD558(unsigned int *result, uint64_t a2)
{
  unsigned int v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  int v6;
  unint64_t v7;
  unint64_t v8;

  v2 = *result;
  v3 = *(_QWORD *)(a2 + 8);
  v4 = *(_QWORD *)(a2 + 24);
  v5 = *(_QWORD *)(a2 + 40);
  if (v3 < 0)
  {
    v6 = *(_DWORD *)(a2 + 48);
    v8 = v4 & 0xFFFFFFFF00000000 | v2;
    v7 = v3 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v6 = 0;
    v7 = v3 & 0x1FFFFFFFFLL;
    v8 = v4 & 0x100000000 | v2;
    v5 = *(_QWORD *)(a2 + 40);
  }
  *(_QWORD *)(a2 + 8) = v7;
  *(_QWORD *)(a2 + 24) = v8;
  *(_QWORD *)(a2 + 40) = v5;
  *(_DWORD *)(a2 + 48) = v6;
  return result;
}

uint64_t sub_1CAAFD5BC@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  uint64_t v2;

  if ((*(_QWORD *)(result + 8) & 0x8000000000000000) != 0)
    LODWORD(v2) = *(_DWORD *)(result + 28);
  else
    v2 = *(_QWORD *)(result + 40);
  *a2 = v2;
  return result;
}

unsigned int *sub_1CAAFD5DC(unsigned int *result, uint64_t a2)
{
  uint64_t v2;
  int v3;
  unint64_t v4;
  unint64_t v5;
  uint64_t v6;

  v2 = *(_QWORD *)(a2 + 8);
  if (v2 < 0)
  {
    v3 = *(_DWORD *)(a2 + 48);
    v6 = *(_QWORD *)(a2 + 40);
    v5 = *(_QWORD *)(a2 + 24) | ((unint64_t)*result << 32);
    v4 = v2 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    v3 = 0;
    v4 = v2 & 0x1FFFFFFFFLL;
    v5 = *(_QWORD *)(a2 + 24) & 0x1FFFFFFFFLL;
    v6 = *result;
  }
  *(_QWORD *)(a2 + 8) = v4;
  *(_QWORD *)(a2 + 24) = v5;
  *(_QWORD *)(a2 + 40) = v6;
  *(_DWORD *)(a2 + 48) = v3;
  return result;
}

_QWORD *sub_1CAAFD628@<X0>(_QWORD *result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *result;
  return result;
}

uint64_t sub_1CAAFD638@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_DWORD *)(result + 4);
  return result;
}

uint64_t sub_1CAAFD648@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_QWORD *)(result + 8);
  return result;
}

uint64_t sub_1CAAFD658@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_DWORD *)(result + 12);
  return result;
}

uint64_t sub_1CAAFD668@<X0>(uint64_t a1@<X8>)
{
  uint64_t result;
  char v3;

  result = BNNS.SGDMomentumOptimizer.gradientBounds.getter();
  *(_QWORD *)a1 = result;
  *(_BYTE *)(a1 + 8) = v3 & 1;
  return result;
}

uint64_t keypath_get_45Tm@<X0>(uint64_t result@<X0>, BOOL *a2@<X8>)
{
  uint64_t v2;

  if ((*(_QWORD *)(result + 16) & 0x80000000) != 0)
    v2 = *(_QWORD *)(result + 16) & 1;
  else
    v2 = *(_QWORD *)(result + 24) & 0x100000000;
  *a2 = v2 != 0;
  return result;
}

unsigned __int8 *keypath_set_46Tm(unsigned __int8 *result, _QWORD *a2)
{
  uint64_t v2;
  unint64_t v3;
  uint64_t v4;
  uint64_t v5;
  unint64_t v6;
  BOOL v7;
  uint64_t v8;

  v2 = *result;
  v4 = a2[2];
  v3 = a2[3];
  if ((v4 & 0x80000000) != 0)
  {
    v5 = a2[5];
    v6 = v4 & 0xFFFFFFFF00000000 | v2 | 0x80000000;
  }
  else
  {
    v5 = 0;
    v6 = v4 & 0xFFFFFFFF00000001;
    v7 = (_DWORD)v2 == 0;
    v8 = 0x100000000;
    if (v7)
      v8 = 0;
    v3 = v8 & 0xFFFFFFFF00000000 | a2[3];
  }
  a2[2] = v6;
  a2[3] = v3;
  a2[5] = v5;
  return result;
}

uint64_t sub_1CAAFD734@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  unint64_t v2;
  unint64_t v3;

  v2 = *(_QWORD *)(result + 16);
  if ((v2 & 0x80000000) != 0)
    v3 = HIDWORD(v2);
  else
    v3 = *(_QWORD *)(result + 32);
  *a2 = v3;
  return result;
}

uint64_t sub_1CAAFD758@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  uint64_t v2;

  if ((*(_BYTE *)(result + 19) & 0x80) != 0)
    v2 = *(_QWORD *)(result + 24);
  else
    LODWORD(v2) = *(_DWORD *)(result + 36);
  *a2 = v2;
  return result;
}

uint64_t dispatch thunk of BNNSOptimizer.step(parameters:gradients:accumulators:filterParameters:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return (*(uint64_t (**)(uint64_t))(a9 + 8))(a1);
}

uint64_t dispatch thunk of BNNSOptimizer.bnnsOptimizerFunction.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

uint64_t dispatch thunk of BNNSOptimizer.accumulatorCountMultiplier.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 24))();
}

uint64_t dispatch thunk of WithOptimizerAlgFields.withOptimizerAlgFields(body:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return (*(uint64_t (**)(void))(a4 + 8))();
}

__n128 __swift_memcpy54_4(uint64_t a1, uint64_t a2)
{
  __n128 result;
  __int128 v3;
  __int128 v4;

  result = *(__n128 *)a2;
  v3 = *(_OWORD *)(a2 + 16);
  v4 = *(_OWORD *)(a2 + 32);
  *(_QWORD *)(a1 + 46) = *(_QWORD *)(a2 + 46);
  *(_OWORD *)(a1 + 16) = v3;
  *(_OWORD *)(a1 + 32) = v4;
  *(__n128 *)a1 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.AdamOptimizer(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  int v4;

  if (!a2)
    return 0;
  if (a2 >= 0xFF && *(_BYTE *)(a1 + 54))
    return (*(_DWORD *)a1 + 255);
  v3 = *(unsigned __int8 *)(a1 + 52);
  if (v3 <= 1)
    v4 = -1;
  else
    v4 = v3 ^ 0xFF;
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.AdamOptimizer(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(_QWORD *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_WORD *)(result + 52) = 0;
    *(_DWORD *)(result + 48) = 0;
    *(_QWORD *)result = a2 - 255;
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 54) = 1;
  }
  else
  {
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 54) = 0;
    if (a2)
      *(_BYTE *)(result + 52) = -(char)a2;
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.AdamOptimizer()
{
  return &type metadata for BNNS.AdamOptimizer;
}

ValueMetadata *type metadata accessor for BNNS.RMSPropOptimizer()
{
  return &type metadata for BNNS.RMSPropOptimizer;
}

ValueMetadata *type metadata accessor for BNNS.SGDMomentumOptimizer()
{
  return &type metadata for BNNS.SGDMomentumOptimizer;
}

uint64_t getEnumTagSinglePayload for BNNS.SGDMomentumOptimizer(uint64_t a1, unsigned int a2)
{
  unsigned int v3;

  if (!a2)
    return 0;
  if (a2 >= 0x7FFFFFFF && *(_BYTE *)(a1 + 48))
    return (*(_DWORD *)a1 + 0x7FFFFFFF);
  v3 = (*(_DWORD *)(a1 + 16) & 0x7FFFFFFE | (*(_DWORD *)(a1 + 16) >> 31)) ^ 0x7FFFFFFF;
  if (v3 >= 0x7FFFFFFE)
    v3 = -1;
  return v3 + 1;
}

uint64_t storeEnumTagSinglePayload for BNNS.SGDMomentumOptimizer(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0x7FFFFFFE)
  {
    *(_QWORD *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_QWORD *)result = a2 - 0x7FFFFFFF;
    if (a3 >= 0x7FFFFFFF)
      *(_BYTE *)(result + 48) = 1;
  }
  else
  {
    if (a3 >= 0x7FFFFFFF)
      *(_BYTE *)(result + 48) = 0;
    if (a2)
    {
      *(_QWORD *)result = 0;
      *(_QWORD *)(result + 8) = 0;
      *(_QWORD *)(result + 16) = -a2 & 0x7FFFFFFE | (a2 << 31);
      *(_QWORD *)(result + 24) = 0;
      *(_QWORD *)(result + 32) = 0;
      *(_QWORD *)(result + 40) = 0;
    }
  }
  return result;
}

uint64_t getEnumTag for BNNS.SGDMomentumOptimizer.Fields(uint64_t a1)
{
  return *(unsigned __int8 *)(a1 + 19) >> 7;
}

uint64_t destructiveProjectEnumData for BNNS.SGDMomentumOptimizer.Fields(uint64_t result)
{
  *(_QWORD *)(result + 16) &= ~0x80000000uLL;
  return result;
}

uint64_t destructiveInjectEnumTag for BNNS.SGDMomentumOptimizer.Fields(uint64_t result, int a2)
{
  *(_QWORD *)(result + 16) = *(_QWORD *)(result + 16) & 0xFFFFFFFF00000001 | (a2 << 31);
  return result;
}

ValueMetadata *type metadata accessor for BNNS.SGDMomentumOptimizer.Fields()
{
  return &type metadata for BNNS.SGDMomentumOptimizer.Fields;
}

uint64_t getEnumTagSinglePayload for BNNS.RMSPropOptimizer(uint64_t a1, unsigned int a2)
{
  unsigned int v3;

  if (!a2)
    return 0;
  if (a2 >= 0x7FFFFFFF && *(_BYTE *)(a1 + 52))
    return (*(_DWORD *)a1 + 0x7FFFFFFF);
  v3 = (((*(_QWORD *)(a1 + 8) >> 33) >> 30) & 0x80000001 | (2 * ((*(_QWORD *)(a1 + 8) >> 33) & 0x3FFFFFFF))) ^ 0x7FFFFFFF;
  if (v3 >= 0x7FFFFFFE)
    v3 = -1;
  return v3 + 1;
}

uint64_t storeEnumTagSinglePayload for BNNS.RMSPropOptimizer(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0x7FFFFFFE)
  {
    *(_QWORD *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_DWORD *)(result + 48) = 0;
    *(_QWORD *)result = a2 - 0x7FFFFFFF;
    if (a3 >= 0x7FFFFFFF)
      *(_BYTE *)(result + 52) = 1;
  }
  else
  {
    if (a3 >= 0x7FFFFFFF)
      *(_BYTE *)(result + 52) = 0;
    if (a2)
    {
      *(_QWORD *)result = 0;
      *(_QWORD *)(result + 8) = (unint64_t)(((-a2 >> 1) & 0x3FFFFFFF) - (a2 << 30)) << 33;
      *(_OWORD *)(result + 16) = 0u;
      *(_OWORD *)(result + 32) = 0u;
      *(_DWORD *)(result + 48) = 0;
    }
  }
  return result;
}

uint64_t getEnumTag for BNNS.RMSPropOptimizer.Fields(uint64_t a1)
{
  return *(_QWORD *)(a1 + 8) >> 63;
}

uint64_t destructiveProjectEnumData for BNNS.RMSPropOptimizer.Fields(uint64_t result)
{
  *(_QWORD *)(result + 8) &= ~0x8000000000000000;
  return result;
}

uint64_t destructiveInjectEnumTag for BNNS.RMSPropOptimizer.Fields(uint64_t result, uint64_t a2)
{
  *(_QWORD *)(result + 8) = *(_QWORD *)(result + 8) & 0x1FFFFFFFFLL | (a2 << 63);
  return result;
}

ValueMetadata *type metadata accessor for BNNS.RMSPropOptimizer.Fields()
{
  return &type metadata for BNNS.RMSPropOptimizer.Fields;
}

__n128 __swift_memcpy53_4(uint64_t a1, uint64_t a2)
{
  __n128 result;
  __int128 v3;
  __int128 v4;

  result = *(__n128 *)a2;
  v3 = *(_OWORD *)(a2 + 16);
  v4 = *(_OWORD *)(a2 + 32);
  *(_QWORD *)(a1 + 45) = *(_QWORD *)(a2 + 45);
  *(_OWORD *)(a1 + 16) = v3;
  *(_OWORD *)(a1 + 32) = v4;
  *(__n128 *)a1 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.AdamOptimizer.Fields(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  int v4;

  if (!a2)
    return 0;
  if (a2 >= 0xFF && *(_BYTE *)(a1 + 53))
    return (*(_DWORD *)a1 + 255);
  v3 = *(unsigned __int8 *)(a1 + 52);
  if (v3 <= 1)
    v4 = -1;
  else
    v4 = v3 ^ 0xFF;
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.AdamOptimizer.Fields(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(_QWORD *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_BYTE *)(result + 52) = 0;
    *(_DWORD *)(result + 48) = 0;
    *(_QWORD *)result = a2 - 255;
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 53) = 1;
  }
  else
  {
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 53) = 0;
    if (a2)
      *(_BYTE *)(result + 52) = -(char)a2;
  }
  return result;
}

uint64_t getEnumTag for BNNS.AdamOptimizer.Fields(uint64_t a1)
{
  return *(unsigned __int8 *)(a1 + 52);
}

uint64_t destructiveInjectEnumTag for BNNS.AdamOptimizer.Fields(uint64_t result, char a2)
{
  *(_BYTE *)(result + 52) = a2 & 1;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.AdamOptimizer.Fields()
{
  return &type metadata for BNNS.AdamOptimizer.Fields;
}

uint64_t sub_1CAAFDBC4()
{
  return swift_deallocObject();
}

uint64_t sub_1CAAFDBD4()
{
  return swift_deallocObject();
}

uint64_t sub_1CAAFDBE4()
{
  return swift_deallocObject();
}

uint64_t outlined init with copy of BNNSOptimizer(uint64_t a1, uint64_t a2)
{
  uint64_t v3;

  v3 = *(_QWORD *)(a1 + 24);
  *(_QWORD *)(a2 + 24) = v3;
  *(_QWORD *)(a2 + 32) = *(_QWORD *)(a1 + 32);
  (**(void (***)(uint64_t, uint64_t))(v3 - 8))(a2, a1);
  return a2;
}

_QWORD *__swift_project_boxed_opaque_existential_1(_QWORD *result, uint64_t a2)
{
  if ((*(_DWORD *)(*(_QWORD *)(a2 - 8) + 80) & 0x20000) != 0)
    return (_QWORD *)(*result
                    + ((*(_DWORD *)(*(_QWORD *)(a2 - 8) + 80) + 16) & ~(unint64_t)*(_DWORD *)(*(_QWORD *)(a2 - 8) + 80)));
  return result;
}

uint64_t partial apply for closure #1 in closure #4 in static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)(uint64_t a1)
{
  uint64_t v1;

  return closure #1 in closure #4 in static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)(a1, *(_DWORD **)(v1 + 16), *(_QWORD **)(v1 + 24), *(_QWORD *)(v1 + 32), *(char ***)(v1 + 40), *(char ***)(v1 + 48), *(char ***)(v1 + 56), *(_QWORD *)(v1 + 64));
}

double BNNS.FusedTernaryArithmeticParameters.layerParameters(inputA:inputB:inputC:output:)@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, _OWORD *a3@<X2>, _OWORD *a4@<X3>, uint64_t *a5@<X8>)
{
  uint64_t v5;
  int v8;
  int v9;
  int v10;
  int v11;
  uint64_t v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  uint64_t v18;
  uint64_t v19;
  double result;
  _BYTE v21[180];
  _BYTE v22[180];
  _BYTE v23[180];

  *(_OWORD *)&v23[116] = a2[7];
  *(_OWORD *)&v23[132] = a2[8];
  *(_OWORD *)&v23[148] = a2[9];
  *(_OWORD *)&v23[164] = a2[10];
  *(_OWORD *)&v23[52] = a2[3];
  *(_OWORD *)&v23[68] = a2[4];
  *(_OWORD *)&v23[84] = a2[5];
  *(_OWORD *)&v23[100] = a2[6];
  *(_OWORD *)&v23[4] = *a2;
  *(_OWORD *)&v23[20] = a2[1];
  *(_OWORD *)&v23[36] = a2[2];
  *(_OWORD *)&v22[116] = a3[7];
  *(_OWORD *)&v22[132] = a3[8];
  *(_OWORD *)&v22[148] = a3[9];
  *(_OWORD *)&v22[164] = a3[10];
  *(_OWORD *)&v22[52] = a3[3];
  *(_OWORD *)&v22[68] = a3[4];
  *(_OWORD *)&v22[84] = a3[5];
  *(_OWORD *)&v22[100] = a3[6];
  *(_OWORD *)&v22[4] = *a3;
  *(_OWORD *)&v22[20] = a3[1];
  *(_OWORD *)&v22[36] = a3[2];
  *(_OWORD *)&v21[100] = a4[6];
  *(_OWORD *)&v21[116] = a4[7];
  *(_OWORD *)&v21[132] = a4[8];
  *(_OWORD *)&v21[148] = a4[9];
  *(_OWORD *)&v21[164] = a4[10];
  *(_OWORD *)&v21[52] = a4[3];
  *(_OWORD *)&v21[68] = a4[4];
  *(_OWORD *)&v21[84] = a4[5];
  *(_OWORD *)&v21[4] = *a4;
  *(_OWORD *)&v21[20] = a4[1];
  v8 = *(unsigned __int8 *)(v5 + 8);
  v9 = *(unsigned __int8 *)(v5 + 9);
  v10 = *(unsigned __int8 *)(v5 + 10);
  v11 = *(unsigned __int8 *)(v5 + 11);
  *(_OWORD *)&v21[36] = a4[2];
  v12 = swift_slowAlloc();
  v13 = a1[9];
  *(_OWORD *)(v12 + 128) = a1[8];
  *(_OWORD *)(v12 + 144) = v13;
  *(_OWORD *)(v12 + 160) = a1[10];
  v14 = a1[5];
  *(_OWORD *)(v12 + 64) = a1[4];
  *(_OWORD *)(v12 + 80) = v14;
  v15 = a1[7];
  *(_OWORD *)(v12 + 96) = a1[6];
  *(_OWORD *)(v12 + 112) = v15;
  v16 = a1[1];
  *(_OWORD *)v12 = *a1;
  *(_OWORD *)(v12 + 16) = v16;
  v17 = a1[3];
  *(_OWORD *)(v12 + 32) = a1[2];
  *(_OWORD *)(v12 + 48) = v17;
  *(_OWORD *)(v12 + 324) = *(_OWORD *)&v23[144];
  *(_OWORD *)(v12 + 340) = *(_OWORD *)&v23[160];
  *(_OWORD *)(v12 + 244) = *(_OWORD *)&v23[64];
  *(_OWORD *)(v12 + 260) = *(_OWORD *)&v23[80];
  *(_OWORD *)(v12 + 276) = *(_OWORD *)&v23[96];
  *(_QWORD *)v5 = v12;
  *(_DWORD *)(v12 + 176) = v8;
  *(_DWORD *)(v12 + 356) = *(_DWORD *)&v23[176];
  *(_OWORD *)(v12 + 292) = *(_OWORD *)&v23[112];
  *(_OWORD *)(v12 + 308) = *(_OWORD *)&v23[128];
  *(_OWORD *)(v12 + 180) = *(_OWORD *)v23;
  *(_OWORD *)(v12 + 196) = *(_OWORD *)&v23[16];
  *(_OWORD *)(v12 + 212) = *(_OWORD *)&v23[32];
  *(_OWORD *)(v12 + 228) = *(_OWORD *)&v23[48];
  *(_DWORD *)(v12 + 360) = v9;
  *(_OWORD *)(v12 + 492) = *(_OWORD *)&v22[128];
  *(_OWORD *)(v12 + 508) = *(_OWORD *)&v22[144];
  *(_OWORD *)(v12 + 524) = *(_OWORD *)&v22[160];
  *(_DWORD *)(v12 + 540) = *(_DWORD *)&v22[176];
  *(_OWORD *)(v12 + 428) = *(_OWORD *)&v22[64];
  *(_OWORD *)(v12 + 444) = *(_OWORD *)&v22[80];
  *(_OWORD *)(v12 + 460) = *(_OWORD *)&v22[96];
  *(_OWORD *)(v12 + 476) = *(_OWORD *)&v22[112];
  *(_OWORD *)(v12 + 364) = *(_OWORD *)v22;
  *(_OWORD *)(v12 + 380) = *(_OWORD *)&v22[16];
  *(_OWORD *)(v12 + 396) = *(_OWORD *)&v22[32];
  *(_OWORD *)(v12 + 412) = *(_OWORD *)&v22[48];
  *(_DWORD *)(v12 + 544) = v10;
  *(_OWORD *)(v12 + 676) = *(_OWORD *)&v21[128];
  *(_OWORD *)(v12 + 692) = *(_OWORD *)&v21[144];
  *(_OWORD *)(v12 + 708) = *(_OWORD *)&v21[160];
  *(_DWORD *)(v12 + 724) = *(_DWORD *)&v21[176];
  *(_OWORD *)(v12 + 612) = *(_OWORD *)&v21[64];
  *(_OWORD *)(v12 + 628) = *(_OWORD *)&v21[80];
  *(_OWORD *)(v12 + 644) = *(_OWORD *)&v21[96];
  *(_OWORD *)(v12 + 660) = *(_OWORD *)&v21[112];
  *(_OWORD *)(v12 + 548) = *(_OWORD *)v21;
  *(_OWORD *)(v12 + 564) = *(_OWORD *)&v21[16];
  *(_OWORD *)(v12 + 580) = *(_OWORD *)&v21[32];
  *(_OWORD *)(v12 + 596) = *(_OWORD *)&v21[48];
  *(_DWORD *)(v12 + 728) = v11;
  type metadata accessor for BNNSLayerParametersArithmetic(0);
  a5[3] = v18;
  a5[4] = (uint64_t)&protocol witness table for BNNSLayerParametersArithmetic;
  v19 = swift_allocObject();
  *a5 = v19;
  *(_DWORD *)(v19 + 16) = 28;
  *(_QWORD *)(v19 + 24) = v12;
  *(_DWORD *)(v19 + 32) = 0;
  *(int32x2_t *)(v19 + 36) = vdup_n_s32(0x7FC00000u);
  *(_DWORD *)(v19 + 44) = 1;
  result = 0.0;
  *(_OWORD *)(v19 + 48) = 0u;
  *(_OWORD *)(v19 + 64) = 0u;
  return result;
}

void BNNS.FusedTernaryArithmeticParameters.inputADescriptorType.getter(_BYTE *a1@<X8>)
{
  uint64_t v1;

  *a1 = *(_BYTE *)(v1 + 8);
}

_BYTE *BNNS.FusedTernaryArithmeticParameters.inputADescriptorType.setter(_BYTE *result)
{
  uint64_t v1;

  *(_BYTE *)(v1 + 8) = *result;
  return result;
}

uint64_t (*BNNS.FusedTernaryArithmeticParameters.inputADescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedTernaryArithmeticParameters.inputBDescriptorType.getter(_BYTE *a1@<X8>)
{
  uint64_t v1;

  *a1 = *(_BYTE *)(v1 + 9);
}

_BYTE *BNNS.FusedTernaryArithmeticParameters.inputBDescriptorType.setter(_BYTE *result)
{
  uint64_t v1;

  *(_BYTE *)(v1 + 9) = *result;
  return result;
}

uint64_t (*BNNS.FusedTernaryArithmeticParameters.inputBDescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedTernaryArithmeticParameters.inputCDescriptorType.getter(_BYTE *a1@<X8>)
{
  uint64_t v1;

  *a1 = *(_BYTE *)(v1 + 10);
}

_BYTE *BNNS.FusedTernaryArithmeticParameters.inputCDescriptorType.setter(_BYTE *result)
{
  uint64_t v1;

  *(_BYTE *)(v1 + 10) = *result;
  return result;
}

uint64_t (*BNNS.FusedTernaryArithmeticParameters.inputCDescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedTernaryArithmeticParameters.outputDescriptorType.getter(_BYTE *a1@<X8>)
{
  uint64_t v1;

  *a1 = *(_BYTE *)(v1 + 11);
}

_BYTE *BNNS.FusedTernaryArithmeticParameters.outputDescriptorType.setter(_BYTE *result)
{
  uint64_t v1;

  *(_BYTE *)(v1 + 11) = *result;
  return result;
}

uint64_t (*BNNS.FusedTernaryArithmeticParameters.outputDescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t (*BNNS.FusedTernaryArithmeticParameters.function.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

char *BNNS.FusedTernaryArithmeticParameters.init(inputADescriptorType:inputBDescriptorType:inputCDescriptorType:outputDescriptorType:function:)@<X0>(char *result@<X0>, char *a2@<X1>, char *a3@<X2>, char *a4@<X3>, uint64_t a5@<X8>)
{
  char v5;
  char v6;
  char v7;
  char v8;

  v5 = *result;
  v6 = *a2;
  v7 = *a3;
  v8 = *a4;
  *(_QWORD *)a5 = 0;
  *(_BYTE *)(a5 + 8) = v5;
  *(_BYTE *)(a5 + 9) = v6;
  *(_BYTE *)(a5 + 10) = v7;
  *(_BYTE *)(a5 + 11) = v8;
  return result;
}

uint64_t protocol witness for FusableLayerParametersWrapperDeallocatable.deallocate() in conformance BNNS.FusedTernaryArithmeticParameters()
{
  uint64_t *v0;
  uint64_t result;

  result = *v0;
  if (*v0)
    JUMPOUT(0x1D1794DA4);
  return result;
}

uint64_t BNNS.FusedParametersLayer.__allocating_init(inputA:inputB:inputC:output:fusedLayerParameters:filterParameters:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint32_t a6, size_t a7, int (__cdecl *a8)(void **, size_t, size_t), void (__cdecl *a9)(void *))
{
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  int v23;
  int v24;
  void *v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  __int128 v30;
  _DWORD v31[6];
  __int128 v32;
  uint64_t v33;
  int v34;
  uint64_t v35;
  int v36;
  __int128 v37;
  int v38;
  __int128 v39;
  uint64_t v40;
  char v41[40];
  _QWORD v42[5];
  _QWORD v43[5];
  _QWORD v44[3];
  uint64_t v45;
  uint64_t v46;
  _BYTE v47[24];
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;

  v50 = *MEMORY[0x1E0C80C00];
  if (*(_QWORD *)(a5 + 16) != 2)
  {
    __break(1u);
    goto LABEL_20;
  }
  outlined init with copy of BNNSOptimizer(a5 + 32, (uint64_t)&v34);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParameters);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableTernaryInputLayerParametersWrapper);
  if ((swift_dynamicCast() & 1) == 0)
  {
    v30 = 0u;
    memset(v31, 0, sizeof(v31));
    swift_bridgeObjectRelease();
    outlined destroy of FusableTernaryInputLayerParametersWrapper?((uint64_t)&v30, &demangling cache variable for type metadata for FusableTernaryInputLayerParametersWrapper?);
    return 0;
  }
  outlined init with take of FusableLayerParametersWrapper(&v30, (uint64_t)v47);
  if (*(_QWORD *)(a5 + 16) < 2uLL)
LABEL_20:
    __break(1u);
  outlined init with copy of BNNSOptimizer(a5 + 72, (uint64_t)&v34);
  swift_bridgeObjectRelease();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParametersWrapper);
  if ((swift_dynamicCast() & 1) == 0)
  {
    v30 = 0u;
    memset(v31, 0, sizeof(v31));
    outlined destroy of FusableTernaryInputLayerParametersWrapper?((uint64_t)&v30, &demangling cache variable for type metadata for FusableLayerParametersWrapper?);
LABEL_16:
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v47);
    return 0;
  }
  outlined init with take of FusableLayerParametersWrapper(&v30, (uint64_t)v44);
  v18 = v48;
  v17 = v49;
  __swift_mutable_project_boxed_opaque_existential_1((uint64_t)v47, v48);
  (*(void (**)(_QWORD *__return_ptr, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))(v17 + 8))(v43, a1, a2, a3, a4, v18, v17);
  v19 = v45;
  v20 = v46;
  __swift_mutable_project_boxed_opaque_existential_1((uint64_t)v44, v45);
  (*(void (**)(_QWORD *__return_ptr, uint64_t, uint64_t, uint64_t, uint64_t))(v20 + 8))(v42, a4, a4, v19, v20);
  v21 = v45;
  v22 = v46;
  __swift_project_boxed_opaque_existential_1(v44, v45);
  v23 = (*(uint64_t (**)(uint64_t, uint64_t))(v22 + 16))(v21, v22);
  if ((v23 - 2) > 3)
  {
LABEL_15:
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v42);
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v43);
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v44);
    goto LABEL_16;
  }
  v24 = v23;
  outlined init with copy of BNNSOptimizer((uint64_t)v43, (uint64_t)v41);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
  type metadata accessor for BNNSLayerParametersArithmetic(0);
  swift_dynamicCast();
  LODWORD(v30) = v34;
  *((_QWORD *)&v30 + 1) = v35;
  v31[0] = v36;
  *(_OWORD *)&v31[1] = v37;
  v31[5] = v38;
  v32 = v39;
  v33 = v40;
  v25 = specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)((uint64_t)&v30, (uint64_t)v42, a6, a7, a8, a9, 8, v24);
  type metadata accessor for BNNS.FusedParametersLayer();
  v26 = swift_allocObject();
  v27 = v26;
  *(_QWORD *)(v26 + 24) = MEMORY[0x1E0DEE9D8];
  if (!v25)
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    goto LABEL_15;
  }
  *(_QWORD *)(v26 + 16) = v25;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<FusableLayerParametersWrapperDeallocatable?>);
  v28 = swift_allocObject();
  *(_OWORD *)(v28 + 16) = xmmword_1CAB5E440;
  outlined init with copy of BNNSOptimizer((uint64_t)v47, (uint64_t)&v34);
  swift_retain();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParametersWrapperDeallocatable);
  if ((swift_dynamicCast() & 1) == 0)
  {
    *(_QWORD *)(v28 + 64) = 0;
    *(_OWORD *)(v28 + 32) = 0u;
    *(_OWORD *)(v28 + 48) = 0u;
  }
  outlined init with copy of BNNSOptimizer((uint64_t)v44, (uint64_t)&v30);
  if ((swift_dynamicCast() & 1) == 0)
  {
    *(_QWORD *)(v28 + 104) = 0;
    *(_OWORD *)(v28 + 72) = 0u;
    *(_OWORD *)(v28 + 88) = 0u;
  }
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v42);
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v43);
  *(_QWORD *)(v27 + 24) = v28;
  swift_release();
  swift_bridgeObjectRelease();
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v44);
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v47);
  return v27;
}

uint64_t BNNS.FusedParametersLayer.apply(batchSize:inputA:inputB:inputC:output:for:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, unint64_t a5, char a6)
{
  uint64_t v6;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  unint64_t v14;
  unint64_t v15;
  unint64_t v16;
  unint64_t v17;
  unint64_t v18;
  unint64_t v19;
  unint64_t v20;
  size_t v21;
  int v22;
  uint64_t result;
  _BYTE *v24;
  _BYTE *v25;
  void *v26;
  const void **v27;
  char v28;
  void *out;
  uint64_t v30;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  unint64_t v46;
  unint64_t v47;
  unint64_t v48;
  unint64_t v49;
  unint64_t v50;
  unint64_t v51;
  unint64_t v52;
  unint64_t v53;
  unint64_t v54;
  unint64_t v55;
  _BYTE v56[136];
  _BYTE v57[136];
  _BYTE v58[136];
  _BYTE v59[136];
  _BYTE v60[136];
  _BYTE v61[136];
  _BYTE v62[8];
  _BYTE v63[8];
  _BYTE v64[8];
  _BYTE v65[8];
  uint64_t v66;
  uint64_t v67;
  uint64_t v68;
  void *v69;
  unint64_t v70;

  v70 = a5;
  outlined init with take of UnsafeMutableRawPointer?(a2 + 136, (uint64_t)v65);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v65, (uint64_t)&v66);
  v10 = v66;
  if (!v66)
    goto LABEL_7;
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v64);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v64, (uint64_t)&v67);
  v11 = v67;
  if (!v67)
    goto LABEL_7;
  outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v63);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v63, (uint64_t)&v68);
  v12 = v68;
  if (v68
    && (outlined init with take of UnsafeMutableRawPointer?(v70 + 136, (uint64_t)v62),
        outlined init with take of UnsafeMutableRawPointer?((uint64_t)v62, (uint64_t)&v69),
        v69))
  {
    v28 = a6;
    out = v69;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
    v13 = swift_allocObject();
    *(_OWORD *)(v13 + 16) = xmmword_1CAB5EF70;
    *(_QWORD *)(v13 + 32) = v10;
    v27 = (const void **)(v13 + 32);
    *(_QWORD *)(v13 + 40) = v11;
    *(_QWORD *)(v13 + 48) = v12;
    v26 = *(void **)(v6 + 16);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
    v30 = swift_allocObject();
    *(_OWORD *)(v30 + 16) = xmmword_1CAB5EF70;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v57);
    outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v58);
    outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)v56);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)v56);
    BNNS.Shape.stride.getter();
    *(_QWORD *)(v30 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v48, v49, v50, v51, v52, v53, v54, v55, v48, v49, v50, v51, v52, v53, v54, v55);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v56);
    outlined init with take of BNNS.Shape((uint64_t)v56, (uint64_t)v59);
    outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)&v48);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)&v48);
    BNNS.Shape.stride.getter();
    *(_QWORD *)(v30 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v40, v41, v42, v43, v44, v45, v46, v47, v40, v41, v42, v43, v44, v45, v46, v47);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v48);
    outlined init with take of BNNS.Shape((uint64_t)&v48, (uint64_t)v60);
    outlined init with take of BNNS.Shape((uint64_t)v60, (uint64_t)&v40);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v60, (uint64_t)&v40);
    BNNS.Shape.stride.getter();
    *(_QWORD *)(v30 + 48) = specialized static BNNS.calculateBatchStride(size:stride:)(v32, v33, v34, v35, v36, v37, v38, v39, v32, v33, v34, v35, v36, v37, v38, v39);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v57);
    outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v61);
    outlined init with take of BNNS.Shape((uint64_t)v61, (uint64_t)v56);
    BNNS.Shape.size.getter();
    v14 = v48;
    v15 = v49;
    v16 = v50;
    v17 = v51;
    v18 = v52;
    v19 = v53;
    v20 = v54;
    v70 = v55;
    outlined init with take of BNNS.Shape((uint64_t)v61, (uint64_t)v56);
    BNNS.Shape.stride.getter();
    v21 = specialized static BNNS.calculateBatchStride(size:stride:)(v14, v15, v16, v17, v18, v19, v20, v70, v48, v49, v50, v51, v52, v53, v54, v55);
    v22 = BNNSFusedFilterApplyMultiInputBatch(v26, a1, 3uLL, v27, (const size_t *)(v30 + 32), out, v21, (v28 & 1) == 0);
    swift_bridgeObjectRelease();
    result = swift_bridgeObjectRelease();
    if (!v22)
      return result;
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v24 = 0;
  }
  else
  {
LABEL_7:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v25 = 2;
  }
  return swift_willThrow();
}

uint64_t BNNS.FusedParametersLayer.applyBackward(batchSize:inputA:inputB:inputC:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingInputCGradient:generatingParameterGradients:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, _OWORD *a6, _OWORD *a7, _OWORD *a8, __int128 *a9, uint64_t a10)
{
  uint64_t v10;
  size_t v11;
  int64_t v12;
  uint64_t v13;
  __int128 *v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  _OWORD *v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  unint64_t v26;
  unint64_t v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  __int128 v52;
  __int128 v53;
  _BYTE *v54;
  char *v55;
  uint64_t v56;
  uint64_t i;
  uint64_t v58;
  char *v59;
  uint64_t v60;
  uint64_t j;
  uint64_t v62;
  _OWORD *v64;
  uint64_t v65;
  _OWORD *v67;
  _OWORD *v68;
  uint64_t v69;
  uint64_t v70;
  uint64_t v71;
  uint64_t v72;
  int v73;
  char *v74;
  BNNSNDArrayDescriptor v75;
  _OWORD v76[11];
  _OWORD v77[11];
  __int128 v78;
  __int128 v79;
  __int128 v80;
  __int128 v81;
  __int128 v82;
  __int128 v83;
  __int128 v84;
  __int128 v85;
  __int128 v86;
  __int128 v87;
  __int128 v88;
  uint64_t v89;

  v11 = a1;
  v89 = *MEMORY[0x1E0C80C00];
  v12 = *(_QWORD *)(a10 + 16);
  v13 = MEMORY[0x1E0DEE9D8];
  if (v12)
  {
    v64 = a6;
    v65 = a5;
    v67 = a8;
    v68 = a7;
    v69 = a2;
    v71 = a3;
    v72 = a4;
    *(_QWORD *)&v77[0] = MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v12, 0);
    v14 = (__int128 *)(a10 + 32);
    v13 = *(_QWORD *)&v77[0];
    do
    {
      v15 = v14[9];
      v86 = v14[8];
      v87 = v15;
      v88 = v14[10];
      v16 = v14[5];
      v82 = v14[4];
      v83 = v16;
      v17 = v14[7];
      v84 = v14[6];
      v85 = v17;
      v18 = v14[1];
      v78 = *v14;
      v79 = v18;
      v19 = v14[3];
      v80 = v14[2];
      v81 = v19;
      v20 = (_OWORD *)swift_slowAlloc();
      v21 = v87;
      v20[8] = v86;
      v20[9] = v21;
      v20[10] = v88;
      v22 = v83;
      v20[4] = v82;
      v20[5] = v22;
      v23 = v85;
      v20[6] = v84;
      v20[7] = v23;
      v24 = v79;
      *v20 = v78;
      v20[1] = v24;
      v25 = v81;
      v20[2] = v80;
      v20[3] = v25;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(_QWORD *)(v13 + 16) + 1, 1);
        v13 = *(_QWORD *)&v77[0];
      }
      v27 = *(_QWORD *)(v13 + 16);
      v26 = *(_QWORD *)(v13 + 24);
      if (v27 >= v26 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v26 > 1), v27 + 1, 1);
        v13 = *(_QWORD *)&v77[0];
      }
      *(_QWORD *)(v13 + 16) = v27 + 1;
      *(_QWORD *)(v13 + 8 * v27 + 32) = v20;
      v14 += 11;
      --v12;
    }
    while (v12);
    a4 = v72;
    v10 = v70;
    a3 = v71;
    a7 = v68;
    a2 = v69;
    v11 = a1;
    a8 = v67;
    a6 = v64;
    a5 = v65;
  }
  v28 = a7[8];
  v29 = a7[9];
  v30 = a7[6];
  v85 = a7[7];
  v86 = v28;
  v31 = a7[10];
  v87 = v29;
  v88 = v31;
  v32 = a7[4];
  v33 = a7[5];
  v34 = a7[2];
  v81 = a7[3];
  v82 = v32;
  v83 = v33;
  v84 = v30;
  v35 = *a7;
  v79 = a7[1];
  v80 = v34;
  v36 = a8[9];
  v77[8] = a8[8];
  v77[9] = v36;
  v77[10] = a8[10];
  v78 = v35;
  v37 = a8[5];
  v77[4] = a8[4];
  v77[5] = v37;
  v38 = a8[7];
  v77[6] = a8[6];
  v77[7] = v38;
  v39 = a8[1];
  v77[0] = *a8;
  v77[1] = v39;
  v40 = a8[3];
  v77[2] = a8[2];
  v77[3] = v40;
  v41 = a9[8];
  v42 = a9[9];
  v43 = a9[6];
  v76[7] = a9[7];
  v76[8] = v41;
  v44 = a9[10];
  v76[9] = v42;
  v76[10] = v44;
  v45 = a9[4];
  v46 = a9[5];
  v47 = a9[2];
  v76[3] = a9[3];
  v76[4] = v45;
  v74 = (char *)v13;
  v73 = 0;
  v76[5] = v46;
  v76[6] = v43;
  v48 = *a9;
  v76[1] = a9[1];
  v76[2] = v47;
  v49 = a6[9];
  *(_OWORD *)&v75.stride[7] = a6[8];
  *(_OWORD *)&v75.data_type = v49;
  *(_OWORD *)&v75.table_data_type = a6[10];
  v76[0] = v48;
  v50 = a6[5];
  *(_OWORD *)&v75.size[7] = a6[4];
  *(_OWORD *)&v75.stride[1] = v50;
  v51 = a6[7];
  *(_OWORD *)&v75.stride[3] = a6[6];
  *(_OWORD *)&v75.stride[5] = v51;
  v52 = a6[1];
  *(_OWORD *)&v75.flags = *a6;
  *(_OWORD *)&v75.size[1] = v52;
  v53 = a6[3];
  *(_OWORD *)&v75.size[3] = a6[2];
  *(_OWORD *)&v75.size[5] = v53;
  closure #1 in closure #1 in closure #1 in closure #2 in BNNS.FusedParametersLayer.applyBackward(batchSize:inputA:inputB:inputC:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingInputCGradient:generatingParameterGradients:)(&v75, a2, a3, a4, (uint64_t)&v78, (uint64_t)v77, (uint64_t)v76, &v73, v10, v11, (uint64_t)a7, (uint64_t)a8, (uint64_t)a9, a5, (uint64_t)a6, &v74);
  if (v73)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v54 = 0;
    swift_willThrow();
    v55 = v74;
    v56 = *((_QWORD *)v74 + 2);
    if (v56)
    {
      swift_bridgeObjectRetain();
      for (i = 0; i != v56; ++i)
      {
        v58 = *(_QWORD *)&v55[8 * i + 32];
        if (v58)
          MEMORY[0x1D1794DA4](v58, -1, -1);
      }
LABEL_20:
      swift_bridgeObjectRelease();
    }
  }
  else
  {
    v59 = v74;
    v60 = *((_QWORD *)v74 + 2);
    if (v60)
    {
      swift_bridgeObjectRetain();
      for (j = 0; j != v60; ++j)
      {
        v62 = *(_QWORD *)&v59[8 * j + 32];
        if (v62)
          MEMORY[0x1D1794DA4](v62, -1, -1);
      }
      goto LABEL_20;
    }
  }
  return swift_bridgeObjectRelease();
}

uint64_t closure #1 in closure #1 in closure #1 in closure #2 in BNNS.FusedParametersLayer.applyBackward(batchSize:inputA:inputB:inputC:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingInputCGradient:generatingParameterGradients:)(BNNSNDArrayDescriptor *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, int *a8, uint64_t a9, size_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15, char **a16)
{
  uint64_t v22;
  _QWORD *v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  uint64_t v30;
  uint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  unint64_t v46;
  unint64_t v47;
  unint64_t v48;
  unint64_t v49;
  unint64_t v50;
  unint64_t v51;
  unint64_t v52;
  unint64_t v53;
  unint64_t v54;
  unint64_t v55;
  size_t out_delta_stride;
  char *v57;
  char isUniquelyReferenced_nonNull_native;
  int v59;
  uint64_t result;
  unint64_t v61;
  void *out;
  size_t out_stride;
  const size_t *v64;
  uint64_t v65;
  void *v66;
  void **in;
  uint64_t v70;
  unint64_t v71;
  unint64_t v72;
  unint64_t v73;
  unint64_t v74;
  unint64_t v75;
  unint64_t v76;
  unint64_t v77;
  unint64_t v78;
  unint64_t v79;
  unint64_t v80;
  unint64_t v81;
  unint64_t v82;
  unint64_t v83;
  unint64_t v84;
  unint64_t v85;
  unint64_t v86;
  unint64_t v87;
  unint64_t v88;
  unint64_t v89;
  unint64_t v90;
  unint64_t v91;
  unint64_t v92;
  unint64_t v93;
  unint64_t v94;
  unint64_t v95;
  unint64_t v96;
  unint64_t v97;
  unint64_t v98;
  unint64_t v99;
  unint64_t v100;
  unint64_t v101;
  unint64_t v102;
  unint64_t v103;
  unint64_t v104;
  unint64_t v105;
  unint64_t v106;
  unint64_t v107;
  unint64_t v108;
  unint64_t v109;
  unint64_t v110;
  _BYTE v111[136];
  _BYTE v112[136];
  _BYTE v113[136];
  unint64_t v114;
  unint64_t v115;
  unint64_t v116;
  unint64_t v117;
  unint64_t v118;
  unint64_t v119;
  unint64_t v120;
  unint64_t v121;
  __int128 v122;
  __int128 v123;
  __int128 v124;
  unint64_t v125;
  unint64_t v126;
  unint64_t v127;
  unint64_t v128;
  unint64_t v129;
  unint64_t v130;
  unint64_t v131;
  unint64_t v132;
  unint64_t v133;
  unint64_t v134;
  _BYTE v135[136];
  _BYTE v136[8];
  _BYTE v137[8];
  _BYTE v138[8];
  _BYTE v139[8];
  uint64_t v140;
  uint64_t v141;
  uint64_t v142;
  void *v143;
  uint64_t v144;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer?>);
  v22 = swift_allocObject();
  *(_OWORD *)(v22 + 16) = xmmword_1CAB5EF70;
  v23 = (_QWORD *)v22;
  outlined init with take of UnsafeMutableRawPointer?(a2 + 136, (uint64_t)v139);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v139, (uint64_t)&v140);
  v23[4] = v140;
  in = (void **)(v23 + 4);
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v138);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v138, (uint64_t)&v141);
  v23[5] = v141;
  outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v137);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v137, (uint64_t)&v142);
  v23[6] = v142;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>>);
  v65 = swift_allocObject();
  *(_OWORD *)(v65 + 16) = xmmword_1CAB5EF70;
  *(_QWORD *)(v65 + 32) = a5;
  *(_QWORD *)(v65 + 40) = a6;
  *(_QWORD *)(v65 + 48) = a7;
  v66 = *(void **)(a9 + 16);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  v144 = swift_allocObject();
  *(_OWORD *)(v144 + 16) = xmmword_1CAB5EF70;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v135);
  outlined init with take of BNNS.Shape((uint64_t)v135, (uint64_t)v112);
  outlined init with take of BNNS.Shape((uint64_t)v112, (uint64_t)&v127);
  BNNS.Shape.size.getter();
  v24 = v122;
  v25 = v123;
  v26 = v124;
  v27 = v125;
  v28 = v126;
  outlined init with take of BNNS.Shape((uint64_t)v112, (uint64_t)&v127);
  BNNS.Shape.stride.getter();
  v29 = specialized static BNNS.calculateBatchStride(size:stride:)(v24, *((unint64_t *)&v24 + 1), v25, *((unint64_t *)&v25 + 1), v26, *((unint64_t *)&v26 + 1), v27, v28, v122, *((unint64_t *)&v122 + 1), v123, *((unint64_t *)&v123 + 1), v124, *((unint64_t *)&v124 + 1), v125, v126);
  v30 = v144;
  v31 = v144;
  *(_QWORD *)(v144 + 32) = v29;
  v64 = (const size_t *)(v31 + 32);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v127);
  outlined init with take of BNNS.Shape((uint64_t)&v127, (uint64_t)v113);
  outlined init with take of BNNS.Shape((uint64_t)v113, (uint64_t)&v122);
  BNNS.Shape.size.getter();
  v61 = v114;
  v32 = v115;
  v33 = v116;
  v34 = v117;
  v35 = v118;
  v36 = v119;
  v37 = v120;
  v38 = v121;
  outlined init with take of BNNS.Shape((uint64_t)v113, (uint64_t)&v122);
  BNNS.Shape.stride.getter();
  *(_QWORD *)(v30 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v61, v32, v33, v34, v35, v36, v37, v38, v114, v115, v116, v117, v118, v119, v120, v121);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v122);
  outlined init with take of BNNS.Shape((uint64_t)&v122, (uint64_t)&v114);
  outlined init with take of BNNS.Shape((uint64_t)&v114, (uint64_t)v111);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)&v114, (uint64_t)v111);
  BNNS.Shape.stride.getter();
  v39 = specialized static BNNS.calculateBatchStride(size:stride:)(v103, v104, v105, v106, v107, v108, v109, v110, v103, v104, v105, v106, v107, v108, v109, v110);
  *(_QWORD *)(v144 + 48) = v39;
  v70 = swift_allocObject();
  *(_OWORD *)(v70 + 16) = xmmword_1CAB5EF70;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v111);
  outlined init with take of BNNS.Shape((uint64_t)v111, (uint64_t)&v122);
  outlined init with take of BNNS.Shape((uint64_t)&v122, (uint64_t)v135);
  BNNS.Shape.size.getter();
  v40 = v127;
  v41 = v128;
  v42 = v129;
  v43 = v130;
  v44 = v131;
  v45 = v132;
  v46 = v133;
  v47 = v134;
  outlined init with take of BNNS.Shape((uint64_t)&v122, (uint64_t)v135);
  BNNS.Shape.stride.getter();
  *(_QWORD *)(v70 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v40, v41, v42, v43, v44, v45, v46, v47, v127, v128, v129, v130, v131, v132, v133, v134);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v103);
  outlined init with take of BNNS.Shape((uint64_t)&v103, (uint64_t)&v127);
  outlined init with take of BNNS.Shape((uint64_t)&v127, (uint64_t)v135);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)&v127, (uint64_t)v135);
  BNNS.Shape.stride.getter();
  *(_QWORD *)(v70 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v95, v96, v97, v98, v99, v100, v101, v102, v95, v96, v97, v98, v99, v100, v101, v102);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v95);
  outlined init with take of BNNS.Shape((uint64_t)&v95, (uint64_t)v135);
  outlined init with take of BNNS.Shape((uint64_t)v135, (uint64_t)&v87);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v135, (uint64_t)&v87);
  BNNS.Shape.stride.getter();
  *(_QWORD *)(v70 + 48) = specialized static BNNS.calculateBatchStride(size:stride:)(v79, v80, v81, v82, v83, v84, v85, v86, v79, v80, v81, v82, v83, v84, v85, v86);
  outlined init with take of UnsafeMutableRawPointer?(a14 + 136, (uint64_t)v136);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v136, (uint64_t)&v143);
  out = v143;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v95);
  outlined init with take of BNNS.Shape((uint64_t)&v95, (uint64_t)&v103);
  outlined init with take of BNNS.Shape((uint64_t)&v103, (uint64_t)v111);
  BNNS.Shape.size.getter();
  v48 = v87;
  v49 = v88;
  v50 = v89;
  v51 = v90;
  v52 = v91;
  v53 = v92;
  v55 = v93;
  v54 = v94;
  outlined init with take of BNNS.Shape((uint64_t)&v103, (uint64_t)v111);
  BNNS.Shape.stride.getter();
  out_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v48, v49, v50, v51, v52, v53, v55, v54, v87, v88, v89, v90, v91, v92, v93, v94);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v87);
  outlined init with take of BNNS.Shape((uint64_t)&v87, (uint64_t)v111);
  outlined init with take of BNNS.Shape((uint64_t)v111, (uint64_t)&v79);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v111, (uint64_t)&v79);
  BNNS.Shape.stride.getter();
  out_delta_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v71, v72, v73, v74, v75, v76, v77, v78, v71, v72, v73, v74, v75, v76, v77, v78);
  v57 = *a16;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
  *a16 = v57;
  if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    v57 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v57 + 2), 0, v57);
  *a16 = v57;
  swift_bridgeObjectRetain();
  v59 = BNNSFusedFilterApplyBackwardMultiInputBatch(v66, a10, 3uLL, (const void **)in, v64, (BNNSNDArrayDescriptor **)(v65 + 32), (const size_t *)(v70 + 32), out, out_stride, a1, out_delta_stride, (BNNSNDArrayDescriptor **)v57 + 4);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease_n();
  result = swift_bridgeObjectRelease_n();
  *a8 = v59;
  return result;
}

uint64_t outlined destroy of FusableTernaryInputLayerParametersWrapper?(uint64_t a1, uint64_t *a2)
{
  uint64_t v3;

  v3 = __swift_instantiateConcreteTypeFromMangledName(a2);
  (*(void (**)(uint64_t, uint64_t))(*(_QWORD *)(v3 - 8) + 8))(a1, v3);
  return a1;
}

uint64_t outlined init with take of FusableLayerParametersWrapper(__int128 *a1, uint64_t a2)
{
  __int128 v2;
  __int128 v3;

  v2 = *a1;
  v3 = a1[1];
  *(_QWORD *)(a2 + 32) = *((_QWORD *)a1 + 4);
  *(_OWORD *)a2 = v2;
  *(_OWORD *)(a2 + 16) = v3;
  return a2;
}

uint64_t __swift_mutable_project_boxed_opaque_existential_1(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t result;

  if ((*(_DWORD *)(*(_QWORD *)(a2 - 8) + 80) & 0x20000) != 0)
  {
    swift_makeBoxUnique();
    return v2;
  }
  return result;
}

void *specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)(uint64_t a1, uint64_t a2, uint32_t a3, size_t a4, int (__cdecl *a5)(void **, size_t, size_t), void (__cdecl *a6)(void *), int a7, int a8)
{
  return specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)(a1, a2, a3, a4, a5, a6, a7, a8);
}

{
  uint64_t v14;
  const void **v15;
  uint64_t v16;
  const BNNSFilterType *v17;
  const void **v18;
  BNNSFilterParameters *p_filter_params;
  uint64_t v20;
  void *FusedLayer;
  BNNSFilterParameters filter_params;
  _BYTE __dst[1128];
  _BYTE __src[1128];
  _BYTE v27[40];
  uint64_t v28;

  v28 = *MEMORY[0x1E0C80C00];
  outlined init with copy of BNNSOptimizer(a2, (uint64_t)v27);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
  type metadata accessor for BNNSLayerParametersNormalization(0);
  swift_dynamicCast();
  memcpy(__dst, __src, sizeof(__dst));
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
  v14 = swift_allocObject();
  *(_OWORD *)(v14 + 16) = xmmword_1CAB5E440;
  if (!a1)
    __break(1u);
  v15 = (const void **)(v14 + 32);
  *(_QWORD *)(v14 + 32) = a1;
  *(_QWORD *)(v14 + 40) = __dst;
  if (a5 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<BNNSFilterType>);
    v16 = swift_allocObject();
    *(_DWORD *)(v16 + 32) = a7;
    v17 = (const BNNSFilterType *)(v16 + 32);
    *(_DWORD *)(v16 + 36) = a8;
    v18 = v15;
    p_filter_params = 0;
  }
  else
  {
    filter_params.flags = a3;
    filter_params.n_threads = a4;
    filter_params.alloc_memory = a5;
    filter_params.free_memory = a6;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<BNNSFilterType>);
    v20 = swift_allocObject();
    *(_DWORD *)(v20 + 32) = a7;
    v17 = (const BNNSFilterType *)(v20 + 32);
    *(_DWORD *)(v20 + 36) = a8;
    p_filter_params = &filter_params;
    v18 = v15;
  }
  FusedLayer = BNNSFilterCreateFusedLayer(2uLL, v17, v18, p_filter_params);
  swift_setDeallocating();
  swift_deallocClassInstance();
  swift_bridgeObjectRelease();
  return FusedLayer;
}

{
  return specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)(a1, a2, a3, a4, a5, a6, a7, a8);
}

{
  uint64_t v14;
  const void **v15;
  uint64_t v16;
  const BNNSFilterType *v17;
  const void **v18;
  BNNSFilterParameters *p_filter_params;
  uint64_t v20;
  void *FusedLayer;
  BNNSFilterParameters filter_params;
  _BYTE __dst[720];
  _BYTE __src[720];
  _BYTE v27[40];
  uint64_t v28;

  v28 = *MEMORY[0x1E0C80C00];
  outlined init with copy of BNNSOptimizer(a2, (uint64_t)v27);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
  type metadata accessor for BNNSLayerParametersQuantization(0);
  swift_dynamicCast();
  memcpy(__dst, __src, sizeof(__dst));
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
  v14 = swift_allocObject();
  *(_OWORD *)(v14 + 16) = xmmword_1CAB5E440;
  if (!a1)
    __break(1u);
  v15 = (const void **)(v14 + 32);
  *(_QWORD *)(v14 + 32) = a1;
  *(_QWORD *)(v14 + 40) = __dst;
  if (a5 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<BNNSFilterType>);
    v16 = swift_allocObject();
    *(_DWORD *)(v16 + 32) = a7;
    v17 = (const BNNSFilterType *)(v16 + 32);
    *(_DWORD *)(v16 + 36) = a8;
    v18 = v15;
    p_filter_params = 0;
  }
  else
  {
    filter_params.flags = a3;
    filter_params.n_threads = a4;
    filter_params.alloc_memory = a5;
    filter_params.free_memory = a6;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<BNNSFilterType>);
    v20 = swift_allocObject();
    *(_DWORD *)(v20 + 32) = a7;
    v17 = (const BNNSFilterType *)(v20 + 32);
    *(_DWORD *)(v20 + 36) = a8;
    p_filter_params = &filter_params;
    v18 = v15;
  }
  FusedLayer = BNNSFilterCreateFusedLayer(2uLL, v17, v18, p_filter_params);
  swift_setDeallocating();
  swift_deallocClassInstance();
  swift_bridgeObjectRelease();
  return FusedLayer;
}

uint64_t dispatch thunk of FusableTernaryInputLayerParametersWrapper.layerParameters(inputA:inputB:inputC:output:)(uint64_t *a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t a5, uint64_t a6)
{
  uint64_t v6;
  int v7;
  uint64_t v8;
  int v9;
  uint64_t v10;
  int v11;
  uint64_t v12;
  int v13;
  uint64_t v14;
  int v15;
  uint64_t v16;
  int v17;
  uint64_t v18;
  int v19;
  uint64_t v20;
  int v21;
  uint64_t (*v22)(uint64_t *, uint64_t *, uint64_t *, uint64_t *);
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  uint64_t v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  uint64_t v40;
  __int128 v41;
  uint64_t v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  uint64_t v52;
  int v53;
  uint64_t v54;
  int v55;
  uint64_t v56;
  uint64_t v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  __int128 v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  __int128 v65;
  uint64_t v66;
  int v67;
  uint64_t v68;
  int v69;
  uint64_t v70;
  uint64_t v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  __int128 v76;
  __int128 v77;
  __int128 v78;
  __int128 v79;
  uint64_t v80;
  int v81;
  uint64_t v82;
  int v83;
  uint64_t v84;
  uint64_t v85;
  __int128 v86;
  __int128 v87;
  __int128 v88;
  __int128 v89;
  __int128 v90;
  __int128 v91;
  __int128 v92;
  __int128 v93;
  uint64_t v94;
  int v95;
  uint64_t v96;
  int v97;
  uint64_t v98;

  v6 = a1[17];
  v7 = *((_DWORD *)a1 + 36);
  v8 = a1[19];
  v9 = *((_DWORD *)a1 + 40);
  v10 = a2[17];
  v11 = *((_DWORD *)a2 + 36);
  v12 = a2[19];
  v13 = *((_DWORD *)a2 + 40);
  v14 = a3[17];
  v15 = *((_DWORD *)a3 + 36);
  v16 = a3[19];
  v17 = *((_DWORD *)a3 + 40);
  v18 = a4[17];
  v19 = *((_DWORD *)a4 + 36);
  v20 = a4[19];
  v21 = *((_DWORD *)a4 + 40);
  v22 = *(uint64_t (**)(uint64_t *, uint64_t *, uint64_t *, uint64_t *))(a6 + 8);
  v85 = *a1;
  v86 = *(_OWORD *)(a1 + 1);
  v87 = *(_OWORD *)(a1 + 3);
  v88 = *(_OWORD *)(a1 + 5);
  v89 = *(_OWORD *)(a1 + 7);
  v90 = *(_OWORD *)(a1 + 9);
  v91 = *(_OWORD *)(a1 + 11);
  v92 = *(_OWORD *)(a1 + 13);
  v93 = *(_OWORD *)(a1 + 15);
  v94 = v6;
  v95 = v7;
  v96 = v8;
  v97 = v9;
  v98 = *(uint64_t *)((char *)a1 + 164);
  v71 = *a2;
  v72 = *(_OWORD *)(a2 + 1);
  v73 = *(_OWORD *)(a2 + 3);
  v74 = *(_OWORD *)(a2 + 5);
  v75 = *(_OWORD *)(a2 + 7);
  v76 = *(_OWORD *)(a2 + 9);
  v77 = *(_OWORD *)(a2 + 11);
  v23 = *(_OWORD *)(a2 + 15);
  v78 = *(_OWORD *)(a2 + 13);
  v24 = *(_OWORD *)(a3 + 1);
  v25 = *(_OWORD *)(a3 + 3);
  v26 = *(_OWORD *)(a3 + 5);
  v27 = *(_OWORD *)(a3 + 7);
  v28 = *(_OWORD *)(a3 + 9);
  v29 = *(_OWORD *)(a3 + 11);
  v30 = *(_OWORD *)(a3 + 13);
  v31 = *a3;
  v32 = *(_OWORD *)(a3 + 15);
  v33 = *(_OWORD *)(a4 + 1);
  v34 = *(_OWORD *)(a4 + 3);
  v35 = *(_OWORD *)(a4 + 5);
  v36 = *(_OWORD *)(a4 + 7);
  v37 = *(_OWORD *)(a4 + 9);
  v38 = *(_OWORD *)(a4 + 11);
  v39 = *(_OWORD *)(a4 + 13);
  v40 = *a4;
  v41 = *(_OWORD *)(a4 + 15);
  v79 = v23;
  v80 = v10;
  v81 = v11;
  v82 = v12;
  v83 = v13;
  v84 = *(uint64_t *)((char *)a2 + 164);
  v57 = v31;
  *(_QWORD *)&v23 = *(uint64_t *)((char *)a4 + 164);
  v58 = v24;
  *(_QWORD *)&v24 = *(uint64_t *)((char *)a3 + 164);
  v59 = v25;
  v60 = v26;
  v61 = v27;
  v62 = v28;
  v63 = v29;
  v64 = v30;
  v65 = v32;
  v66 = v14;
  v67 = v15;
  v68 = v16;
  v69 = v17;
  v70 = v24;
  v43 = v40;
  v44 = v33;
  v45 = v34;
  v46 = v35;
  v47 = v36;
  v48 = v37;
  v49 = v38;
  v50 = v39;
  v51 = v41;
  v52 = v18;
  v53 = v19;
  v54 = v20;
  v55 = v21;
  v56 = v23;
  return v22(&v85, &v71, &v57, &v43);
}

uint64_t dispatch thunk of FusableTernaryInputLayerParametersWrapper.filterType.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

uint64_t __swift_memcpy12_8(uint64_t result, uint64_t *a2)
{
  uint64_t v2;

  v2 = *a2;
  *(_DWORD *)(result + 8) = *((_DWORD *)a2 + 2);
  *(_QWORD *)result = v2;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.FusedTernaryArithmeticParameters(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  BOOL v4;
  int v5;

  if (!a2)
    return 0;
  if (a2 >= 0xFE && *(_BYTE *)(a1 + 12))
    return (*(_DWORD *)a1 + 254);
  v3 = *(unsigned __int8 *)(a1 + 8);
  v4 = v3 >= 3;
  v5 = v3 - 3;
  if (!v4)
    v5 = -1;
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.FusedTernaryArithmeticParameters(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFD)
  {
    *(_DWORD *)(result + 8) = 0;
    *(_QWORD *)result = a2 - 254;
    if (a3 >= 0xFE)
      *(_BYTE *)(result + 12) = 1;
  }
  else
  {
    if (a3 >= 0xFE)
      *(_BYTE *)(result + 12) = 0;
    if (a2)
      *(_BYTE *)(result + 8) = a2 + 2;
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.FusedTernaryArithmeticParameters()
{
  return &type metadata for BNNS.FusedTernaryArithmeticParameters;
}

uint64_t sub_1CAAFF7D0()
{
  return swift_deallocObject();
}

uint64_t BNNS.NormalizationLayer.__allocating_init(type:input:output:beta:gamma:momentum:epsilon:activation:filterParameters:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, int (__cdecl *a9)(void **, size_t, size_t), void (__cdecl *a10)(void *))
{
  const void *v10;
  uint32_t v11;
  size_t v12;
  uint64_t *v13;
  uint64_t *v14;
  float v15;
  float v16;
  float v17;
  float v18;
  __int128 *v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 *v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 *v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 *v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  uint64_t v43;
  char v44;
  unsigned int v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  BNNSDataType v50;
  size_t v51;
  uint64_t v52;
  uint64_t v53;
  BNNSActivationFunction v54;
  int32_t v55;
  const int32_t *v56;
  const int32_t *v57;
  const int32_t *v58;
  size_t v59;
  BNNSFilterType v60;
  BNNSFilterType v61;
  BNNSFilterParameters *p_filter_params;
  BNNSFilterType v63;
  void *v64;
  uint64_t v65;
  uint64_t v66;
  int v68;
  int v69;
  uint32_t v70;
  size_t v71;
  uint64_t v72;
  uint64_t v73;
  void *v74;
  void *v75;
  uint64_t v76;
  size_t v77;
  size_t v78;
  size_t v79;
  size_t v80;
  size_t v81;
  size_t v82;
  size_t v83;
  size_t v84;
  size_t v85;
  size_t v86;
  size_t v87;
  size_t v88;
  size_t v89;
  size_t v90;
  size_t v91;
  size_t v92;
  void *v93;
  size_t v94;
  size_t v95;
  size_t v96;
  size_t v97;
  BNNSDataType v98;
  size_t v99;
  void *v100;
  size_t v101;
  size_t v102;
  size_t v103;
  size_t v104;
  size_t v105;
  size_t v106;
  size_t v107;
  uint64_t v108;
  size_t v109;
  size_t v110;
  size_t v111;
  size_t v112;
  BNNSFilterParameters filter_params;
  _BYTE v114[277];
  _OWORD __src[44];
  BNNSLayerParametersNormalization __dst;
  _QWORD v117[20];
  BNNSDataType v118;
  uint64_t v119;
  int v120;
  BNNSActivationFunction v121;
  uint64_t v122;
  uint64_t v123;
  int32_t v124;
  const int32_t *v125;
  const int32_t *v126;
  const int32_t *v127;
  _BYTE v128[184];
  _BYTE v129[184];
  __int128 v130;
  __int128 v131;
  __int128 v132;
  __int128 v133;
  __int128 v134;
  __int128 v135;
  __int128 v136;
  __int128 v137;
  __int128 v138;
  __int128 v139;
  __int128 v140;
  __int128 v141;
  __int128 v142;
  __int128 v143;
  __int128 v144;
  __int128 v145;
  __int128 v146;
  __int128 v147;
  __int128 v148;
  __int128 v149;
  __int128 v150;
  __int128 v151;
  __int128 v152;
  __int128 v153;
  __int128 v154;
  __int128 v155;
  __int128 v156;
  __int128 v157;
  __int128 v158;
  __int128 v159;
  __int128 v160;
  __int128 v161;
  __int128 v162;
  __int128 v163;
  __int128 v164;
  __int128 v165;
  __int128 v166;
  __int128 v167;
  __int128 v168;
  __int128 v169;
  __int128 v170;
  __int128 v171;
  __int128 v172;
  __int128 v173;
  _BYTE v174[368];
  _QWORD v175[49];

  v10 = (const void *)MEMORY[0x1E0C80A78](a1);
  v70 = v11;
  v71 = v12;
  v14 = v13;
  v16 = v15;
  v18 = v17;
  v175[46] = *MEMORY[0x1E0C80C00];
  v20 = v19[9];
  v138 = v19[8];
  v139 = v20;
  v140 = v19[10];
  v21 = v19[5];
  v134 = v19[4];
  v135 = v21;
  v22 = v19[6];
  v137 = v19[7];
  v136 = v22;
  v23 = v19[1];
  v130 = *v19;
  v131 = v23;
  v24 = v19[2];
  v133 = v19[3];
  v132 = v24;
  v26 = v25[9];
  v149 = v25[8];
  v150 = v26;
  v151 = v25[10];
  v27 = v25[5];
  v145 = v25[4];
  v146 = v27;
  v28 = v25[6];
  v148 = v25[7];
  v147 = v28;
  v29 = v25[1];
  v141 = *v25;
  v142 = v29;
  v30 = v25[2];
  v144 = v25[3];
  v143 = v30;
  v32 = v31[9];
  v160 = v31[8];
  v161 = v32;
  v162 = v31[10];
  v33 = v31[5];
  v156 = v31[4];
  v157 = v33;
  v34 = v31[6];
  v159 = v31[7];
  v158 = v34;
  v35 = v31[1];
  v152 = *v31;
  v153 = v35;
  v36 = v31[2];
  v155 = v31[3];
  v154 = v36;
  v38 = v37[9];
  v171 = v37[8];
  v172 = v38;
  v173 = v37[10];
  v39 = v37[5];
  v167 = v37[4];
  v168 = v39;
  v40 = v37[6];
  v170 = v37[7];
  v169 = v40;
  v41 = v37[1];
  v163 = *v37;
  v164 = v41;
  v42 = v37[2];
  v166 = v37[3];
  v165 = v42;
  outlined init with take of BNNS.NormalizationType(v10, v174);
  v43 = *v14;
  v44 = *((_BYTE *)v14 + 8);
  outlined init with take of BNNS.NormalizationType(v174, v175);
  v45 = _s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v175);
  if (v45 >= 2)
  {
    if (v45 == 2)
    {
      _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v175);
      v91 = 0;
      v92 = 0;
      v89 = 0;
      v90 = 0;
      v87 = 0;
      v88 = 0;
      v85 = 0;
      v86 = 0;
      v83 = 0;
      v84 = 0;
      v81 = 0;
      v82 = 0;
      v79 = 0;
      v80 = 0;
      v77 = 0;
      v78 = 0;
      v74 = 0;
      v75 = 0;
      v50 = 0;
      v111 = 0;
      v112 = 0;
      v109 = 0;
      v110 = 0;
      v106 = 0;
      v107 = 0;
      v104 = 0;
      v105 = 0;
      v102 = 0;
      v103 = 0;
      v100 = 0;
      v101 = 0;
      v99 = 0;
      v96 = 0;
      v97 = 0;
      v94 = 0;
      v95 = 0;
      v93 = 0;
      v98 = 0;
      v51 = 0;
      v108 = 0;
    }
    else
    {
      v91 = 0;
      v92 = 0;
      v89 = 0;
      v90 = 0;
      v87 = 0;
      v88 = 0;
      v85 = 0;
      v86 = 0;
      v83 = 0;
      v84 = 0;
      v81 = 0;
      v82 = 0;
      v79 = 0;
      v80 = 0;
      v77 = 0;
      v78 = 0;
      v74 = 0;
      v75 = 0;
      v50 = 0;
      v111 = 0;
      v112 = 0;
      v109 = 0;
      v110 = 0;
      v106 = 0;
      v107 = 0;
      v104 = 0;
      v105 = 0;
      v102 = 0;
      v103 = 0;
      v100 = 0;
      v101 = 0;
      v99 = 0;
      v96 = 0;
      v97 = 0;
      v94 = 0;
      v95 = 0;
      v93 = 0;
      v98 = 0;
      v108 = 0;
      v51 = *(_QWORD *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v175);
    }
    v49 = 0;
    v48 = 0;
    v76 = 0;
    v72 = 0;
    v73 = 0;
  }
  else
  {
    v46 = _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v175);
    v47 = v46 + 184;
    outlined init with take of BNNSNDArrayDescriptor?(v46, (uint64_t)v128, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    outlined init with take of BNNSNDArrayDescriptor?(v47, (uint64_t)v129, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v128, (uint64_t)&__dst, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    v48 = 0;
    if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)&__dst) == 1)
    {
      v98 = 0;
      v99 = 0;
      v100 = 0;
      v93 = 0;
      v94 = 0;
      v95 = 0;
      v96 = 0;
      v97 = 0;
      v101 = 0;
      v102 = 0;
      v103 = 0;
      v104 = 0;
      v105 = 0;
      v106 = 0;
      v107 = 0;
      v109 = 0;
      v110 = 0;
      v111 = 0;
      v112 = 0;
      v108 = 0;
      v49 = 0;
      v76 = 0;
    }
    else
    {
      outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v128, (uint64_t)__src, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      v108 = *(_QWORD *)&__src[0];
      v111 = *(_QWORD *)&__src[1];
      v112 = *((_QWORD *)&__src[0] + 1);
      v109 = *(_QWORD *)&__src[2];
      v110 = *((_QWORD *)&__src[1] + 1);
      v106 = *(_QWORD *)&__src[3];
      v107 = *((_QWORD *)&__src[2] + 1);
      v104 = *(_QWORD *)&__src[4];
      v105 = *((_QWORD *)&__src[3] + 1);
      v102 = *(_QWORD *)&__src[5];
      v103 = *((_QWORD *)&__src[4] + 1);
      v99 = *(_QWORD *)&__src[6];
      v96 = *(_QWORD *)&__src[7];
      v97 = *((_QWORD *)&__src[6] + 1);
      v94 = *(_QWORD *)&__src[8];
      v95 = *((_QWORD *)&__src[7] + 1);
      v93 = (void *)*((_QWORD *)&__src[8] + 1);
      v100 = (void *)*((_QWORD *)&__src[9] + 1);
      v76 = *(_QWORD *)&__src[9];
      v101 = *((_QWORD *)&__src[5] + 1);
      v98 = __src[10];
      v49 = *(_QWORD *)((char *)&__src[10] + 4);
      LODWORD(v47) = HIDWORD(__src[10]);
    }
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v129, (uint64_t)__src, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    v69 = v47;
    if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)__src) == 1)
    {
      v50 = 0;
      v74 = 0;
      v75 = 0;
      v77 = 0;
      v78 = 0;
      v79 = 0;
      v80 = 0;
      v81 = 0;
      v82 = 0;
      v83 = 0;
      v84 = 0;
      v85 = 0;
      v86 = 0;
      v87 = 0;
      v88 = 0;
      v89 = 0;
      v90 = 0;
      v91 = 0;
      v92 = 0;
      v72 = 0;
      v73 = 0;
    }
    else
    {
      outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v129, (uint64_t)v117, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      v73 = v117[0];
      v91 = v117[2];
      v92 = v117[1];
      v89 = v117[4];
      v90 = v117[3];
      v87 = v117[6];
      v88 = v117[5];
      v85 = v117[8];
      v86 = v117[7];
      v83 = v117[10];
      v84 = v117[9];
      v81 = v117[12];
      v82 = v117[11];
      v79 = v117[14];
      v80 = v117[13];
      v77 = v117[16];
      v78 = v117[15];
      v72 = v117[18];
      v74 = (void *)v117[19];
      v75 = (void *)v117[17];
      v50 = v118;
      v48 = v119;
      v68 = v120;
    }
    v51 = 0;
  }
  *(_QWORD *)&__src[0] = v43;
  BYTE8(__src[0]) = v44;
  BNNS.ActivationFunction.bnnsActivation.getter();
  v52 = v122;
  v53 = v123;
  v54 = v121;
  v55 = v124;
  v56 = v125;
  v57 = v126;
  v58 = v127;
  outlined init with take of BNNS.NormalizationType(v174, v117);
  if (_s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v117) == 2)
    v59 = *(_QWORD *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v117);
  else
    v59 = 0;
  __src[8] = v171;
  __src[9] = v172;
  __src[4] = v167;
  __src[5] = v168;
  __src[6] = v169;
  __src[7] = v170;
  __src[0] = v163;
  __src[1] = v164;
  __src[2] = v165;
  __src[3] = v166;
  __src[18] = v159;
  __src[19] = v160;
  __src[20] = v161;
  __src[21] = v162;
  __src[13] = v154;
  __src[14] = v155;
  __src[15] = v156;
  __src[16] = v157;
  __src[17] = v158;
  __src[10] = v173;
  __src[11] = v152;
  __src[12] = v153;
  __src[30] = v149;
  __src[31] = v150;
  __src[26] = v145;
  __src[27] = v146;
  __src[29] = v148;
  __src[28] = v147;
  __src[22] = v141;
  __src[23] = v142;
  __src[25] = v144;
  __src[24] = v143;
  __src[40] = v137;
  __src[41] = v138;
  __src[42] = v139;
  __src[43] = v140;
  __src[36] = v133;
  __src[37] = v134;
  __src[38] = v135;
  __src[39] = v136;
  __src[32] = v151;
  __src[33] = v130;
  __src[34] = v131;
  __src[35] = v132;
  memcpy(&__dst, __src, 0x2C0uLL);
  __dst.moving_mean_desc.size[0] = v112;
  __dst.moving_mean_desc.size[1] = v111;
  __dst.moving_mean_desc.size[2] = v110;
  __dst.moving_mean_desc.size[3] = v109;
  __dst.moving_mean_desc.size[4] = v107;
  __dst.moving_mean_desc.size[5] = v106;
  __dst.moving_mean_desc.size[6] = v105;
  __dst.moving_mean_desc.size[7] = v104;
  __dst.moving_mean_desc.stride[0] = v103;
  __dst.moving_mean_desc.stride[1] = v102;
  __dst.moving_mean_desc.stride[2] = v101;
  __dst.moving_mean_desc.stride[3] = v99;
  __dst.moving_mean_desc.stride[4] = v97;
  __dst.moving_mean_desc.stride[5] = v96;
  __dst.moving_mean_desc.stride[6] = v95;
  __dst.moving_mean_desc.stride[7] = v94;
  __dst.moving_mean_desc.data = v93;
  *(_QWORD *)&__dst.moving_mean_desc.flags = v108;
  *(_QWORD *)&__dst.moving_mean_desc.data_type = v76;
  __dst.moving_mean_desc.table_data = v100;
  __dst.moving_mean_desc.table_data_type = v98;
  *(_QWORD *)&__dst.moving_mean_desc.data_scale = v49;
  *((_DWORD *)&__dst.moving_mean_desc.data_bias + 1) = v69;
  *(_QWORD *)&__dst.moving_variance_desc.flags = v73;
  __dst.moving_variance_desc.size[0] = v92;
  __dst.moving_variance_desc.size[1] = v91;
  __dst.moving_variance_desc.size[2] = v90;
  __dst.moving_variance_desc.size[3] = v89;
  __dst.moving_variance_desc.size[4] = v88;
  __dst.moving_variance_desc.size[5] = v87;
  __dst.moving_variance_desc.size[6] = v86;
  __dst.moving_variance_desc.size[7] = v85;
  __dst.moving_variance_desc.stride[0] = v84;
  __dst.moving_variance_desc.stride[1] = v83;
  __dst.moving_variance_desc.stride[2] = v82;
  __dst.moving_variance_desc.stride[3] = v81;
  __dst.moving_variance_desc.stride[4] = v80;
  __dst.moving_variance_desc.stride[5] = v79;
  __dst.moving_variance_desc.stride[6] = v78;
  __dst.moving_variance_desc.stride[7] = v77;
  __dst.moving_variance_desc.data = v75;
  *(_QWORD *)&__dst.moving_variance_desc.data_type = v72;
  __dst.moving_variance_desc.table_data = v74;
  __dst.moving_variance_desc.table_data_type = v50;
  *(_QWORD *)&__dst.moving_variance_desc.data_scale = v48;
  *((_DWORD *)&__dst.moving_variance_desc.data_bias + 1) = v68;
  __dst.momentum = v18;
  __dst.epsilon = v16;
  __dst.activation.function = v54;
  *(_QWORD *)&__dst.activation.alpha = v52;
  *(_QWORD *)&__dst.activation.iscale = v53;
  __dst.activation.ishift = v55;
  __dst.activation.iscale_per_channel = v56;
  __dst.activation.ioffset_per_channel = v57;
  __dst.activation.ishift_per_channel = v58;
  __dst.num_groups = v51;
  __dst.normalization_axis = v59;
  if (a9 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    outlined init with take of BNNS.NormalizationType(v174, v114);
    v60 = _s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v114) + 2;
    _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v114);
    v61 = v60;
    p_filter_params = 0;
  }
  else
  {
    filter_params.flags = v70;
    filter_params.n_threads = v71;
    filter_params.alloc_memory = a9;
    filter_params.free_memory = a10;
    outlined init with take of BNNS.NormalizationType(v174, v114);
    v63 = _s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v114) + 2;
    _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v114);
    p_filter_params = &filter_params;
    v61 = v63;
  }
  v64 = BNNSFilterCreateLayerNormalization(v61, &__dst, p_filter_params);
  type metadata accessor for BNNS.NormalizationLayer();
  v65 = swift_allocObject();
  v66 = v65;
  if (v64)
  {
    *(_QWORD *)(v65 + 16) = v64;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v66;
}

uint64_t BNNS.NormalizationLayer.apply(batchSize:input:output:for:)(size_t a1, uint64_t a2, uint64_t a3, char a4)
{
  uint64_t v4;

  return specialized static BNNS.normalizationFilterApply(_:batchSize:input:output:for:)(v4, a1, a2, a3, a4 & 1);
}

uint64_t BNNS.NormalizationLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingBetaGradient:generatingGammaGradient:)(size_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, _OWORD *a5, uint64_t a6, uint64_t a7)
{
  uint64_t v7;

  return specialized static BNNS.normalizationLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingBetaGradient:generatingGammaGradient:)(v7, a1, a3, a4, a5, a6, a7);
}

uint64_t type metadata accessor for BNNS.NormalizationLayer()
{
  return objc_opt_self();
}

uint64_t BNNS.NormalizationLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.NormalizationLayer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.normalizationLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingBetaGradient:generatingGammaGradient:)(BNNSNDArrayDescriptor *a1, uint64_t a2, size_t a3, BNNSNDArrayDescriptor *a4, uint64_t a5, uint64_t a6, BNNSNDArrayDescriptor *a7, uint64_t a8, BNNSNDArrayDescriptor *beta_delta)
{
  size_t v10;
  size_t v11;
  const void *v13;
  size_t v14;
  void *v15;
  unint64_t v20;
  unint64_t v21;
  unint64_t v22;
  unint64_t v23;
  unint64_t v24;
  unint64_t v25;
  unint64_t v26;
  unint64_t v27;
  _BYTE v28[136];
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  _BYTE v37[136];
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  _BYTE v46[136];
  _BYTE v47[136];
  _BYTE v48[8];
  const void *v49;
  _BYTE v50[144];

  v15 = *(void **)(a2 + 16);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v47);
  outlined init with take of BNNS.Shape((uint64_t)v47, (uint64_t)v50);
  outlined init with take of BNNS.Shape((uint64_t)v50, (uint64_t)v46);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v50, (uint64_t)v46);
  BNNS.Shape.stride.getter();
  v14 = specialized static BNNS.calculateBatchStride(size:stride:)(v38, v39, v40, v41, v42, v43, v44, v45, v38, v39, v40, v41, v42, v43, v44, v45);
  outlined init with take of BNNSNDArrayDescriptor?(a6 + 136, (uint64_t)v48, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v48, (uint64_t)&v49, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v13 = v49;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v37);
  outlined init with take of BNNS.Shape((uint64_t)v37, (uint64_t)&v38);
  outlined init with take of BNNS.Shape((uint64_t)&v38, (uint64_t)v46);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)&v38, (uint64_t)v46);
  BNNS.Shape.stride.getter();
  v10 = specialized static BNNS.calculateBatchStride(size:stride:)(v29, v30, v31, v32, v33, v34, v35, v36, v29, v30, v31, v32, v33, v34, v35, v36);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v29);
  outlined init with take of BNNS.Shape((uint64_t)&v29, (uint64_t)v46);
  outlined init with take of BNNS.Shape((uint64_t)v46, (uint64_t)v28);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v46, (uint64_t)v28);
  BNNS.Shape.stride.getter();
  v11 = specialized static BNNS.calculateBatchStride(size:stride:)(v20, v21, v22, v23, v24, v25, v26, v27, v20, v21, v22, v23, v24, v25, v26, v27);
  return BNNSNormalizationFilterApplyBackwardBatch(v15, a3, a4, v14, v13, v10, a7, v11, beta_delta, a1);
}

uint64_t specialized static BNNS.normalizationFilterApply(_:batchSize:input:output:for:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4, char a5)
{
  uint64_t result;
  size_t v6;
  _BYTE *v7;
  void *v8;
  size_t v9;
  void *in;
  void *v12;
  unint64_t v14;
  unint64_t v15;
  unint64_t v16;
  unint64_t v17;
  unint64_t v18;
  unint64_t v19;
  unint64_t v20;
  unint64_t v21;
  _BYTE v22[136];
  unint64_t v23;
  unint64_t v24;
  unint64_t v25;
  unint64_t v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  _BYTE v31[136];
  _BYTE v32[136];
  _BYTE v33[136];
  _BYTE v34[8];
  _BYTE v35[8];
  void *v36;
  void *v37;
  uint64_t v38;

  v38 = a4;
  v12 = *(void **)(a1 + 16);
  outlined init with take of BNNSNDArrayDescriptor?(a3 + 136, (uint64_t)v35, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  result = outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v35, (uint64_t)&v36, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  in = v36;
  if (!v36)
  {
    __break(1u);
    goto LABEL_7;
  }
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v32);
  outlined init with take of BNNS.Shape((uint64_t)v32, (uint64_t)v33);
  outlined init with take of BNNS.Shape((uint64_t)v33, (uint64_t)v31);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v33, (uint64_t)v31);
  BNNS.Shape.stride.getter();
  v9 = specialized static BNNS.calculateBatchStride(size:stride:)(v23, v24, v25, v26, v27, v28, v29, v30, v23, v24, v25, v26, v27, v28, v29, v30);
  outlined init with take of BNNSNDArrayDescriptor?(v38 + 136, (uint64_t)v34, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  result = outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v34, (uint64_t)&v37, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v8 = v37;
  if (!v37)
  {
LABEL_7:
    __break(1u);
    return result;
  }
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v23);
  outlined init with take of BNNS.Shape((uint64_t)&v23, (uint64_t)v31);
  outlined init with take of BNNS.Shape((uint64_t)v31, (uint64_t)v22);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v31, (uint64_t)v22);
  BNNS.Shape.stride.getter();
  v6 = specialized static BNNS.calculateBatchStride(size:stride:)(v14, v15, v16, v17, v18, v19, v20, v21, v14, v15, v16, v17, v18, v19, v20, v21);
  result = BNNSNormalizationFilterApplyBatch(v12, a2, in, v9, v8, v6, (a5 & 1) == 0);
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v7 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t specialized static BNNS.normalizationLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingBetaGradient:generatingGammaGradient:)(uint64_t a1, size_t a2, uint64_t a3, _OWORD *a4, _OWORD *a5, uint64_t a6, uint64_t a7)
{
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  BNNSNDArrayDescriptor *v23;
  uint64_t result;
  _BYTE *v25;
  BNNSNDArrayDescriptor *v26;
  _OWORD v27[11];
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  BNNSNDArrayDescriptor v39;
  BNNSNDArrayDescriptor v40;
  _OWORD v41[11];
  _OWORD v42[11];
  _OWORD v43[11];
  uint64_t v44;

  v44 = *MEMORY[0x1E0C80C00];
  v13 = a5[9];
  *(_OWORD *)&v40.stride[7] = a5[8];
  *(_OWORD *)&v40.data_type = v13;
  *(_OWORD *)&v40.table_data_type = a5[10];
  v14 = a5[5];
  *(_OWORD *)&v40.size[7] = a5[4];
  *(_OWORD *)&v40.stride[1] = v14;
  v15 = a5[7];
  *(_OWORD *)&v40.stride[3] = a5[6];
  *(_OWORD *)&v40.stride[5] = v15;
  v16 = a5[1];
  *(_OWORD *)&v40.flags = *a5;
  *(_OWORD *)&v40.size[1] = v16;
  v17 = a5[3];
  *(_OWORD *)&v40.size[3] = a5[2];
  *(_OWORD *)&v40.size[5] = v17;
  v18 = a4[9];
  *(_OWORD *)&v39.stride[7] = a4[8];
  *(_OWORD *)&v39.data_type = v18;
  *(_OWORD *)&v39.table_data_type = a4[10];
  v19 = a4[5];
  *(_OWORD *)&v39.size[7] = a4[4];
  *(_OWORD *)&v39.stride[1] = v19;
  v20 = a4[7];
  *(_OWORD *)&v39.stride[3] = a4[6];
  *(_OWORD *)&v39.stride[5] = v20;
  v21 = a4[1];
  *(_OWORD *)&v39.flags = *a4;
  *(_OWORD *)&v39.size[1] = v21;
  v22 = a4[3];
  *(_OWORD *)&v39.size[3] = a4[2];
  *(_OWORD *)&v39.size[5] = v22;
  outlined init with take of BNNSNDArrayDescriptor?(a6, (uint64_t)v43, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v43) == 1)
  {
    outlined init with take of BNNSNDArrayDescriptor?(a7, (uint64_t)v42, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v42) == 1)
    {
      v26 = 0;
LABEL_6:
      v23 = 0;
      goto LABEL_9;
    }
    v36 = v42[8];
    v37 = v42[9];
    v38 = v42[10];
    v32 = v42[4];
    v33 = v42[5];
    v34 = v42[6];
    v35 = v42[7];
    v28 = v42[0];
    v29 = v42[1];
    v30 = v42[2];
    v31 = v42[3];
    v26 = 0;
    v23 = (BNNSNDArrayDescriptor *)&v28;
  }
  else
  {
    v36 = v43[8];
    v37 = v43[9];
    v38 = v43[10];
    v32 = v43[4];
    v33 = v43[5];
    v34 = v43[6];
    v35 = v43[7];
    v28 = v43[0];
    v29 = v43[1];
    v30 = v43[2];
    v31 = v43[3];
    outlined init with take of BNNSNDArrayDescriptor?(a7, (uint64_t)v41, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v41) == 1)
    {
      v26 = (BNNSNDArrayDescriptor *)&v28;
      goto LABEL_6;
    }
    v27[8] = v41[8];
    v27[9] = v41[9];
    v27[10] = v41[10];
    v27[4] = v41[4];
    v27[5] = v41[5];
    v27[6] = v41[6];
    v27[7] = v41[7];
    v27[0] = v41[0];
    v27[1] = v41[1];
    v27[2] = v41[2];
    v27[3] = v41[3];
    v26 = (BNNSNDArrayDescriptor *)&v28;
    v23 = (BNNSNDArrayDescriptor *)v27;
  }
LABEL_9:
  result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.normalizationLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingBetaGradient:generatingGammaGradient:)(v23, a1, a2, &v40, (uint64_t)a5, a3, &v39, (uint64_t)a4, v26);
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v25 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t method lookup function for BNNS.NormalizationLayer()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of BNNS.NormalizationLayer.apply(batchSize:input:output:for:)(uint64_t a1, uint64_t *a2, uint64_t *a3, char a4)
{
  uint64_t v4;
  uint64_t v5;
  int v6;
  uint64_t v7;
  int v8;
  uint64_t v9;
  int v10;
  uint64_t v11;
  int v12;
  uint64_t (*v13)(uint64_t, uint64_t *, uint64_t *, _QWORD);
  uint64_t v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  uint64_t v24;
  int v25;
  uint64_t v26;
  int v27;
  uint64_t v28;
  uint64_t v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  uint64_t v38;
  int v39;
  uint64_t v40;
  int v41;
  uint64_t v42;

  v5 = a2[17];
  v6 = *((_DWORD *)a2 + 36);
  v7 = a2[19];
  v8 = *((_DWORD *)a2 + 40);
  v9 = a3[17];
  v10 = *((_DWORD *)a3 + 36);
  v11 = a3[19];
  v12 = *((_DWORD *)a3 + 40);
  v13 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, _QWORD))(*(_QWORD *)v4 + 96);
  v29 = *a2;
  v30 = *(_OWORD *)(a2 + 1);
  v31 = *(_OWORD *)(a2 + 3);
  v32 = *(_OWORD *)(a2 + 5);
  v33 = *(_OWORD *)(a2 + 7);
  v34 = *(_OWORD *)(a2 + 9);
  v35 = *(_OWORD *)(a2 + 11);
  v36 = *(_OWORD *)(a2 + 13);
  v37 = *(_OWORD *)(a2 + 15);
  v38 = v5;
  v39 = v6;
  v40 = v7;
  v41 = v8;
  v42 = *(uint64_t *)((char *)a2 + 164);
  v15 = *a3;
  v16 = *(_OWORD *)(a3 + 1);
  v17 = *(_OWORD *)(a3 + 3);
  v18 = *(_OWORD *)(a3 + 5);
  v19 = *(_OWORD *)(a3 + 7);
  v20 = *(_OWORD *)(a3 + 9);
  v21 = *(_OWORD *)(a3 + 11);
  v22 = *(_OWORD *)(a3 + 13);
  v23 = *(_OWORD *)(a3 + 15);
  v24 = v9;
  v25 = v10;
  v26 = v11;
  v27 = v12;
  v28 = *(uint64_t *)((char *)a3 + 164);
  return v13(a1, &v29, &v15, a4 & 1);
}

uint64_t dispatch thunk of BNNS.NormalizationLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingBetaGradient:generatingGammaGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5, uint64_t a6, uint64_t a7)
{
  uint64_t v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  _OWORD v43[11];
  char v44;
  _OWORD v45[11];
  char v46;
  uint64_t v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  uint64_t v56;
  int v57;
  uint64_t v58;
  int v59;
  uint64_t v60;
  uint64_t v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  __int128 v65;
  __int128 v66;
  __int128 v67;
  __int128 v68;
  __int128 v69;
  uint64_t v70;
  int v71;
  uint64_t v72;
  int v73;
  uint64_t v74;
  uint64_t v75;
  __int128 v76;
  __int128 v77;
  __int128 v78;
  __int128 v79;
  __int128 v80;
  __int128 v81;
  __int128 v82;
  __int128 v83;
  uint64_t v84;
  int v85;
  uint64_t v86;
  int v87;
  uint64_t v88;
  uint64_t v89;
  __int128 v90;
  __int128 v91;
  __int128 v92;
  __int128 v93;
  __int128 v94;
  __int128 v95;
  __int128 v96;
  __int128 v97;
  uint64_t v98;
  int v99;
  uint64_t v100;
  int v101;
  uint64_t v102;

  v8 = *(_OWORD *)(a2 + 13);
  v95 = *(_OWORD *)(a2 + 11);
  v9 = *(_OWORD *)(a2 + 15);
  v96 = v8;
  v97 = v9;
  v102 = *(uint64_t *)((char *)a2 + 164);
  v90 = *(_OWORD *)(a2 + 1);
  v91 = *(_OWORD *)(a2 + 3);
  v92 = *(_OWORD *)(a2 + 5);
  v93 = *(_OWORD *)(a2 + 7);
  v94 = *(_OWORD *)(a2 + 9);
  v10 = *(_OWORD *)(a3 + 13);
  v81 = *(_OWORD *)(a3 + 11);
  v11 = *(_OWORD *)(a3 + 15);
  v82 = v10;
  *(_QWORD *)&v10 = *(uint64_t *)((char *)a3 + 164);
  v83 = v11;
  v88 = v10;
  v12 = *(_OWORD *)(a3 + 3);
  v76 = *(_OWORD *)(a3 + 1);
  v13 = *(_OWORD *)(a3 + 5);
  v77 = v12;
  v14 = *(_OWORD *)(a3 + 7);
  v78 = v13;
  v15 = *(_OWORD *)(a3 + 9);
  v79 = v14;
  v16 = *(_OWORD *)(a4 + 11);
  v80 = v15;
  v17 = *(_OWORD *)(a4 + 13);
  v67 = v16;
  v18 = *(_OWORD *)(a4 + 15);
  v68 = v17;
  *(_QWORD *)&v17 = *(uint64_t *)((char *)a4 + 164);
  v69 = v18;
  v74 = v17;
  v89 = *a2;
  v19 = *(_OWORD *)(a4 + 1);
  v75 = *a3;
  v61 = *a4;
  v20 = *(_OWORD *)(a4 + 3);
  v62 = v19;
  v21 = *(_OWORD *)(a4 + 5);
  v63 = v20;
  v22 = *(_OWORD *)(a4 + 7);
  v64 = v21;
  v23 = *(_OWORD *)(a4 + 9);
  v65 = v22;
  v24 = *(_OWORD *)(a5 + 1);
  v66 = v23;
  v47 = *a5;
  v25 = *(_OWORD *)(a5 + 3);
  v48 = v24;
  v26 = *(_OWORD *)(a5 + 5);
  v49 = v25;
  v27 = *(_OWORD *)(a5 + 7);
  v50 = v26;
  v28 = *(_OWORD *)(a5 + 9);
  v51 = v27;
  v29 = *(_OWORD *)(a5 + 11);
  v52 = v28;
  v30 = *(_OWORD *)(a5 + 13);
  v53 = v29;
  v31 = *(_OWORD *)(a5 + 15);
  v54 = v30;
  *(_QWORD *)&v30 = *(uint64_t *)((char *)a5 + 164);
  v55 = v31;
  v60 = v30;
  v32 = *(_OWORD *)(a6 + 16);
  v45[0] = *(_OWORD *)a6;
  v45[1] = v32;
  v33 = *(_OWORD *)(a6 + 48);
  v45[2] = *(_OWORD *)(a6 + 32);
  v45[3] = v33;
  v34 = *(_OWORD *)(a6 + 80);
  v45[4] = *(_OWORD *)(a6 + 64);
  v45[5] = v34;
  v35 = *(_OWORD *)(a6 + 112);
  v45[6] = *(_OWORD *)(a6 + 96);
  v45[7] = v35;
  v36 = *(_OWORD *)(a6 + 144);
  v45[8] = *(_OWORD *)(a6 + 128);
  v45[9] = v36;
  v45[10] = *(_OWORD *)(a6 + 160);
  v37 = *(_OWORD *)(a7 + 16);
  v43[0] = *(_OWORD *)a7;
  v43[1] = v37;
  v38 = *(_OWORD *)(a7 + 48);
  v43[2] = *(_OWORD *)(a7 + 32);
  v43[3] = v38;
  v39 = *(_OWORD *)(a7 + 80);
  v43[4] = *(_OWORD *)(a7 + 64);
  v43[5] = v39;
  v40 = *(_OWORD *)(a7 + 112);
  v43[6] = *(_OWORD *)(a7 + 96);
  v43[7] = v40;
  v41 = *(_OWORD *)(a7 + 144);
  v43[8] = *(_OWORD *)(a7 + 128);
  v43[9] = v41;
  v43[10] = *(_OWORD *)(a7 + 160);
  v98 = a2[17];
  v99 = *((_DWORD *)a2 + 36);
  v100 = a2[19];
  v101 = *((_DWORD *)a2 + 40);
  v84 = a3[17];
  v85 = *((_DWORD *)a3 + 36);
  v86 = a3[19];
  v87 = *((_DWORD *)a3 + 40);
  v70 = a4[17];
  v71 = *((_DWORD *)a4 + 36);
  v72 = a4[19];
  v73 = *((_DWORD *)a4 + 40);
  v56 = a5[17];
  v57 = *((_DWORD *)a5 + 36);
  v58 = a5[19];
  v59 = *((_DWORD *)a5 + 40);
  v46 = *(_BYTE *)(a6 + 176);
  v44 = *(_BYTE *)(a7 + 176);
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, _OWORD *, _OWORD *))(*(_QWORD *)v7 + 104))(a1, &v89, &v75, &v61, &v47, v45, v43);
}

void *__swift_memcpy361_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x169uLL);
}

uint64_t getEnumTagSinglePayload for BNNS.NormalizationType(uint64_t a1, int a2)
{
  uint64_t v2;
  int v3;

  if (!a2)
    return 0;
  if (a2 < 0 && *(_BYTE *)(a1 + 361))
    return *(_DWORD *)a1 + 0x80000000;
  v2 = *(_QWORD *)(a1 + 176) >> 1;
  if (v2 > 0x80000000)
    v3 = ~(_DWORD)v2;
  else
    v3 = -1;
  return (v3 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.NormalizationType(uint64_t result, int a2, int a3)
{
  if (a2 < 0)
  {
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_BYTE *)(result + 360) = 0;
    *(_OWORD *)(result + 344) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(_QWORD *)result = a2 ^ 0x80000000;
    if (a3 < 0)
      *(_BYTE *)(result + 361) = 1;
  }
  else
  {
    if ((a3 & 0x80000000) == 0)
    {
      if (!a2)
        return result;
LABEL_8:
      *(_OWORD *)(result + 144) = 0u;
      *(_OWORD *)(result + 160) = 0u;
      *(_OWORD *)(result + 112) = 0u;
      *(_OWORD *)(result + 128) = 0u;
      *(_OWORD *)(result + 80) = 0u;
      *(_OWORD *)(result + 96) = 0u;
      *(_OWORD *)(result + 48) = 0u;
      *(_OWORD *)(result + 64) = 0u;
      *(_OWORD *)(result + 16) = 0u;
      *(_OWORD *)(result + 32) = 0u;
      *(_OWORD *)result = 0u;
      *(_QWORD *)(result + 176) = 2 * -a2;
      *(_OWORD *)(result + 200) = 0u;
      *(_OWORD *)(result + 216) = 0u;
      *(_OWORD *)(result + 232) = 0u;
      *(_OWORD *)(result + 248) = 0u;
      *(_BYTE *)(result + 360) = 0;
      *(_OWORD *)(result + 184) = 0u;
      result += 184;
      *(_OWORD *)(result + 80) = 0u;
      *(_OWORD *)(result + 96) = 0u;
      *(_OWORD *)(result + 112) = 0u;
      *(_OWORD *)(result + 128) = 0u;
      *(_OWORD *)(result + 144) = 0u;
      *(_OWORD *)(result + 160) = 0u;
      return result;
    }
    *(_BYTE *)(result + 361) = 0;
    if (a2)
      goto LABEL_8;
  }
  return result;
}

uint64_t destructiveInjectEnumTag for BNNS.NormalizationType(uint64_t result, char a2)
{
  char v2;

  v2 = *(_BYTE *)(result + 360) & 1 | (a2 << 6);
  *(_QWORD *)(result + 176) &= 1uLL;
  *(_BYTE *)(result + 360) = v2;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.NormalizationType()
{
  return &type metadata for BNNS.NormalizationType;
}

uint64_t static vDSP.rectangularToPolar<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.rectangularToPolar<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.rectangularToPolar<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vDSP.rectangularToPolar<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:));
}

uint64_t static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:));
}

{
  return static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:));
}

uint64_t static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v10;
  uint64_t result;

  v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  result = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a6 + 8) + 16))(a4);
  if (result == v10)
  {
    MEMORY[0x1E0C80A78](result);
    return (*(uint64_t (**)(uint64_t))(a6 + 16))(a7);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t static vDSP.polarToRectangular<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.polarToRectangular<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.polarToRectangular<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:));
}

{
  return static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:));
}

uint64_t static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v12;
  uint64_t v13;
  char *v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t result;
  _QWORD v20[2];

  v20[0] = a7;
  v12 = *(_QWORD *)(a3 - 8);
  MEMORY[0x1E0C80A78](a1);
  v14 = (char *)v20 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  v17 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v15 + 8) + 16))(v16);
  (*(void (**)(char *, uint64_t, uint64_t))(v12 + 16))(v14, a1, a3);
  v18 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  result = (*(uint64_t (**)(char *, uint64_t))(v12 + 8))(v14, a3);
  if (v18 == v17)
  {
    MEMORY[0x1E0C80A78](result);
    v20[-6] = a3;
    v20[-5] = a4;
    v20[-4] = a5;
    v20[-3] = a6;
    v20[-2] = a1;
    v20[-1] = v17;
    return (*(uint64_t (**)(_QWORD))(a6 + 16))(v20[0]);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(uint64_t result, uint64_t a2, _QWORD *a3, uint64_t a4, uint64_t (*a5)(void))
{
  if (!result)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*a3)
  {
    if (a4 >= -1)
      return a5();
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:));
}

uint64_t partial apply for closure #1 in static vDSP.polarToRectangular<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:));
}

uint64_t partial apply for closure #1 in static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(a1, a2, *(_QWORD **)(v2 + 16), *(_QWORD *)(v2 + 24), MEMORY[0x1E0C8C2B8]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(a1, a2, *(_QWORD **)(v2 + 16), *(_QWORD *)(v2 + 24), MEMORY[0x1E0C8C2B0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(a1, a2, *(_QWORD **)(v2 + 16), *(_QWORD *)(v2 + 24), MEMORY[0x1E0C8C2A8]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(a1, a2, *(_QWORD **)(v2 + 16), *(_QWORD *)(v2 + 24), MEMORY[0x1E0C8C2A0]);
}

BOOL static vDSP.DFTTransformType.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void vDSP.DFTTransformType.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

Swift::Int vDSP.DFTTransformType.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t vDSP.DFT.__allocating_init(previous:count:direction:transformType:ofType:)(uint64_t a1, uint64_t a2, char *a3, char *a4)
{
  swift_allocObject();
  return vDSP.DFT.init(previous:count:direction:transformType:ofType:)(a1, a2, a3, a4);
}

uint64_t vDSP.DFT.init(previous:count:direction:transformType:ofType:)(uint64_t a1, uint64_t a2, char *a3, char *a4)
{
  uint64_t v4;
  uint64_t v5;
  char v8;
  char v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t AssociatedTypeWitness;
  uint64_t AssociatedConformanceWitness;
  uint64_t v14;
  uint64_t v15;
  char v17;
  char v18;

  v5 = v4;
  v8 = *a3;
  v9 = *a4;
  v11 = *(_QWORD *)(*(_QWORD *)v5 + 80);
  v10 = *(_QWORD *)(*(_QWORD *)v5 + 88);
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v18 = v8;
  v17 = v9;
  AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  v14 = (*(uint64_t (**)(uint64_t, uint64_t, char *, char *, uint64_t, uint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness + 16))(a1, a2, &v18, &v17, v11, v10, AssociatedTypeWitness, AssociatedConformanceWitness);
  swift_release();
  if (v14)
  {
    *(_BYTE *)(v5 + 24) = v9;
    *(_QWORD *)(v5 + 16) = v14;
  }
  else
  {
    type metadata accessor for vDSP.DFT(0, v11, v10, v15);
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v5;
}

uint64_t type metadata accessor for vDSP.DFT(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return __swift_instantiateGenericMetadata(a1, a2, a3, a4, (uint64_t)&nominal type descriptor for vDSP.DFT);
}

uint64_t vDSP.DFT.transform<A>(inputReal:inputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t result;

  (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  result = Array.init(unsafeUninitializedCapacity:initializingWith:)();
  __break(1u);
  return result;
}

uint64_t closure #1 in vDSP.DFT.transform<A>(inputReal:inputImaginary:)(uint64_t a1, _QWORD *a2, uint64_t *a3, uint64_t a4)
{
  uint64_t result;

  *a3 = Array.init(unsafeUninitializedCapacity:initializingWith:)();
  result = swift_bridgeObjectRelease();
  *a2 = a4;
  return result;
}

uint64_t partial apply for closure #1 in vDSP.DFT.transform<A>(inputReal:inputImaginary:)(uint64_t a1, _QWORD *a2)
{
  uint64_t v2;

  return closure #1 in vDSP.DFT.transform<A>(inputReal:inputImaginary:)(a1, a2, *(uint64_t **)(v2 + 32), *(_QWORD *)(v2 + 40));
}

uint64_t closure #1 in closure #1 in vDSP.DFT.transform<A>(inputReal:inputImaginary:)(uint64_t a1, _QWORD *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  uint64_t v15;
  uint64_t v16;
  uint64_t result;

  v15 = type metadata accessor for UnsafeMutableBufferPointer();
  v16 = MEMORY[0x1D1794D08](&protocol conformance descriptor for UnsafeMutableBufferPointer<A>, v15);
  result = vDSP.DFT.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a4, a5, a6, a1, a8, v15, a9, v16);
  *a2 = a7;
  return result;
}

uint64_t vDSP.DFT.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v8;
  uint64_t v9;
  uint64_t AssociatedTypeWitness;
  uint64_t v16;
  uint64_t AssociatedConformanceWitness;

  v9 = v8;
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v16 = *(_QWORD *)(v9 + 16);
  AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  return (*(uint64_t (**)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness + 24))(v16, a1, a2, a3, a4, a5, a6, a7, a8, AssociatedTypeWitness, AssociatedConformanceWitness);
}

uint64_t vDSP.DFT.deinit()
{
  uint64_t v0;
  uint64_t AssociatedTypeWitness;
  uint64_t v2;
  uint64_t AssociatedConformanceWitness;

  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v2 = *(_QWORD *)(v0 + 16);
  AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  (*(void (**)(uint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness + 32))(v2, AssociatedTypeWitness, AssociatedConformanceWitness);
  return v0;
}

uint64_t vDSP.DFT.__deallocating_deinit()
{
  vDSP.DFT.deinit();
  return swift_deallocClassInstance();
}

uint64_t static vDSP.VectorizableFloat.makeDFTSetup<A>(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, uint64_t a3, _BYTE *a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.VectorizableFloat.makeDFTSetup<A>(previous:count:direction:transformType:)(a1, a2, a3, a4, a5, a6, MEMORY[0x1E0C8BFA0], MEMORY[0x1E0C8BFB0]);
}

uint64_t static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t partial apply for closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t protocol witness for static vDSP_DFTFunctions.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:) in conformance vDSP.VectorizableFloat(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

uint64_t static vDSP.VectorizableDouble.makeDFTSetup<A>(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, uint64_t a3, _BYTE *a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.VectorizableFloat.makeDFTSetup<A>(previous:count:direction:transformType:)(a1, a2, a3, a4, a5, a6, MEMORY[0x1E0C8BFA8], MEMORY[0x1E0C8BFB8]);
}

uint64_t static vDSP.VectorizableFloat.makeDFTSetup<A>(previous:count:direction:transformType:)(uint64_t result, uint64_t a2, uint64_t a3, _BYTE *a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(uint64_t), uint64_t (*a8)(uint64_t))
{
  if ((*a4 & 1) != 0)
  {
    if (result)
      result = *(_QWORD *)(result + 16);
    if ((a2 & 0x8000000000000000) == 0)
      return a8(result);
    __break(1u);
  }
  else
  {
    if (result)
      result = *(_QWORD *)(result + 16);
    if ((a2 & 0x8000000000000000) == 0)
      return a7(result);
  }
  __break(1u);
  return result;
}

uint64_t static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  _QWORD v11[12];

  v11[2] = a6;
  v11[3] = a7;
  v11[4] = a8;
  v11[5] = a9;
  v11[6] = a3;
  v11[7] = a4;
  v11[8] = a5;
  v11[9] = a1;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t, uint64_t))(a8 + 24))(a10, v11, MEMORY[0x1E0DEE9C0] + 8, a6, a8);
}

uint64_t partial apply for closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:));
}

_QWORD *closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(_QWORD *result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, _QWORD *a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t))
{
  if (!a3)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (!a5)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (!*a7)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (*result)
    return (_QWORD *)a8(a2, a3, a5);
LABEL_9:
  __break(1u);
  return result;
}

unint64_t lazy protocol witness table accessor for type vDSP.DFTTransformType and conformance vDSP.DFTTransformType()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vDSP.DFTTransformType and conformance vDSP.DFTTransformType;
  if (!lazy protocol witness table cache variable for type vDSP.DFTTransformType and conformance vDSP.DFTTransformType)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vDSP.DFTTransformType, &type metadata for vDSP.DFTTransformType);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.DFTTransformType and conformance vDSP.DFTTransformType);
  }
  return result;
}

_UNKNOWN **associated type witness table accessor for vDSP_FloatingPointDiscreteFourierTransformable.DFTFunctions : vDSP_DFTFunctions in Float()
{
  return &protocol witness table for vDSP.VectorizableFloat;
}

unint64_t instantiation function for generic protocol witness table for Float(uint64_t a1)
{
  unint64_t result;

  result = lazy protocol witness table accessor for type Float and conformance Float();
  *(_QWORD *)(a1 + 8) = result;
  return result;
}

_UNKNOWN **associated type witness table accessor for vDSP_FloatingPointDiscreteFourierTransformable.DFTFunctions : vDSP_DFTFunctions in Double()
{
  return &protocol witness table for vDSP.VectorizableDouble;
}

unint64_t instantiation function for generic protocol witness table for Double(uint64_t a1)
{
  unint64_t result;

  result = lazy protocol witness table accessor for type Double and conformance Double();
  *(_QWORD *)(a1 + 8) = result;
  return result;
}

unint64_t lazy protocol witness table accessor for type Double and conformance Double()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type Double and conformance Double;
  if (!lazy protocol witness table cache variable for type Double and conformance Double)
  {
    result = MEMORY[0x1D1794D08](MEMORY[0x1E0DEB078], MEMORY[0x1E0DEB070]);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Double and conformance Double);
  }
  return result;
}

uint64_t protocol witness for static vDSP_DFTFunctions.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:) in conformance vDSP.VectorizableDouble(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

uint64_t storeEnumTagSinglePayload for vDSP.DFTTransformType(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 1 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 1) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFF)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFE)
    return ((uint64_t (*)(void))((char *)&loc_1CAB01FE0 + 4 * byte_1CAB60A35[v4]))();
  *a1 = a2 + 1;
  return ((uint64_t (*)(void))((char *)sub_1CAB02014 + 4 * byte_1CAB60A30[v4]))();
}

uint64_t sub_1CAB02014(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB0201C(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB02024);
  return result;
}

uint64_t sub_1CAB02030(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB02038);
  *(_BYTE *)result = a2 + 1;
  return result;
}

uint64_t sub_1CAB0203C(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB02044(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vDSP.DFTTransformType()
{
  return &type metadata for vDSP.DFTTransformType;
}

uint64_t type metadata completion function for vDSP.DFT()
{
  return swift_initClassMetadata2();
}

uint64_t method lookup function for vDSP.DFT()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of vDSP.DFT.__allocating_init(previous:count:direction:transformType:ofType:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(v0 + 112))();
}

uint64_t dispatch thunk of vDSP.DFT.transform<A>(inputReal:inputImaginary:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 120))();
}

uint64_t dispatch thunk of vDSP.DFT.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 128))();
}

uint64_t dispatch thunk of static vDSP_DFTFunctions.makeDFTSetup<A>(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  return (*(uint64_t (**)(void))(a8 + 16))();
}

uint64_t dispatch thunk of static vDSP_DFTFunctions.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  return (*(uint64_t (**)(void))(a11 + 24))();
}

uint64_t dispatch thunk of static vDSP_DFTFunctions.destroySetup(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 32))();
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:));
}

_QWORD *partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(_QWORD *a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, MEMORY[0x1E0C8BF68]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  __int128 v3;
  uint64_t v4;
  uint64_t v5;
  _QWORD v7[3];
  __int128 v8;
  __int128 v9;
  uint64_t v10;

  v10 = a1;
  v3 = *(_OWORD *)(v2 + 80);
  v8 = *(_OWORD *)(v2 + 64);
  v4 = *(_QWORD *)(v2 + 24);
  v5 = *(_QWORD *)(v2 + 40);
  v7[2] = *(_QWORD *)(v2 + 56);
  v9 = v3;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v5 + 16))(a2, v7, MEMORY[0x1E0DEE9C0] + 8, v4);
}

_QWORD *partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(_QWORD *a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, MEMORY[0x1E0C8BF60]);
}

_QWORD *partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(_QWORD *a1, uint64_t (*a2)(uint64_t, uint64_t, uint64_t))
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), *(_QWORD *)(v2 + 48), *(_QWORD **)(v2 + 56), a2);
}

uint64_t partial apply for closure #1 in closure #1 in vDSP.DFT.transform<A>(inputReal:inputImaginary:)(uint64_t a1, _QWORD *a2)
{
  uint64_t *v2;

  return closure #1 in closure #1 in vDSP.DFT.transform<A>(inputReal:inputImaginary:)(a1, a2, v2[4], v2[5], v2[6], v2[7], v2[8], v2[2], v2[3]);
}

BOOL static vDSP.IntegrationRule.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void vDSP.IntegrationRule.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

Swift::Int vDSP.IntegrationRule.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t static vDSP.integrate<A>(_:using:stepSize:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4;

  v4 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v4, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.integrate<A>(_:using:stepSize:));
}

{
  uint64_t v4;

  v4 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v4, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.integrate<A>(_:using:stepSize:));
}

uint64_t closure #1 in static vDSP.integrate<A>(_:using:stepSize:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v12;
  uint64_t v13;
  uint64_t result;

  v12 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v13 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.integrate<A, B>(_:using:stepSize:result:)(a3, a4, a1, a5, v12, a6, v13);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

{
  uint64_t v12;
  uint64_t v13;
  uint64_t result;

  v12 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v13 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.integrate<A, B>(_:using:stepSize:result:)(a3, a4, a1, a5, v12, a6, v13);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.integrate<A>(_:using:stepSize:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.integrate<A>(_:using:stepSize:)(a1, a2, v2[4], v2[5], v2[2], v2[3]);
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.integrate<A>(_:using:stepSize:)(a1, a2, v2[4], v2[5], v2[2], v2[3]);
}

uint64_t static vDSP.integrate<A, B>(_:using:stepSize:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v9;
  uint64_t result;
  uint64_t v11;

  v9 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  result = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a7 + 8) + 16))(a5);
  if (result >= v9)
    v11 = v9;
  else
    v11 = result;
  if (v11 < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1E0C80A78](result);
    return (*(uint64_t (**)(_QWORD))(a7 + 16))(partial apply for closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:));
  }
  return result;
}

{
  uint64_t v9;
  uint64_t result;
  uint64_t v11;

  v9 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  result = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a7 + 8) + 16))(a5);
  if (result >= v9)
    v11 = v9;
  else
    v11 = result;
  if (v11 < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1E0C80A78](result);
    return (*(uint64_t (**)(_QWORD))(a7 + 16))(partial apply for closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:));
  }
  return result;
}

void closure #1 in closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:)(const float *a1, int a2, char a3, float **a4, vDSP_Length __N, float a6)
{
  float *v6;
  float *v7;
  float *v8;
  float v9;
  float v10;
  float v11;

  if (!a3)
  {
    if (a1)
    {
      v9 = a6;
      v7 = *a4;
      if (v7)
      {
        vDSP_vrsum(a1, 1, &v9, v7, 1, __N);
        return;
      }
      goto LABEL_16;
    }
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (a3 == 1)
  {
    if (a1)
    {
      v10 = a6;
      v6 = *a4;
      if (v6)
      {
        vDSP_vsimps(a1, 1, &v10, v6, 1, __N);
        return;
      }
    }
    else
    {
      __break(1u);
    }
    __break(1u);
    goto LABEL_15;
  }
  if (!a1)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v11 = a6;
  v8 = *a4;
  if (!v8)
  {
LABEL_18:
    __break(1u);
    return;
  }
  vDSP_vtrapz(a1, 1, &v11, v8, 1, __N);
}

void closure #1 in closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:)(const double *a1, int a2, char a3, double **a4, vDSP_Length __N, double a6)
{
  double *v6;
  double *v7;
  double *v8;
  double v9;
  double v10;
  double v11;

  if (!a3)
  {
    if (a1)
    {
      v9 = a6;
      v7 = *a4;
      if (v7)
      {
        vDSP_vrsumD(a1, 1, &v9, v7, 1, __N);
        return;
      }
      goto LABEL_16;
    }
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (a3 == 1)
  {
    if (a1)
    {
      v10 = a6;
      v6 = *a4;
      if (v6)
      {
        vDSP_vsimpsD(a1, 1, &v10, v6, 1, __N);
        return;
      }
    }
    else
    {
      __break(1u);
    }
    __break(1u);
    goto LABEL_15;
  }
  if (!a1)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v11 = a6;
  v8 = *a4;
  if (!v8)
  {
LABEL_18:
    __break(1u);
    return;
  }
  vDSP_vtrapzD(a1, 1, &v11, v8, 1, __N);
}

uint64_t partial apply for closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:)(uint64_t a1)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  int v4;
  uint64_t v5;
  _BYTE v7[20];
  int v8;
  uint64_t v9;
  uint64_t v10;

  v2 = *(_QWORD *)(v1 + 16);
  v3 = *(_QWORD *)(v1 + 32);
  v4 = *(_DWORD *)(v1 + 60);
  v5 = *(_QWORD *)(v1 + 64);
  v7[16] = *(_BYTE *)(v1 + 56);
  v8 = v4;
  v9 = a1;
  v10 = v5;
  return (*(uint64_t (**)(void (*)(const float *, int), _BYTE *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:), v7, MEMORY[0x1E0DEE9C0] + 8, v2);
}

{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  _BYTE v6[24];
  uint64_t v7;
  uint64_t v8;

  v2 = *(_QWORD *)(v1 + 16);
  v3 = *(_QWORD *)(v1 + 32);
  v4 = *(_QWORD *)(v1 + 64);
  v6[16] = *(_BYTE *)(v1 + 56);
  v7 = v4;
  v8 = a1;
  return (*(uint64_t (**)(void (*)(const double *, int), _BYTE *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:), v6, MEMORY[0x1E0DEE9C0] + 8, v2);
}

unint64_t lazy protocol witness table accessor for type vDSP.IntegrationRule and conformance vDSP.IntegrationRule()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vDSP.IntegrationRule and conformance vDSP.IntegrationRule;
  if (!lazy protocol witness table cache variable for type vDSP.IntegrationRule and conformance vDSP.IntegrationRule)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vDSP.IntegrationRule, &type metadata for vDSP.IntegrationRule);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.IntegrationRule and conformance vDSP.IntegrationRule);
  }
  return result;
}

uint64_t getEnumTagSinglePayload for vDSP.IntegrationRule(unsigned __int8 *a1, unsigned int a2)
{
  int v2;
  int v3;
  int v4;
  unsigned int v6;
  BOOL v7;
  int v8;

  if (!a2)
    return 0;
  if (a2 < 0xFE)
    goto LABEL_17;
  if (a2 + 2 >= 0xFFFF00)
    v2 = 4;
  else
    v2 = 2;
  if ((a2 + 2) >> 8 < 0xFF)
    v3 = 1;
  else
    v3 = v2;
  if (v3 == 4)
  {
    v4 = *(_DWORD *)(a1 + 1);
    if (v4)
      return (*a1 | (v4 << 8)) - 2;
  }
  else
  {
    if (v3 == 2)
    {
      v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1))
        goto LABEL_17;
      return (*a1 | (v4 << 8)) - 2;
    }
    v4 = a1[1];
    if (a1[1])
      return (*a1 | (v4 << 8)) - 2;
  }
LABEL_17:
  v6 = *a1;
  v7 = v6 >= 3;
  v8 = v6 - 3;
  if (!v7)
    v8 = -1;
  return (v8 + 1);
}

uint64_t storeEnumTagSinglePayload for vDSP.IntegrationRule(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 2 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 2) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFE)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFD)
    return ((uint64_t (*)(void))((char *)&loc_1CAB02AD8 + 4 * byte_1CAB60BD1[v4]))();
  *a1 = a2 + 2;
  return ((uint64_t (*)(void))((char *)sub_1CAB02B0C + 4 * asc_1CAB60BCC[v4]))();
}

uint64_t sub_1CAB02B0C(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB02B14(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB02B1CLL);
  return result;
}

uint64_t sub_1CAB02B28(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB02B30);
  *(_BYTE *)result = a2 + 2;
  return result;
}

uint64_t sub_1CAB02B34(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB02B3C(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vDSP.IntegrationRule()
{
  return &type metadata for vDSP.IntegrationRule;
}

void partial apply for closure #1 in closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:)(const double *a1, int a2)
{
  uint64_t v2;

  closure #1 in closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:)(a1, a2, *(_BYTE *)(v2 + 16), *(double ***)(v2 + 32), *(_QWORD *)(v2 + 40), *(double *)(v2 + 24));
}

void partial apply for closure #1 in closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:)(const float *a1, int a2)
{
  uint64_t v2;

  closure #1 in closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:)(a1, a2, *(_BYTE *)(v2 + 16), *(float ***)(v2 + 24), *(_QWORD *)(v2 + 32), *(float *)(v2 + 20));
}

uint64_t BNNS.AdamWOptimizer.accumulatorCountMultiplier.getter()
{
  uint64_t v0;

  if (*(_BYTE *)(v0 + 52))
    return 3;
  else
    return 2;
}

float BNNS.AdamWOptimizer.learningRate.getter()
{
  uint64_t v0;

  return *(float *)v0;
}

void BNNS.AdamWOptimizer.learningRate.setter(float a1)
{
  float *v1;

  *v1 = a1;
}

float (*BNNS.AdamWOptimizer.learningRate.modify(uint64_t a1))(uint64_t a1)
{
  _DWORD *v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *v1;
  return BNNS.AdamWOptimizer.learningRate.modify;
}

float BNNS.AdamWOptimizer.learningRate.modify(uint64_t a1)
{
  float result;

  result = *(float *)(a1 + 8);
  **(float **)a1 = result;
  return result;
}

float BNNS.AdamWOptimizer.beta1.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 4);
}

void BNNS.AdamWOptimizer.beta1.setter(float a1)
{
  uint64_t v1;

  *(float *)(v1 + 4) = a1;
}

float (*BNNS.AdamWOptimizer.beta1.modify(uint64_t a1))(float *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 4);
  return BNNS.AdamWOptimizer.beta1.modify;
}

float BNNS.AdamWOptimizer.beta1.modify(float *a1)
{
  float result;

  result = a1[2];
  *(float *)(*(_QWORD *)a1 + 4) = result;
  return result;
}

float BNNS.AdamWOptimizer.beta2.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 8);
}

void BNNS.AdamWOptimizer.beta2.setter(float a1)
{
  uint64_t v1;

  *(float *)(v1 + 8) = a1;
}

float (*BNNS.AdamWOptimizer.beta2.modify(uint64_t a1))(float *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 8);
  return BNNS.AdamWOptimizer.beta2.modify;
}

float BNNS.AdamWOptimizer.beta2.modify(float *a1)
{
  float result;

  result = a1[2];
  *(float *)(*(_QWORD *)a1 + 8) = result;
  return result;
}

float BNNS.AdamWOptimizer.timeStep.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 12);
}

void BNNS.AdamWOptimizer.timeStep.setter(float a1)
{
  uint64_t v1;

  *(float *)(v1 + 12) = a1;
}

float (*BNNS.AdamWOptimizer.timeStep.modify(uint64_t a1))(float *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 12);
  return BNNS.AdamWOptimizer.timeStep.modify;
}

float BNNS.AdamWOptimizer.timeStep.modify(float *a1)
{
  float result;

  result = a1[2];
  *(float *)(*(_QWORD *)a1 + 12) = result;
  return result;
}

float BNNS.AdamWOptimizer.epsilon.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 16);
}

void BNNS.AdamWOptimizer.epsilon.setter(float a1)
{
  uint64_t v1;

  *(float *)(v1 + 16) = a1;
}

float (*BNNS.AdamWOptimizer.epsilon.modify(uint64_t a1))(float *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 16);
  return BNNS.AdamWOptimizer.epsilon.modify;
}

float BNNS.AdamWOptimizer.epsilon.modify(float *a1)
{
  float result;

  result = a1[2];
  *(float *)(*(_QWORD *)a1 + 16) = result;
  return result;
}

float BNNS.AdamWOptimizer.gradientScale.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 20);
}

void BNNS.AdamWOptimizer.gradientScale.setter(float a1)
{
  uint64_t v1;

  *(float *)(v1 + 20) = a1;
}

float (*BNNS.AdamWOptimizer.gradientScale.modify(uint64_t a1))(float *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 20);
  return BNNS.AdamWOptimizer.gradientScale.modify;
}

float BNNS.AdamWOptimizer.gradientScale.modify(float *a1)
{
  float result;

  result = a1[2];
  *(float *)(*(_QWORD *)a1 + 20) = result;
  return result;
}

float BNNS.AdamWOptimizer.weightDecay.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 24);
}

void BNNS.AdamWOptimizer.weightDecay.setter(float a1)
{
  uint64_t v1;

  *(float *)(v1 + 24) = a1;
}

float (*BNNS.AdamWOptimizer.weightDecay.modify(uint64_t a1))(float *a1)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 24);
  return BNNS.AdamWOptimizer.weightDecay.modify;
}

float BNNS.AdamWOptimizer.weightDecay.modify(float *a1)
{
  float result;

  result = a1[2];
  *(float *)(*(_QWORD *)a1 + 24) = result;
  return result;
}

void BNNS.AdamWOptimizer.gradientClipping.getter(uint64_t a1@<X8>)
{
  uint64_t v1;
  int v2;
  unsigned int v3;
  float v4;
  float v5;
  char v6;
  unint64_t v7;

  v2 = *(_DWORD *)(v1 + 32);
  v3 = *(_DWORD *)(v1 + 44);
  if (v2 == 3)
  {
    v7 = v3 | ((unint64_t)*(unsigned int *)(v1 + 48) << 32);
    v6 = 2;
    goto LABEL_9;
  }
  if (v2 == 2)
  {
    v7 = v3;
    v6 = 1;
    goto LABEL_9;
  }
  if (v2 != 1)
  {
    v7 = 0;
    v6 = 3;
    goto LABEL_9;
  }
  v5 = *(float *)(v1 + 36);
  v4 = *(float *)(v1 + 40);
  if (v5 <= v4)
  {
    v6 = 0;
    v7 = LODWORD(v5) | ((unint64_t)LODWORD(v4) << 32);
LABEL_9:
    *(_QWORD *)a1 = v7;
    *(_BYTE *)(a1 + 8) = v6;
    return;
  }
  __break(1u);
}

void key path getter for BNNS.AdamWOptimizer.gradientClipping : BNNS.AdamWOptimizer(uint64_t a1@<X8>)
{
  char v2;
  uint64_t v3;
  char v4;

  BNNS.AdamWOptimizer.gradientClipping.getter((uint64_t)&v3);
  v2 = v4;
  *(_QWORD *)a1 = v3;
  *(_BYTE *)(a1 + 8) = v2;
}

uint64_t BNNS.AdamWOptimizer.gradientClipping.setter(uint64_t a1)
{
  return ((uint64_t (*)(void))((char *)sub_1CAB034D0 + 4 * byte_1CAB60C60[*(unsigned __int8 *)(a1 + 8)]))();
}

void sub_1CAB034D0()
{
  uint64_t v0;
  uint64_t v1;

  *(_DWORD *)(v1 + 32) = 1;
  *(_DWORD *)(v1 + 36) = v0;
  *(_QWORD *)(v1 + 40) = HIDWORD(v0);
  *(_DWORD *)(v1 + 48) = 0;
}

uint64_t (*BNNS.AdamWOptimizer.gradientClipping.modify(uint64_t (*result)(uint64_t a1, char a2)))(uint64_t a1, char a2)
{
  uint64_t v1;
  int v2;
  unsigned int v3;
  float v4;
  float v5;
  char v6;
  unint64_t v7;

  *((_QWORD *)result + 2) = v1;
  v2 = *(_DWORD *)(v1 + 32);
  v3 = *(_DWORD *)(v1 + 44);
  if (v2 == 3)
  {
    v7 = v3 | ((unint64_t)*(unsigned int *)(v1 + 48) << 32);
    v6 = 2;
    goto LABEL_9;
  }
  if (v2 == 2)
  {
    v7 = v3;
    v6 = 1;
    goto LABEL_9;
  }
  if (v2 != 1)
  {
    v7 = 0;
    v6 = 3;
    goto LABEL_9;
  }
  v5 = *(float *)(v1 + 36);
  v4 = *(float *)(v1 + 40);
  if (v5 <= v4)
  {
    v6 = 0;
    v7 = LODWORD(v5) | ((unint64_t)LODWORD(v4) << 32);
LABEL_9:
    *(_QWORD *)result = v7;
    *((_BYTE *)result + 8) = v6;
    return BNNS.AdamWOptimizer.gradientClipping.modify;
  }
  __break(1u);
  return result;
}

uint64_t BNNS.AdamWOptimizer.gradientClipping.modify(uint64_t a1, char a2)
{
  uint64_t v2;

  v2 = *(unsigned __int8 *)(a1 + 8);
  if ((a2 & 1) != 0)
    return ((uint64_t (*)(void))((char *)&loc_1CAB035E0 + 4 * byte_1CAB60C68[v2]))();
  else
    return ((uint64_t (*)(void))((char *)sub_1CAB03620 + 4 * byte_1CAB60C64[v2]))();
}

uint64_t sub_1CAB03620@<X0>(uint64_t result@<X0>, unint64_t a2@<X8>)
{
  unint64_t v2;
  int v3;
  uint64_t v4;

  v2 = HIDWORD(a2);
  v3 = a2;
  v4 = *(_QWORD *)(result + 16);
  *(_DWORD *)(v4 + 32) = 1;
  *(_DWORD *)(v4 + 36) = v3;
  *(_QWORD *)(v4 + 40) = v2;
  *(_DWORD *)(v4 + 48) = 0;
  return result;
}

uint64_t sub_1CAB03638@<X0>(uint64_t result@<X0>, int a2@<W8>)
{
  _DWORD *v3;

  v3 = *(_DWORD **)(result + 16);
  v3[8] = 2;
  v3[9] = 0;
  v3[10] = 0;
  v3[11] = a2;
  v3[12] = 0;
  return result;
}

uint64_t BNNS.AdamWOptimizer.init(learningRate:beta1:beta2:timeStep:epsilon:gradientScale:weightDecay:gradientClipping:usesAMSGrad:)(uint64_t a1)
{
  return ((uint64_t (*)(void))((char *)sub_1CAB036C0 + 4 * byte_1CAB60C6C[*(unsigned __int8 *)(a1 + 8)]))();
}

void sub_1CAB036C0(char a1@<W1>, uint64_t a2@<X8>, float a3@<S0>, float a4@<S1>, float a5@<S2>, float a6@<S3>, float a7@<S4>, float a8@<S5>, float a9@<S6>)
{
  uint64_t v9;

  *(float *)a2 = a3;
  *(float *)(a2 + 4) = a4;
  *(float *)(a2 + 8) = a5;
  *(float *)(a2 + 12) = a6;
  *(float *)(a2 + 16) = a7;
  *(float *)(a2 + 20) = a8;
  *(float *)(a2 + 24) = a9;
  *(_DWORD *)(a2 + 28) = 0;
  *(_DWORD *)(a2 + 32) = 1;
  *(_DWORD *)(a2 + 36) = v9;
  *(_QWORD *)(a2 + 40) = HIDWORD(v9);
  *(_DWORD *)(a2 + 48) = 0;
  *(_BYTE *)(a2 + 52) = a1 & 1;
}

uint64_t BNNS.AdamWOptimizer.bnnsOptimizerFunction.getter()
{
  uint64_t v0;

  if (*(_BYTE *)(v0 + 52))
    return 12;
  else
    return 10;
}

uint64_t protocol witness for BNNSOptimizer.bnnsOptimizerFunction.getter in conformance BNNS.AdamWOptimizer()
{
  uint64_t v0;

  if (*(_BYTE *)(v0 + 52))
    return 12;
  else
    return 10;
}

uint64_t protocol witness for BNNSOptimizer.accumulatorCountMultiplier.getter in conformance BNNS.AdamWOptimizer()
{
  uint64_t v0;

  if (*(_BYTE *)(v0 + 52))
    return 3;
  else
    return 2;
}

uint64_t protocol witness for WithOptimizerAlgFields.withOptimizerAlgFields(body:) in conformance BNNS.AdamWOptimizer(uint64_t (*a1)(__int128 *))
{
  __int128 *v1;
  int v2;
  __int128 v4;
  uint64_t v5;
  int v6;
  uint64_t v7;
  __int128 v8;
  uint64_t v9;

  v9 = *MEMORY[0x1E0C80C00];
  v2 = *((_DWORD *)v1 + 6);
  v4 = *v1;
  v5 = *((_QWORD *)v1 + 2);
  v6 = v2;
  v7 = *(_QWORD *)((char *)v1 + 28);
  v8 = *(__int128 *)((char *)v1 + 36);
  return a1(&v4);
}

uint64_t BNNS.AdamOptimizer.init(learningRate:beta1:beta2:timeStep:epsilon:gradientScale:regularizationScale:gradientClipping:regularizationFunction:usesAMSGrad:)(uint64_t a1)
{
  return ((uint64_t (*)(void))((char *)sub_1CAB03824 + 4 * byte_1CAB60C70[*(unsigned __int8 *)(a1 + 8)]))();
}

int8x16_t sub_1CAB03824@<Q0>(uint64_t a1@<X1>, char a2@<W2>, int8x16_t *a3@<X8>, unsigned int a4@<S0>, int32x2_t a5@<D1>, unsigned int a6@<S2>, __int32 a7@<S3>, unsigned int a8@<S4>, unsigned int a9@<S5>, unsigned int a10@<S6>)
{
  unint64_t v10;
  int8x16_t v11;
  int8x16_t result;

  a5.i32[1] = a7;
  v11.i64[0] = a4;
  v11.i64[1] = a6;
  result = vorrq_s8((int8x16_t)vshll_n_s32(a5, 0x20uLL), v11);
  *a3 = result;
  a3[1].i64[0] = a8 | ((unint64_t)a9 << 32);
  a3[1].i64[1] = a10 | (unint64_t)(a1 << 32);
  a3[2].i64[0] = (v10 << 32) | 1;
  a3[2].i64[1] = HIDWORD(v10);
  a3[3].i32[0] = 0;
  a3[3].i8[4] = 1;
  a3[3].i8[5] = a2 & 1;
  return result;
}

uint64_t BNNS.AdamOptimizer.usesAMSGrad.getter()
{
  uint64_t v0;

  return *(unsigned __int8 *)(v0 + 53);
}

uint64_t BNNS.AdamOptimizer.usesAMSGrad.setter(uint64_t result)
{
  uint64_t v1;

  *(_BYTE *)(v1 + 53) = result;
  return result;
}

_BYTE *(*BNNS.AdamOptimizer.usesAMSGrad.modify(uint64_t a1))(_BYTE *result)
{
  uint64_t v1;

  *(_QWORD *)a1 = v1;
  *(_BYTE *)(a1 + 8) = *(_BYTE *)(v1 + 53);
  return BNNS.AdamOptimizer.usesAMSGrad.modify;
}

_BYTE *BNNS.AdamOptimizer.usesAMSGrad.modify(_BYTE *result)
{
  *(_BYTE *)(*(_QWORD *)result + 53) = result[8];
  return result;
}

void BNNS.AdamOptimizer.gradientClipping.getter(uint64_t a1@<X8>)
{
  uint64_t v1;
  unint64_t v2;
  unint64_t v3;
  unint64_t v4;
  unint64_t v5;
  char v6;
  BOOL v7;
  char v8;
  unint64_t v9;

  v2 = *(_QWORD *)(v1 + 32);
  if (*(_BYTE *)(v1 + 52) != 1)
  {
    if ((*(_BYTE *)(v1 + 28) & 1) != 0)
    {
      if (*(float *)&v2 > *((float *)&v2 + 1))
      {
        __break(1u);
        goto LABEL_20;
      }
      v8 = 0;
    }
    else
    {
      v2 = 0;
      v8 = 3;
    }
LABEL_18:
    *(_QWORD *)a1 = v2;
    *(_BYTE *)(a1 + 8) = v8;
    return;
  }
  v3 = *(_QWORD *)(v1 + 40);
  if ((_DWORD)v2 != 1)
  {
    v4 = HIDWORD(v3);
    v5 = v4 | ((unint64_t)*(unsigned int *)(v1 + 48) << 32);
    if ((_DWORD)v2 == 3)
    {
      v6 = 2;
    }
    else
    {
      v5 = 0;
      v6 = 3;
    }
    v7 = (_DWORD)v2 == 2;
    if ((_DWORD)v2 == 2)
      v2 = v4;
    else
      v2 = v5;
    if (v7)
      v8 = 1;
    else
      v8 = v6;
    goto LABEL_18;
  }
  v9 = HIDWORD(v2);
  if (*(float *)&v9 <= *(float *)&v3)
  {
    v8 = 0;
    v2 = v9 | (v3 << 32);
    goto LABEL_18;
  }
LABEL_20:
  __break(1u);
}

void key path getter for BNNS.AdamOptimizer.gradientClipping : BNNS.AdamOptimizer(uint64_t a1@<X8>)
{
  char v2;
  uint64_t v3;
  char v4;

  BNNS.AdamOptimizer.gradientClipping.getter((uint64_t)&v3);
  v2 = v4;
  *(_QWORD *)a1 = v3;
  *(_BYTE *)(a1 + 8) = v2;
}

uint64_t key path setter for BNNS.AdamOptimizer.gradientClipping : BNNS.AdamOptimizer(uint64_t a1)
{
  return ((uint64_t (*)(void))((char *)sub_1CAB03A40 + 4 * byte_1CAB60C74[*(unsigned __int8 *)(a1 + 8)]))();
}

void sub_1CAB03A40(uint64_t a1, uint64_t a2)
{
  unint64_t v2;
  unint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  unint64_t v7;

  v3 = HIDWORD(v2);
  v4 = v2;
  v5 = *(_QWORD *)(a2 + 24);
  if (*(_BYTE *)(a2 + 52) == 1)
  {
    v6 = (v4 << 32) | 1;
    v7 = v3;
  }
  else
  {
    v7 = *(unsigned int *)(a2 + 40);
    v5 &= 0x1FFFFFFFFuLL;
    v6 = v4 | (v3 << 32);
  }
  *(_QWORD *)(a2 + 24) = v5;
  *(_QWORD *)(a2 + 32) = v6;
  *(_QWORD *)(a2 + 40) = v7;
  *(_DWORD *)(a2 + 48) = 0;
}

uint64_t BNNS.AdamOptimizer.gradientClipping.setter(uint64_t a1)
{
  return ((uint64_t (*)(void))((char *)sub_1CAB03AF0 + 4 * byte_1CAB60C78[*(unsigned __int8 *)(a1 + 8)]))();
}

void sub_1CAB03AF0()
{
  unint64_t v0;
  uint64_t v1;
  unint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  unint64_t v6;

  v2 = HIDWORD(v0);
  v3 = v0;
  v4 = *(_QWORD *)(v1 + 24);
  if (*(_BYTE *)(v1 + 52) == 1)
  {
    v5 = (v3 << 32) | 1;
    v6 = v2;
  }
  else
  {
    v6 = *(unsigned int *)(v1 + 40);
    v4 &= 0x1FFFFFFFFuLL;
    v5 = v3 | (v2 << 32);
  }
  *(_QWORD *)(v1 + 24) = v4;
  *(_QWORD *)(v1 + 32) = v5;
  *(_QWORD *)(v1 + 40) = v6;
  *(_DWORD *)(v1 + 48) = 0;
}

uint64_t (*BNNS.AdamOptimizer.gradientClipping.modify(uint64_t a1))(uint64_t a1, char a2)
{
  uint64_t v1;

  *(_QWORD *)(a1 + 16) = v1;
  BNNS.AdamOptimizer.gradientClipping.getter(a1);
  return BNNS.AdamOptimizer.gradientClipping.modify;
}

uint64_t BNNS.AdamOptimizer.gradientClipping.modify(uint64_t a1, char a2)
{
  uint64_t v2;

  v2 = *(unsigned __int8 *)(a1 + 8);
  if ((a2 & 1) != 0)
    return ((uint64_t (*)(void))((char *)sub_1CAB03C30 + 4 * byte_1CAB60C80[v2]))();
  else
    return ((uint64_t (*)(void))((char *)sub_1CAB03C30 + 4 * byte_1CAB60C7C[v2]))();
}

uint64_t sub_1CAB03C30(uint64_t result)
{
  unint64_t v1;
  unint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  unint64_t v7;

  v2 = HIDWORD(v1);
  v3 = v1;
  v4 = *(_QWORD *)(result + 16);
  v5 = *(_QWORD *)(v4 + 24);
  if (*(_BYTE *)(v4 + 52) == 1)
  {
    v6 = (v3 << 32) | 1;
    v7 = v2;
  }
  else
  {
    v7 = *(unsigned int *)(v4 + 40);
    v5 &= 0x1FFFFFFFFuLL;
    v6 = v3 | (v2 << 32);
  }
  *(_QWORD *)(v4 + 24) = v5;
  *(_QWORD *)(v4 + 32) = v6;
  *(_QWORD *)(v4 + 40) = v7;
  *(_DWORD *)(v4 + 48) = 0;
  return result;
}

uint64_t BNNS.SGDMomentumOptimizer.init(learningRate:momentum:gradientScale:regularizationScale:gradientClipping:usesNesterovMomentum:regularizationFunction:sgdMomentumVariant:)(uint64_t a1)
{
  return ((uint64_t (*)(void))((char *)sub_1CAB03CE4 + 4 * byte_1CAB60C84[*(unsigned __int8 *)(a1 + 8)]))();
}

int8x16_t sub_1CAB03CE4@<Q0>(char a1@<W1>, uint64_t a2@<X2>, unsigned int a3@<W3>, int8x16_t *a4@<X8>, unsigned int a5@<S0>, int32x2_t a6@<D1>, unsigned int a7@<S2>, __int32 a8@<S3>)
{
  uint64_t v8;
  int8x16_t v9;
  int8x16_t result;

  a6.i32[1] = a8;
  v9.i64[0] = a5;
  v9.i64[1] = a7;
  result = vorrq_s8((int8x16_t)vshll_n_s32(a6, 0x20uLL), v9);
  *a4 = result;
  a4[1].i64[0] = a1 & 1 | (unint64_t)(a2 << 32) | 0x80000000;
  a4[1].i64[1] = a3 | 0x100000000;
  a4[2].i64[0] = v8;
  a4[2].i64[1] = 0;
  return result;
}

void BNNS.SGDMomentumOptimizer.gradientClipping.getter(uint64_t a1@<X8>)
{
  _QWORD *v1;
  unint64_t v2;
  unint64_t v3;
  uint64_t v4;
  char v5;
  unint64_t v6;
  uint64_t v7;
  char v8;
  BOOL v9;
  unint64_t v10;

  v3 = v1[2];
  v2 = v1[3];
  if ((v3 & 0x80000000) != 0)
  {
    v6 = HIDWORD(v2);
    if ((_DWORD)v6 != 1)
    {
      v7 = v1[5];
      if ((_DWORD)v6 == 3)
      {
        v8 = 2;
      }
      else
      {
        v7 = 0;
        v8 = 3;
      }
      v9 = (_DWORD)v6 == 2;
      if ((_DWORD)v6 == 2)
        v4 = v1[5];
      else
        v4 = v7;
      if (v9)
        v5 = 1;
      else
        v5 = v8;
      goto LABEL_18;
    }
    v4 = v1[4];
    if (*(float *)&v4 <= *((float *)&v4 + 1))
    {
      v5 = 0;
      goto LABEL_18;
    }
  }
  else
  {
    if ((v3 & 1) == 0)
    {
      v4 = 0;
      v5 = 3;
LABEL_18:
      *(_QWORD *)a1 = v4;
      *(_BYTE *)(a1 + 8) = v5;
      return;
    }
    v10 = HIDWORD(v3);
    if (*((float *)&v3 + 1) <= *(float *)&v2)
    {
      v5 = 0;
      v4 = v10 | (v2 << 32);
      goto LABEL_18;
    }
    __break(1u);
  }
  __break(1u);
}

void key path getter for BNNS.SGDMomentumOptimizer.gradientClipping : BNNS.SGDMomentumOptimizer(uint64_t a1@<X8>)
{
  char v2;
  uint64_t v3;
  char v4;

  BNNS.SGDMomentumOptimizer.gradientClipping.getter((uint64_t)&v3);
  v2 = v4;
  *(_QWORD *)a1 = v3;
  *(_BYTE *)(a1 + 8) = v2;
}

uint64_t key path setter for BNNS.SGDMomentumOptimizer.gradientClipping : BNNS.SGDMomentumOptimizer(uint64_t a1)
{
  return ((uint64_t (*)(void))((char *)sub_1CAB03E90 + 4 * byte_1CAB60C88[*(unsigned __int8 *)(a1 + 8)]))();
}

void sub_1CAB03E90(uint64_t a1, _QWORD *a2)
{
  unint64_t v2;
  unint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  unint64_t v7;
  uint64_t v8;

  v3 = HIDWORD(v2);
  v4 = v2;
  v5 = a2[2];
  if ((v5 & 0x80000000) != 0)
  {
    v8 = a2[3] | 0x100000000;
    v6 = v4 | (v3 << 32);
    v7 = v5 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v6 = a2[4];
    v7 = a2[2] & 1 | ((unint64_t)v2 << 32);
    v8 = a2[3] & 0x100000000 | v3;
  }
  a2[2] = v7;
  a2[3] = v8;
  a2[4] = v6;
  a2[5] = 0;
}

uint64_t BNNS.SGDMomentumOptimizer.gradientClipping.setter(uint64_t a1)
{
  return ((uint64_t (*)(void))((char *)sub_1CAB03F54 + 4 * byte_1CAB60C8C[*(unsigned __int8 *)(a1 + 8)]))();
}

void sub_1CAB03F54()
{
  unint64_t v0;
  _QWORD *v1;
  unint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  unint64_t v6;
  uint64_t v7;

  v2 = HIDWORD(v0);
  v3 = v0;
  v4 = v1[2];
  if ((v4 & 0x80000000) != 0)
  {
    v7 = v1[3] | 0x100000000;
    v5 = v3 | (v2 << 32);
    v6 = v4 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v5 = v1[4];
    v6 = v1[2] & 1 | ((unint64_t)v0 << 32);
    v7 = v1[3] & 0x100000000 | v2;
  }
  v1[2] = v6;
  v1[3] = v7;
  v1[4] = v5;
  v1[5] = 0;
}

uint64_t (*BNNS.SGDMomentumOptimizer.gradientClipping.modify(uint64_t a1))(uint64_t a1, char a2)
{
  uint64_t v1;

  *(_QWORD *)(a1 + 16) = v1;
  BNNS.SGDMomentumOptimizer.gradientClipping.getter(a1);
  return BNNS.SGDMomentumOptimizer.gradientClipping.modify;
}

uint64_t BNNS.SGDMomentumOptimizer.gradientClipping.modify(uint64_t a1, char a2)
{
  uint64_t v2;

  v2 = *(unsigned __int8 *)(a1 + 8);
  if ((a2 & 1) != 0)
    return ((uint64_t (*)(void))((char *)sub_1CAB04094 + 4 * byte_1CAB60C94[v2]))();
  else
    return ((uint64_t (*)(void))((char *)sub_1CAB04094 + 4 * byte_1CAB60C90[v2]))();
}

uint64_t sub_1CAB04094@<X0>(uint64_t result@<X0>, unint64_t a2@<X8>)
{
  unint64_t v2;
  uint64_t v3;
  _QWORD *v4;
  uint64_t v5;
  uint64_t v6;
  unint64_t v7;
  uint64_t v8;

  v2 = HIDWORD(a2);
  v3 = a2;
  v4 = *(_QWORD **)(result + 16);
  v5 = v4[2];
  if ((v5 & 0x80000000) != 0)
  {
    v8 = v4[3] | 0x100000000;
    v6 = v3 | (v2 << 32);
    v7 = v5 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    v6 = v4[4];
    v7 = v4[2] & 1 | (v3 << 32);
    v8 = v4[3] & 0x100000000 | v2;
  }
  v4[2] = v7;
  v4[3] = v8;
  v4[4] = v6;
  v4[5] = 0;
  return result;
}

uint64_t BNNS.RMSPropOptimizer.init(learningRate:alpha:epsilon:centered:momentum:gradientScale:regularizationScale:gradientClipping:regularizationFunction:)(uint64_t a1, uint64_t a2)
{
  return ((uint64_t (*)(void))((char *)sub_1CAB04158 + 4 * byte_1CAB60C98[*(unsigned __int8 *)(a2 + 8)]))();
}

unint64_t sub_1CAB04158@<X0>(char a1@<W0>, uint64_t a2@<X2>, uint64_t a3@<X8>, unsigned int a4@<S0>, unsigned int a5@<S1>, unsigned int a6@<S2>, unsigned int a7@<S3>, unsigned int a8@<S4>, unsigned int a9@<S5>)
{
  unint64_t v9;
  uint64_t v10;
  unint64_t result;

  v10 = 0x100000000;
  if ((a1 & 1) == 0)
    v10 = 0;
  result = a9 | (unint64_t)(a2 << 32);
  *(_QWORD *)a3 = a4 | ((unint64_t)a5 << 32);
  *(_QWORD *)(a3 + 8) = v10 | a6 | 0x8000000000000000;
  *(_QWORD *)(a3 + 16) = a7 | ((unint64_t)a8 << 32);
  *(_QWORD *)(a3 + 24) = result;
  *(_QWORD *)(a3 + 32) = (v9 << 32) | 1;
  *(_QWORD *)(a3 + 40) = HIDWORD(v9);
  *(_DWORD *)(a3 + 48) = 0;
  return result;
}

void BNNS.RMSPropOptimizer.gradientClipping.getter(uint64_t a1@<X8>)
{
  uint64_t v1;
  unint64_t v2;
  char v3;
  unint64_t v4;
  unint64_t v5;
  unint64_t v6;
  char v7;
  BOOL v8;
  unint64_t v9;

  v2 = *(_QWORD *)(v1 + 32);
  if ((*(_QWORD *)(v1 + 8) & 0x8000000000000000) != 0)
  {
    v4 = *(_QWORD *)(v1 + 40);
    if ((_DWORD)v2 != 1)
    {
      v5 = HIDWORD(v4);
      v6 = v5 | ((unint64_t)*(unsigned int *)(v1 + 48) << 32);
      if ((_DWORD)v2 == 3)
      {
        v7 = 2;
      }
      else
      {
        v6 = 0;
        v7 = 3;
      }
      v8 = (_DWORD)v2 == 2;
      if ((_DWORD)v2 == 2)
        v2 = v5;
      else
        v2 = v6;
      if (v8)
        v3 = 1;
      else
        v3 = v7;
      goto LABEL_18;
    }
    v9 = HIDWORD(v2);
    if (*(float *)&v9 <= *(float *)&v4)
    {
      v3 = 0;
      v2 = v9 | (v4 << 32);
      goto LABEL_18;
    }
  }
  else
  {
    if ((*(_BYTE *)(v1 + 28) & 1) == 0)
    {
      v2 = 0;
      v3 = 3;
LABEL_18:
      *(_QWORD *)a1 = v2;
      *(_BYTE *)(a1 + 8) = v3;
      return;
    }
    if (*(float *)&v2 <= *((float *)&v2 + 1))
    {
      v3 = 0;
      goto LABEL_18;
    }
    __break(1u);
  }
  __break(1u);
}

void key path getter for BNNS.RMSPropOptimizer.gradientClipping : BNNS.RMSPropOptimizer(uint64_t a1@<X8>)
{
  char v2;
  uint64_t v3;
  char v4;

  BNNS.RMSPropOptimizer.gradientClipping.getter((uint64_t)&v3);
  v2 = v4;
  *(_QWORD *)a1 = v3;
  *(_BYTE *)(a1 + 8) = v2;
}

uint64_t key path setter for BNNS.RMSPropOptimizer.gradientClipping : BNNS.RMSPropOptimizer(uint64_t a1)
{
  return ((uint64_t (*)(void))((char *)sub_1CAB04324 + 4 * byte_1CAB60C9C[*(unsigned __int8 *)(a1 + 8)]))();
}

void sub_1CAB04324(uint64_t a1, uint64_t a2)
{
  unint64_t v2;
  unint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  unint64_t v7;
  unint64_t v8;
  uint64_t v9;

  v3 = HIDWORD(v2);
  v4 = v2;
  v5 = *(_QWORD *)(a2 + 8);
  v6 = *(_QWORD *)(a2 + 24);
  if (v5 < 0)
  {
    v9 = (v4 << 32) | 1;
    v8 = v5 & 0x1FFFFFFFFLL | 0x8000000000000000;
    v7 = v3;
  }
  else
  {
    v7 = *(unsigned int *)(a2 + 40);
    v8 = v5 & 0x1FFFFFFFFLL;
    v6 &= 0x1FFFFFFFFuLL;
    v9 = v4 | (v3 << 32);
  }
  *(_QWORD *)(a2 + 8) = v8;
  *(_QWORD *)(a2 + 24) = v6;
  *(_QWORD *)(a2 + 32) = v9;
  *(_QWORD *)(a2 + 40) = v7;
  *(_DWORD *)(a2 + 48) = 0;
}

uint64_t BNNS.RMSPropOptimizer.gradientClipping.setter(uint64_t a1)
{
  return ((uint64_t (*)(void))((char *)sub_1CAB043E0 + 4 * byte_1CAB60CA0[*(unsigned __int8 *)(a1 + 8)]))();
}

void sub_1CAB043E0()
{
  unint64_t v0;
  uint64_t v1;
  unint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  unint64_t v6;
  unint64_t v7;
  uint64_t v8;

  v2 = HIDWORD(v0);
  v3 = v0;
  v4 = *(_QWORD *)(v1 + 8);
  v5 = *(_QWORD *)(v1 + 24);
  if (v4 < 0)
  {
    v8 = (v3 << 32) | 1;
    v7 = v4 & 0x1FFFFFFFFLL | 0x8000000000000000;
    v6 = v2;
  }
  else
  {
    v6 = *(unsigned int *)(v1 + 40);
    v7 = v4 & 0x1FFFFFFFFLL;
    v5 &= 0x1FFFFFFFFuLL;
    v8 = v3 | (v2 << 32);
  }
  *(_QWORD *)(v1 + 8) = v7;
  *(_QWORD *)(v1 + 24) = v5;
  *(_QWORD *)(v1 + 32) = v8;
  *(_QWORD *)(v1 + 40) = v6;
  *(_DWORD *)(v1 + 48) = 0;
}

uint64_t (*BNNS.RMSPropOptimizer.gradientClipping.modify(uint64_t a1))(uint64_t a1, char a2)
{
  uint64_t v1;

  *(_QWORD *)(a1 + 16) = v1;
  BNNS.RMSPropOptimizer.gradientClipping.getter(a1);
  return BNNS.RMSPropOptimizer.gradientClipping.modify;
}

uint64_t BNNS.RMSPropOptimizer.gradientClipping.modify(uint64_t a1, char a2)
{
  uint64_t v2;

  v2 = *(unsigned __int8 *)(a1 + 8);
  if ((a2 & 1) != 0)
    return ((uint64_t (*)(void))((char *)sub_1CAB0451C + 4 * byte_1CAB60CA8[v2]))();
  else
    return ((uint64_t (*)(void))((char *)sub_1CAB0451C + 4 * byte_1CAB60CA4[v2]))();
}

uint64_t sub_1CAB0451C(uint64_t result)
{
  unint64_t v1;
  unint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  unint64_t v7;
  unint64_t v8;
  uint64_t v9;

  v2 = HIDWORD(v1);
  v3 = v1;
  v4 = *(_QWORD *)(result + 16);
  v5 = *(_QWORD *)(v4 + 8);
  v6 = *(_QWORD *)(v4 + 24);
  if (v5 < 0)
  {
    v9 = (v3 << 32) | 1;
    v8 = v5 & 0x1FFFFFFFFLL | 0x8000000000000000;
    v7 = v2;
  }
  else
  {
    v7 = *(unsigned int *)(v4 + 40);
    v8 = v5 & 0x1FFFFFFFFLL;
    v6 &= 0x1FFFFFFFFuLL;
    v9 = v3 | (v2 << 32);
  }
  *(_QWORD *)(v4 + 8) = v8;
  *(_QWORD *)(v4 + 24) = v6;
  *(_QWORD *)(v4 + 32) = v9;
  *(_QWORD *)(v4 + 40) = v7;
  *(_DWORD *)(v4 + 48) = 0;
  return result;
}

float sub_1CAB045A8@<S0>(float *a1@<X0>, _DWORD *a2@<X8>)
{
  float result;

  result = *a1;
  *a2 = *(_DWORD *)a1;
  return result;
}

float sub_1CAB045B4(float *a1, _DWORD *a2)
{
  float result;

  result = *a1;
  *a2 = *(_DWORD *)a1;
  return result;
}

float sub_1CAB045C0@<S0>(uint64_t a1@<X0>, float *a2@<X8>)
{
  float result;

  result = *(float *)(a1 + 4);
  *a2 = result;
  return result;
}

float sub_1CAB045CC(float *a1, uint64_t a2)
{
  float result;

  result = *a1;
  *(float *)(a2 + 4) = *a1;
  return result;
}

float sub_1CAB045D8@<S0>(uint64_t a1@<X0>, float *a2@<X8>)
{
  float result;

  result = *(float *)(a1 + 8);
  *a2 = result;
  return result;
}

float sub_1CAB045E4(float *a1, uint64_t a2)
{
  float result;

  result = *a1;
  *(float *)(a2 + 8) = *a1;
  return result;
}

float sub_1CAB045F0@<S0>(uint64_t a1@<X0>, float *a2@<X8>)
{
  float result;

  result = *(float *)(a1 + 12);
  *a2 = result;
  return result;
}

float sub_1CAB045FC(float *a1, uint64_t a2)
{
  float result;

  result = *a1;
  *(float *)(a2 + 12) = *a1;
  return result;
}

float sub_1CAB04608@<S0>(uint64_t a1@<X0>, float *a2@<X8>)
{
  float result;

  result = *(float *)(a1 + 16);
  *a2 = result;
  return result;
}

float sub_1CAB04614(float *a1, uint64_t a2)
{
  float result;

  result = *a1;
  *(float *)(a2 + 16) = *a1;
  return result;
}

float sub_1CAB04620@<S0>(uint64_t a1@<X0>, float *a2@<X8>)
{
  float result;

  result = *(float *)(a1 + 20);
  *a2 = result;
  return result;
}

float sub_1CAB0462C(float *a1, uint64_t a2)
{
  float result;

  result = *a1;
  *(float *)(a2 + 20) = *a1;
  return result;
}

float sub_1CAB04638@<S0>(uint64_t a1@<X0>, float *a2@<X8>)
{
  float result;

  result = *(float *)(a1 + 24);
  *a2 = result;
  return result;
}

float sub_1CAB04644(float *a1, uint64_t a2)
{
  float result;

  result = *a1;
  *(float *)(a2 + 24) = *a1;
  return result;
}

uint64_t sub_1CAB04654(uint64_t a1)
{
  return ((uint64_t (*)(void))((char *)sub_1CAB04688 + 4 * byte_1CAB60CAC[*(unsigned __int8 *)(a1 + 8)]))();
}

void sub_1CAB04688(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  *(_DWORD *)(a2 + 32) = 1;
  *(_DWORD *)(a2 + 36) = v2;
  *(_QWORD *)(a2 + 40) = HIDWORD(v2);
  *(_DWORD *)(a2 + 48) = 0;
}

uint64_t sub_1CAB046D8@<X0>(uint64_t result@<X0>, _BYTE *a2@<X8>)
{
  *a2 = *(_BYTE *)(result + 53);
  return result;
}

_BYTE *sub_1CAB046E4(_BYTE *result, uint64_t a2)
{
  *(_BYTE *)(a2 + 53) = *result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.GradientClipping(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  int v4;

  if (!a2)
    return 0;
  if (a2 >= 0xFD && *(_BYTE *)(a1 + 9))
    return (*(_DWORD *)a1 + 253);
  v3 = *(unsigned __int8 *)(a1 + 8);
  if (v3 <= 3)
    v4 = -1;
  else
    v4 = v3 ^ 0xFF;
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.GradientClipping(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFC)
  {
    *(_BYTE *)(result + 8) = 0;
    *(_QWORD *)result = a2 - 253;
    if (a3 >= 0xFD)
      *(_BYTE *)(result + 9) = 1;
  }
  else
  {
    if (a3 >= 0xFD)
      *(_BYTE *)(result + 9) = 0;
    if (a2)
      *(_BYTE *)(result + 8) = -(char)a2;
  }
  return result;
}

uint64_t getEnumTag for BNNS.GradientClipping(uint64_t a1)
{
  if (*(unsigned __int8 *)(a1 + 8) <= 2u)
    return *(unsigned __int8 *)(a1 + 8);
  else
    return (*(_DWORD *)a1 + 3);
}

uint64_t destructiveInjectEnumTag for BNNS.GradientClipping(uint64_t result, unsigned int a2)
{
  if (a2 >= 3)
  {
    *(_QWORD *)result = a2 - 3;
    LOBYTE(a2) = 3;
  }
  *(_BYTE *)(result + 8) = a2;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.GradientClipping()
{
  return &type metadata for BNNS.GradientClipping;
}

uint64_t getEnumTagSinglePayload for BNNS.AdamWOptimizer(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  BOOL v4;
  int v5;

  if (!a2)
    return 0;
  if (a2 >= 0xFF && *(_BYTE *)(a1 + 53))
    return (*(_DWORD *)a1 + 255);
  v3 = *(unsigned __int8 *)(a1 + 52);
  v4 = v3 >= 2;
  v5 = (v3 + 2147483646) & 0x7FFFFFFF;
  if (!v4)
    v5 = -1;
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.AdamWOptimizer(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(_QWORD *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_BYTE *)(result + 52) = 0;
    *(_DWORD *)(result + 48) = 0;
    *(_QWORD *)result = a2 - 255;
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 53) = 1;
  }
  else
  {
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 53) = 0;
    if (a2)
      *(_BYTE *)(result + 52) = a2 + 1;
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.AdamWOptimizer()
{
  return &type metadata for BNNS.AdamWOptimizer;
}

uint64_t static vDSP.clip<A>(_:to:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.clip<A>(_:to:)(a1, a2, a3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.clip<A>(_:to:));
}

{
  return static vDSP.clip<A>(_:to:)(a1, a2, a3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.clip<A>(_:to:));
}

uint64_t partial apply for closure #1 in static vDSP.clip<A>(_:to:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.clip<A>(_:to:)(a1, a2, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, float))static vDSP.clip<A, B>(_:to:result:));
}

{
  return partial apply for closure #1 in static vDSP.clip<A>(_:to:)(a1, a2, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, double))static vDSP.clip<A, B>(_:to:result:));
}

uint64_t static vDSP.clip<A, B>(_:to:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7, float a8)
{
  return static vDSP.clip<A, B>(_:to:result:)(a1, a7, a8, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:));
}

uint64_t static vDSP.clip<A, B>(_:to:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7, double a8)
{
  return static vDSP.clip<A, B>(_:to:result:)(a1, a7, a8, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:));
}

uint64_t static vDSP.invertedClip<A>(_:to:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.clip<A>(_:to:)(a1, a2, a3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.invertedClip<A>(_:to:));
}

{
  return static vDSP.clip<A>(_:to:)(a1, a2, a3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.invertedClip<A>(_:to:));
}

uint64_t closure #1 in static vDSP.clip<A>(_:to:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, void (*a6)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, float), float a7, float a8)
{
  uint64_t v16;
  uint64_t v17;
  uint64_t result;

  v16 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  a6(a3, a1, a4, v16, a5, v17, a7, a8);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.invertedClip<A, B>(_:to:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7, float a8)
{
  return static vDSP.clip<A, B>(_:to:result:)(a1, a7, a8, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.invertedClip<A, B>(_:to:result:));
}

uint64_t static vDSP.clip<A, B>(_:to:result:)(uint64_t a1, float a2, float a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v24;
  uint64_t v25;
  float v26;
  float v27;
  _BYTE v28[16];
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  float *v34;
  float *v35;
  uint64_t v36;
  uint64_t v37;

  v25 = a9;
  v37 = *MEMORY[0x1E0C80C00];
  v16 = *(_QWORD *)(a5 - 8);
  MEMORY[0x1E0C80A78](a1);
  v18 = (char *)&v24 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v21 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v19 + 8) + 16))(v20);
  (*(void (**)(char *, uint64_t, uint64_t))(v16 + 16))(v18, a1, a5);
  v22 = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  (*(void (**)(char *, uint64_t))(v16 + 8))(v18, a5);
  if (v22 != v21)
    __break(1u);
  v26 = a2;
  v27 = a3;
  v29 = a5;
  v30 = a6;
  v31 = a7;
  v32 = a8;
  v33 = a1;
  v34 = &v26;
  v35 = &v27;
  v36 = v21;
  return (*(uint64_t (**)(uint64_t, _BYTE *, uint64_t, uint64_t, uint64_t))(a8 + 16))(v25, v28, MEMORY[0x1E0DEE9C0] + 8, a6, a8);
}

uint64_t closure #1 in static vDSP.clip<A>(_:to:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, void (*a6)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, double), double a7, double a8)
{
  uint64_t v16;
  uint64_t v17;
  uint64_t result;

  v16 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  a6(a3, a1, a4, v16, a5, v17, a7, a8);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.invertedClip<A, B>(_:to:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7, double a8)
{
  return static vDSP.clip<A, B>(_:to:result:)(a1, a7, a8, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.invertedClip<A, B>(_:to:result:));
}

uint64_t static vDSP.clip<A, B>(_:to:result:)(uint64_t a1, double a2, double a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v24;
  double v25;
  double v26;
  _BYTE v27[16];
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  double *v33;
  double *v34;
  uint64_t v35;
  uint64_t v36;

  v24 = a9;
  v36 = *MEMORY[0x1E0C80C00];
  v16 = *(_QWORD *)(a5 - 8);
  MEMORY[0x1E0C80A78](a1);
  v18 = (char *)&v24 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v21 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v19 + 8) + 16))(v20);
  (*(void (**)(char *, uint64_t, uint64_t))(v16 + 16))(v18, a1, a5);
  v22 = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  (*(void (**)(char *, uint64_t))(v16 + 8))(v18, a5);
  if (v22 != v21)
    __break(1u);
  v25 = a2;
  v26 = a3;
  v28 = a5;
  v29 = a6;
  v30 = a7;
  v31 = a8;
  v32 = a1;
  v33 = &v25;
  v34 = &v26;
  v35 = v21;
  return (*(uint64_t (**)(uint64_t, _BYTE *, uint64_t, uint64_t, uint64_t))(a8 + 16))(v24, v27, MEMORY[0x1E0DEE9C0] + 8, a6, a8);
}

uint64_t static vDSP.threshold<A>(_:to:with:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4;

  v4 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v4, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.threshold<A>(_:to:with:));
}

{
  uint64_t v4;

  v4 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v4, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.threshold<A>(_:to:with:));
}

uint64_t closure #1 in static vDSP.threshold<A>(_:to:with:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  uint64_t v14;
  uint64_t v15;
  uint64_t result;

  v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.threshold<A, B>(_:to:with:result:)(a3, a7, a4, a1, a5, v14, a6, v15);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t static vDSP.threshold<A, B>(_:to:with:result:)(uint64_t a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v14;
  uint64_t v15;
  char *v16;
  int *v17;
  int v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v24;
  int v25;
  float v26;
  _BYTE v27[16];
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  int v33;
  char v34;
  float *v35;
  uint64_t v36;
  uint64_t v37;

  v37 = *MEMORY[0x1E0C80C00];
  v14 = *(_QWORD *)(a5 - 8);
  MEMORY[0x1E0C80A78](a1);
  v16 = (char *)&v24 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  v18 = *v17;
  HIDWORD(v24) = *((unsigned __int8 *)v17 + 4);
  v25 = v18;
  v21 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v19 + 8) + 16))(v20);
  (*(void (**)(char *, uint64_t, uint64_t))(v14 + 16))(v16, a1, a5);
  v22 = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  (*(void (**)(char *, uint64_t))(v14 + 8))(v16, a5);
  if (v22 != v21)
    __break(1u);
  v26 = a2;
  v28 = a5;
  v29 = a6;
  v30 = a7;
  v31 = a8;
  v32 = a1;
  v33 = v25;
  v34 = BYTE4(v24);
  v35 = &v26;
  v36 = v21;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t), _BYTE *, uint64_t, uint64_t, uint64_t))(a8 + 16))(partial apply for closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:), v27, MEMORY[0x1E0DEE9C0] + 8, a6, a8);
}

void closure #1 in closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:)(const float *a1, int a2, uint64_t a3, const float *__B, float **a5, vDSP_Length __N)
{
  float *v6;
  int v7;

  if ((a3 & 0x100000000) == 0)
  {
    if (!a1)
    {
LABEL_19:
      __break(1u);
      goto LABEL_20;
    }
    v7 = a3;
    v6 = *a5;
    if (!v6)
    {
LABEL_20:
      __break(1u);
      goto LABEL_21;
    }
    if ((__N & 0x8000000000000000) == 0)
    {
      vDSP_vthrsc(a1, 1, __B, (const float *)&v7, v6, 1, __N);
      return;
    }
    __break(1u);
    goto LABEL_17;
  }
  if ((_DWORD)a3)
  {
    if (!a1)
    {
LABEL_21:
      __break(1u);
      goto LABEL_22;
    }
    if (!*a5)
    {
LABEL_22:
      __break(1u);
      goto LABEL_23;
    }
    if ((__N & 0x8000000000000000) == 0)
    {
      vDSP_vthres(a1, 1, __B, *a5, 1, __N);
      return;
    }
LABEL_17:
    __break(1u);
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!a1)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!*a5)
  {
LABEL_24:
    __break(1u);
    return;
  }
  if ((__N & 0x8000000000000000) != 0)
    goto LABEL_18;
  vDSP_vthr(a1, 1, __B, *a5, 1, __N);
}

uint64_t closure #1 in static vDSP.threshold<A>(_:to:with:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  uint64_t v14;
  uint64_t v15;
  uint64_t result;

  v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.threshold<A, B>(_:to:with:result:)(a3, a7, a4, a1, a5, v14, a6, v15);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t static vDSP.threshold<A, B>(_:to:with:result:)(uint64_t a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t *v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v23;
  uint64_t v24;
  double v25;
  _BYTE v26[16];
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  char v33;
  double *v34;
  uint64_t v35;
  uint64_t v36;

  v36 = *MEMORY[0x1E0C80C00];
  v14 = *(_QWORD *)(a5 - 8);
  MEMORY[0x1E0C80A78](a1);
  v16 = (char *)&v23 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  v24 = *v17;
  HIDWORD(v23) = *((unsigned __int8 *)v17 + 8);
  v20 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v18 + 8) + 16))(v19);
  (*(void (**)(char *, uint64_t, uint64_t))(v14 + 16))(v16, a1, a5);
  v21 = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  (*(void (**)(char *, uint64_t))(v14 + 8))(v16, a5);
  if (v21 != v20)
    __break(1u);
  v25 = a2;
  v27 = a5;
  v28 = a6;
  v29 = a7;
  v30 = a8;
  v31 = a1;
  v32 = v24;
  v33 = BYTE4(v23);
  v34 = &v25;
  v35 = v20;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t), _BYTE *, uint64_t, uint64_t, uint64_t))(a8 + 16))(partial apply for closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:), v26, MEMORY[0x1E0DEE9C0] + 8, a6, a8);
}

void closure #1 in closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:)(const double *a1, int a2, uint64_t a3, char a4, const double *__B, double **a6, vDSP_Length __N)
{
  uint64_t v7;

  if ((a4 & 1) == 0)
  {
    if (!a1)
    {
LABEL_19:
      __break(1u);
      goto LABEL_20;
    }
    v7 = a3;
    if (!*a6)
    {
LABEL_20:
      __break(1u);
      goto LABEL_21;
    }
    if ((__N & 0x8000000000000000) == 0)
    {
      vDSP_vthrscD(a1, 1, __B, (const double *)&v7, *a6, 1, __N);
      return;
    }
    __break(1u);
    goto LABEL_17;
  }
  if (a3)
  {
    if (!a1)
    {
LABEL_21:
      __break(1u);
      goto LABEL_22;
    }
    if (!*a6)
    {
LABEL_22:
      __break(1u);
      goto LABEL_23;
    }
    if ((__N & 0x8000000000000000) == 0)
    {
      vDSP_vthresD(a1, 1, __B, *a6, 1, __N);
      return;
    }
LABEL_17:
    __break(1u);
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!a1)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!*a6)
  {
LABEL_24:
    __break(1u);
    return;
  }
  if ((__N & 0x8000000000000000) != 0)
    goto LABEL_18;
  vDSP_vthrD(a1, 1, __B, *a6, 1, __N);
}

uint64_t static vDSP.limit<A>(_:limit:withOutputConstant:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.clip<A>(_:to:)(a1, a2, a3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.limit<A>(_:limit:withOutputConstant:));
}

{
  return static vDSP.clip<A>(_:to:)(a1, a2, a3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.limit<A>(_:limit:withOutputConstant:));
}

uint64_t static vDSP.clip<A>(_:to:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(_QWORD *, uint64_t *))
{
  uint64_t v5;

  v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, a4);
}

{
  uint64_t v5;

  v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, a4);
}

uint64_t closure #1 in static vDSP.limit<A>(_:limit:withOutputConstant:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, float a6, float a7)
{
  uint64_t v14;
  uint64_t v15;
  uint64_t result;

  v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a3, a6, a7, a1, a4, v14, a5, v15);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(uint64_t a1, float a2, float a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v23;
  float v24;
  float v25;
  _BYTE v26[16];
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  float *v32;
  float *v33;
  uint64_t v34;
  uint64_t v35;

  v35 = *MEMORY[0x1E0C80C00];
  v15 = *(_QWORD *)(a5 - 8);
  MEMORY[0x1E0C80A78](a1);
  v17 = (char *)&v23 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  v20 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v18 + 8) + 16))(v19);
  (*(void (**)(char *, uint64_t, uint64_t))(v15 + 16))(v17, a1, a5);
  v21 = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  (*(void (**)(char *, uint64_t))(v15 + 8))(v17, a5);
  if (v21 != v20)
    __break(1u);
  v24 = a2;
  v25 = a3;
  v27 = a5;
  v28 = a6;
  v29 = a7;
  v30 = a8;
  v31 = a1;
  v32 = &v24;
  v33 = &v25;
  v34 = v20;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t), _BYTE *, uint64_t, uint64_t, uint64_t))(a8 + 16))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:), v26, MEMORY[0x1E0DEE9C0] + 8, a6, a8);
}

uint64_t closure #1 in static vDSP.limit<A>(_:limit:withOutputConstant:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, double a6, double a7)
{
  uint64_t v14;
  uint64_t v15;
  uint64_t result;

  v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a3, a6, a7, a1, a4, v14, a5, v15);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(uint64_t a1, double a2, double a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v23;
  double v24;
  double v25;
  _BYTE v26[16];
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  double *v32;
  double *v33;
  uint64_t v34;
  uint64_t v35;

  v35 = *MEMORY[0x1E0C80C00];
  v15 = *(_QWORD *)(a5 - 8);
  MEMORY[0x1E0C80A78](a1);
  v17 = (char *)&v23 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  v20 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v18 + 8) + 16))(v19);
  (*(void (**)(char *, uint64_t, uint64_t))(v15 + 16))(v17, a1, a5);
  v21 = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  (*(void (**)(char *, uint64_t))(v15 + 8))(v17, a5);
  if (v21 != v20)
    __break(1u);
  v24 = a2;
  v25 = a3;
  v27 = a5;
  v28 = a6;
  v29 = a7;
  v30 = a8;
  v31 = a1;
  v32 = &v24;
  v33 = &v25;
  v34 = v20;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t), _BYTE *, uint64_t, uint64_t, uint64_t))(a8 + 16))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:), v26, MEMORY[0x1E0DEE9C0] + 8, a6, a8);
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, _QWORD *a5, uint64_t a6, uint64_t (*a7)(void))
{
  if (!result)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*a5)
  {
    if ((a6 & 0x8000000000000000) == 0)
      return a7();
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.invertedClip<A>(_:to:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.clip<A>(_:to:)(a1, a2, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, float))static vDSP.invertedClip<A, B>(_:to:result:));
}

{
  return partial apply for closure #1 in static vDSP.clip<A>(_:to:)(a1, a2, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, double))static vDSP.invertedClip<A, B>(_:to:result:));
}

uint64_t partial apply for closure #1 in static vDSP.clip<A>(_:to:)(uint64_t a1, uint64_t *a2, void (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, float))
{
  uint64_t v3;

  return closure #1 in static vDSP.clip<A>(_:to:)(a1, a2, *(_QWORD *)(v3 + 32), *(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24), a3, *(float *)(v3 + 40), *(float *)(v3 + 44));
}

uint64_t partial apply for closure #1 in static vDSP.clip<A>(_:to:)(uint64_t a1, uint64_t *a2, void (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, double))
{
  uint64_t v3;

  return closure #1 in static vDSP.clip<A>(_:to:)(a1, a2, *(_QWORD *)(v3 + 32), *(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24), a3, *(double *)(v3 + 40), *(double *)(v3 + 48));
}

uint64_t partial apply for closure #1 in static vDSP.threshold<A>(_:to:with:)(uint64_t a1, uint64_t *a2)
{
  uint64_t v2;

  return closure #1 in static vDSP.threshold<A>(_:to:with:)(a1, a2, *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 48), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(float *)(v2 + 40));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.threshold<A>(_:to:with:)(a1, a2, *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 48), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(double *)(v2 + 40));
}

uint64_t partial apply for closure #1 in static vDSP.limit<A>(_:limit:withOutputConstant:)(uint64_t a1, uint64_t *a2)
{
  uint64_t v2;

  return closure #1 in static vDSP.limit<A>(_:limit:withOutputConstant:)(a1, a2, *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(float *)(v2 + 40), *(float *)(v2 + 44));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.limit<A>(_:limit:withOutputConstant:)(a1, a2, *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(double *)(v2 + 40), *(double *)(v2 + 48));
}

uint64_t type metadata instantiation function for vDSP.ThresholdRule()
{
  return swift_allocateGenericValueMetadata();
}

uint64_t type metadata completion function for vDSP.ThresholdRule()
{
  uint64_t result;
  unint64_t v1;

  result = swift_checkMetadataState();
  if (v1 <= 0x3F)
  {
    swift_initEnumMetadataSinglePayload();
    return 0;
  }
  return result;
}

uint64_t *initializeBufferWithCopyOfBuffer for vDSP.ThresholdRule(uint64_t *a1, uint64_t *a2, uint64_t a3)
{
  uint64_t v5;
  uint64_t v6;
  unsigned int v7;
  size_t v8;
  unint64_t v9;
  uint64_t v10;
  unsigned int v11;
  _BOOL8 v12;
  BOOL v13;
  uint64_t v14;
  uint64_t v17;
  uint64_t v18;
  unsigned int v19;
  _BOOL8 v20;

  v5 = *(_QWORD *)(a3 + 16);
  v6 = *(_QWORD *)(v5 - 8);
  v7 = *(_DWORD *)(v6 + 84);
  v8 = *(_QWORD *)(v6 + 64);
  v9 = v8;
  if (v7 <= 1)
  {
    if (v8 <= 3)
    {
      v11 = (~(-1 << (8 * v8)) - v7 + 2) >> (8 * v8);
      if (v11 > 0xFFFE)
      {
        v10 = 4;
      }
      else
      {
        v12 = v11 != 0;
        v13 = v11 >= 0xFF;
        v10 = 2;
        if (!v13)
          v10 = v12;
      }
    }
    else
    {
      v10 = 1;
    }
    v9 = v10 + v8;
  }
  v14 = *(_DWORD *)(v6 + 80);
  if (v14 <= 7 && v9 <= 0x18 && (*(_DWORD *)(v6 + 80) & 0x100000) == 0)
  {
    if ((*(unsigned int (**)(uint64_t *, uint64_t, _QWORD))(v6 + 48))(a2, 2, *(_QWORD *)(a3 + 16)))
    {
      if (v7 <= 1)
      {
        if (v8 <= 3)
        {
          v19 = (~(-1 << (8 * v8)) - v7 + 2) >> (8 * v8);
          if (v19 > 0xFFFE)
          {
            v18 = 4;
          }
          else
          {
            v20 = v19 != 0;
            v13 = v19 >= 0xFF;
            v18 = 2;
            if (!v13)
              v18 = v20;
          }
        }
        else
        {
          v18 = 1;
        }
        v8 += v18;
      }
      memcpy(a1, a2, v8);
    }
    else
    {
      (*(void (**)(uint64_t *, uint64_t *, uint64_t))(v6 + 16))(a1, a2, v5);
      (*(void (**)(uint64_t *, _QWORD, uint64_t, uint64_t))(v6 + 56))(a1, 0, 2, v5);
    }
  }
  else
  {
    v17 = *a2;
    *a1 = *a2;
    a1 = (uint64_t *)(v17 + ((v14 + 16) & ~v14));
    swift_retain();
  }
  return a1;
}

uint64_t destroy for vDSP.ThresholdRule(uint64_t a1, uint64_t a2)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t result;

  v3 = *(_QWORD *)(a2 + 16);
  v4 = *(_QWORD *)(v3 - 8);
  result = (*(uint64_t (**)(uint64_t, uint64_t, uint64_t))(v4 + 48))(a1, 2, v3);
  if (!(_DWORD)result)
    return (*(uint64_t (**)(uint64_t, uint64_t))(v4 + 8))(a1, v3);
  return result;
}

void *initializeWithCopy for vDSP.ThresholdRule(void *a1, const void *a2, uint64_t a3)
{
  uint64_t v5;
  uint64_t v6;
  unsigned int v7;
  size_t v8;
  uint64_t v9;
  unsigned int v10;
  _BOOL8 v11;
  BOOL v12;

  v5 = *(_QWORD *)(a3 + 16);
  v6 = *(_QWORD *)(v5 - 8);
  if ((*(unsigned int (**)(const void *, uint64_t, uint64_t))(v6 + 48))(a2, 2, v5))
  {
    v7 = *(_DWORD *)(v6 + 84);
    v8 = *(_QWORD *)(v6 + 64);
    if (v7 <= 1)
    {
      if (v8 <= 3)
      {
        v10 = (~(-1 << (8 * v8)) - v7 + 2) >> (8 * v8);
        if (v10 > 0xFFFE)
        {
          v9 = 4;
        }
        else
        {
          v11 = v10 != 0;
          v12 = v10 >= 0xFF;
          v9 = 2;
          if (!v12)
            v9 = v11;
        }
      }
      else
      {
        v9 = 1;
      }
      v8 += v9;
    }
    memcpy(a1, a2, v8);
  }
  else
  {
    (*(void (**)(void *, const void *, uint64_t))(v6 + 16))(a1, a2, v5);
    (*(void (**)(void *, _QWORD, uint64_t, uint64_t))(v6 + 56))(a1, 0, 2, v5);
  }
  return a1;
}

void *assignWithCopy for vDSP.ThresholdRule(void *a1, void *a2, uint64_t a3)
{
  uint64_t v5;
  uint64_t v6;
  uint64_t (*v7)(void *, uint64_t, uint64_t);
  int v8;
  int v9;
  unsigned int v10;
  size_t v11;
  uint64_t v12;
  unsigned int v13;
  _BOOL8 v14;
  BOOL v15;

  v5 = *(_QWORD *)(a3 + 16);
  v6 = *(_QWORD *)(v5 - 8);
  v7 = *(uint64_t (**)(void *, uint64_t, uint64_t))(v6 + 48);
  v8 = v7(a1, 2, v5);
  v9 = v7(a2, 2, v5);
  if (v8)
  {
    if (v9)
    {
      v10 = *(_DWORD *)(v6 + 84);
      v11 = *(_QWORD *)(v6 + 64);
      if (v10 <= 1)
      {
        if (v11 > 3)
        {
LABEL_5:
          v12 = 1;
LABEL_16:
          v11 += v12;
          goto LABEL_17;
        }
LABEL_9:
        v13 = (~(-1 << (8 * v11)) - v10 + 2) >> (8 * v11);
        if (v13 > 0xFFFE)
        {
          v12 = 4;
        }
        else
        {
          v14 = v13 != 0;
          v15 = v13 >= 0xFF;
          v12 = 2;
          if (!v15)
            v12 = v14;
        }
        goto LABEL_16;
      }
      goto LABEL_17;
    }
    (*(void (**)(void *, void *, uint64_t))(v6 + 16))(a1, a2, v5);
    (*(void (**)(void *, _QWORD, uint64_t, uint64_t))(v6 + 56))(a1, 0, 2, v5);
  }
  else
  {
    if (v9)
    {
      (*(void (**)(void *, uint64_t))(v6 + 8))(a1, v5);
      v10 = *(_DWORD *)(v6 + 84);
      v11 = *(_QWORD *)(v6 + 64);
      if (v10 <= 1)
      {
        if (v11 > 3)
          goto LABEL_5;
        goto LABEL_9;
      }
LABEL_17:
      memcpy(a1, a2, v11);
      return a1;
    }
    (*(void (**)(void *, void *, uint64_t))(v6 + 24))(a1, a2, v5);
  }
  return a1;
}

void *initializeWithTake for vDSP.ThresholdRule(void *a1, const void *a2, uint64_t a3)
{
  uint64_t v5;
  uint64_t v6;
  unsigned int v7;
  size_t v8;
  uint64_t v9;
  unsigned int v10;
  _BOOL8 v11;
  BOOL v12;

  v5 = *(_QWORD *)(a3 + 16);
  v6 = *(_QWORD *)(v5 - 8);
  if ((*(unsigned int (**)(const void *, uint64_t, uint64_t))(v6 + 48))(a2, 2, v5))
  {
    v7 = *(_DWORD *)(v6 + 84);
    v8 = *(_QWORD *)(v6 + 64);
    if (v7 <= 1)
    {
      if (v8 <= 3)
      {
        v10 = (~(-1 << (8 * v8)) - v7 + 2) >> (8 * v8);
        if (v10 > 0xFFFE)
        {
          v9 = 4;
        }
        else
        {
          v11 = v10 != 0;
          v12 = v10 >= 0xFF;
          v9 = 2;
          if (!v12)
            v9 = v11;
        }
      }
      else
      {
        v9 = 1;
      }
      v8 += v9;
    }
    memcpy(a1, a2, v8);
  }
  else
  {
    (*(void (**)(void *, const void *, uint64_t))(v6 + 32))(a1, a2, v5);
    (*(void (**)(void *, _QWORD, uint64_t, uint64_t))(v6 + 56))(a1, 0, 2, v5);
  }
  return a1;
}

void *assignWithTake for vDSP.ThresholdRule(void *a1, void *a2, uint64_t a3)
{
  uint64_t v5;
  uint64_t v6;
  uint64_t (*v7)(void *, uint64_t, uint64_t);
  int v8;
  int v9;
  unsigned int v10;
  size_t v11;
  uint64_t v12;
  unsigned int v13;
  _BOOL8 v14;
  BOOL v15;

  v5 = *(_QWORD *)(a3 + 16);
  v6 = *(_QWORD *)(v5 - 8);
  v7 = *(uint64_t (**)(void *, uint64_t, uint64_t))(v6 + 48);
  v8 = v7(a1, 2, v5);
  v9 = v7(a2, 2, v5);
  if (v8)
  {
    if (v9)
    {
      v10 = *(_DWORD *)(v6 + 84);
      v11 = *(_QWORD *)(v6 + 64);
      if (v10 <= 1)
      {
        if (v11 > 3)
        {
LABEL_5:
          v12 = 1;
LABEL_16:
          v11 += v12;
          goto LABEL_17;
        }
LABEL_9:
        v13 = (~(-1 << (8 * v11)) - v10 + 2) >> (8 * v11);
        if (v13 > 0xFFFE)
        {
          v12 = 4;
        }
        else
        {
          v14 = v13 != 0;
          v15 = v13 >= 0xFF;
          v12 = 2;
          if (!v15)
            v12 = v14;
        }
        goto LABEL_16;
      }
      goto LABEL_17;
    }
    (*(void (**)(void *, void *, uint64_t))(v6 + 32))(a1, a2, v5);
    (*(void (**)(void *, _QWORD, uint64_t, uint64_t))(v6 + 56))(a1, 0, 2, v5);
  }
  else
  {
    if (v9)
    {
      (*(void (**)(void *, uint64_t))(v6 + 8))(a1, v5);
      v10 = *(_DWORD *)(v6 + 84);
      v11 = *(_QWORD *)(v6 + 64);
      if (v10 <= 1)
      {
        if (v11 > 3)
          goto LABEL_5;
        goto LABEL_9;
      }
LABEL_17:
      memcpy(a1, a2, v11);
      return a1;
    }
    (*(void (**)(void *, void *, uint64_t))(v6 + 40))(a1, a2, v5);
  }
  return a1;
}

uint64_t getEnumTagSinglePayload for vDSP.ThresholdRule(uint64_t a1, unsigned int a2, uint64_t a3)
{
  uint64_t v4;
  unsigned int v5;
  unsigned int v6;
  uint64_t v7;
  uint64_t v8;
  unsigned int v9;
  _BOOL8 v10;
  BOOL v11;
  int v12;
  char v13;
  int v14;
  unsigned int v15;
  int v16;
  int v17;
  unsigned int v18;

  v4 = *(_QWORD *)(*(_QWORD *)(a3 + 16) - 8);
  v5 = *(_DWORD *)(v4 + 84);
  v6 = v5 - 2;
  v7 = *(_QWORD *)(v4 + 64);
  if (v5 <= 1)
  {
    v6 = 0;
    if (v7 <= 3)
    {
      v9 = (~(-1 << (8 * v7)) - v5 + 2) >> (8 * v7);
      if (v9 > 0xFFFE)
      {
        v8 = 4;
      }
      else
      {
        v10 = v9 != 0;
        v11 = v9 >= 0xFF;
        v8 = 2;
        if (!v11)
          v8 = v10;
      }
    }
    else
    {
      v8 = 1;
    }
    v7 += v8;
  }
  if (!a2)
    return 0;
  v12 = a2 - v6;
  if (a2 <= v6)
    goto LABEL_29;
  v13 = 8 * v7;
  if (v7 <= 3)
  {
    v15 = ((v12 + ~(-1 << v13)) >> v13) + 1;
    if (HIWORD(v15))
    {
      v14 = *(_DWORD *)(a1 + v7);
      if (!v14)
        goto LABEL_29;
      goto LABEL_20;
    }
    if (v15 > 0xFF)
    {
      v14 = *(unsigned __int16 *)(a1 + v7);
      if (!*(_WORD *)(a1 + v7))
        goto LABEL_29;
      goto LABEL_20;
    }
    if (v15 < 2)
    {
LABEL_29:
      if (v6)
      {
        v18 = (*(uint64_t (**)(void))(v4 + 48))();
        if (v18 >= 3)
          return v18 - 2;
        else
          return 0;
      }
      return 0;
    }
  }
  v14 = *(unsigned __int8 *)(a1 + v7);
  if (!*(_BYTE *)(a1 + v7))
    goto LABEL_29;
LABEL_20:
  v16 = (v14 - 1) << v13;
  if (v7 > 3)
    v16 = 0;
  if ((_DWORD)v7)
  {
    if (v7 <= 3)
      v17 = v7;
    else
      v17 = 4;
    __asm { BR              X12 }
  }
  return v6 + v16 + 1;
}

void storeEnumTagSinglePayload for vDSP.ThresholdRule(_WORD *a1, unsigned int a2, unsigned int a3, uint64_t a4)
{
  uint64_t v6;
  unsigned int v7;
  unsigned int v8;
  size_t v9;
  uint64_t v10;
  unsigned int v11;
  _BOOL8 v12;
  BOOL v13;
  unsigned int v14;
  unsigned int v15;
  int v16;
  unsigned int v17;
  int v18;

  v6 = *(_QWORD *)(*(_QWORD *)(a4 + 16) - 8);
  v7 = *(_DWORD *)(v6 + 84);
  v8 = v7 - 2;
  v9 = *(_QWORD *)(v6 + 64);
  if (v7 <= 1)
  {
    v8 = 0;
    if (v9 <= 3)
    {
      v11 = (~(-1 << (8 * v9)) - v7 + 2) >> (8 * v9);
      if (v11 > 0xFFFE)
      {
        v10 = 4;
      }
      else
      {
        v12 = v11 != 0;
        v13 = v11 >= 0xFF;
        v10 = 2;
        if (!v13)
          v10 = v12;
      }
    }
    else
    {
      v10 = 1;
    }
    v9 += v10;
  }
  v13 = a3 >= v8;
  v14 = a3 - v8;
  if (v14 != 0 && v13)
  {
    if (v9 <= 3)
    {
      v17 = ((v14 + ~(-1 << (8 * v9))) >> (8 * v9)) + 1;
      if (HIWORD(v17))
      {
        v15 = 4u;
      }
      else if (v17 >= 0x100)
      {
        v15 = 2;
      }
      else
      {
        v15 = v17 > 1;
      }
    }
    else
    {
      v15 = 1u;
    }
  }
  else
  {
    v15 = 0u;
  }
  if (v8 < a2)
  {
    v16 = ~v8 + a2;
    if (v9 < 4)
    {
      if ((_DWORD)v9)
      {
        v18 = v16 & ~(-1 << (8 * v9));
        bzero(a1, v9);
        if ((_DWORD)v9 == 3)
        {
          *a1 = v18;
          *((_BYTE *)a1 + 2) = BYTE2(v18);
        }
        else if ((_DWORD)v9 == 2)
        {
          *a1 = v18;
        }
        else
        {
          *(_BYTE *)a1 = v18;
        }
      }
    }
    else
    {
      bzero(a1, v9);
      *(_DWORD *)a1 = v16;
    }
    __asm { BR              X10 }
  }
  __asm { BR              X11 }
}

uint64_t getEnumTag for vDSP.ThresholdRule(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(uint64_t, uint64_t))(*(_QWORD *)(*(_QWORD *)(a2 + 16) - 8) + 48))(a1, 2);
}

uint64_t destructiveInjectEnumTag for vDSP.ThresholdRule(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(uint64_t, uint64_t, uint64_t))(*(_QWORD *)(*(_QWORD *)(a3 + 16) - 8) + 56))(a1, a2, 2);
}

uint64_t type metadata accessor for vDSP.ThresholdRule(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return __swift_instantiateGenericMetadata(a1, a2, a3, a4, (uint64_t)&nominal type descriptor for vDSP.ThresholdRule);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C688]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), (uint64_t (*)(void))MEMORY[0x1E0C8C680]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:)(uint64_t a1)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  char v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD v8[3];
  char v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;

  v2 = *(_QWORD *)(v1 + 16);
  v3 = *(_QWORD *)(v1 + 32);
  v4 = *(_BYTE *)(v1 + 64);
  v5 = *(_QWORD *)(v1 + 72);
  v6 = *(_QWORD *)(v1 + 80);
  v8[2] = *(_QWORD *)(v1 + 56);
  v9 = v4;
  v10 = v5;
  v11 = a1;
  v12 = v6;
  return (*(uint64_t (**)(void (*)(const double *, int), _QWORD *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:), v8, MEMORY[0x1E0DEE9C0] + 8, v2);
}

{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  char v4;
  uint64_t v5;
  _DWORD v7[5];
  char v8;
  uint64_t v9;
  uint64_t v10;

  v2 = *(_QWORD *)(v1 + 16);
  v3 = *(_QWORD *)(v1 + 32);
  v4 = *(_BYTE *)(v1 + 60);
  v5 = *(_QWORD *)(v1 + 64);
  v7[4] = *(_DWORD *)(v1 + 56);
  v8 = v4;
  v9 = v5;
  v10 = a1;
  return (*(uint64_t (**)(void (*)(const float *, int), _DWORD *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:), v7, MEMORY[0x1E0DEE9C0] + 8, v2);
}

void partial apply for closure #1 in closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:)(const double *a1, int a2)
{
  uint64_t v2;

  closure #1 in closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_BYTE *)(v2 + 24), *(const double **)(v2 + 32), *(double ***)(v2 + 40), *(_QWORD *)(v2 + 48));
}

void partial apply for closure #1 in closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:)(const float *a1, int a2)
{
  uint64_t v2;

  closure #1 in closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:)(a1, a2, *(unsigned int *)(v2 + 16) | ((unint64_t)*(unsigned __int8 *)(v2 + 20) << 32), *(const float **)(v2 + 24), *(float ***)(v2 + 32), *(_QWORD *)(v2 + 40));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.invertedClip<A, B>(_:to:result:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.invertedClip<A, B>(_:to:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.invertedClip<A, B>(_:to:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.invertedClip<A, B>(_:to:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C660]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C658]);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C3E8]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), (uint64_t (*)(void))MEMORY[0x1E0C8C3D8]);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  _OWORD v6[2];
  uint64_t v7;

  v3 = *(_QWORD *)(v2 + 16);
  v4 = *(_QWORD *)(v2 + 32);
  v6[1] = *(_OWORD *)(v2 + 56);
  v7 = a1;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, uint64_t))(v4 + 24))(a2, v6, MEMORY[0x1E0DEE9C0] + 8, v3);
}

uint64_t vImage.PixelBuffer<>.permuteChannels(to:destination:)(char a1, char a2, char a3, char a4, _QWORD *a5)
{
  return vImage.PixelBuffer<>.permuteChannels(to:destination:)(a1, a2, a3, a4, a5, MEMORY[0x1E0C8D520]);
}

{
  return vImage.PixelBuffer<>.permuteChannels(to:destination:)(a1, a2, a3, a4, a5, MEMORY[0x1E0C8D510]);
}

{
  return vImage.PixelBuffer<>.permuteChannels(to:destination:)(a1, a2, a3, a4, a5, MEMORY[0x1E0C8D508]);
}

{
  return vImage.PixelBuffer<>.permuteChannels(to:destination:)(a1, a2, a3, a4, a5, MEMORY[0x1E0C8D528]);
}

vImage_Error specialized vImage.PixelBuffer<>._permute<A>(permuteArray:destination:permuteFunc:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  vImagePixelCount v6;
  vImagePixelCount v7;
  uint64_t v8;
  uint64_t v9;
  size_t v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  if ((specialized Sequence<>.max()(a1) & 0x1FC) != 0)
  {
    __break(1u);
    goto LABEL_16;
  }
  if (!*(_QWORD *)(a3 + 16))
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v6 = *(_QWORD *)(a3 + 48);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v7 = *(_QWORD *)(a3 + 40);
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v6)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v7)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!*(_QWORD *)(a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v8 = *(_QWORD *)(a2 + 48);
  if (v8 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v9 = *(_QWORD *)(a2 + 40);
  if (v9 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v8)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v9)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v6 != v8)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v7 != v9)
    goto LABEL_27;
  v10 = *(_QWORD *)(a3 + 56);
  src.data = *(void **)(a3 + 32);
  src.height = v7;
  src.width = v6;
  src.rowBytes = v10;
  v11 = *(_QWORD *)(a2 + 56);
  dest.data = *(void **)(a2 + 32);
  dest.height = v7;
  dest.width = v6;
  dest.rowBytes = v11;
  return vImagePermuteChannels_RGB888(&src, &dest, (const uint8_t *)(a1 + 32), 0);
}

uint64_t vImage.PixelBuffer<>._permute<A>(permuteArray:destination:permuteFunc:)(uint64_t a1, uint64_t *a2, uint64_t (*a3)(uint64_t *, uint64_t *, uint64_t, _QWORD), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t *v8;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t (*v25)(uint64_t *, uint64_t *, uint64_t, _QWORD);
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;

  v34 = *MEMORY[0x1E0C80C00];
  v13 = *a2;
  v14 = *v8;
  if ((specialized Sequence<>.max()(a1) & 0x1FC) != 0)
  {
    __break(1u);
    goto LABEL_6;
  }
  v25 = a3;
  v26 = v14;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&v30);
  v16 = v30;
  v15 = v31;
  type metadata accessor for vImage.PixelBuffer(0, a6, *(_QWORD *)(*(_QWORD *)(a8 + 8) + 8), v17);
  vImage.PixelBuffer.size.getter(&v26);
  swift_bridgeObjectRelease();
  if (v16 != v26 || v15 != v27)
LABEL_6:
    __break(1u);
  v30 = v14;
  v26 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v27 = v18;
  v28 = v19;
  v29 = v20;
  v30 = v13;
  v30 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v31 = v21;
  v32 = v22;
  v33 = v23;
  return v25(&v26, &v30, a1 + 32, 0);
}

uint64_t vImage.PixelBuffer<>.permuteChannels(to:destination:)(char a1, char a2, char a3, uint64_t *a4)
{
  uint64_t *v4;
  uint64_t inited;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5EF70;
  *(_BYTE *)(inited + 32) = a1;
  *(_BYTE *)(inited + 33) = a2;
  *(_BYTE *)(inited + 34) = a3;
  specialized vImage.PixelBuffer<>._permute<A>(permuteArray:destination:permuteFunc:)(inited, *a4, *v4);
  return swift_setDeallocating();
}

uint64_t vImage.PixelBuffer<>.permuteChannels(to:destination:)(char a1, char a2, char a3, char a4, _QWORD *a5, void (*a6)(_QWORD *, _QWORD *, uint64_t, _QWORD))
{
  _QWORD *v6;
  uint64_t inited;
  uint64_t v14;
  _QWORD *v15;
  _QWORD *v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  _QWORD v24[4];
  _QWORD v25[10];

  v25[9] = *MEMORY[0x1E0C80C00];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
  inited = swift_initStackObject();
  *(_BYTE *)(inited + 32) = a1;
  v14 = inited + 32;
  *(_OWORD *)(inited + 16) = xmmword_1CAB5F170;
  *(_BYTE *)(inited + 33) = a2;
  *(_BYTE *)(inited + 34) = a3;
  *(_BYTE *)(inited + 35) = a4;
  v15 = (_QWORD *)*a5;
  v16 = (_QWORD *)*v6;
  if ((specialized Sequence<>.max()(inited) & 0x1FC) != 0)
  {
    __break(1u);
    goto LABEL_16;
  }
  if (!v16[2])
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v17 = v16[6];
  if (v17 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v18 = v16[5];
  if (v18 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v17)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v18)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v15[2])
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v19 = v15[6];
  if (v19 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v20 = v15[5];
  if (v20 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v19)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v20)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v17 != v19)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v18 != v20)
    goto LABEL_27;
  v21 = v16[7];
  v25[0] = v16[4];
  v25[1] = v18;
  v25[2] = v17;
  v25[3] = v21;
  v22 = v15[7];
  v24[0] = v15[4];
  v24[1] = v18;
  v24[2] = v17;
  v24[3] = v22;
  a6(v25, v24, v14, 0);
  return swift_setDeallocating();
}

float _swift_vDSP_dotpr(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  char *v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  void (*v19)(char *);
  uint64_t v20;
  void (*v21)(char *, uint64_t, uint64_t);
  uint64_t (*v22)(uint64_t, uint64_t);
  uint64_t (*v23)(uint64_t, uint64_t);
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  float v35;
  uint64_t v36;

  v36 = *MEMORY[0x1E0C80C00];
  v11 = *(_QWORD *)(a4 - 8);
  v12 = MEMORY[0x1E0C80A78](a1);
  v14 = (char *)&v30 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  v16 = *(_QWORD *)(v15 - 8);
  MEMORY[0x1E0C80A78](v12);
  v18 = (char *)&v30 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = *(void (**)(char *))(v16 + 16);
  v33 = v20;
  v19(v18);
  v21 = *(void (**)(char *, uint64_t, uint64_t))(v11 + 16);
  v31 = a2;
  v21(v14, a2, a4);
  v22 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  v34 = v22(a3, a5);
  v23 = *(uint64_t (**)(uint64_t, uint64_t))(a6 + 16);
  v32 = a6;
  v24 = v23(a4, a6);
  (*(void (**)(char *, uint64_t))(v11 + 8))(v14, a4);
  (*(void (**)(char *, uint64_t))(v16 + 8))(v18, a3);
  if (v34 != v24)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  v25 = v22(a3, a5);
  if (v25 < 0)
    goto LABEL_5;
  v35 = NAN;
  v26 = MEMORY[0x1E0C80A78](v25);
  *(&v30 - 8) = a3;
  *(&v30 - 7) = a4;
  v28 = v31;
  v27 = v32;
  *(&v30 - 6) = a5;
  *(&v30 - 5) = v27;
  *(&v30 - 4) = v28;
  *(&v30 - 3) = (uint64_t)&v35;
  *(&v30 - 2) = v26;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t)))(a5 + 24))(partial apply for closure #1 in static vDSP.dot<A, B>(_:_:));
  return v35;
}

double _swift_vDSP_dotprD(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  char *v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  void (*v19)(char *);
  uint64_t v20;
  void (*v21)(char *, uint64_t, uint64_t);
  uint64_t (*v22)(uint64_t, uint64_t);
  uint64_t (*v23)(uint64_t, uint64_t);
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  _QWORD v35[2];

  v35[1] = *MEMORY[0x1E0C80C00];
  v11 = *(_QWORD *)(a4 - 8);
  v12 = MEMORY[0x1E0C80A78](a1);
  v14 = (char *)&v30 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  v16 = *(_QWORD *)(v15 - 8);
  MEMORY[0x1E0C80A78](v12);
  v18 = (char *)&v30 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = *(void (**)(char *))(v16 + 16);
  v33 = v20;
  v19(v18);
  v21 = *(void (**)(char *, uint64_t, uint64_t))(v11 + 16);
  v31 = a2;
  v21(v14, a2, a4);
  v22 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  v34 = v22(a3, a5);
  v23 = *(uint64_t (**)(uint64_t, uint64_t))(a6 + 16);
  v32 = a6;
  v24 = v23(a4, a6);
  (*(void (**)(char *, uint64_t))(v11 + 8))(v14, a4);
  (*(void (**)(char *, uint64_t))(v16 + 8))(v18, a3);
  if (v34 != v24)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  v25 = v22(a3, a5);
  if (v25 < 0)
    goto LABEL_5;
  v35[0] = 0x7FF8000000000000;
  v26 = MEMORY[0x1E0C80A78](v25);
  *(&v30 - 8) = a3;
  *(&v30 - 7) = a4;
  v28 = v31;
  v27 = v32;
  *(&v30 - 6) = a5;
  *(&v30 - 5) = v27;
  *(&v30 - 4) = v28;
  *(&v30 - 3) = (uint64_t)v35;
  *(&v30 - 2) = v26;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t)))(a5 + 24))(partial apply for closure #1 in static vDSP.dot<A, B>(_:_:));
  return *(double *)v35;
}

float static vDSP.dot<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return _swift_vDSP_dotpr(a1, a2, a3, a3, a4, a4);
}

double static vDSP.dot<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return _swift_vDSP_dotprD(a1, a2, a3, a3, a4, a4);
}

uint64_t static vDSP.hypot<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.hypot<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.hypot<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.hypot<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.hypot<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vDSP.hypot<A, B>(_:_:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.hypot<A, B>(_:_:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.hypot<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.hypot<A, B>(_:_:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.hypot<A, B, C>(_:_:result:));
}

uint64_t static vDSP.hypot<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.hypot<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vDSP.hypot<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  void (*v24)(char *);
  uint64_t v25;
  void (*v26)(char *, uint64_t, uint64_t);
  uint64_t (*v27)(uint64_t, uint64_t);
  uint64_t v28;
  uint64_t (*v29)(uint64_t, uint64_t);
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t (*v33)(uint64_t, uint64_t);
  uint64_t v34;
  void (*v35)(char *, uint64_t);
  uint64_t result;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  void (*v42)(char *, uint64_t);
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;

  v50 = a6;
  v51 = a3;
  v14 = *(_QWORD *)(a5 - 8);
  v15 = MEMORY[0x1E0C80A78](a1);
  v17 = (char *)&v43 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  v18 = MEMORY[0x1E0C80A78](v15);
  v20 = (char *)&v43 - v19;
  MEMORY[0x1E0C80A78](v18);
  v22 = (char *)&v43 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  v49 = v23;
  v24 = *(void (**)(char *))(v23 + 16);
  v46 = v25;
  v24(v22);
  v48 = v14;
  v26 = *(void (**)(char *, uint64_t, uint64_t))(v14 + 16);
  v45 = a2;
  v26(v20, a2, a5);
  v27 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  v47 = a7;
  v28 = v27(a4, a7);
  v29 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  v30 = v29(a5, a8);
  v26(v17, (uint64_t)v20, a5);
  if (v28 != v30)
  {
    v42 = *(void (**)(char *, uint64_t))(v48 + 8);
    v42(v17, a5);
    v42(v20, a5);
    result = (*(uint64_t (**)(char *, uint64_t))(v49 + 8))(v22, a4);
    goto LABEL_6;
  }
  v44 = a8;
  v31 = v29(a5, a8);
  v43 = a9;
  v32 = *(_QWORD *)(a9 + 8);
  v33 = *(uint64_t (**)(uint64_t, uint64_t))(v32 + 16);
  v34 = v33(v50, v32);
  v35 = *(void (**)(char *, uint64_t))(v48 + 8);
  v35(v17, a5);
  v35(v20, a5);
  result = (*(uint64_t (**)(char *, uint64_t))(v49 + 8))(v22, a4);
  if (v31 != v34)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  result = v33(v50, v32);
  if ((result & 0x8000000000000000) == 0)
  {
    v37 = MEMORY[0x1E0C80A78](result);
    *(&v43 - 10) = a4;
    *(&v43 - 9) = a5;
    v38 = v47;
    *(&v43 - 8) = v50;
    *(&v43 - 7) = v38;
    v39 = v43;
    *(&v43 - 6) = v44;
    *(&v43 - 5) = v39;
    v40 = v51;
    *(&v43 - 4) = v45;
    *(&v43 - 3) = v40;
    *(&v43 - 2) = v37;
    return (*(uint64_t (**)(uint64_t))(v38 + 24))(v41);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vDSP.hypot<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t))
{
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  void (*v21)(char *);
  uint64_t v22;
  void (*v23)(char *, uint64_t, uint64_t);
  uint64_t (*v24)(uint64_t, uint64_t);
  uint64_t v25;
  uint64_t (*v26)(uint64_t, uint64_t);
  uint64_t v27;
  uint64_t result;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t (*v42)(uint64_t, uint64_t);

  v41 = a7;
  v42 = a8;
  v13 = *(_QWORD *)(a4 - 8);
  v14 = MEMORY[0x1E0C80A78](a1);
  v16 = (char *)&v36 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  v18 = *(_QWORD *)(v17 - 8);
  MEMORY[0x1E0C80A78](v14);
  v20 = (char *)&v36 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  v21 = *(void (**)(char *))(v18 + 16);
  v37 = v22;
  v21(v20);
  v23 = *(void (**)(char *, uint64_t, uint64_t))(v13 + 16);
  v39 = a2;
  v23(v16, a2, a4);
  v24 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  v38 = a5;
  v25 = v24(a3, a5);
  v26 = *(uint64_t (**)(uint64_t, uint64_t))(a6 + 16);
  v40 = a6;
  v27 = v26(a4, a6);
  (*(void (**)(char *, uint64_t))(v13 + 8))(v16, a4);
  result = (*(uint64_t (**)(char *, uint64_t))(v18 + 8))(v20, a3);
  if (v25 == v27)
  {
    v29 = v37;
    v30 = v38;
    v31 = v24(a3, v38);
    v32 = MEMORY[0x1E0C80A78](v31);
    *(&v36 - 6) = a3;
    *(&v36 - 5) = a4;
    v34 = v40;
    v33 = v41;
    *(&v36 - 4) = v30;
    *(&v36 - 3) = v34;
    v35 = v39;
    *(&v36 - 2) = v29;
    *(&v36 - 1) = v35;
    return v42(v32, v33);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in static vDSP.hypot<A, B>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, unint64_t *a10, void (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16;
  uint64_t v17;
  uint64_t result;

  v16 = __swift_instantiateConcreteTypeFromMangledName(a9);
  v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a10, a9);
  a11(a3, a4, a1, a5, a6, v16, a7, a8, v17);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  *a2 = result;
  return result;
}

uint64_t static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  return static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, char *a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15)
{
  return static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15);
}

{
  return static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15);
}

{
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  char *v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  char *v34;
  uint64_t v35;
  char *v36;
  void (*v37)(char *);
  uint64_t v38;
  void (*v39)(char *, uint64_t, uint64_t);
  uint64_t (*v40)(char *, uint64_t);
  uint64_t v41;
  uint64_t (*v42)(uint64_t);
  uint64_t v43;
  uint64_t v44;
  uint64_t (*v45)(uint64_t, uint64_t);
  uint64_t v46;
  void (*v47)(char *, char *);
  void (*v48)(char *, uint64_t, uint64_t);
  uint64_t (*v49)(uint64_t, uint64_t);
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t (*v55)(uint64_t, uint64_t);
  uint64_t v56;
  void (*v57)(char *, uint64_t);
  uint64_t result;
  uint64_t v59;
  uint64_t v60;
  uint64_t v61;
  uint64_t v62;
  uint64_t v63;
  uint64_t v64;
  uint64_t v65;
  void (*v66)(char *, char *);
  void (*v67)(char *, uint64_t);
  uint64_t v68;
  uint64_t v69;
  uint64_t v70;
  char *v71;
  uint64_t v72;
  uint64_t v73;
  uint64_t v74;
  uint64_t (*v75)(uint64_t, uint64_t);
  uint64_t v76;
  uint64_t v77;
  char *v78;
  char *v79;
  uint64_t v80;
  uint64_t v81;
  uint64_t v82;
  uint64_t v83;
  uint64_t v84;
  uint64_t v85;
  uint64_t v86;
  uint64_t v87;
  uint64_t v88;
  char *v89;
  uint64_t v90;
  uint64_t v91;

  v83 = a3;
  v84 = a4;
  v87 = a12;
  v80 = a9;
  v90 = *(_QWORD *)(a9 - 8);
  v91 = a5;
  v18 = MEMORY[0x1E0C80A78](a1);
  v89 = (char *)&v68 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  v81 = v20;
  v88 = *(_QWORD *)(v20 - 8);
  v21 = MEMORY[0x1E0C80A78](v18);
  v78 = (char *)&v68 - ((v22 + 15) & 0xFFFFFFFFFFFFFFF0);
  v23 = MEMORY[0x1E0C80A78](v21);
  v79 = (char *)&v68 - v24;
  v26 = *(_QWORD *)(v25 - 8);
  v27 = MEMORY[0x1E0C80A78](v23);
  v29 = (char *)&v68 - ((v28 + 15) & 0xFFFFFFFFFFFFFFF0);
  v31 = *(_QWORD *)(v30 - 8);
  v32 = MEMORY[0x1E0C80A78](v27);
  v34 = (char *)&v68 - ((v33 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1E0C80A78](v32);
  v36 = (char *)&v68 - v35;
  v37 = *(void (**)(char *))(v31 + 16);
  v74 = v38;
  v37((char *)&v68 - v35);
  v82 = v26;
  v39 = *(void (**)(char *, uint64_t, uint64_t))(v26 + 16);
  v73 = a2;
  v39(v29, a2, a7);
  v40 = *(uint64_t (**)(char *, uint64_t))(a11 + 16);
  v85 = a11;
  v41 = v40(a6, a11);
  v42 = *(uint64_t (**)(uint64_t))(v87 + 16);
  v86 = a7;
  v43 = v42(a7);
  ((void (*)(char *, char *, char *))v37)(v34, v36, a6);
  if (v41 != v43)
  {
    v66 = *(void (**)(char *, char *))(v31 + 8);
    v66(v34, a6);
    (*(void (**)(char *, uint64_t))(v82 + 8))(v29, v86);
    v66(v36, a6);
    goto LABEL_8;
  }
  a7 = a10;
  v44 = v40(a6, v85);
  v45 = *(uint64_t (**)(uint64_t, uint64_t))(*(_QWORD *)(a15 + 8) + 16);
  v76 = *(_QWORD *)(a15 + 8);
  v77 = a10;
  v75 = v45;
  v46 = ((uint64_t (*)(uint64_t))v45)(a10);
  v47 = *(void (**)(char *, char *))(v31 + 8);
  v47(v34, a6);
  (*(void (**)(char *, uint64_t))(v82 + 8))(v29, v86);
  v47(v36, a6);
  if (v44 != v46)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v71 = a6;
  v72 = a15;
  v48 = *(void (**)(char *, uint64_t, uint64_t))(v88 + 16);
  a6 = v79;
  v31 = v81;
  v48(v79, v83, v81);
  a7 = v80;
  (*(void (**)(char *, uint64_t, uint64_t))(v90 + 16))(v89, v84, v80);
  v49 = *(uint64_t (**)(uint64_t, uint64_t))(a13 + 16);
  v50 = v49(v31, a13);
  v51 = (*(uint64_t (**)(uint64_t, uint64_t))(a14 + 16))(a7, a14);
  v36 = v78;
  v48(v78, (uint64_t)a6, v31);
  if (v50 != v51)
  {
LABEL_9:
    v67 = *(void (**)(char *, uint64_t))(v88 + 8);
    v67(v36, v31);
    (*(void (**)(char *, uint64_t))(v90 + 8))(v89, a7);
    result = ((uint64_t (*)(char *, uint64_t))v67)(a6, v31);
    goto LABEL_10;
  }
  v70 = a14;
  v69 = a13;
  v82 = v49(v31, a13);
  v52 = v91;
  v53 = v76;
  v54 = v77;
  v55 = v75;
  v56 = v75(v77, v76);
  v57 = *(void (**)(char *, uint64_t))(v88 + 8);
  v57(v36, v31);
  (*(void (**)(char *, uint64_t))(v90 + 8))(v89, a7);
  result = ((uint64_t (*)(char *, uint64_t))v57)(a6, v31);
  if (v82 != v56)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  result = v55(v54, v53);
  if ((result & 0x8000000000000000) == 0)
  {
    v59 = MEMORY[0x1E0C80A78](result);
    v60 = v85;
    v61 = v86;
    *(&v68 - 16) = (uint64_t)v71;
    *(&v68 - 15) = v61;
    *(&v68 - 14) = v31;
    *(&v68 - 13) = a7;
    *(&v68 - 12) = v54;
    *(&v68 - 11) = v60;
    v62 = v69;
    *(&v68 - 10) = v87;
    *(&v68 - 9) = v62;
    v63 = v72;
    *(&v68 - 8) = v70;
    *(&v68 - 7) = v63;
    v64 = v83;
    *(&v68 - 6) = v73;
    *(&v68 - 5) = v64;
    *(&v68 - 4) = v84;
    *(&v68 - 3) = v52;
    *(&v68 - 2) = v59;
    return (*(uint64_t (**)(uint64_t))(v60 + 24))(v65);
  }
LABEL_11:
  __break(1u);
  return result;
}

uint64_t static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t (*a14)(uint64_t, uint64_t))
{
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  char *v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  char *v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  char *v35;
  void (*v36)(char *, uint64_t, uint64_t);
  uint64_t v37;
  void (*v38)(char *, uint64_t, uint64_t);
  uint64_t (*v39)(uint64_t, uint64_t);
  uint64_t v40;
  uint64_t (*v41)(uint64_t, uint64_t);
  uint64_t v42;
  void (*v43)(char *, uint64_t);
  uint64_t (*v44)(char *, uint64_t);
  uint64_t result;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  char *v49;
  uint64_t v50;
  uint64_t (*v51)(uint64_t, uint64_t);
  uint64_t v52;
  uint64_t (*v53)(uint64_t, uint64_t);
  uint64_t v54;
  uint64_t (*v55)(char *, uint64_t);
  char *v56;
  uint64_t v57;
  uint64_t v58;
  char *v59;
  uint64_t v60;
  uint64_t v61;
  uint64_t (*v62)(uint64_t, uint64_t);
  uint64_t v63;
  uint64_t v64;
  uint64_t v65;
  uint64_t v66;
  uint64_t v67;
  uint64_t v68;
  uint64_t v69;
  uint64_t v70;
  uint64_t v71;
  uint64_t v72;
  uint64_t v73;
  uint64_t (*v74)(uint64_t, uint64_t);
  void (*v75)(char *, uint64_t, uint64_t);
  uint64_t (*v76)(char *, uint64_t);
  uint64_t v77;
  uint64_t v78;
  uint64_t v79;
  void (*v80)(char *, uint64_t, uint64_t);
  uint64_t v81;
  uint64_t (*v82)(uint64_t, uint64_t);
  char *v83;
  char *v84;
  uint64_t v85;
  uint64_t v86;
  char *v87;
  uint64_t v88;
  uint64_t v89;
  uint64_t v90;
  uint64_t v91;
  uint64_t v92;
  uint64_t v93;
  uint64_t v94;

  v91 = a4;
  v92 = a8;
  v94 = a3;
  v89 = *(_QWORD *)(a8 - 8);
  v17 = MEMORY[0x1E0C80A78](a1);
  v87 = (char *)&v73 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  v93 = v19;
  v88 = *(_QWORD *)(v19 - 8);
  v20 = MEMORY[0x1E0C80A78](v17);
  v84 = (char *)&v73 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  v22 = MEMORY[0x1E0C80A78](v20);
  v24 = (char *)&v73 - v23;
  v26 = *(_QWORD *)(v25 - 8);
  v27 = MEMORY[0x1E0C80A78](v22);
  v29 = (char *)&v73 - ((v28 + 15) & 0xFFFFFFFFFFFFFFF0);
  v31 = *(_QWORD *)(v30 - 8);
  v32 = MEMORY[0x1E0C80A78](v27);
  v83 = (char *)&v73 - ((v33 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1E0C80A78](v32);
  v35 = (char *)&v73 - v34;
  v36 = *(void (**)(char *, uint64_t, uint64_t))(v31 + 16);
  v86 = v37;
  v80 = v36;
  ((void (*)(char *))v36)((char *)&v73 - v34);
  v38 = *(void (**)(char *, uint64_t, uint64_t))(v26 + 16);
  v79 = a2;
  v38(v29, a2, a6);
  v39 = *(uint64_t (**)(uint64_t, uint64_t))(a9 + 16);
  v85 = a9;
  v82 = v39;
  v40 = v39(a5, a9);
  v41 = *(uint64_t (**)(uint64_t, uint64_t))(a10 + 16);
  v77 = a10;
  v42 = v41(a6, a10);
  v43 = *(void (**)(char *, uint64_t))(v26 + 8);
  v78 = a6;
  v43(v29, a6);
  v44 = *(uint64_t (**)(char *, uint64_t))(v31 + 8);
  v81 = a5;
  result = v44(v35, a5);
  if (v40 != v42)
  {
    __break(1u);
    goto LABEL_6;
  }
  v46 = v88;
  v47 = v93;
  v75 = *(void (**)(char *, uint64_t, uint64_t))(v88 + 16);
  v76 = v44;
  v75(v24, v94, v93);
  v48 = v89;
  v49 = v87;
  v50 = v92;
  (*(void (**)(char *, uint64_t, uint64_t))(v89 + 16))(v87, v91, v92);
  v51 = *(uint64_t (**)(uint64_t, uint64_t))(a11 + 16);
  v90 = a11;
  v74 = v51;
  v52 = v51(v47, a11);
  v53 = *(uint64_t (**)(uint64_t, uint64_t))(a12 + 16);
  v73 = a12;
  v54 = v53(v50, a12);
  (*(void (**)(char *, uint64_t))(v48 + 8))(v49, v50);
  v55 = *(uint64_t (**)(char *, uint64_t))(v46 + 8);
  result = v55(v24, v47);
  if (v52 != v54)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  v56 = v83;
  v57 = v86;
  v58 = v81;
  v80(v83, v86, v81);
  v59 = v84;
  v60 = v93;
  v75(v84, v94, v93);
  v61 = v85;
  v62 = v82;
  v63 = v82(v58, v85);
  v64 = v74(v60, v90);
  v55(v59, v60);
  result = v76(v56, v58);
  if (v63 == v64)
  {
    v65 = v62(v58, v61);
    v66 = MEMORY[0x1E0C80A78](v65);
    v67 = v78;
    *(&v73 - 12) = v58;
    *(&v73 - 11) = v67;
    v68 = v92;
    *(&v73 - 10) = v60;
    *(&v73 - 9) = v68;
    v69 = v77;
    *(&v73 - 8) = v61;
    *(&v73 - 7) = v69;
    v70 = v73;
    *(&v73 - 6) = v90;
    *(&v73 - 5) = v70;
    v71 = v79;
    *(&v73 - 4) = v57;
    *(&v73 - 3) = v71;
    v72 = v91;
    *(&v73 - 2) = v94;
    *(&v73 - 1) = v72;
    return a14(v66, a13);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t closure #1 in static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t *a15, unint64_t *a16, void (*a17)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v19;
  uint64_t v20;
  uint64_t result;

  v19 = __swift_instantiateConcreteTypeFromMangledName(a15);
  v20 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a16, a15);
  a17(a3, a4, a5, a6, a1, a7, a8, a9, a10, v19, a11, a12, a13, a14, v20);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a11 + 16))(a7, a11);
  *a2 = result;
  return result;
}

_QWORD *closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(_QWORD *result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, _QWORD, uint64_t, uint64_t))
{
  if (!a2)
  {
    __break(1u);
    goto LABEL_8;
  }
  if (!a4)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (!a6)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (!a8)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (*result)
    return (_QWORD *)a11(a2, 1, a4, 1, a6, 1, a8, 1, *result, 1, a10);
LABEL_11:
  __break(1u);
  return result;
}

float static vDSP.distanceSquared<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  char *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  void (*v18)(char *);
  uint64_t v19;
  void (*v20)(char *, uint64_t, uint64_t);
  uint64_t (*v21)(uint64_t, uint64_t);
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  _QWORD v28[2];
  uint64_t v29;
  float v30;
  uint64_t v31;

  v29 = a6;
  v31 = *MEMORY[0x1E0C80C00];
  v10 = *(_QWORD *)(a4 - 8);
  v11 = MEMORY[0x1E0C80A78](a1);
  v13 = (char *)v28 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  v15 = *(_QWORD *)(v14 - 8);
  MEMORY[0x1E0C80A78](v11);
  v17 = (char *)v28 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  v18 = *(void (**)(char *))(v15 + 16);
  v28[1] = v19;
  v18(v17);
  v20 = *(void (**)(char *, uint64_t, uint64_t))(v10 + 16);
  v28[0] = a2;
  v20(v13, a2, a4);
  v21 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  v22 = v21(a3, a5);
  v23 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v29 + 8) + 16))(a4);
  (*(void (**)(char *, uint64_t))(v10 + 8))(v13, a4);
  (*(void (**)(char *, uint64_t))(v15 + 8))(v17, a3);
  if (v22 != v23)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  v24 = v21(a3, a5);
  if (v24 < 0)
    goto LABEL_5;
  v30 = NAN;
  v25 = MEMORY[0x1E0C80A78](v24);
  v28[-8] = a3;
  v28[-7] = a4;
  v26 = v29;
  v28[-6] = a5;
  v28[-5] = v26;
  v28[-4] = v28[0];
  v28[-3] = &v30;
  v28[-2] = v25;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t)))(a5 + 24))(partial apply for closure #1 in static vDSP.distanceSquared<A, B>(_:_:));
  return v30;
}

double static vDSP.distanceSquared<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  char *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  void (*v18)(char *);
  uint64_t v19;
  void (*v20)(char *, uint64_t, uint64_t);
  uint64_t (*v21)(uint64_t, uint64_t);
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  _QWORD v28[2];
  uint64_t v29;
  _QWORD v30[2];

  v29 = a6;
  v30[1] = *MEMORY[0x1E0C80C00];
  v10 = *(_QWORD *)(a4 - 8);
  v11 = MEMORY[0x1E0C80A78](a1);
  v13 = (char *)v28 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  v15 = *(_QWORD *)(v14 - 8);
  MEMORY[0x1E0C80A78](v11);
  v17 = (char *)v28 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  v18 = *(void (**)(char *))(v15 + 16);
  v28[1] = v19;
  v18(v17);
  v20 = *(void (**)(char *, uint64_t, uint64_t))(v10 + 16);
  v28[0] = a2;
  v20(v13, a2, a4);
  v21 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  v22 = v21(a3, a5);
  v23 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v29 + 8) + 16))(a4);
  (*(void (**)(char *, uint64_t))(v10 + 8))(v13, a4);
  (*(void (**)(char *, uint64_t))(v15 + 8))(v17, a3);
  if (v22 != v23)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  v24 = v21(a3, a5);
  if (v24 < 0)
    goto LABEL_5;
  v30[0] = 0x7FF8000000000000;
  v25 = MEMORY[0x1E0C80A78](v24);
  v28[-8] = a3;
  v28[-7] = a4;
  v26 = v29;
  v28[-6] = a5;
  v28[-5] = v26;
  v28[-4] = v28[0];
  v28[-3] = v30;
  v28[-2] = v25;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t)))(a5 + 24))(partial apply for closure #1 in static vDSP.distanceSquared<A, B>(_:_:));
  return *(double *)v30;
}

uint64_t partial apply for closure #1 in static vDSP.hypot<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.hypot<A, B>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t *v5;

  return closure #1 in static vDSP.hypot<A, B>(_:_:)(a1, a2, v5[6], v5[7], v5[2], v5[3], v5[4], v5[5], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

{
  return partial apply for closure #1 in static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

uint64_t partial apply for closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

{
  return partial apply for closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

uint64_t partial apply for closure #1 in static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t *v5;

  return closure #1 in static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(a1, a2, v5[10], v5[11], v5[12], v5[13], v5[2], v5[3], v5[4], v5[5], v5[6], v5[7], v5[8], v5[9], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  __int128 v7;
  _QWORD v9[3];
  __int128 v10;
  __int128 v11;
  uint64_t v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;

  v4 = *(_QWORD *)(v3 + 56);
  v5 = *(_QWORD *)(v3 + 120);
  v6 = *(_QWORD *)(v3 + 128);
  v9[2] = *(_QWORD *)(v3 + 16);
  v10 = *(_OWORD *)(v3 + 24);
  v11 = *(_OWORD *)(v3 + 40);
  v12 = v4;
  v7 = *(_OWORD *)(v3 + 80);
  v13 = *(_OWORD *)(v3 + 64);
  v14 = v7;
  v15 = *(_OWORD *)(v3 + 104);
  v16 = v5;
  v17 = a1;
  v18 = a2;
  v19 = v6;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, _QWORD))(v13 + 24))(a3, v9, MEMORY[0x1E0DEE9C0] + 8, v10);
}

uint64_t partial apply for closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.distanceSquared<A, B>(_:_:));
}

{
  return partial apply for closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.distanceSquared<A, B>(_:_:));
}

uint64_t partial apply for closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  _QWORD *v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD v8[5];

  v4 = v3[3];
  v5 = v3[5];
  v6 = v3[7];
  v8[2] = a1;
  v8[3] = a2;
  v8[4] = v6;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(*(_QWORD *)(v5 + 8) + 24))(a3, v8, MEMORY[0x1E0DEE9C0] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(a1, a2, MEMORY[0x1E0C8C0C0]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(a1, a2, MEMORY[0x1E0C8C0B0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

_QWORD *partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(_QWORD *a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, MEMORY[0x1E0C8C790]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, MEMORY[0x1E0C8C788]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  __int128 v7;
  __int128 v8;
  _OWORD v10[4];
  uint64_t v11;
  __int128 v12;
  uint64_t v13;
  __int128 v14;
  __int128 v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;

  v4 = *(_QWORD *)(v3 + 64);
  v5 = *(_QWORD *)(v3 + 88);
  v6 = *(_QWORD *)(v3 + 136);
  v7 = *(_OWORD *)(v3 + 32);
  v10[1] = *(_OWORD *)(v3 + 16);
  v10[2] = v7;
  v10[3] = *(_OWORD *)(v3 + 48);
  v11 = v4;
  v12 = *(_OWORD *)(v3 + 72);
  v13 = v5;
  v8 = *(_OWORD *)(v3 + 120);
  v14 = *(_OWORD *)(v3 + 104);
  v15 = v8;
  v16 = a1;
  v17 = a2;
  v18 = v6;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, _QWORD))(v12 + 24))(a3, v10, MEMORY[0x1E0DEE9C0] + 8, v7);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  __int128 v8;
  _OWORD v10[2];
  uint64_t v11;
  __int128 v12;
  __int128 v13;
  uint64_t v14;
  __int128 v15;
  uint64_t v16;
  __int128 v17;
  uint64_t v18;
  uint64_t v19;
  __int128 v20;
  uint64_t v21;

  v4 = *(_QWORD *)(v3 + 32);
  v5 = *(_QWORD *)(v3 + 72);
  v6 = *(_QWORD *)(v3 + 104);
  v7 = *(_QWORD *)(v3 + 144);
  v10[1] = *(_OWORD *)(v3 + 16);
  v11 = v4;
  v12 = *(_OWORD *)(v3 + 40);
  v13 = *(_OWORD *)(v3 + 56);
  v14 = v5;
  v8 = *(_OWORD *)(v3 + 128);
  v17 = *(_OWORD *)(v3 + 112);
  v15 = *(_OWORD *)(v3 + 80);
  v16 = v6;
  v18 = a1;
  v19 = a2;
  v20 = v8;
  v21 = v7;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, _QWORD))(v15 + 24))(a3, v10, MEMORY[0x1E0DEE9C0] + 8, v12);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  __int128 v7;
  _OWORD v9[4];
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;

  v4 = *(_QWORD *)(v3 + 48);
  v5 = *(_QWORD *)(v3 + 88);
  v6 = *(_QWORD *)(v3 + 152);
  v7 = *(_OWORD *)(v3 + 120);
  v9[1] = *(_OWORD *)(v3 + 104);
  v9[2] = v7;
  v9[3] = *(_OWORD *)(v3 + 136);
  v10 = a1;
  v11 = a2;
  v12 = v6;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, uint64_t))(v5 + 16))(a3, v9, MEMORY[0x1E0DEE9C0] + 8, v4);
}

_QWORD *partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(_QWORD *a1, uint64_t (*a2)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, _QWORD, uint64_t, uint64_t))
{
  uint64_t *v2;

  return closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, v2[2], v2[3], v2[4], v2[5], v2[6], v2[7], v2[8], v2[9], v2[10], a2);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:));
}

_QWORD *partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:)(_QWORD *a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, MEMORY[0x1E0C8C438]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, MEMORY[0x1E0C8C430]);
}

uint64_t partial apply for closure #1 in static vDSP.dot<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.dot<A, B>(_:_:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.dot<A, B>(_:_:));
}

{
  return partial apply for closure #1 in static vDSP.dot<A, B>(_:_:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.dot<A, B>(_:_:));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.dot<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(a1, a2, MEMORY[0x1E0C8C0E0]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(a1, a2, MEMORY[0x1E0C8C0D0]);
}

uint64_t partial apply for closure #1 in static vDSP.dot<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  _QWORD *v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD v8[5];

  v4 = v3[3];
  v5 = v3[5];
  v6 = v3[7];
  v8[2] = a1;
  v8[3] = a2;
  v8[4] = v6;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E0DEE9C0] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(void))
{
  uint64_t v3;
  uint64_t result;

  result = *(_QWORD *)(v3 + 16);
  if (result)
  {
    if (a1)
      return a3();
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

void one-time initialization function for gaussian1Dx3()
{
  static vImage.ConvolutionKernel.gaussian1Dx3 = (uint64_t)&outlined read-only object #0 of one-time initialization function for gaussian1Dx3;
}

uint64_t static vImage.ConvolutionKernel.gaussian1Dx3.getter()
{
  return static vImage.ConvolutionKernel.gaussian1Dx3.getter(&one-time initialization token for gaussian1Dx3);
}

void one-time initialization function for gaussian1Dx5()
{
  static vImage.ConvolutionKernel.gaussian1Dx5 = (uint64_t)&outlined read-only object #0 of one-time initialization function for gaussian1Dx5;
}

uint64_t static vImage.ConvolutionKernel.gaussian1Dx5.getter()
{
  return static vImage.ConvolutionKernel.gaussian1Dx3.getter(&one-time initialization token for gaussian1Dx5);
}

void one-time initialization function for gaussian1Dx7()
{
  static vImage.ConvolutionKernel.gaussian1Dx7 = (uint64_t)&outlined read-only object #0 of one-time initialization function for gaussian1Dx7;
}

uint64_t static vImage.ConvolutionKernel.gaussian1Dx7.getter()
{
  return static vImage.ConvolutionKernel.gaussian1Dx3.getter(&one-time initialization token for gaussian1Dx7);
}

uint64_t static vImage.ConvolutionKernel.gaussian1Dx3.getter(_QWORD *a1)
{
  if (*a1 != -1)
    swift_once();
  return swift_bridgeObjectRetain();
}

const vImage_Buffer *vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(uint64_t a1, uint64_t a2, Pixel_16U *a3, uint64_t *a4, uint64_t a5, uint64_t a6, float a7)
{
  uint64_t *v7;
  uint64_t v11;
  const vImage_Buffer *result;
  const vImage_Buffer *v13;
  unint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  __int128 v26;
  __int128 v27;
  uint64_t v28;
  __int128 v29;
  uint64_t v30;
  uint64_t v31;
  vImage_Buffer v32;
  uint64_t v33;

  v33 = *MEMORY[0x1E0C80C00];
  v11 = *v7;
  v30 = *a4;
  v31 = v11;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&v32);
  vImage.PixelBuffer.size.getter(&v29);
  v28 = v11;
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  if (*(_OWORD *)&v32.data != v29)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if ((*(_BYTE *)(a1 + 16) & 1) == 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if ((*(_BYTE *)(a2 + 16) & 1) == 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  v21 = *(_QWORD *)(a5 + 16);
  result = (const vImage_Buffer *)(*(uint64_t (**)(void))(a6 + 32))();
  if (((unint64_t)result & 0x8000000000000000) != 0)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  v13 = result;
  if (result)
  {
    v14 = 0;
    v15 = 32;
    while (v13 != (const vImage_Buffer *)v14)
    {
      *(_QWORD *)&v29 = v28;
      swift_bridgeObjectRetain();
      swift_bridgeObjectRetain();
      v16 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v14 >= *(_QWORD *)(v16 + 16))
        goto LABEL_19;
      v17 = *(_QWORD *)(v16 + v15);
      swift_bridgeObjectRelease();
      v18 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v14 >= *(_QWORD *)(v18 + 16))
        goto LABEL_20;
      v19 = *(_QWORD *)(v18 + v15);
      swift_bridgeObjectRelease();
      swift_bridgeObjectRelease();
      swift_bridgeObjectRelease();
      if (v17)
      {
        if (v19 && v17 == v19)
          goto LABEL_22;
      }
      else if (!v19)
      {
        goto LABEL_27;
      }
      v20 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v14 >= *(_QWORD *)(v20 + 16))
        goto LABEL_21;
      v26 = *(_OWORD *)(v20 + v15 + 16);
      v27 = *(_OWORD *)(v20 + v15);
      swift_bridgeObjectRelease();
      *(_OWORD *)&v32.data = v27;
      *(_OWORD *)&v32.width = v26;
      result = closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(&v32, a7, (uint64_t)a4, v14, a1, a2, a3, v21, a6);
      v15 += 32;
      if (v13 == (const vImage_Buffer *)++v14)
        return result;
    }
    __break(1u);
LABEL_19:
    __break(1u);
LABEL_20:
    __break(1u);
LABEL_21:
    __break(1u);
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  return result;
}

const vImage_Buffer *closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(vImage_Buffer *a1, float a2, uint64_t a3, unint64_t a4, uint64_t a5, uint64_t a6, Pixel_16U *a7, uint64_t a8, uint64_t a9)
{
  uint64_t v15;
  uint64_t v16;
  __int128 v18;
  __int128 v19;
  vImage_Buffer dest;
  uint64_t v21;

  v21 = *MEMORY[0x1E0C80C00];
  type metadata accessor for vImage.PixelBuffer(0, a8, *(_QWORD *)(a9 + 8), a5);
  v15 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a4 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  if (*(_QWORD *)(v15 + 16) <= a4)
    goto LABEL_5;
  v16 = v15 + 32 * a4;
  v18 = *(_OWORD *)(v16 + 48);
  v19 = *(_OWORD *)(v16 + 32);
  swift_bridgeObjectRelease();
  *(_OWORD *)&dest.data = v19;
  *(_OWORD *)&dest.width = v18;
  return closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(&dest, a1, a5, a6, a7, a2);
}

const vImage_Buffer *closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(const vImage_Buffer *dest, vImage_Buffer *src, uint64_t a3, uint64_t a4, Pixel_16U *a5, float a6)
{
  unint64_t v6;
  unint64_t kernelY_width;
  Pixel_16U backgroundColor;
  vImage_Flags flags;

  v6 = *(_QWORD *)(a3 + 16);
  if (HIDWORD(v6))
  {
    __break(1u);
    goto LABEL_8;
  }
  kernelY_width = *(_QWORD *)(a4 + 16);
  if (HIDWORD(kernelY_width))
  {
LABEL_8:
    __break(1u);
    return dest;
  }
  if (*((_BYTE *)a5 + 2) == 1)
  {
    backgroundColor = 0;
    flags = dword_1CAB60E54[(__int16)*a5];
  }
  else
  {
    backgroundColor = *a5;
    flags = 4;
  }
  return (const vImage_Buffer *)vImageSepConvolve_Planar8(src, dest, 0, 0, 0, (const float *)(a3 + 32), v6, (const float *)(a4 + 32), kernelY_width, a6, backgroundColor, flags);
}

uint64_t vImage.EdgeMode.backgroundColor.getter@<X0>(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  uint64_t v4;
  uint64_t v5;
  char *v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v10;

  v4 = *(_QWORD *)(a1 - 8);
  MEMORY[0x1E0C80A78](a1);
  v6 = (char *)&v10 - ((v5 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(char *))(v4 + 16))(v6);
  v7 = *(_QWORD *)(a1 + 16);
  v8 = *(_QWORD *)(v7 - 8);
  if ((*(unsigned int (**)(char *, uint64_t, uint64_t))(v8 + 48))(v6, 3, v7))
  {
    (*(void (**)(uint64_t, uint64_t, uint64_t, uint64_t))(v8 + 56))(a2, 1, 1, v7);
    return (*(uint64_t (**)(char *, uint64_t))(v4 + 8))(v6, a1);
  }
  else
  {
    (*(void (**)(uint64_t, char *, uint64_t))(v8 + 32))(a2, v6, v7);
    return (*(uint64_t (**)(uint64_t, _QWORD, uint64_t, uint64_t))(v8 + 56))(a2, 0, 1, v7);
  }
}

void vImage.EdgeMode.vImageFlags.getter(uint64_t a1)
{
  uint64_t v2;
  char *v3;
  uint64_t v4;
  char *v5;
  uint64_t v6;

  MEMORY[0x1E0C80A78](a1);
  v3 = (char *)&v6 - ((v2 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(char *))(v4 + 16))(v3);
  v5 = (char *)sub_1CAB09768
     + 4
     * byte_1CAB60D60[(*(unsigned int (**)(char *, uint64_t, _QWORD))(*(_QWORD *)(*(_QWORD *)(a1 + 16) - 8)
                                                                              + 48))(v3, 3, *(_QWORD *)(a1 + 16))];
  __asm { BR              X10 }
}

uint64_t sub_1CAB09768()
{
  uint64_t v0;
  uint64_t v1;
  uint64_t v2;

  (*(void (**)(uint64_t, uint64_t))(v2 + 8))(v0, v1);
  return 4;
}

const vImage_Buffer *vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(const vImage_Buffer *result, uint64_t a2, Pixel_16U *a3, uint64_t a4, float a5)
{
  uint64_t v5;
  _QWORD *v6;
  vImagePixelCount v7;
  vImagePixelCount v8;
  _QWORD *v9;
  uint64_t v10;
  uint64_t v11;
  void *v12;
  void *v13;
  size_t v14;
  size_t v15;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v18;

  v18 = *MEMORY[0x1E0C80C00];
  v6 = *(_QWORD **)v5;
  if (!*(_QWORD *)(*(_QWORD *)v5 + 16))
  {
    __break(1u);
    goto LABEL_22;
  }
  v7 = v6[6];
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v8 = v6[5];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v8)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  v9 = *(_QWORD **)a4;
  if (!*(_QWORD *)(*(_QWORD *)a4 + 16))
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  v10 = v9[6];
  if (v10 < 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v11 = v9[5];
  if (v11 < 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (!v10)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v11)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (v7 != v10)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (v8 != v11)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if ((result->width & 1) == 0)
  {
LABEL_33:
    __break(1u);
LABEL_34:
    __break(1u);
  }
  if ((*(_BYTE *)(a2 + 16) & 1) == 0)
    goto LABEL_34;
  v12 = (void *)v6[4];
  v13 = (void *)v9[4];
  if (v12)
  {
    if (!v13 || v12 != v13)
      goto LABEL_20;
    __break(1u);
  }
  if (v13)
  {
LABEL_20:
    v14 = v6[7];
    src.data = v12;
    src.height = v8;
    src.width = v7;
    src.rowBytes = v14;
    v15 = v9[7];
    dest.data = v13;
    dest.height = v8;
    dest.width = v7;
    dest.rowBytes = v15;
    return closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(&dest, &src, (uint64_t)result, a2, a3, a5);
  }
  __break(1u);
  return result;
}

const vImage_Buffer *vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:useFloat16Accumulator:destination:)(const vImage_Buffer *result, uint64_t a2, Pixel_16F *a3, char a4, uint64_t a5, float a6)
{
  uint64_t v6;
  _QWORD *v7;
  vImagePixelCount v8;
  vImagePixelCount v9;
  _QWORD *v10;
  uint64_t v11;
  uint64_t v12;
  void *v13;
  void *v14;
  unsigned int v15;
  size_t v16;
  size_t v17;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v20;

  v20 = *MEMORY[0x1E0C80C00];
  v7 = *(_QWORD **)v6;
  if (!*(_QWORD *)(*(_QWORD *)v6 + 16))
  {
    __break(1u);
    goto LABEL_25;
  }
  v8 = v7[6];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  v9 = v7[5];
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v8)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (!v9)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  v10 = *(_QWORD **)a5;
  if (!*(_QWORD *)(*(_QWORD *)a5 + 16))
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v11)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v12)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (v8 != v11)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (v9 != v12)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if ((result->width & 1) == 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if ((*(_BYTE *)(a2 + 16) & 1) == 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v13 = (void *)v7[4];
  v14 = (void *)v10[4];
  if (v13)
  {
    if (v14)
    {
      if (v13 != v14)
      {
        if ((a4 & 1) == 0)
        {
LABEL_19:
          v15 = 0;
LABEL_23:
          v16 = v7[7];
          src.data = v13;
          src.height = v9;
          src.width = v8;
          src.rowBytes = v16;
          v17 = v10[7];
          dest.data = v14;
          dest.height = v9;
          dest.width = v8;
          dest.rowBytes = v17;
          return closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:useFloat16Accumulator:destination:)(&dest, &src, (uint64_t)result, a2, a3, v15, a6);
        }
LABEL_22:
        v15 = 4096;
        goto LABEL_23;
      }
LABEL_38:
      __break(1u);
    }
LABEL_21:
    if ((a4 & 1) == 0)
      goto LABEL_19;
    goto LABEL_22;
  }
  if (v14)
    goto LABEL_21;
  __break(1u);
  return result;
}

const vImage_Buffer *closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:useFloat16Accumulator:destination:)(const vImage_Buffer *dest, vImage_Buffer *src, uint64_t a3, uint64_t a4, Pixel_16F *a5, unsigned int a6, float a7)
{
  unint64_t v7;
  unint64_t kernelY_width;
  Pixel_16F backgroundColor;
  int v10;

  v7 = *(_QWORD *)(a3 + 16);
  if (HIDWORD(v7))
  {
    __break(1u);
    goto LABEL_8;
  }
  kernelY_width = *(_QWORD *)(a4 + 16);
  if (HIDWORD(kernelY_width))
  {
LABEL_8:
    __break(1u);
    return dest;
  }
  if (*((_BYTE *)a5 + 2) == 1)
  {
    backgroundColor = 0;
    v10 = dword_1CAB60E54[(__int16)*a5];
  }
  else
  {
    backgroundColor = *a5;
    v10 = 4;
  }
  return (const vImage_Buffer *)vImageSepConvolve_Planar16F(src, dest, 0, 0, 0, (const float *)(a3 + 32), v7, (const float *)(a4 + 32), kernelY_width, a7, backgroundColor, v10 | a6);
}

const vImage_Buffer *vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(uint64_t a1, uint64_t a2, int *a3, uint64_t *a4, uint64_t a5, uint64_t a6, float a7)
{
  uint64_t *v7;
  uint64_t v11;
  const vImage_Buffer *result;
  const vImage_Buffer *v13;
  unint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  __int128 v26;
  __int128 v27;
  uint64_t v28;
  __int128 v29;
  uint64_t v30;
  uint64_t v31;
  vImage_Buffer v32;
  uint64_t v33;

  v33 = *MEMORY[0x1E0C80C00];
  v11 = *v7;
  v30 = *a4;
  v31 = v11;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&v32);
  vImage.PixelBuffer.size.getter(&v29);
  v28 = v11;
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  if (*(_OWORD *)&v32.data != v29)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if ((*(_BYTE *)(a1 + 16) & 1) == 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if ((*(_BYTE *)(a2 + 16) & 1) == 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  v21 = *(_QWORD *)(a5 + 16);
  result = (const vImage_Buffer *)(*(uint64_t (**)(void))(a6 + 32))();
  if (((unint64_t)result & 0x8000000000000000) != 0)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  v13 = result;
  if (result)
  {
    v14 = 0;
    v15 = 32;
    while (v13 != (const vImage_Buffer *)v14)
    {
      *(_QWORD *)&v29 = v28;
      swift_bridgeObjectRetain();
      swift_bridgeObjectRetain();
      v16 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v14 >= *(_QWORD *)(v16 + 16))
        goto LABEL_19;
      v17 = *(_QWORD *)(v16 + v15);
      swift_bridgeObjectRelease();
      v18 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v14 >= *(_QWORD *)(v18 + 16))
        goto LABEL_20;
      v19 = *(_QWORD *)(v18 + v15);
      swift_bridgeObjectRelease();
      swift_bridgeObjectRelease();
      swift_bridgeObjectRelease();
      if (v17)
      {
        if (v19 && v17 == v19)
          goto LABEL_22;
      }
      else if (!v19)
      {
        goto LABEL_27;
      }
      v20 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v14 >= *(_QWORD *)(v20 + 16))
        goto LABEL_21;
      v26 = *(_OWORD *)(v20 + v15 + 16);
      v27 = *(_OWORD *)(v20 + v15);
      swift_bridgeObjectRelease();
      *(_OWORD *)&v32.data = v27;
      *(_OWORD *)&v32.width = v26;
      result = closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(&v32, a7, (uint64_t)a4, v14, a1, a2, a3, v21, a6);
      v15 += 32;
      if (v13 == (const vImage_Buffer *)++v14)
        return result;
    }
    __break(1u);
LABEL_19:
    __break(1u);
LABEL_20:
    __break(1u);
LABEL_21:
    __break(1u);
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  return result;
}

const vImage_Buffer *closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(vImage_Buffer *a1, float a2, uint64_t a3, unint64_t a4, uint64_t a5, uint64_t a6, int *a7, uint64_t a8, uint64_t a9)
{
  uint64_t v15;
  uint64_t v16;
  __int128 v18;
  __int128 v19;
  vImage_Buffer dest;
  uint64_t v21;

  v21 = *MEMORY[0x1E0C80C00];
  type metadata accessor for vImage.PixelBuffer(0, a8, *(_QWORD *)(a9 + 8), a5);
  v15 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a4 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  if (*(_QWORD *)(v15 + 16) <= a4)
    goto LABEL_5;
  v16 = v15 + 32 * a4;
  v18 = *(_OWORD *)(v16 + 48);
  v19 = *(_OWORD *)(v16 + 32);
  swift_bridgeObjectRelease();
  *(_OWORD *)&dest.data = v19;
  *(_OWORD *)&dest.width = v18;
  return closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(&dest, a1, a5, a6, a7, a2);
}

const vImage_Buffer *closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(const vImage_Buffer *dest, vImage_Buffer *src, uint64_t a3, uint64_t a4, int *a5, float a6)
{
  unint64_t v6;
  unint64_t kernelY_width;
  vImage_Flags v8;
  Pixel_F v9;

  v6 = *(_QWORD *)(a3 + 16);
  if (HIDWORD(v6))
  {
    __break(1u);
    goto LABEL_8;
  }
  kernelY_width = *(_QWORD *)(a4 + 16);
  if (HIDWORD(kernelY_width))
  {
LABEL_8:
    __break(1u);
    return dest;
  }
  if (*((_BYTE *)a5 + 4) == 1)
  {
    v8 = dword_1CAB60E54[*a5];
    v9 = 0.0;
  }
  else
  {
    v9 = *(float *)a5;
    v8 = 4;
  }
  return (const vImage_Buffer *)vImageSepConvolve_PlanarF(src, dest, 0, 0, 0, (const float *)(a3 + 32), v6, (const float *)(a4 + 32), kernelY_width, a6, v9, v8);
}

const vImage_Buffer *vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(const vImage_Buffer *result, uint64_t a2, int *a3, uint64_t a4, float a5)
{
  uint64_t v5;
  _QWORD *v6;
  vImagePixelCount v7;
  vImagePixelCount v8;
  _QWORD *v9;
  uint64_t v10;
  uint64_t v11;
  void *v12;
  void *v13;
  size_t v14;
  size_t v15;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v18;

  v18 = *MEMORY[0x1E0C80C00];
  v6 = *(_QWORD **)v5;
  if (!*(_QWORD *)(*(_QWORD *)v5 + 16))
  {
    __break(1u);
    goto LABEL_22;
  }
  v7 = v6[6];
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v8 = v6[5];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v8)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  v9 = *(_QWORD **)a4;
  if (!*(_QWORD *)(*(_QWORD *)a4 + 16))
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  v10 = v9[6];
  if (v10 < 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v11 = v9[5];
  if (v11 < 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (!v10)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v11)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (v7 != v10)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (v8 != v11)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if ((result->width & 1) == 0)
  {
LABEL_33:
    __break(1u);
LABEL_34:
    __break(1u);
  }
  if ((*(_BYTE *)(a2 + 16) & 1) == 0)
    goto LABEL_34;
  v12 = (void *)v6[4];
  v13 = (void *)v9[4];
  if (v12)
  {
    if (!v13 || v12 != v13)
      goto LABEL_20;
    __break(1u);
  }
  if (v13)
  {
LABEL_20:
    v14 = v6[7];
    src.data = v12;
    src.height = v8;
    src.width = v7;
    src.rowBytes = v14;
    v15 = v9[7];
    dest.data = v13;
    dest.height = v8;
    dest.width = v7;
    dest.rowBytes = v15;
    return closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(&dest, &src, (uint64_t)result, a2, a3, a5);
  }
  __break(1u);
  return result;
}

unint64_t *vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:)(unint64_t *result, char *a2, uint64_t a3)
{
  uint64_t v3;
  _QWORD *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  void *v10;
  void *v11;
  unint64_t v12;
  unint64_t v13;
  uint64_t v14;
  char v15;
  size_t v16;
  size_t v17;
  Pixel_8 v18;
  vImage_Flags flags;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  v4 = *(_QWORD **)v3;
  if (!*(_QWORD *)(*(_QWORD *)v3 + 16))
  {
    __break(1u);
    goto LABEL_27;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (!v5)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v6)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v7 = *(_QWORD **)a3;
  if (!*(_QWORD *)(*(_QWORD *)a3 + 16))
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v8)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (!v9)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (v5 != v8)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (v6 != v9)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v10 = (void *)v4[4];
  v11 = (void *)v7[4];
  if (v10)
  {
    if (!v11 || v10 != v11)
      goto LABEL_18;
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_18:
  v12 = *result;
  if ((*result & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (HIDWORD(v12))
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v13 = result[1];
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_40:
    __break(1u);
LABEL_41:
    __break(1u);
  }
  if (HIDWORD(v13))
    goto LABEL_41;
  v14 = *a2;
  v15 = a2[1];
  v16 = v4[7];
  src.data = v10;
  src.height = v6;
  src.width = v5;
  src.rowBytes = v16;
  v17 = v7[7];
  dest.data = v11;
  dest.height = v6;
  dest.width = v5;
  dest.rowBytes = v17;
  if ((v15 & 1) != 0)
  {
    v18 = 0;
    flags = dword_1CAB60E54[v14];
  }
  else
  {
    v18 = v14;
    flags = 4;
  }
  return (unint64_t *)vImageBoxConvolve_Planar8(&src, &dest, 0, 0, 0, v13 | 1, v12 | 1, v18, flags);
}

unint64_t *vImage.PixelBuffer<>.tentConvolve(kernelSize:edgeMode:destination:)(unint64_t *result, char *a2, uint64_t a3)
{
  uint64_t v3;
  _QWORD *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  void *v10;
  void *v11;
  unint64_t v12;
  unint64_t v13;
  uint64_t v14;
  char v15;
  size_t v16;
  size_t v17;
  Pixel_8 v18;
  vImage_Flags flags;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  v4 = *(_QWORD **)v3;
  if (!*(_QWORD *)(*(_QWORD *)v3 + 16))
  {
    __break(1u);
    goto LABEL_27;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (!v5)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v6)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v7 = *(_QWORD **)a3;
  if (!*(_QWORD *)(*(_QWORD *)a3 + 16))
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v8)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (!v9)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (v5 != v8)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (v6 != v9)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v10 = (void *)v4[4];
  v11 = (void *)v7[4];
  if (v10)
  {
    if (!v11 || v10 != v11)
      goto LABEL_18;
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_18:
  v12 = *result;
  if ((*result & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (HIDWORD(v12))
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v13 = result[1];
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_40:
    __break(1u);
LABEL_41:
    __break(1u);
  }
  if (HIDWORD(v13))
    goto LABEL_41;
  v14 = *a2;
  v15 = a2[1];
  v16 = v4[7];
  src.data = v10;
  src.height = v6;
  src.width = v5;
  src.rowBytes = v16;
  v17 = v7[7];
  dest.data = v11;
  dest.height = v6;
  dest.width = v5;
  dest.rowBytes = v17;
  if ((v15 & 1) != 0)
  {
    v18 = 0;
    flags = dword_1CAB60E54[v14];
  }
  else
  {
    v18 = v14;
    flags = 4;
  }
  return (unint64_t *)vImageTentConvolve_Planar8(&src, &dest, 0, 0, 0, v13 | 1, v12 | 1, v18, flags);
}

uint64_t vImage.PixelBuffer<>.boxConvolved(kernelSize:edgeMode:)@<X0>(uint64_t *a1@<X0>, int *a2@<X1>, uint64_t *a3@<X8>)
{
  return vImage.PixelBuffer<>.boxConvolved(kernelSize:edgeMode:)(a1, a2, (uint64_t (*)(_QWORD *, int *, uint64_t *))vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:), a3);
}

unint64_t *vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:)(unint64_t *result, int *a2, uint64_t a3)
{
  uint64_t v3;
  _QWORD *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  void *v10;
  void *v11;
  unint64_t v12;
  unint64_t v13;
  uint64_t v14;
  int v15;
  uint64_t v16;
  void *v17;
  uint64_t v18;
  size_t v19;
  size_t v20;
  vImage_Flags flags;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v24;

  v24 = *MEMORY[0x1E0C80C00];
  v4 = *(_QWORD **)v3;
  if (!*(_QWORD *)(*(_QWORD *)v3 + 16))
  {
    __break(1u);
    goto LABEL_30;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v5)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v6)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  v7 = *(_QWORD **)a3;
  if (!*(_QWORD *)(*(_QWORD *)a3 + 16))
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v8)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v9)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (v5 != v8)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v6 != v9)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v10 = (void *)v4[4];
  v11 = (void *)v7[4];
  if (v10)
  {
    if (!v11 || v10 != v11)
      goto LABEL_18;
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_18:
  v12 = *result;
  if ((*result & 0x8000000000000000) != 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (HIDWORD(v12))
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v13 = result[1];
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_43:
    __break(1u);
LABEL_44:
    __break(1u);
  }
  if (HIDWORD(v13))
    goto LABEL_44;
  v14 = *a2;
  v15 = *((unsigned __int8 *)a2 + 4);
  if ((v15 & 1) != 0)
  {
    v16 = MEMORY[0x1E0DEE9D8];
  }
  else
  {
    v17 = v11;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    v18 = swift_allocObject();
    v11 = v17;
    v16 = v18;
    *(_OWORD *)(v18 + 16) = xmmword_1CAB5F170;
    *(_WORD *)(v18 + 32) = v14;
    *(_BYTE *)(v18 + 34) = BYTE2(v14);
    *(_BYTE *)(v18 + 35) = BYTE3(v14);
  }
  v19 = v4[7];
  src.data = v10;
  src.height = v6;
  src.width = v5;
  src.rowBytes = v19;
  v20 = v7[7];
  dest.data = v11;
  dest.height = v6;
  dest.width = v5;
  dest.rowBytes = v20;
  if (v15)
    flags = dword_1CAB60E54[v14];
  else
    flags = 4;
  vImageBoxConvolve_ARGB8888(&src, &dest, 0, 0, 0, v13 | 1, v12 | 1, (const uint8_t *)(v16 + 32), flags);
  return (unint64_t *)swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.tentConvolved(kernelSize:edgeMode:)@<X0>(uint64_t *a1@<X0>, int *a2@<X1>, uint64_t *a3@<X8>)
{
  return vImage.PixelBuffer<>.boxConvolved(kernelSize:edgeMode:)(a1, a2, (uint64_t (*)(_QWORD *, int *, uint64_t *))vImage.PixelBuffer<>.tentConvolve(kernelSize:edgeMode:destination:), a3);
}

uint64_t vImage.PixelBuffer<>.boxConvolved(kernelSize:edgeMode:)@<X0>(uint64_t *a1@<X0>, int *a2@<X1>, uint64_t (*a3)(_QWORD *, int *, uint64_t *)@<X2>, uint64_t *a4@<X8>)
{
  uint64_t *v4;
  vImagePixelCount v5;
  uint64_t v6;
  vImagePixelCount v7;
  uint64_t v8;
  uint64_t v9;
  int v10;
  uint64_t v11;
  void *v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  _QWORD *v19;
  uint64_t result;
  char v21;
  uint64_t v24;
  int v25;
  char v26;
  _QWORD v27[4];

  v6 = *v4;
  if (!*(_QWORD *)(*v4 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  v7 = *(_QWORD *)(v6 + 48);
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v5 = *(_QWORD *)(v6 + 40);
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (!v7)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (v5)
  {
    v9 = *a1;
    v8 = a1[1];
    v10 = *a2;
    v21 = *((_BYTE *)a2 + 4);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    v11 = swift_allocObject();
    *(_OWORD *)(v11 + 16) = xmmword_1CAB5E430;
    v12 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v7, v5, 0x20u);
    v14 = v13;
    v16 = v15;
    v18 = v17;
    type metadata accessor for vImage.BufferReference();
    v19 = (_QWORD *)swift_allocObject();
    v19[2] = v12;
    v19[3] = v14;
    v19[4] = v16;
    v19[5] = v18;
    *(_QWORD *)(v11 + 32) = v12;
    *(_QWORD *)(v11 + 40) = v14;
    *(_QWORD *)(v11 + 48) = v16;
    *(_QWORD *)(v11 + 56) = v18;
    *(_QWORD *)(v11 + 64) = v19;
    v27[1] = v8;
    v27[2] = v6;
    v27[0] = v9;
    v25 = v10;
    v26 = v21;
    v24 = v11;
    result = a3(v27, &v25, &v24);
    *a4 = v11;
    return result;
  }
LABEL_11:
  __break(1u);

  result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

unint64_t *vImage.PixelBuffer<>.tentConvolve(kernelSize:edgeMode:destination:)(unint64_t *result, int *a2, uint64_t a3)
{
  uint64_t v3;
  _QWORD *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  void *v10;
  void *v11;
  unint64_t v12;
  unint64_t v13;
  uint64_t v14;
  int v15;
  uint64_t v16;
  void *v17;
  uint64_t v18;
  size_t v19;
  size_t v20;
  vImage_Flags flags;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v24;

  v24 = *MEMORY[0x1E0C80C00];
  v4 = *(_QWORD **)v3;
  if (!*(_QWORD *)(*(_QWORD *)v3 + 16))
  {
    __break(1u);
    goto LABEL_30;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v5)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v6)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  v7 = *(_QWORD **)a3;
  if (!*(_QWORD *)(*(_QWORD *)a3 + 16))
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v8)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v9)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (v5 != v8)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v6 != v9)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v10 = (void *)v4[4];
  v11 = (void *)v7[4];
  if (v10)
  {
    if (!v11 || v10 != v11)
      goto LABEL_18;
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_18:
  v12 = *result;
  if ((*result & 0x8000000000000000) != 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (HIDWORD(v12))
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  v13 = result[1];
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_43:
    __break(1u);
LABEL_44:
    __break(1u);
  }
  if (HIDWORD(v13))
    goto LABEL_44;
  v14 = *a2;
  v15 = *((unsigned __int8 *)a2 + 4);
  if ((v15 & 1) != 0)
  {
    v16 = MEMORY[0x1E0DEE9D8];
  }
  else
  {
    v17 = v11;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    v18 = swift_allocObject();
    v11 = v17;
    v16 = v18;
    *(_OWORD *)(v18 + 16) = xmmword_1CAB5F170;
    *(_WORD *)(v18 + 32) = v14;
    *(_BYTE *)(v18 + 34) = BYTE2(v14);
    *(_BYTE *)(v18 + 35) = BYTE3(v14);
  }
  v19 = v4[7];
  src.data = v10;
  src.height = v6;
  src.width = v5;
  src.rowBytes = v19;
  v20 = v7[7];
  dest.data = v11;
  dest.height = v6;
  dest.width = v5;
  dest.rowBytes = v20;
  if (v15)
    flags = dword_1CAB60E54[v14];
  else
    flags = 4;
  vImageTentConvolve_ARGB8888(&src, &dest, 0, 0, 0, v13 | 1, v12 | 1, (const uint8_t *)(v16 + 32), flags);
  return (unint64_t *)swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:)(unint64_t *a1, unsigned __int8 *a2, uint64_t *a3, uint64_t a4, uint64_t a5)
{
  return vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:)(a1, a2, a3, a4, a5, (uint64_t (*)(_OWORD *, uint64_t *, unint64_t, _QWORD, _QWORD, _QWORD, unsigned __int8 *, uint64_t, uint64_t))closure #1 in vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:));
}

vImage_Error closure #1 in vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:)(const vImage_Buffer *a1, uint64_t a2, unint64_t a3, uint64_t a4, uint32_t a5, Pixel_8 a6, char *a7, uint64_t a8, uint64_t a9)
{
  uint32_t v12;
  uint64_t v15;
  uint64_t v16;
  vImage_Flags flags;
  __int128 v19;
  __int128 v20;
  vImage_Buffer dest;
  uint64_t v22;

  v12 = a4;
  v22 = *MEMORY[0x1E0C80C00];
  type metadata accessor for vImage.PixelBuffer(0, a8, *(_QWORD *)(a9 + 8), a4);
  v15 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_8:
    __break(1u);
  }
  if (*(_QWORD *)(v15 + 16) <= a3)
    goto LABEL_8;
  v16 = v15 + 32 * a3;
  v19 = *(_OWORD *)(v16 + 48);
  v20 = *(_OWORD *)(v16 + 32);
  swift_bridgeObjectRelease();
  *(_OWORD *)&dest.data = v20;
  *(_OWORD *)&dest.width = v19;
  if (a7[1] == 1)
    flags = dword_1CAB60E54[*a7];
  else
    flags = 4;
  return vImageBoxConvolve_Planar8(a1, &dest, 0, 0, 0, v12, a5, a6, flags);
}

uint64_t vImage.PixelBuffer<>.tentConvolve(kernelSize:edgeMode:destination:)(unint64_t *a1, unsigned __int8 *a2, uint64_t *a3, uint64_t a4, uint64_t a5)
{
  return vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:)(a1, a2, a3, a4, a5, (uint64_t (*)(_OWORD *, uint64_t *, unint64_t, _QWORD, _QWORD, _QWORD, unsigned __int8 *, uint64_t, uint64_t))closure #1 in vImage.PixelBuffer<>.tentConvolve(kernelSize:edgeMode:destination:));
}

uint64_t vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:)(unint64_t *a1, unsigned __int8 *a2, uint64_t *a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(_OWORD *, uint64_t *, unint64_t, _QWORD, _QWORD, _QWORD, unsigned __int8 *, uint64_t, uint64_t))
{
  uint64_t *v6;
  uint64_t v10;
  unint64_t v11;
  unint64_t v12;
  unsigned int v13;
  uint64_t result;
  uint64_t v15;
  unint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  unsigned int v23;
  unsigned int v24;
  uint64_t v25;
  unsigned int v26;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  uint64_t v33;
  uint64_t v34;
  _OWORD v35[2];
  uint64_t v36;

  v36 = *MEMORY[0x1E0C80C00];
  v10 = *v6;
  v33 = *a3;
  v34 = v10;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(v35);
  vImage.PixelBuffer.size.getter(&v32);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  if (v35[0] != v32)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v11 = *a1;
  if ((*a1 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (HIDWORD(v11))
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  v12 = a1[1];
  if ((v12 & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (HIDWORD(v12))
  {
LABEL_31:
    __break(1u);
LABEL_32:
    __break(1u);
LABEL_33:
    __break(1u);
  }
  v13 = *a2;
  if (a2[1])
    v13 = 0;
  v26 = v13;
  v25 = *(_QWORD *)(a4 + 16);
  result = (*(uint64_t (**)(void))(a5 + 32))();
  if (result < 0)
    goto LABEL_32;
  v15 = result;
  if (result)
  {
    v16 = 0;
    v23 = v12 | 1;
    v24 = v11 | 1;
    v17 = 32;
    while (v15 != v16)
    {
      *(_QWORD *)&v32 = v10;
      swift_bridgeObjectRetain();
      swift_bridgeObjectRetain();
      v18 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v16 >= *(_QWORD *)(v18 + 16))
        goto LABEL_23;
      v19 = *(_QWORD *)(v18 + v17);
      swift_bridgeObjectRelease();
      v20 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v16 >= *(_QWORD *)(v20 + 16))
        goto LABEL_24;
      v21 = *(_QWORD *)(v20 + v17);
      swift_bridgeObjectRelease();
      swift_bridgeObjectRelease();
      swift_bridgeObjectRelease();
      if (v19)
      {
        if (v21 && v19 == v21)
          goto LABEL_26;
      }
      else if (!v21)
      {
        goto LABEL_33;
      }
      v22 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v16 >= *(_QWORD *)(v22 + 16))
        goto LABEL_25;
      v30 = *(_OWORD *)(v22 + v17 + 16);
      v31 = *(_OWORD *)(v22 + v17);
      swift_bridgeObjectRelease();
      v35[0] = v31;
      v35[1] = v30;
      result = a6(v35, a3, v16, v23, v24, v26, a2, v25, a5);
      v17 += 32;
      if (v15 == ++v16)
        return result;
    }
    __break(1u);
LABEL_23:
    __break(1u);
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  return result;
}

vImage_Error closure #1 in vImage.PixelBuffer<>.tentConvolve(kernelSize:edgeMode:destination:)(const vImage_Buffer *a1, uint64_t a2, unint64_t a3, uint64_t a4, uint32_t a5, Pixel_8 a6, char *a7, uint64_t a8, uint64_t a9)
{
  uint32_t v12;
  uint64_t v15;
  uint64_t v16;
  vImage_Flags flags;
  __int128 v19;
  __int128 v20;
  vImage_Buffer dest;
  uint64_t v22;

  v12 = a4;
  v22 = *MEMORY[0x1E0C80C00];
  type metadata accessor for vImage.PixelBuffer(0, a8, *(_QWORD *)(a9 + 8), a4);
  v15 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_8:
    __break(1u);
  }
  if (*(_QWORD *)(v15 + 16) <= a3)
    goto LABEL_8;
  v16 = v15 + 32 * a3;
  v19 = *(_OWORD *)(v16 + 48);
  v20 = *(_OWORD *)(v16 + 32);
  swift_bridgeObjectRelease();
  *(_OWORD *)&dest.data = v20;
  *(_OWORD *)&dest.width = v19;
  if (a7[1] == 1)
    flags = dword_1CAB60E54[*a7];
  else
    flags = 4;
  return vImageTentConvolve_Planar8(a1, &dest, 0, 0, 0, v12, a5, a6, flags);
}

uint64_t vImage.ConvolutionKernel2D.width.getter()
{
  uint64_t v0;

  return *(_QWORD *)v0;
}

uint64_t vImage.ConvolutionKernel2D.height.getter()
{
  uint64_t v0;

  return *(_QWORD *)(v0 + 8);
}

uint64_t vImage.ConvolutionKernel2D.values.getter()
{
  return swift_bridgeObjectRetain();
}

uint64_t vImage.ConvolutionKernel2D.init(values:width:height:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, unint64_t *a5@<X8>)
{
  unint64_t v5;
  unint64_t v6;
  uint64_t v7;

  v5 = a3 & ~(a3 >> 63);
  if ((v5 & 1) == 0 || a3 < 1 || a2 < 1 || (v6 = a2 & ~(a2 >> 63), (v6 & 1) == 0))
  {
    __break(1u);
    goto LABEL_10;
  }
  v7 = result;
  result = MEMORY[0x1D1794114](result, a4);
  if (!is_mul_ok(v6, v5))
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if ((result & 0x8000000000000000) == 0 && result == v6 * v5)
  {
    *a5 = v6;
    a5[1] = v5;
    a5[2] = v7;
    return result;
  }
LABEL_11:
  __break(1u);
  return result;
}

double vImage.ConvolutionKernel2D.init(values:size:)@<D0>(uint64_t a1@<X0>, uint64_t *a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X8>)
{
  uint64_t v5;
  double result;
  __int128 v7;
  uint64_t v8;

  vImage.ConvolutionKernel2D.init(values:width:height:)(a1, *a2, a2[1], a3, (unint64_t *)&v7);
  v5 = v8;
  result = *(double *)&v7;
  *(_OWORD *)a4 = v7;
  *(_QWORD *)(a4 + 16) = v5;
  return result;
}

uint64_t vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(uint64_t result, unint64_t a2, uint64_t a3, uint64_t a4)
{
  _QWORD *v4;
  _QWORD *v5;
  _QWORD *v6;
  _QWORD *v9;
  void *v10;
  void *v11;
  BOOL v12;
  vImagePixelCount v13;
  vImagePixelCount v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  int v18;
  _QWORD *v19;
  uint64_t v20;
  size_t v21;
  size_t v22;
  int v23;
  int v24;
  int v25;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v28;

  v28 = *MEMORY[0x1E0C80C00];
  v5 = (_QWORD *)*v4;
  if (!v5[2])
  {
    __break(1u);
    goto LABEL_26;
  }
  v6 = *(_QWORD **)a4;
  if (!*(_QWORD *)(*(_QWORD *)a4 + 16))
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  v9 = (_QWORD *)result;
  v10 = (void *)v5[4];
  v11 = (void *)v6[4];
  if (v10)
  {
    if (v11)
      v12 = v10 == v11;
    else
      v12 = 0;
    if (!v12)
      goto LABEL_11;
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_11:
  v13 = v5[6];
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v14 = v5[5];
  if ((v14 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (!v13)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v14)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v15 = v6[6];
  if (v15 < 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v16 = v6[5];
  if (v16 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v15)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v16)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (v13 != v15)
  {
LABEL_35:
    __break(1u);
LABEL_36:
    __break(1u);
  }
  if (v14 != v16)
    goto LABEL_36;
  if ((*(_BYTE *)(a3 + 4) & 1) != 0)
  {
    v17 = MEMORY[0x1E0DEE9D8];
  }
  else
  {
    v18 = *(_DWORD *)a3;
    v24 = HIWORD(*(_DWORD *)a3);
    v25 = *(_DWORD *)a3 >> 8;
    v23 = HIBYTE(*(_DWORD *)a3);
    v19 = (_QWORD *)result;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    v20 = swift_allocObject();
    v9 = v19;
    v17 = v20;
    *(_OWORD *)(v20 + 16) = xmmword_1CAB5F170;
    *(_BYTE *)(v20 + 32) = v18;
    *(_BYTE *)(v20 + 33) = v25;
    *(_BYTE *)(v20 + 34) = v24;
    *(_BYTE *)(v20 + 35) = v23;
  }
  v21 = v5[7];
  src.data = v10;
  src.height = v14;
  src.width = v13;
  src.rowBytes = v21;
  v22 = v6[7];
  dest.data = v11;
  dest.height = v14;
  dest.width = v13;
  dest.rowBytes = v22;
  closure #1 in closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(&dest, &src, v9, a2 | ((HIDWORD(a2) & 1) << 32), v17, (int *)a3);
  return swift_bridgeObjectRelease();
}

const vImage_Buffer *closure #1 in closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(const vImage_Buffer *dest, vImage_Buffer *src, _QWORD *a3, uint64_t a4, uint64_t a5, int *a6)
{
  unint64_t v6;
  vImage_Flags flags;
  float v8;

  v6 = a3[1];
  if (HIDWORD(v6))
  {
    __break(1u);
    goto LABEL_10;
  }
  if (HIDWORD(*a3))
  {
LABEL_10:
    __break(1u);
    return dest;
  }
  if (*((_BYTE *)a6 + 4) == 1)
    flags = dword_1CAB60E54[*a6];
  else
    flags = 4;
  v8 = *(float *)&a4;
  if ((a4 & 0x100000000) != 0)
    v8 = 0.0;
  return (const vImage_Buffer *)vImageConvolveFloatKernel_ARGB8888(src, dest, 0, 0, 0, (const float *)(a3[2] + 32), v6, *a3, v8, (const uint8_t *)(a5 + 32), flags);
}

uint64_t vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, float a5)
{
  uint64_t v5;
  _QWORD *v6;
  vImagePixelCount v7;
  vImagePixelCount v8;
  _QWORD *v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v13;
  uint64_t v14;
  void *v16;
  void *v17;
  uint64_t v18;
  int v19;
  uint64_t v21;
  uint64_t v22;
  size_t v23;
  size_t v24;
  int v25;
  int v26;
  int v27;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v30;

  v30 = *MEMORY[0x1E0C80C00];
  v6 = *(_QWORD **)v5;
  if (!*(_QWORD *)(*(_QWORD *)v5 + 16))
  {
    __break(1u);
    goto LABEL_25;
  }
  v7 = v6[6];
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  v8 = v6[5];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (!v8)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  v9 = *(_QWORD **)a4;
  if (!*(_QWORD *)(*(_QWORD *)a4 + 16))
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  v10 = v9[6];
  if (v10 < 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v11 = v9[5];
  if (v11 < 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v10)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v11)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (v7 != v10)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (v8 != v11)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v13 = result;
  if ((*(_BYTE *)(result + 16) & 1) == 0)
  {
LABEL_36:
    __break(1u);
LABEL_37:
    __break(1u);
  }
  v14 = a2;
  if ((*(_BYTE *)(a2 + 16) & 1) == 0)
    goto LABEL_37;
  v16 = (void *)v6[4];
  v17 = (void *)v9[4];
  if (v16)
  {
    if (!v17 || v16 != v17)
      goto LABEL_20;
    __break(1u);
  }
  if (!v17)
  {
    __break(1u);
    return result;
  }
LABEL_20:
  if ((*(_BYTE *)(a3 + 4) & 1) != 0)
  {
    v18 = MEMORY[0x1E0DEE9D8];
  }
  else
  {
    v19 = *(_DWORD *)a3;
    v26 = HIWORD(*(_DWORD *)a3);
    v27 = *(_DWORD *)a3 >> 8;
    v25 = HIBYTE(*(_DWORD *)a3);
    v21 = result;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    v22 = swift_allocObject();
    v13 = v21;
    v14 = a2;
    v18 = v22;
    *(_OWORD *)(v22 + 16) = xmmword_1CAB5F170;
    *(_BYTE *)(v22 + 32) = v19;
    *(_BYTE *)(v22 + 33) = v27;
    *(_BYTE *)(v22 + 34) = v26;
    *(_BYTE *)(v22 + 35) = v25;
  }
  v23 = v6[7];
  src.data = v16;
  src.height = v8;
  src.width = v7;
  src.rowBytes = v23;
  v24 = v9[7];
  dest.data = v17;
  dest.height = v8;
  dest.width = v7;
  dest.rowBytes = v24;
  closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(&dest, &src, v13, v14, v18, (int *)a3, a5);
  return swift_bridgeObjectRelease();
}

const vImage_Buffer *closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(const vImage_Buffer *dest, vImage_Buffer *src, uint64_t a3, uint64_t a4, uint64_t a5, int *a6, float a7)
{
  unint64_t v7;
  unint64_t kernelY_width;
  vImage_Flags flags;

  v7 = *(_QWORD *)(a3 + 16);
  if (HIDWORD(v7))
  {
    __break(1u);
    goto LABEL_8;
  }
  kernelY_width = *(_QWORD *)(a4 + 16);
  if (HIDWORD(kernelY_width))
  {
LABEL_8:
    __break(1u);
    return dest;
  }
  if (*((_BYTE *)a6 + 4) == 1)
    flags = dword_1CAB60E54[*a6];
  else
    flags = 4;
  return (const vImage_Buffer *)vImageSepConvolve_ARGB8888(src, dest, 0, 0, 0, (const float *)(a3 + 32), v7, (const float *)(a4 + 32), kernelY_width, a7, (const uint8_t *)(a5 + 32), flags);
}

uint64_t vImage.PixelBuffer<>.convolve(with:divisor:bias:edgeMode:destination:)(uint64_t *a1, uint64_t a2, unint64_t a3, int *a4, _QWORD **a5)
{
  _QWORD **v5;
  _QWORD **v6;
  unint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  int v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t result;
  uint64_t v16;
  __int16 *v17;
  unint64_t v18;
  int v19;
  int v20;
  unint64_t v21;
  unint64_t v22;
  uint64_t v23;
  int v24;
  BOOL v25;
  uint64_t v26;
  int v27;
  uint64_t v28;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;

  v6 = a5;
  v8 = a3;
  v9 = *a1;
  v10 = a1[1];
  v11 = a1[2];
  if ((a2 & 0x100000000) == 0)
  {
    v12 = a2;
    v13 = *a1;
LABEL_15:
    v26 = *a4;
    if ((a4[1] & 1) != 0)
    {
      v27 = dword_1CAB60E54[v26];
      v28 = MEMORY[0x1E0DEE9D8];
    }
    else
    {
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
      v28 = swift_allocObject();
      *(_OWORD *)(v28 + 16) = xmmword_1CAB5F170;
      *(_WORD *)(v28 + 32) = v26;
      *(_BYTE *)(v28 + 34) = BYTE2(v26);
      *(_BYTE *)(v28 + 35) = BYTE3(v26);
      v27 = 4;
    }
    specialized vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(v13, v10, v11, v12, v8 | ((HIDWORD(v8) & 1) << 32), v28 + 32, v27, *v6, (void (*)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, int *, uint64_t *, int))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int), 0, (void (*)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, int *, int *, uint64_t *, int))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned Int32, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int), 0, *v5);
    return swift_bridgeObjectRelease();
  }
  v32 = a1[1];
  v14 = *(_QWORD *)(v11 + 16);
  if (v14)
  {
    v31 = *a1;
    v33 = MEMORY[0x1E0DEE9D8];
    swift_bridgeObjectRetain();
    result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v14, 0);
    v16 = v33;
    v17 = (__int16 *)(v11 + 32);
    v18 = *(_QWORD *)(v33 + 16);
    do
    {
      v20 = *v17++;
      v19 = v20;
      v21 = *(_QWORD *)(v33 + 24);
      v22 = v18 + 1;
      if (v18 >= v21 >> 1)
        result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v21 > 1, v18 + 1, 1);
      *(_QWORD *)(v33 + 16) = v22;
      *(_DWORD *)(v33 + 4 * v18++ + 32) = v19;
      --v14;
    }
    while (v14);
    v8 = a3;
    v9 = v31;
    v6 = a5;
  }
  else
  {
    v16 = MEMORY[0x1E0DEE9D8];
    v22 = *(_QWORD *)(MEMORY[0x1E0DEE9D8] + 16);
    result = swift_bridgeObjectRetain();
    if (!v22)
    {
      swift_bridgeObjectRelease();
      v12 = 0;
LABEL_14:
      v13 = v9;
      swift_bridgeObjectRelease();
      v10 = v32;
      goto LABEL_15;
    }
  }
  v23 = 0;
  v12 = 0;
  while (1)
  {
    v24 = *(_DWORD *)(v16 + 4 * v23 + 32);
    v25 = __OFADD__(v12, v24);
    v12 += v24;
    if (v25)
      break;
    if (v22 == ++v23)
    {
      swift_bridgeObjectRelease();
      goto LABEL_14;
    }
  }
  __break(1u);
  return result;
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const int16_t *a6, uint32_t a7, uint32_t a8, int32_t *divisor, uint8_t *backgroundColor, vImage_Flags flags)
{
  return vImageConvolve_ARGB8888(a1, a2, a3, a4, a5, a6, a7, a8, *divisor, *(const uint8_t **)backgroundColor, flags);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned Int32, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const int16_t *a6, uint32_t a7, uint32_t a8, int32_t *divisor, uint8_t *backgroundColor, const uint8_t *__attribute__((__org_typedef(Pixel_8888))) *flags, vImage_Flags a12)
{
  return vImageConvolveWithBias_ARGB8888(a1, a2, a3, a4, a5, a6, a7, a8, *divisor, *(_DWORD *)backgroundColor, *flags, a12);
}

uint64_t specialized vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t result, uint64_t a2, uint64_t a3, int a4, unint64_t a5, uint64_t a6, int a7, _QWORD *a8, void (*a9)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, int *, uint64_t *, int), uint64_t a10, void (*a11)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, int *, int *, uint64_t *, int), uint64_t a12, _QWORD *a13)
{
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  _QWORD v20[5];

  v20[4] = *MEMORY[0x1E0C80C00];
  if (!a13[2])
  {
    __break(1u);
    goto LABEL_20;
  }
  if (!a8[2])
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v13 = a13[4];
  v14 = a8[4];
  if (v13)
  {
    if (!v14 || v13 != v14)
    {
LABEL_8:
      v15 = a13[6];
      if ((v15 & 0x8000000000000000) == 0)
      {
        v16 = a13[5];
        if ((v16 & 0x8000000000000000) == 0)
        {
          if (v15)
          {
            if (v16)
            {
              v17 = a8[6];
              if ((v17 & 0x8000000000000000) == 0)
              {
                v18 = a8[5];
                if ((v18 & 0x8000000000000000) == 0)
                {
                  if (v17)
                  {
                    if (v18)
                    {
                      if (v15 == v17)
                      {
                        if (v16 == v18)
                        {
                          v19 = a13[7];
                          v20[0] = v13;
                          v20[1] = v16;
                          v20[2] = v15;
                          v20[3] = v19;
                          return specialized closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)((uint64_t)v20, (uint64_t)a8, a5 | ((HIDWORD(a5) & 1) << 32), a11, a12, result, a2, a3, a2, result, a4, a6, a7, a9);
                        }
LABEL_30:
                        __break(1u);
                      }
LABEL_29:
                      __break(1u);
                      goto LABEL_30;
                    }
LABEL_28:
                    __break(1u);
                    goto LABEL_29;
                  }
LABEL_27:
                  __break(1u);
                  goto LABEL_28;
                }
LABEL_26:
                __break(1u);
                goto LABEL_27;
              }
LABEL_25:
              __break(1u);
              goto LABEL_26;
            }
LABEL_24:
            __break(1u);
            goto LABEL_25;
          }
LABEL_23:
          __break(1u);
          goto LABEL_24;
        }
LABEL_22:
        __break(1u);
        goto LABEL_23;
      }
LABEL_21:
      __break(1u);
      goto LABEL_22;
    }
    __break(1u);
  }
  if (v14)
    goto LABEL_8;
  __break(1u);
  return result;
}

uint64_t specialized vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t result, uint64_t a2, uint64_t a3, int a4, unint64_t a5, char a6, int a7, _QWORD *a8, void (*a9)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, int *, char *, int), uint64_t a10, void (*a11)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, int *, int *, char *, int), uint64_t a12, _QWORD *a13)
{
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  _QWORD v20[5];

  v20[4] = *MEMORY[0x1E0C80C00];
  if (!a13[2])
  {
    __break(1u);
    goto LABEL_20;
  }
  if (!a8[2])
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v13 = a13[4];
  v14 = a8[4];
  if (v13)
  {
    if (!v14 || v13 != v14)
    {
LABEL_8:
      v15 = a13[6];
      if ((v15 & 0x8000000000000000) == 0)
      {
        v16 = a13[5];
        if ((v16 & 0x8000000000000000) == 0)
        {
          if (v15)
          {
            if (v16)
            {
              v17 = a8[6];
              if ((v17 & 0x8000000000000000) == 0)
              {
                v18 = a8[5];
                if ((v18 & 0x8000000000000000) == 0)
                {
                  if (v17)
                  {
                    if (v18)
                    {
                      if (v15 == v17)
                      {
                        if (v16 == v18)
                        {
                          v19 = a13[7];
                          v20[0] = v13;
                          v20[1] = v16;
                          v20[2] = v15;
                          v20[3] = v19;
                          return specialized closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)((uint64_t)v20, (uint64_t)a8, a5 | ((HIDWORD(a5) & 1) << 32), a11, a12, result, a2, a3, a2, result, a4, a6, a7, a9);
                        }
LABEL_30:
                        __break(1u);
                      }
LABEL_29:
                      __break(1u);
                      goto LABEL_30;
                    }
LABEL_28:
                    __break(1u);
                    goto LABEL_29;
                  }
LABEL_27:
                  __break(1u);
                  goto LABEL_28;
                }
LABEL_26:
                __break(1u);
                goto LABEL_27;
              }
LABEL_25:
              __break(1u);
              goto LABEL_26;
            }
LABEL_24:
            __break(1u);
            goto LABEL_25;
          }
LABEL_23:
          __break(1u);
          goto LABEL_24;
        }
LABEL_22:
        __break(1u);
        goto LABEL_23;
      }
LABEL_21:
      __break(1u);
      goto LABEL_22;
    }
    __break(1u);
  }
  if (v14)
    goto LABEL_8;
  __break(1u);
  return result;
}

uint64_t specialized vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t result, uint64_t a2, uint64_t a3, unint64_t a4, uint64_t a5, int a6, _QWORD *a7, void (*a8)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, float *, uint64_t *, int), float a9, uint64_t a10, void (*a11)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, float *, int *, uint64_t *, int), uint64_t a12, _QWORD *a13)
{
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  _QWORD v20[5];

  v20[4] = *MEMORY[0x1E0C80C00];
  if (!a13[2])
  {
    __break(1u);
    goto LABEL_20;
  }
  if (!a7[2])
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v13 = a13[4];
  v14 = a7[4];
  if (v13)
  {
    if (!v14 || v13 != v14)
    {
LABEL_8:
      v15 = a13[6];
      if ((v15 & 0x8000000000000000) == 0)
      {
        v16 = a13[5];
        if ((v16 & 0x8000000000000000) == 0)
        {
          if (v15)
          {
            if (v16)
            {
              v17 = a7[6];
              if ((v17 & 0x8000000000000000) == 0)
              {
                v18 = a7[5];
                if ((v18 & 0x8000000000000000) == 0)
                {
                  if (v17)
                  {
                    if (v18)
                    {
                      if (v15 == v17)
                      {
                        if (v16 == v18)
                        {
                          v19 = a13[7];
                          v20[0] = v13;
                          v20[1] = v16;
                          v20[2] = v15;
                          v20[3] = v19;
                          return specialized closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)((uint64_t)v20, (uint64_t)a7, a4 | ((HIDWORD(a4) & 1) << 32), a11, a9, a12, result, a2, a3, a2, result, a5, a6, a8);
                        }
LABEL_30:
                        __break(1u);
                      }
LABEL_29:
                      __break(1u);
                      goto LABEL_30;
                    }
LABEL_28:
                    __break(1u);
                    goto LABEL_29;
                  }
LABEL_27:
                  __break(1u);
                  goto LABEL_28;
                }
LABEL_26:
                __break(1u);
                goto LABEL_27;
              }
LABEL_25:
              __break(1u);
              goto LABEL_26;
            }
LABEL_24:
            __break(1u);
            goto LABEL_25;
          }
LABEL_23:
          __break(1u);
          goto LABEL_24;
        }
LABEL_22:
        __break(1u);
        goto LABEL_23;
      }
LABEL_21:
      __break(1u);
      goto LABEL_22;
    }
    __break(1u);
  }
  if (v14)
    goto LABEL_8;
  __break(1u);
  return result;
}

uint64_t specialized vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t result, uint64_t a2, uint64_t a3, unint64_t a4, unint64_t a5, int a6, _QWORD *a7, void (*a8)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, float *, int *, int), float a9, uint64_t a10, void (*a11)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, float *, int *, int *, int), uint64_t a12, _QWORD *a13)
{
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  _QWORD v21[5];

  v21[4] = *MEMORY[0x1E0C80C00];
  if (!a13[2])
  {
    __break(1u);
    goto LABEL_20;
  }
  if (!a7[2])
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v13 = a13[4];
  v14 = a7[4];
  if (v13)
  {
    if (!v14 || v13 != v14)
    {
LABEL_8:
      v15 = a13[6];
      if ((v15 & 0x8000000000000000) == 0)
      {
        v16 = a13[5];
        if ((v16 & 0x8000000000000000) == 0)
        {
          if (v15)
          {
            if (v16)
            {
              v17 = a7[6];
              if ((v17 & 0x8000000000000000) == 0)
              {
                v18 = a7[5];
                if ((v18 & 0x8000000000000000) == 0)
                {
                  if (v17)
                  {
                    if (v18)
                    {
                      if (v15 == v17)
                      {
                        if (v16 == v18)
                        {
                          v19 = a13[7];
                          v21[0] = v13;
                          v21[1] = v16;
                          v21[2] = v15;
                          v21[3] = v19;
                          v20 = a5 | ((HIDWORD(a5) & 1) << 32);
                          return specialized closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)((uint64_t)v21, (uint64_t)a7, a4 | ((HIDWORD(a4) & 1) << 32), a11, a9, a12, result, a2, a3, a2, result, v20, SBYTE4(v20), a6, a8);
                        }
LABEL_30:
                        __break(1u);
                      }
LABEL_29:
                      __break(1u);
                      goto LABEL_30;
                    }
LABEL_28:
                    __break(1u);
                    goto LABEL_29;
                  }
LABEL_27:
                  __break(1u);
                  goto LABEL_28;
                }
LABEL_26:
                __break(1u);
                goto LABEL_27;
              }
LABEL_25:
              __break(1u);
              goto LABEL_26;
            }
LABEL_24:
            __break(1u);
            goto LABEL_25;
          }
LABEL_23:
          __break(1u);
          goto LABEL_24;
        }
LABEL_22:
        __break(1u);
        goto LABEL_23;
      }
LABEL_21:
      __break(1u);
      goto LABEL_22;
    }
    __break(1u);
  }
  if (v14)
    goto LABEL_8;
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t *a1, uint64_t a2, uint64_t a3, uint64_t a4, int a5, uint64_t *a6, void (*a7)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int), uint64_t a8, void (*a9)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int), uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, void (*a14)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int), uint64_t a15, uint64_t a16, uint64_t a17)
{
  uint64_t *v17;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t result;
  BOOL v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  _QWORD v38[2];
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;

  v43 = *MEMORY[0x1E0C80C00];
  v32 = a1[1];
  v33 = *a1;
  v34 = a1[2];
  v21 = *a6;
  v22 = *v17;
  v23 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v39 = v21;
  type metadata accessor for vImage.PixelBuffer(0, a12, *(_QWORD *)(*(_QWORD *)(a17 + 8) + 8), v24);
  result = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (v23)
  {
    if (result)
      v26 = v23 == result;
    else
      v26 = 0;
    if (!v26)
      goto LABEL_9;
    __break(1u);
  }
  if (!result)
  {
    __break(1u);
    return result;
  }
LABEL_9:
  v38[0] = v22;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&v39);
  v28 = v39;
  v27 = v40;
  vImage.PixelBuffer.size.getter(v38);
  swift_bridgeObjectRelease();
  if (v28 != v38[0] || v27 != v38[1])
    __break(1u);
  v39 = v22;
  v39 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v40 = v29;
  v41 = v30;
  v42 = v31;
  return closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)((uint64_t)&v39, v21, a3, a9, a10, v33, v32, v34, v32, v33, a2, a4, a5, a7, a8, *(_QWORD *)(a11 + 16), a12, a13, a14,
           a15,
           a16,
           a17);
}

uint64_t vImage.PixelBuffer<>.convolve(with:divisor:bias:edgeMode:destination:)(uint64_t *a1, uint64_t a2, unint64_t a3, char *a4, _QWORD **a5)
{
  _QWORD **v5;
  _QWORD **v6;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  int v12;
  uint64_t v13;
  uint64_t result;
  uint64_t v15;
  __int16 *v16;
  unint64_t v17;
  int v18;
  int v19;
  unint64_t v20;
  unint64_t v21;
  uint64_t v22;
  int v23;
  BOOL v24;
  char v25;
  int v26;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;

  v6 = a5;
  v9 = *a1;
  v10 = a1[1];
  v11 = a1[2];
  if ((a2 & 0x100000000) == 0)
  {
    v12 = a2;
LABEL_15:
    if (a4[1] == 1)
    {
      v25 = 0;
      v26 = dword_1CAB60E54[*a4];
    }
    else
    {
      v25 = *a4;
      v26 = 4;
    }
    return specialized vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(v9, v10, v11, v12, a3 | ((HIDWORD(a3) & 1) << 32), v25, v26, *v6, (void (*)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, int *, char *, int))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned UInt8, @unowned UInt32) -> (@unowned Int), 0, (void (*)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, int *, int *, char *, int))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned Int32, @unowned UInt8, @unowned UInt32) -> (@unowned Int), 0, *v5);
  }
  v13 = *(_QWORD *)(v11 + 16);
  if (v13)
  {
    v28 = a1[1];
    v29 = *a1;
    v30 = MEMORY[0x1E0DEE9D8];
    swift_bridgeObjectRetain();
    result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v13, 0);
    v15 = v30;
    v16 = (__int16 *)(v11 + 32);
    v17 = *(_QWORD *)(v30 + 16);
    do
    {
      v19 = *v16++;
      v18 = v19;
      v20 = *(_QWORD *)(v30 + 24);
      v21 = v17 + 1;
      if (v17 >= v20 >> 1)
        result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v20 > 1, v17 + 1, 1);
      *(_QWORD *)(v30 + 16) = v21;
      *(_DWORD *)(v30 + 4 * v17++ + 32) = v18;
      --v13;
    }
    while (v13);
    v10 = v28;
    v9 = v29;
    v6 = a5;
  }
  else
  {
    v15 = MEMORY[0x1E0DEE9D8];
    v21 = *(_QWORD *)(MEMORY[0x1E0DEE9D8] + 16);
    result = swift_bridgeObjectRetain();
    if (!v21)
    {
      swift_bridgeObjectRelease();
      v12 = 0;
LABEL_14:
      swift_bridgeObjectRelease();
      goto LABEL_15;
    }
  }
  v22 = 0;
  v12 = 0;
  while (1)
  {
    v23 = *(_DWORD *)(v15 + 4 * v22 + 32);
    v24 = __OFADD__(v12, v23);
    v12 += v23;
    if (v24)
      break;
    if (v21 == ++v22)
    {
      swift_bridgeObjectRelease();
      goto LABEL_14;
    }
  }
  __break(1u);
  return result;
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned UInt8, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const int16_t *a6, uint32_t a7, uint32_t a8, int32_t *divisor, Pixel_8 *flags, vImage_Flags a11)
{
  return vImageConvolve_Planar8(a1, a2, a3, a4, a5, a6, a7, a8, *divisor, *flags, a11);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned Int32, @unowned UInt8, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const int16_t *a6, uint32_t a7, uint32_t a8, int32_t *divisor, int32_t *flags, Pixel_8 *a11, vImage_Flags a12)
{
  return vImageConvolveWithBias_Planar8(a1, a2, a3, a4, a5, a6, a7, a8, *divisor, *flags, *a11, a12);
}

uint64_t vImage.PixelBuffer<>.convolve(with:divisor:bias:edgeMode:destination:)(uint64_t *a1, uint64_t a2, uint64_t a3, char *a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v8;
  int v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t result;
  uint64_t v14;
  __int16 *v15;
  unint64_t v16;
  int v17;
  int v18;
  unint64_t v19;
  unint64_t v20;
  uint64_t v21;
  int v22;
  BOOL v23;
  uint64_t v24;
  int v25;
  char v26;
  int v27;
  unint64_t v28;
  int v29;
  unint64_t v30;
  uint64_t v31;
  uint64_t AssociatedTypeWitness;
  uint64_t AssociatedConformanceWitness;
  uint64_t v34;
  uint64_t v35;
  char v39;
  int v40;
  char v41;
  int v42;
  uint64_t v43;
  uint64_t v44;

  v8 = a6;
  v10 = a2;
  if ((a2 & 0x100000000) != 0)
  {
    v11 = a1[2];
    v12 = *(_QWORD *)(v11 + 16);
    if (v12)
    {
      v44 = MEMORY[0x1E0DEE9D8];
      swift_bridgeObjectRetain();
      result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v12, 0);
      v14 = v44;
      v15 = (__int16 *)(v11 + 32);
      v16 = *(_QWORD *)(v44 + 16);
      do
      {
        v18 = *v15++;
        v17 = v18;
        v44 = v14;
        v19 = *(_QWORD *)(v14 + 24);
        v20 = v16 + 1;
        if (v16 >= v19 >> 1)
        {
          result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v19 > 1, v16 + 1, 1);
          v14 = v44;
        }
        *(_QWORD *)(v14 + 16) = v20;
        *(_DWORD *)(v14 + 4 * v16++ + 32) = v17;
        --v12;
      }
      while (v12);
LABEL_9:
      v21 = 0;
      v10 = 0;
      while (1)
      {
        v22 = *(_DWORD *)(v14 + 4 * v21 + 32);
        v23 = __OFADD__(v10, v22);
        v10 += v22;
        if (v23)
          break;
        if (v20 == ++v21)
        {
          swift_bridgeObjectRelease();
          goto LABEL_13;
        }
      }
      __break(1u);
      goto LABEL_27;
    }
    v14 = MEMORY[0x1E0DEE9D8];
    v20 = *(_QWORD *)(MEMORY[0x1E0DEE9D8] + 16);
    result = swift_bridgeObjectRetain();
    if (v20)
      goto LABEL_9;
    swift_bridgeObjectRelease();
    v10 = 0;
LABEL_13:
    swift_bridgeObjectRelease();
    v8 = a6;
  }
  v24 = *a4;
  v25 = a4[1];
  if (a4[1])
    v26 = 0;
  else
    v26 = *a4;
  result = (*(uint64_t (**)(_QWORD, uint64_t))(a7 + 32))(*(_QWORD *)(v8 + 16), a7);
  if (result < 0)
  {
LABEL_27:
    __break(1u);
    return result;
  }
  if (result)
  {
    if (v25)
      v27 = dword_1CAB60E54[v24];
    else
      v27 = 4;
    v28 = 0;
    v29 = v27;
    do
    {
      v30 = v28 + 1;
      v31 = result;
      vImage.PixelBuffer<>.subscript.getter(v28, &v44);
      vImage.PixelBuffer<>.subscript.getter(v28, &v43);
      v42 = v10;
      v40 = a3;
      v41 = BYTE4(a3) & 1;
      v39 = v26;
      AssociatedTypeWitness = swift_getAssociatedTypeWitness();
      AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
      v35 = type metadata accessor for vImage.PixelBuffer(0, AssociatedTypeWitness, *(_QWORD *)(*(_QWORD *)(AssociatedConformanceWitness + 8) + 8), v34);
      vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(a1, (uint64_t)&v42, (uint64_t)&v40, (uint64_t)&v39, v29, &v43, (void (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned UInt8, @unowned UInt32) -> (@unowned Int), 0, (void (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned Int32, @unowned UInt8, @unowned UInt32) -> (@unowned Int), 0, v35, AssociatedTypeWitness, MEMORY[0x1E0DEDB58], MEMORY[0x1E0DEDBC8], MEMORY[0x1E0DEDE70], AssociatedConformanceWitness, AssociatedConformanceWitness);
      swift_bridgeObjectRelease();
      swift_bridgeObjectRelease();
      result = v31;
      v28 = v30;
    }
    while (v31 != v30);
  }
  return result;
}

uint64_t vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(uint64_t *a1, unint64_t a2, __int128 *a3, _QWORD **a4)
{
  _QWORD **v4;
  uint64_t v7;
  __int128 v8;
  _QWORD *v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  _QWORD *v16;
  _QWORD v18[4];
  _QWORD v19[6];
  int v20;
  _QWORD **v21;
  unint64_t v22;

  v21 = a4;
  v22 = a2;
  if ((a3[1] & 1) != 0)
  {
    v20 = dword_1CAB60E54[*(_QWORD *)a3];
    v7 = MEMORY[0x1E0DEE9D8];
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
    v7 = swift_allocObject();
    v8 = *a3;
    *(_OWORD *)(v7 + 16) = xmmword_1CAB5F170;
    *(_OWORD *)(v7 + 32) = v8;
    v20 = 4;
  }
  v19[5] = v7;
  v9 = *v4;
  v10 = swift_allocObject();
  *(_QWORD *)(v10 + 16) = v9;
  v19[2] = partial apply for implicit closure #2 in implicit closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:);
  v19[3] = MEMORY[0x1E0C80A78](v10);
  v11 = swift_allocObject();
  *(_QWORD *)(v11 + 16) = v9;
  v18[2] = partial apply for implicit closure #4 in implicit closure #3 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:);
  v18[3] = MEMORY[0x1E0C80A78](v11);
  v12 = v7 + 32;
  v14 = *a1;
  v13 = a1[1];
  v15 = a1[2];
  v16 = *v21;
  swift_bridgeObjectRetain_n();
  specialized vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(v14, v13, v15, v22 | ((HIDWORD(v22) & 1) << 32), v12, v20, v16, (void (*)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, float *, uint64_t *, int))partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned UnsafePointer<Float>?, @unowned UInt32) -> (@unowned Int), 0.0, (uint64_t)v19, (void (*)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, float *, int *, uint64_t *, int))partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float, @unowned UnsafePointer<Float>?, @unowned UInt32) -> (@unowned Int), (uint64_t)v18, v9);
  swift_release();
  swift_release();
  return swift_bridgeObjectRelease();
}

vImage_Error vImage.PixelBuffer<>.vImageConvolve_ARGBFFFF_wrapper(_:_:_:_:_:_:_:_:unusedDivisor:_:_:)(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, const Pixel_FFFF backgroundColor, vImage_Flags flags)
{
  return vImageConvolve_ARGBFFFF(src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, backgroundColor, flags);
}

vImage_Error vImage.PixelBuffer<>.vImageConvolveWithBias_ARGBFFFF_wrapper(_:_:_:_:_:_:_:_:unusedDivisor:_:_:_:)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const float *a6, uint32_t a7, uint32_t a8, double a9, float a10, float *backgroundColor, vImage_Flags flags)
{
  return vImageConvolveWithBias_ARGBFFFF(a1, a2, a3, a4, a5, a6, a7, a8, a10, backgroundColor, flags);
}

uint64_t vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(uint64_t *a1, unint64_t a2, unsigned int *a3, _QWORD **a4)
{
  _QWORD **v4;
  unsigned int v7;
  _QWORD *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  _QWORD *v14;
  _QWORD v16[4];
  _QWORD v17[5];
  unint64_t v18;
  int v19;
  char v20;

  v18 = a2;
  v7 = *a3;
  if (*((_BYTE *)a3 + 4) == 1)
  {
    v19 = dword_1CAB60E54[v7];
    v7 = 0;
  }
  else
  {
    v19 = 4;
  }
  v8 = *v4;
  v9 = swift_allocObject();
  *(_QWORD *)(v9 + 16) = v8;
  v17[2] = partial apply for implicit closure #2 in implicit closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:);
  v17[3] = MEMORY[0x1E0C80A78](v9);
  v10 = swift_allocObject();
  *(_QWORD *)(v10 + 16) = v8;
  v16[2] = partial apply for implicit closure #4 in implicit closure #3 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:);
  v16[3] = MEMORY[0x1E0C80A78](v10);
  v11 = *a1;
  v12 = a1[1];
  v13 = a1[2];
  v14 = *a4;
  swift_bridgeObjectRetain_n();
  v20 = 0;
  specialized vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(v11, v12, v13, v18 | ((HIDWORD(v18) & 1) << 32), v7, v19, v14, (void (*)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, float *, int *, int))partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float?, @unowned UInt32) -> (@unowned Int), 0.0, (uint64_t)v17, (void (*)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, float *, int *, int *, int))partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float, @unowned Float?, @unowned UInt32) -> (@unowned Int), (uint64_t)v16, v8);
  swift_release();
  return swift_release();
}

const vImage_Buffer *vImage.PixelBuffer<>.vImageConvolve_PlanarF_wrapper(_:_:_:_:_:_:_:_:unusedDivisor:_:_:)(const vImage_Buffer *result, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const float *a6, uint32_t a7, uint32_t a8, uint64_t flags, vImage_Flags a10)
{
  if ((flags & 0x100000000) == 0)
    return (const vImage_Buffer *)vImageConvolve_PlanarF(result, a2, a3, a4, a5, a6, a7, a8, *(Pixel_F *)&flags, a10);
  __break(1u);
  return result;
}

const vImage_Buffer *vImage.PixelBuffer<>.vImageConvolveWithBias_PlanarF_wrapper(_:_:_:_:_:_:_:_:unusedDivisor:_:_:_:)(const vImage_Buffer *result, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const float *a6, uint32_t a7, uint32_t a8, double a9, float a10, uint64_t flags, vImage_Flags a12)
{
  if ((flags & 0x100000000) == 0)
    return (const vImage_Buffer *)vImageConvolveWithBias_PlanarF(result, a2, a3, a4, a5, a6, a7, a8, a10, *(Pixel_F *)&flags, a12);
  __break(1u);
  return result;
}

uint64_t *vImage.PixelBuffer<>.convolve(with:bias:edgeMode:useFloat16Accumulator:destination:)(uint64_t *result, unint64_t a2, uint64_t a3, char a4, uint64_t a5)
{
  _QWORD *v5;
  _QWORD *v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  void *v10;
  void *v11;
  BOOL v12;
  vImagePixelCount v13;
  vImagePixelCount v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  unint64_t v18;
  uint64_t v19;
  uint64_t v20;
  void *v21;
  uint64_t v22;
  unsigned int v23;
  size_t v24;
  size_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  char v29;
  unint64_t v30;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v34;

  v34 = *MEMORY[0x1E0C80C00];
  v6 = (_QWORD *)*v5;
  if (!v6[2])
  {
    __break(1u);
    goto LABEL_29;
  }
  v7 = *(_QWORD **)a5;
  if (!*(_QWORD *)(*(_QWORD *)a5 + 16))
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  v8 = a3;
  v9 = (uint64_t)result;
  v10 = (void *)v6[4];
  v11 = (void *)v7[4];
  if (v10)
  {
    if (v11)
      v12 = v10 == v11;
    else
      v12 = 0;
    if (!v12)
      goto LABEL_11;
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_11:
  v13 = v6[6];
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v14 = v6[5];
  if ((v14 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v13)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v14)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  v15 = v7[6];
  if (v15 < 0)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v16 = v7[5];
  if (v16 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (!v15)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v16)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (v13 != v15)
  {
LABEL_38:
    __break(1u);
LABEL_39:
    __break(1u);
  }
  if (v14 != v16)
    goto LABEL_39;
  v17 = *result;
  v18 = result[1];
  if ((*(_BYTE *)(a3 + 8) & 1) != 0)
  {
    v19 = MEMORY[0x1E0DEE9D8];
  }
  else
  {
    v20 = *(_QWORD *)a3;
    v26 = *(_QWORD *)a3 >> 16;
    v27 = HIDWORD(*(_QWORD *)a3);
    v28 = HIWORD(*(_QWORD *)a3);
    v30 = a2;
    v29 = a4;
    v21 = v10;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    v22 = swift_allocObject();
    v10 = v21;
    a4 = v29;
    a2 = v30;
    v8 = a3;
    v19 = v22;
    *(_OWORD *)(v22 + 16) = xmmword_1CAB5F170;
    *(_WORD *)(v22 + 32) = v20;
    *(_WORD *)(v22 + 34) = v26;
    *(_WORD *)(v22 + 36) = v27;
    *(_WORD *)(v22 + 38) = v28;
  }
  if ((a4 & 1) != 0)
    v23 = 4096;
  else
    v23 = 0;
  v24 = v6[7];
  src.data = v10;
  src.height = v14;
  src.width = v13;
  src.rowBytes = v24;
  v25 = v7[7];
  dest.data = v11;
  dest.height = v14;
  dest.width = v13;
  dest.rowBytes = v25;
  closure #1 in closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:useFloat16Accumulator:destination:)(&dest, a2 | ((HIDWORD(a2) & 1) << 32), &src, v9, v18, v17, v19, v8, v23);
  return (uint64_t *)swift_bridgeObjectRelease();
}

const vImage_Buffer *closure #1 in closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:useFloat16Accumulator:destination:)(const vImage_Buffer *dest, uint64_t a2, vImage_Buffer *src, uint64_t a4, unint64_t kernel_height, uint64_t kernel_width, uint64_t a7, uint64_t a8, unsigned int a9)
{
  unint64_t v9;
  int v10;
  int v11;

  v9 = (kernel_width | kernel_height) >> 32;
  if ((a2 & 0x100000000) == 0)
  {
    if (!v9)
    {
      if (*(_BYTE *)(a8 + 8) == 1)
        v10 = dword_1CAB60E54[*(_QWORD *)a8];
      else
        v10 = 4;
      return (const vImage_Buffer *)vImageConvolveWithBias_ARGB16F(src, dest, 0, 0, 0, (const float *)(*(_QWORD *)(a4 + 16) + 32), kernel_height, kernel_width, *(float *)&a2, (const uint16_t *)(a7 + 32), v10 | a9);
    }
    __break(1u);
LABEL_13:
    __break(1u);
    return dest;
  }
  if (v9)
    goto LABEL_13;
  if (*(_BYTE *)(a8 + 8) == 1)
    v11 = dword_1CAB60E54[*(_QWORD *)a8];
  else
    v11 = 4;
  return (const vImage_Buffer *)vImageConvolve_ARGB16F(src, dest, 0, 0, 0, (const float *)(*(_QWORD *)(a4 + 16) + 32), kernel_height, kernel_width, (const uint16_t *)(a7 + 32), v11 | a9);
}

const vImage_Buffer *vImage.PixelBuffer<>.convolve(with:bias:edgeMode:useFloat16Accumulator:destination:)(const vImage_Buffer *result, unint64_t a2, uint64_t a3, char a4, uint64_t a5)
{
  uint64_t v5;
  _QWORD *v6;
  _QWORD *v7;
  void *v8;
  void *v9;
  BOOL v10;
  vImagePixelCount v11;
  vImagePixelCount v12;
  uint64_t v13;
  uint64_t v14;
  unsigned int v15;
  unint64_t height;
  uint64_t data;
  Pixel_16F v18;
  size_t v19;
  size_t v20;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v23;

  v23 = *MEMORY[0x1E0C80C00];
  v6 = *(_QWORD **)v5;
  if (!*(_QWORD *)(*(_QWORD *)v5 + 16))
  {
    __break(1u);
    goto LABEL_29;
  }
  v7 = *(_QWORD **)a5;
  if (!*(_QWORD *)(*(_QWORD *)a5 + 16))
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  v8 = (void *)v6[4];
  v9 = (void *)v7[4];
  if (v8)
  {
    if (v9)
      v10 = v8 == v9;
    else
      v10 = 0;
    if (!v10)
      goto LABEL_11;
    __break(1u);
  }
  if (!v9)
  {
    __break(1u);
    return result;
  }
LABEL_11:
  v11 = v6[6];
  if ((v11 & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v12 = v6[5];
  if ((v12 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v11)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v12)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  v13 = v7[6];
  if (v13 < 0)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v14 = v7[5];
  if (v14 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (!v13)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v14)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (v11 != v13)
  {
LABEL_38:
    __break(1u);
LABEL_39:
    __break(1u);
  }
  if (v12 != v14)
    goto LABEL_39;
  if ((a4 & 1) != 0)
    v15 = 4096;
  else
    v15 = 0;
  data = (uint64_t)result->data;
  height = result->height;
  if (*(_BYTE *)(a3 + 2))
    v18 = 0;
  else
    v18 = *(_WORD *)a3;
  v19 = v6[7];
  src.data = v8;
  src.height = v12;
  src.width = v11;
  src.rowBytes = v19;
  v20 = v7[7];
  dest.data = v9;
  dest.height = v12;
  dest.width = v11;
  dest.rowBytes = v20;
  return closure #1 in closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:useFloat16Accumulator:destination:)(&dest, a2 | ((HIDWORD(a2) & 1) << 32), &src, (uint64_t)result, height, data, v18, (__int16 *)a3, v15);
}

const vImage_Buffer *closure #1 in closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:useFloat16Accumulator:destination:)(const vImage_Buffer *dest, uint64_t a2, vImage_Buffer *src, uint64_t a4, unint64_t kernel_height, uint64_t kernel_width, Pixel_16F backgroundColor, __int16 *a8, unsigned int a9)
{
  unint64_t v9;
  int v10;
  int v11;

  v9 = (kernel_width | kernel_height) >> 32;
  if ((a2 & 0x100000000) == 0)
  {
    if (!v9)
    {
      if (*((_BYTE *)a8 + 2) == 1)
        v10 = dword_1CAB60E54[*a8];
      else
        v10 = 4;
      return (const vImage_Buffer *)vImageConvolveWithBias_Planar16F(src, dest, 0, 0, 0, (const float *)(*(_QWORD *)(a4 + 16) + 32), kernel_height, kernel_width, *(float *)&a2, backgroundColor, v10 | a9);
    }
    __break(1u);
LABEL_13:
    __break(1u);
    return dest;
  }
  if (v9)
    goto LABEL_13;
  if (*((_BYTE *)a8 + 2) == 1)
    v11 = dword_1CAB60E54[*a8];
  else
    v11 = 4;
  return (const vImage_Buffer *)vImageConvolve_Planar16F(src, dest, 0, 0, 0, (const float *)(*(_QWORD *)(a4 + 16) + 32), kernel_height, kernel_width, backgroundColor, v11 | a9);
}

uint64_t vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(uint64_t *a1, unint64_t a2, int *a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t *v6;
  uint64_t *v7;
  int *v9;
  int v10;
  int v11;
  uint64_t (*v12)(void);
  uint64_t result;
  unint64_t v14;
  int v15;
  int v16;
  uint64_t v17;
  _QWORD *v18;
  uint64_t v19;
  uint64_t v20;
  _QWORD *v21;
  uint64_t v22;
  uint64_t AssociatedTypeWitness;
  uint64_t AssociatedConformanceWitness;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  _QWORD v28[4];
  _QWORD v29[4];
  uint64_t v30;
  int v31;
  uint64_t v32;
  int *v33;
  uint64_t *v34;
  uint64_t v35;
  uint64_t *v36;
  uint64_t v37;
  uint64_t v38;
  unint64_t v39;
  uint64_t *v40;
  int v41;
  int v42;
  char v43;
  int v44;
  char v45;
  int v46;
  uint64_t v47;
  uint64_t v48;

  v7 = v6;
  v36 = a1;
  v37 = a4;
  v9 = a3;
  v38 = a5;
  v39 = a2;
  v10 = *a3;
  v11 = *((unsigned __int8 *)a3 + 4);
  v12 = *(uint64_t (**)(void))(a6 + 32);
  v35 = *(_QWORD *)(a5 + 16);
  result = v12();
  if (result < 0)
  {
    __break(1u);
  }
  else if (result)
  {
    v14 = 0;
    if (v11)
      v15 = 0;
    else
      v15 = v10;
    v31 = v15;
    v30 = HIDWORD(v39) & 1;
    v33 = v9;
    v34 = v6;
    v32 = result;
    do
    {
      vImage.PixelBuffer<>.subscript.getter(v14, &v48);
      vImage.PixelBuffer<>.subscript.getter(v14, &v47);
      v46 = 0;
      v44 = v39;
      v45 = v30;
      v42 = v31;
      v43 = 0;
      if (*((_BYTE *)v9 + 4) == 1)
        v16 = dword_1CAB60E54[*v9];
      else
        v16 = 4;
      v41 = v16;
      ++v14;
      v17 = *v7;
      v18 = (_QWORD *)swift_allocObject();
      v19 = v35;
      v18[2] = v35;
      v18[3] = a6;
      v18[4] = v17;
      v20 = swift_bridgeObjectRetain();
      v40 = &v30;
      MEMORY[0x1E0C80A78](v20);
      v29[2] = partial apply for implicit closure #2 in implicit closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:);
      v29[3] = v18;
      v21 = (_QWORD *)swift_allocObject();
      v21[2] = v19;
      v21[3] = a6;
      v21[4] = v17;
      v22 = MEMORY[0x1E0C80A78](v21);
      v28[2] = partial apply for implicit closure #4 in implicit closure #3 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:);
      v28[3] = v22;
      AssociatedTypeWitness = swift_getAssociatedTypeWitness();
      AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
      v26 = type metadata accessor for vImage.PixelBuffer(0, AssociatedTypeWitness, *(_QWORD *)(*(_QWORD *)(AssociatedConformanceWitness + 8) + 8), v25);
      swift_bridgeObjectRetain();
      v27 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Float?);
      v7 = v34;
      vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(v36, (uint64_t)&v46, (uint64_t)&v44, (uint64_t)&v42, v41, &v47, (void (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int))partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float?, @unowned UInt32) -> (@unowned Int), (uint64_t)v29, (void (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int))partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float, @unowned Float?, @unowned UInt32) -> (@unowned Int), (uint64_t)v28, v26, AssociatedTypeWitness, MEMORY[0x1E0DEB188], MEMORY[0x1E0DEB188], v27, AssociatedConformanceWitness, AssociatedConformanceWitness);
      swift_bridgeObjectRelease();
      swift_release();
      swift_release();
      swift_bridgeObjectRelease();
      result = v32;
      v9 = v33;
    }
    while (v32 != v14);
  }
  return result;
}

uint64_t specialized closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, void (*a4)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, int *, int *, uint64_t *, int), uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, int a11, uint64_t a12, int a13, void (*a14)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, int *, uint64_t *, int))
{
  __int128 v16;
  unint64_t v17;
  int v20;
  int v21;
  uint64_t v22;
  _OWORD v23[2];
  uint64_t v24;

  v24 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a2 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  v16 = *(_OWORD *)(a2 + 48);
  v23[0] = *(_OWORD *)(a2 + 32);
  v23[1] = v16;
  v21 = a11;
  v17 = a10 | a9;
  v22 = a12;
  if ((a3 & 0x100000000) != 0)
  {
    if (!HIDWORD(v17))
    {
      swift_bridgeObjectRetain();
      a14(a1, v23, 0, 0, 0, a8 + 32, a9, a10, &v21, &v22, a13);
      return swift_bridgeObjectRelease();
    }
LABEL_10:
    __break(1u);
  }
  v20 = a3;
  if (HIDWORD(v17))
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  swift_bridgeObjectRetain();
  a4(a1, v23, 0, 0, 0, a8 + 32, a9, a10, &v21, &v20, &v22, a13);
  return swift_bridgeObjectRelease();
}

uint64_t specialized closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, void (*a4)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, int *, int *, char *, int), uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, int a11, char a12, int a13, void (*a14)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, int *, char *, int))
{
  __int128 v16;
  unint64_t v17;
  int v20;
  char v21;
  int v22;
  _OWORD v23[2];
  uint64_t v24;

  v24 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a2 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  v16 = *(_OWORD *)(a2 + 48);
  v23[0] = *(_OWORD *)(a2 + 32);
  v23[1] = v16;
  v22 = a11;
  v17 = a10 | a9;
  v21 = a12;
  if ((a3 & 0x100000000) != 0)
  {
    if (!HIDWORD(v17))
    {
      swift_bridgeObjectRetain();
      a14(a1, v23, 0, 0, 0, a8 + 32, a9, a10, &v22, &v21, a13);
      return swift_bridgeObjectRelease();
    }
LABEL_10:
    __break(1u);
  }
  v20 = a3;
  if (HIDWORD(v17))
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  swift_bridgeObjectRetain();
  a4(a1, v23, 0, 0, 0, a8 + 32, a9, a10, &v22, &v20, &v21, a13);
  return swift_bridgeObjectRelease();
}

uint64_t specialized closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, void (*a4)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, float *, int *, uint64_t *, int), float a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, int a13, void (*a14)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, float *, uint64_t *, int))
{
  __int128 v16;
  unint64_t v17;
  int v20;
  float v21;
  uint64_t v22;
  _OWORD v23[2];
  uint64_t v24;

  v24 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a2 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  v16 = *(_OWORD *)(a2 + 48);
  v23[0] = *(_OWORD *)(a2 + 32);
  v23[1] = v16;
  v21 = a5;
  v22 = a12;
  v17 = a11 | a10;
  if ((a3 & 0x100000000) != 0)
  {
    if (!HIDWORD(v17))
    {
      swift_bridgeObjectRetain();
      a14(a1, v23, 0, 0, 0, a9 + 32, a10, a11, &v21, &v22, a13);
      return swift_bridgeObjectRelease();
    }
LABEL_10:
    __break(1u);
  }
  v20 = a3;
  if (HIDWORD(v17))
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  swift_bridgeObjectRetain();
  a4(a1, v23, 0, 0, 0, a9 + 32, a10, a11, &v21, &v20, &v22, a13);
  return swift_bridgeObjectRelease();
}

uint64_t specialized closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, void (*a4)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, float *, int *, int *, int), float a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, int a12, char a13, int a14, void (*a15)(uint64_t, _OWORD *, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, float *, int *, int))
{
  __int128 v17;
  unint64_t v18;
  int v21;
  float v22;
  int v23;
  char v24;
  _OWORD v25[2];
  uint64_t v26;

  v26 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a2 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  v17 = *(_OWORD *)(a2 + 48);
  v25[0] = *(_OWORD *)(a2 + 32);
  v25[1] = v17;
  v22 = a5;
  v23 = a12;
  v24 = a13 & 1;
  v18 = a11 | a10;
  if ((a3 & 0x100000000) != 0)
  {
    if (!HIDWORD(v18))
    {
      swift_bridgeObjectRetain();
      a15(a1, v25, 0, 0, 0, a9 + 32, a10, a11, &v22, &v23, a14);
      return swift_bridgeObjectRelease();
    }
LABEL_10:
    __break(1u);
  }
  v21 = a3;
  if (HIDWORD(v18))
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  swift_bridgeObjectRetain();
  a4(a1, v25, 0, 0, 0, a9 + 32, a10, a11, &v22, &v21, &v23, a14);
  return swift_bridgeObjectRelease();
}

uint64_t closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, void (*a4)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int), uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, int a13, void (*a14)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int), uint64_t a15, uint64_t a16, uint64_t a17, uint64_t a18, void (*a19)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int), uint64_t a20,uint64_t a21,uint64_t a22)
{
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  _QWORD v33[6];

  v33[4] = *MEMORY[0x1E0C80C00];
  v33[0] = a2;
  type metadata accessor for vImage.PixelBuffer(0, a17, *(_QWORD *)(*(_QWORD *)(a22 + 8) + 8), (uint64_t)a4);
  v33[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v33[1] = v22;
  v33[2] = v23;
  v33[3] = v24;
  return closure #1 in closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)((uint64_t)v33, a3, a4, a5, a1, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, a16, a17, a18, a19);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t a1, uint64_t a2, void (*a3)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, int a13, void (*a14)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int), uint64_t a15, uint64_t a16, uint64_t a17, uint64_t a18, void (*a19)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int))
{
  void (*v20)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int);
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  char *v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  int v29;
  unint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  _QWORD v44[2];
  void (*v45)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int);
  uint64_t v46;
  uint64_t v47;
  int v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;

  v45 = a3;
  v46 = a8;
  v53 = a5;
  v44[1] = a4;
  v20 = a19;
  v47 = a18;
  v48 = a13;
  v49 = a12;
  v50 = a11;
  v51 = a1;
  v21 = type metadata accessor for Optional();
  v22 = *(_QWORD *)(v21 - 8);
  v23 = MEMORY[0x1E0C80A78](v21);
  v25 = (char *)v44 - v24;
  v26 = *((_QWORD *)a19 - 1);
  MEMORY[0x1E0C80A78](v23);
  v28 = (uint64_t)v44 - ((v27 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(char *, uint64_t, uint64_t))(v22 + 16))(v25, a2, v21);
  v29 = (*(uint64_t (**)(char *, uint64_t, void (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int)))(v26 + 48))(v25, 1, a19);
  v54 = a10;
  v52 = a9;
  v30 = a10 | a9;
  if (v29 != 1)
  {
    (*(void (**)(uint64_t, char *, void (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int)))(v26 + 32))(v28, v25, a19);
    if (HIDWORD(v30))
    {
LABEL_30:
      __break(1u);
      goto LABEL_31;
    }
    v33 = v46;
    swift_bridgeObjectRetain();
    v34 = v47;
    if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && (v33 < 0 || (v33 & 0x4000000000000000) != 0))
    {
      if (MEMORY[0x1D1794120](v33, v34))
      {
        v55 = v33;
        v42 = type metadata accessor for _ArrayBuffer();
        MEMORY[0x1D1794D08](MEMORY[0x1E0DEC400], v42);
        v43 = Array.init<A>(_:)();
        destructiveProjectEnumData for BNNS.ActivationFunction(v43);
        swift_unknownObjectRetain();
        v36 = _ContiguousArrayBuffer.firstElementAddress.getter();
        swift_release();
        goto LABEL_27;
      }
      swift_bridgeObjectRelease();
      v36 = 0;
    }
    else
    {
      swift_bridgeObjectRelease();
      if ((_swift_isClassOrObjCExistentialType() & 1) != 0)
      {
        v35 = *(unsigned __int8 *)(*(_QWORD *)(v34 - 8) + 80);
        v36 = (v33 & 0xFFFFFFFFFFFFFF8) + ((v35 + 32) & ~v35);
      }
      else
      {
        v38 = *(unsigned __int8 *)(*(_QWORD *)(v34 - 8) + 80);
        v36 = v33 + ((v38 + 32) & ~v38);
      }
    }
    if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && (v33 < 0 || (v33 & 0x4000000000000000) != 0))
    {
      specialized _ArrayBuffer._nonNative.getter(v33);
      swift_unknownObjectRetain();
      if (v36)
        goto LABEL_27;
    }
    else
    {
      _swift_isClassOrObjCExistentialType();
      swift_bridgeObjectRetain();
      if (v36)
      {
LABEL_27:
        v45(v53, v51, 0, 0, 0, v36, v52, v54, v50, v28, v49, v48);
        (*(void (**)(uint64_t, void (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int)))(v26 + 8))(v28, a19);
        return swift_unknownObjectRelease();
      }
    }
    v36 = ~*(_DWORD *)(*(_QWORD *)(v34 - 8) + 80) | 0xFFFFFFFFFFFFFF00;
    goto LABEL_27;
  }
  (*(void (**)(char *, uint64_t))(v22 + 8))(v25, v21);
  if (HIDWORD(v30))
  {
    __break(1u);
    goto LABEL_30;
  }
  v20 = a14;
  v28 = v46;
  swift_bridgeObjectRetain();
  v22 = v47;
  if ((_swift_isClassOrObjCExistentialType() & 1) == 0
    || (v28 & 0x8000000000000000) == 0 && (v28 & 0x4000000000000000) == 0)
  {
    swift_bridgeObjectRelease();
    if ((_swift_isClassOrObjCExistentialType() & 1) != 0)
    {
      v31 = *(unsigned __int8 *)(*(_QWORD *)(v22 - 8) + 80);
      v32 = (v28 & 0xFFFFFFFFFFFFFF8) + ((v31 + 32) & ~v31);
    }
    else
    {
      v37 = *(unsigned __int8 *)(*(_QWORD *)(v22 - 8) + 80);
      v32 = v28 + ((v37 + 32) & ~v37);
    }
    goto LABEL_15;
  }
LABEL_31:
  if (MEMORY[0x1D1794120](v28, v22))
  {
    v55 = v28;
    v40 = type metadata accessor for _ArrayBuffer();
    MEMORY[0x1D1794D08](MEMORY[0x1E0DEC400], v40);
    v41 = Array.init<A>(_:)();
    destructiveProjectEnumData for BNNS.ActivationFunction(v41);
    swift_unknownObjectRetain();
    v32 = _ContiguousArrayBuffer.firstElementAddress.getter();
    swift_release();
    goto LABEL_20;
  }
  swift_bridgeObjectRelease();
  v32 = 0;
LABEL_15:
  if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && (v28 < 0 || (v28 & 0x4000000000000000) != 0))
  {
    specialized _ArrayBuffer._nonNative.getter(v28);
    swift_unknownObjectRetain();
    if (v32)
      goto LABEL_20;
    goto LABEL_19;
  }
  _swift_isClassOrObjCExistentialType();
  swift_bridgeObjectRetain();
  if (!v32)
LABEL_19:
    v32 = ~*(_DWORD *)(*(_QWORD *)(v22 - 8) + 80) | 0xFFFFFFFFFFFFFF00;
LABEL_20:
  v20(v53, v51, 0, 0, 0, v32, v52, v54, v50, v49, v48);
  return swift_unknownObjectRelease();
}

vImage_Buffer **vImage.PixelBuffer<>.convolve(with:divisors:biases:edgeMode:destination:)(vImage_Buffer **result, unint64_t *a2, unint64_t *a3, unint64_t *a4, _QWORD *a5, int a6, char a7, unsigned int a8, unsigned int a9, unsigned int a10, unsigned int a11, unsigned int *a12, uint64_t a13)
{
  unint64_t v13;
  unint64_t v14;
  uint64_t v15;
  unint64_t v16;
  unint64_t v17;
  vImage_Buffer *v18;
  vImage_Buffer *v19;
  uint64_t v20;
  _QWORD *v21;
  _QWORD *v22;
  unint64_t v24;
  unint64_t v25;
  unsigned __int8 v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  _QWORD *data;
  int64_t v32;
  void *v33;
  int64_t v34;
  size_t rowBytes;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  uint64_t v39;
  vImagePixelCount width;
  unint64_t v41;
  int64_t v42;
  int64_t v43;
  uint64_t v44;
  unint64_t v45;
  _QWORD *v46;
  int64_t v47;
  vImagePixelCount height;
  int64_t v49;
  void *v50;
  unint64_t v51;
  unint64_t v52;
  unint64_t v53;
  uint64_t v54;
  size_t v55;
  unint64_t v56;
  int64_t v57;
  unint64_t v58;
  uint64_t v59;
  _QWORD *v60;
  vImagePixelCount v61;
  unint64_t v62;
  vImagePixelCount v63;
  unint64_t v64;
  unint64_t v65;
  _QWORD *v66;
  void *v67;
  unint64_t v68;
  int64_t v69;
  uint64_t v70;
  int64_t v71;
  _QWORD *p_data;
  __int16 *v73;
  int v74;
  int v75;
  unint64_t v76;
  unint64_t v77;
  uint64_t v78;
  BOOL v79;
  unint64_t v80;
  unint64_t v81;
  unsigned __int8 v82;
  uint64_t v83;
  uint64_t v84;
  __int128 v85;
  __int128 v86;
  uint64_t v87;
  uint64_t v88;
  uint64_t v89;
  uint64_t v90;
  _QWORD *v91;
  _QWORD *v92;
  _QWORD *v93;
  unint64_t v94;
  unint64_t v95;
  unint64_t v96;
  unint64_t v97;
  unint64_t v98;
  unint64_t v99;
  unint64_t v100;
  unsigned int v101;
  uint64_t v102;
  unsigned __int8 v103;
  unint64_t v104;
  unint64_t v105;
  unint64_t v106;
  unint64_t v107;
  unint64_t v108;
  char *v109;
  unint64_t v110[13];
  vImage_Buffer v111;
  vImage_Buffer v112;
  vImage_Buffer *v113;
  vImage_Buffer *v114;
  unint64_t v115;
  unint64_t v116;
  unint64_t v117;
  unint64_t v118;
  unint64_t v119;
  unint64_t v120;
  unint64_t v121;
  unint64_t v122;
  unint64_t v123;
  unint64_t v124;
  uint64_t v125;

  v125 = *MEMORY[0x1E0C80C00];
  v21 = *(_QWORD **)v15;
  if (!*(_QWORD *)(*(_QWORD *)v15 + 16))
    goto LABEL_68;
  v22 = *(_QWORD **)a13;
  if (!*(_QWORD *)(*(_QWORD *)a13 + 16))
    goto LABEL_69;
  LOBYTE(v20) = a7;
  v18 = *result;
  v19 = result[1];
  v24 = (unint64_t)result[2];
  v104 = a2[1];
  v105 = *a2;
  v16 = *a3;
  v17 = a3[1];
  v108 = a3[2];
  v14 = a4[1];
  v106 = a2[2];
  v107 = *a4;
  v25 = a4[2];
  v102 = *a12;
  v26 = *((_BYTE *)a12 + 4);
  v13 = v21[4];
  v27 = v22[4];
  if (!v13)
  {
LABEL_7:
    if (!v27)
    {
      __break(1u);
      return result;
    }
    goto LABEL_8;
  }
  if (v27 && v13 == v27)
  {
    __break(1u);
    goto LABEL_7;
  }
LABEL_8:
  v28 = v21[6];
  if (v28 < 0)
    goto LABEL_70;
  v13 = v21[5];
  if ((v13 & 0x8000000000000000) != 0)
    goto LABEL_71;
  if (!v28)
    goto LABEL_72;
  if (!v13)
    goto LABEL_73;
  v29 = v22[6];
  if (v29 < 0)
    goto LABEL_74;
  v30 = v22[5];
  if (v30 < 0)
    goto LABEL_75;
  if (!v29)
    goto LABEL_76;
  if (!v30)
    goto LABEL_77;
  if (v28 != v29)
    goto LABEL_78;
  if (v13 != v30)
    goto LABEL_79;
  v91 = a5;
  v92 = v22;
  LODWORD(v109) = a6;
  v93 = v21;
  v103 = v26;
  swift_bridgeObjectRetain();
  v94 = (unint64_t)v19;
  v95 = (unint64_t)v18;
  v113 = v18;
  v114 = v19;
  v115 = v24;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRelease();
  v116 = v105;
  v117 = v104;
  v118 = v106;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  v97 = v17;
  v98 = v16;
  v119 = v16;
  v120 = v17;
  v121 = v108;
  v18 = &v112;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  v96 = v24;
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  v122 = v107;
  v123 = v14;
  v99 = v25;
  v100 = v14;
  v124 = v25;
  v19 = (vImage_Buffer *)MEMORY[0x1E0DEE9D8];
  v112.data = (void *)MEMORY[0x1E0DEE9D8];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 4, 0);
  data = v112.data;
  v17 = (unint64_t)v113;
  v16 = *((_QWORD *)v112.data + 2);
  v13 = *((_QWORD *)v112.data + 3);
  v32 = v13 >> 1;
  v14 = v16 + 1;
  v101 = a8;
  if (v13 >> 1 <= v16)
    goto LABEL_80;
  while (1)
  {
    data[2] = v14;
    data[v16 + 4] = v17;
    v33 = v18[3].data;
    v34 = v16 + 2;
    if (v32 < v34)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v13 > 1), v34, 1);
      data = v18->data;
    }
    data[2] = v34;
    data[v14 + 4] = v33;
    rowBytes = v18[3].rowBytes;
    v37 = data[2];
    v36 = data[3];
    v38 = v37 + 1;
    if (v37 >= v36 >> 1)
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v36 > 1), v37 + 1, 1);
    v39 = (uint64_t)v18->data;
    *(_QWORD *)(v39 + 16) = v38;
    *(_QWORD *)(v39 + 8 * v37 + 32) = rowBytes;
    width = v18[4].width;
    v41 = *(_QWORD *)(v39 + 24);
    v42 = v37 + 2;
    if (v42 > (uint64_t)(v41 >> 1))
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v41 > 1), v42, 1);
      v39 = (uint64_t)v18->data;
    }
    *(_QWORD *)(v39 + 16) = v42;
    *(_QWORD *)(v39 + 8 * v38 + 32) = width;
    v43 = specialized Set.init<A>(_:)(v39);
    swift_release();
    v44 = *(_QWORD *)(v43 + 16);
    swift_bridgeObjectRelease();
    if (v44 == 1)
    {
      v18->data = v19;
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 4, 0);
      v46 = v18->data;
      v17 = v18[2].width;
      v42 = *((_QWORD *)v18->data + 2);
      v45 = *((_QWORD *)v18->data + 3);
      v47 = v45 >> 1;
      v43 = v42 + 1;
      if (v45 >> 1 > v42)
        goto LABEL_27;
    }
    else
    {
      __break(1u);
    }
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v45 > 1), v43, 1);
    v46 = v18->data;
    v45 = *((_QWORD *)v18->data + 3);
    v47 = v45 >> 1;
LABEL_27:
    v46[2] = v43;
    v46[v42 + 4] = v17;
    height = v18[3].height;
    v49 = v42 + 2;
    if (v47 < v49)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v45 > 1), v49, 1);
      v46 = v18->data;
    }
    v46[2] = v49;
    v46[v43 + 4] = height;
    v50 = v18[4].data;
    v52 = v46[2];
    v51 = v46[3];
    v53 = v52 + 1;
    if (v52 >= v51 >> 1)
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v51 > 1), v52 + 1, 1);
    v54 = (uint64_t)v18->data;
    *(_QWORD *)(v54 + 16) = v53;
    *(_QWORD *)(v54 + 8 * v52 + 32) = v50;
    v55 = v18[4].rowBytes;
    v56 = *(_QWORD *)(v54 + 24);
    v57 = v52 + 2;
    if (v57 > (uint64_t)(v56 >> 1))
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v56 > 1), v57, 1);
      v54 = (uint64_t)v18->data;
    }
    *(_QWORD *)(v54 + 16) = v57;
    *(_QWORD *)(v54 + 8 * v53 + 32) = v55;
    v58 = specialized Set.init<A>(_:)(v54);
    swift_release();
    v59 = *(_QWORD *)(v58 + 16);
    swift_bridgeObjectRelease();
    if (v59 != 1)
    {
      __break(1u);
LABEL_84:
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v17 > 1), v58, 1);
      v60 = v18->data;
      goto LABEL_37;
    }
    if ((v20 & 1) == 0)
    {
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for vImage.ConvolutionKernel2D<Int16>);
      swift_arrayDestroy();
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int32>);
      v20 = swift_allocObject();
      *(_OWORD *)(v20 + 16) = xmmword_1CAB5F170;
      *(_QWORD *)(v20 + 32) = v91;
      *(_DWORD *)(v20 + 40) = (_DWORD)v109;
      *(_DWORD *)(v20 + 44) = (_DWORD)v109;
      goto LABEL_63;
    }
    v18->data = v19;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 4, 0);
    v60 = v18->data;
    v57 = v18[2].rowBytes;
    v55 = *((_QWORD *)v18->data + 2);
    v17 = *((_QWORD *)v18->data + 3);
    v58 = v55 + 1;
    swift_bridgeObjectRetain();
    if (v55 >= v17 >> 1)
      goto LABEL_84;
LABEL_37:
    v60[2] = v58;
    v60[v55 + 4] = v57;
    v61 = v18[3].width;
    v62 = v60[3];
    swift_bridgeObjectRetain();
    if (v58 >= v62 >> 1)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v62 > 1), v55 + 2, 1);
      v60 = v18->data;
    }
    v60[2] = v55 + 2;
    v60[v58 + 4] = v61;
    v63 = v18[4].height;
    v65 = v60[2];
    v64 = v60[3];
    v14 = v65 + 1;
    swift_bridgeObjectRetain();
    if (v65 >= v64 >> 1)
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v64 > 1), v65 + 1, 1);
    v66 = v18->data;
    v66[2] = v14;
    v66[v65 + 4] = v63;
    v67 = v18[5].data;
    v68 = v66[3];
    swift_bridgeObjectRetain();
    v19 = (vImage_Buffer *)MEMORY[0x1E0DEE9D8];
    if (v14 >= v68 >> 1)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v68 > 1), v65 + 2, 1);
      v66 = v18->data;
    }
    v66[2] = v65 + 2;
    v109 = (char *)(v66 + 4);
    v66[v14 + 4] = v67;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for vImage.ConvolutionKernel2D<Int16>);
    swift_arrayDestroy();
    v69 = v66[2];
    if (!v69)
      break;
    v91 = v66;
    v18->data = v19;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v69, 0);
    v17 = 0;
    v20 = (uint64_t)v18->data;
    while (1)
    {
      v70 = *(_QWORD *)&v109[8 * v17];
      v71 = *(_QWORD *)(v70 + 16);
      if (v71)
        break;
      v77 = v19->width;
      swift_bridgeObjectRetain();
      if (v77)
      {
        p_data = &v19->data;
        goto LABEL_53;
      }
      swift_bridgeObjectRelease();
      LODWORD(v16) = 0;
LABEL_57:
      swift_bridgeObjectRelease();
      v18->data = (void *)v20;
      v81 = *(_QWORD *)(v20 + 16);
      v80 = *(_QWORD *)(v20 + 24);
      v14 = v81 + 1;
      if (v81 >= v80 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v80 > 1), v81 + 1, 1);
        v20 = (uint64_t)v18->data;
      }
      ++v17;
      *(_QWORD *)(v20 + 16) = v14;
      *(_DWORD *)(v20 + 4 * v81 + 32) = v16;
      if (v17 == v69)
      {
        swift_release();
        goto LABEL_63;
      }
    }
    v111.data = v19;
    swift_bridgeObjectRetain();
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v71, 0);
    p_data = &v19->data;
    v73 = (__int16 *)(v70 + 32);
    v14 = p_data[2];
    do
    {
      v75 = *v73++;
      v74 = v75;
      v111.data = p_data;
      v76 = p_data[3];
      v77 = v14 + 1;
      if (v14 >= v76 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v76 > 1), v14 + 1, 1);
        p_data = v111.data;
      }
      p_data[2] = v77;
      *((_DWORD *)p_data + v14++ + 8) = v74;
      --v71;
    }
    while (v71);
    v18 = &v112;
    v19 = (vImage_Buffer *)MEMORY[0x1E0DEE9D8];
LABEL_53:
    v78 = 0;
    LODWORD(v16) = 0;
    while (1)
    {
      v13 = *((unsigned int *)p_data + v78 + 8);
      v79 = __OFADD__((_DWORD)v16, (_DWORD)v13);
      v16 = (v16 + v13);
      if (v79)
        break;
      if (v77 == ++v78)
      {
        swift_bridgeObjectRelease();
        goto LABEL_57;
      }
    }
    __break(1u);
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
LABEL_70:
    __break(1u);
LABEL_71:
    __break(1u);
LABEL_72:
    __break(1u);
LABEL_73:
    __break(1u);
LABEL_74:
    __break(1u);
LABEL_75:
    __break(1u);
LABEL_76:
    __break(1u);
LABEL_77:
    __break(1u);
LABEL_78:
    __break(1u);
LABEL_79:
    __break(1u);
LABEL_80:
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v13 > 1), v14, 1);
    data = v18->data;
    v13 = *((_QWORD *)v18->data + 3);
    v32 = v13 >> 1;
  }
  swift_release();
  v20 = MEMORY[0x1E0DEE9D8];
LABEL_63:
  v82 = v103;
  if ((v103 & 1) != 0)
  {
    v83 = MEMORY[0x1E0DEE9D8];
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    v84 = swift_allocObject();
    v82 = v103;
    v83 = v84;
    *(_OWORD *)(v84 + 16) = xmmword_1CAB5F170;
    *(_WORD *)(v84 + 32) = v102;
    *(_BYTE *)(v84 + 34) = BYTE2(v102);
    v18 = &v112;
    *(_BYTE *)(v84 + 35) = BYTE3(v102);
  }
  v85 = *((_OWORD *)v93 + 3);
  *(_OWORD *)&v18->data = *((_OWORD *)v93 + 2);
  *(_OWORD *)&v18->width = v85;
  v86 = *((_OWORD *)v92 + 3);
  *(_OWORD *)&v111.data = *((_OWORD *)v92 + 2);
  *(_OWORD *)&v111.width = v86;
  v87 = *(_QWORD *)(v96 + 16);
  v88 = *(_QWORD *)(v106 + 16);
  v89 = *(_QWORD *)(v108 + 16);
  v90 = *(_QWORD *)(v99 + 16);
  v110[0] = v95;
  v110[1] = v94;
  v110[2] = v96;
  v110[3] = v105;
  v110[4] = v104;
  v110[5] = v106;
  v110[6] = v98;
  v110[7] = v97;
  v110[8] = v108;
  v110[9] = v107;
  v110[10] = v100;
  v110[11] = v99;
  closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in closure #5 in vImage.PixelBuffer<>.convolve(with:divisors:biases:edgeMode:destination:)((const int16_t *)(v99 + 32), v90, (const int16_t *)(v96 + 32), v87, (const int16_t *)(v106 + 32), v88, (const int16_t *)(v108 + 32), v89, &v112, &v111, v110, v20, v101 | ((unint64_t)a9 << 32), a10 | ((unint64_t)a11 << 32), v83, v102 | ((unint64_t)v82 << 32));
  swift_bridgeObjectRelease();
  return (vImage_Buffer **)swift_bridgeObjectRelease();
}

const int16_t *closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in closure #5 in vImage.PixelBuffer<>.convolve(with:divisors:biases:edgeMode:destination:)(const int16_t *result, uint64_t a2, const int16_t *a3, uint64_t a4, const int16_t *a5, uint64_t a6, const int16_t *a7, uint64_t a8, const vImage_Buffer *a9, const vImage_Buffer *a10, unint64_t *a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16)
{
  unint64_t v17;
  unint64_t v18;
  vImage_Flags flags;
  _QWORD biases[2];
  const int16_t *v21[4];

  v18 = *a11;
  v17 = a11[1];
  v21[0] = a3;
  v21[1] = a5;
  v21[2] = a7;
  v21[3] = result;
  if ((v17 | v18) >> 32)
  {
    __break(1u);
  }
  else
  {
    biases[0] = a13;
    biases[1] = a14;
    if ((a16 & 0x100000000) != 0)
      flags = dword_1CAB60E54[(int)a16];
    else
      flags = 4;
    return (const int16_t *)vImageConvolveMultiKernel_ARGB8888(a9, a10, 0, 0, 0, v21, v17, v18, (const int32_t *)(a12 + 32), (const int32_t *)biases, (const uint8_t *)(a15 + 32), flags);
  }
  return result;
}

uint64_t sub_1CAB0E62C()
{
  swift_bridgeObjectRelease();
  return swift_deallocObject();
}

vImage_Error partial apply for implicit closure #2 in implicit closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, const Pixel_FFFF backgroundColor, vImage_Flags flags)
{
  return vImageConvolve_ARGBFFFF(src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, backgroundColor, flags);
}

uint64_t partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned UnsafePointer<Float>?, @unowned UInt32) -> (@unowned Int)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float *a9)
{
  uint64_t v9;

  return (*(uint64_t (**)(float))(v9 + 16))(*a9);
}

vImage_Error partial apply for implicit closure #4 in implicit closure #3 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const float *a6, uint32_t a7, uint32_t a8, double a9, float a10, float *backgroundColor, vImage_Flags flags)
{
  return vImageConvolveWithBias_ARGBFFFF(a1, a2, a3, a4, a5, a6, a7, a8, a10, backgroundColor, flags);
}

uint64_t partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float, @unowned UnsafePointer<Float>?, @unowned UInt32) -> (@unowned Int)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float *a9, float *a10)
{
  uint64_t v10;

  return (*(uint64_t (**)(float, float))(v10 + 16))(*a9, *a10);
}

const vImage_Buffer *partial apply for implicit closure #2 in implicit closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(const vImage_Buffer *result, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const float *a6, uint32_t a7, uint32_t a8, double a9, double a10, double a11, double a12, double a13, double a14, double a15, double a16, Pixel_F a17, char a18, vImage_Flags a19)
{
  if ((a18 & 1) == 0)
    return (const vImage_Buffer *)vImageConvolve_PlanarF(result, a2, a3, a4, a5, a6, a7, a8, a17, a19);
  __break(1u);
  return result;
}

uint64_t partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float?, @unowned UInt32) -> (@unowned Int)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float *a9)
{
  return partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float?, @unowned UInt32) -> (@unowned Int)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v9;

  return (*(uint64_t (**)(float))(v9 + 16))(*a9);
}

const vImage_Buffer *partial apply for implicit closure #4 in implicit closure #3 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(const vImage_Buffer *result, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const float *a6, uint32_t a7, uint32_t a8, double a9, float a10, double a11, double a12, double a13, double a14, double a15, double a16, Pixel_F a17, char a18, vImage_Flags a19)
{
  if ((a18 & 1) == 0)
    return (const vImage_Buffer *)vImageConvolveWithBias_PlanarF(result, a2, a3, a4, a5, a6, a7, a8, a10, a17, a19);
  __break(1u);
  return result;
}

uint64_t partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float, @unowned Float?, @unowned UInt32) -> (@unowned Int)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float *a9, float *a10)
{
  return partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float, @unowned Float?, @unowned UInt32) -> (@unowned Int)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10);
}

{
  uint64_t v10;

  return (*(uint64_t (**)(float, float))(v10 + 16))(*a9, *a10);
}

ValueMetadata *type metadata accessor for vImage.ConvolutionKernel()
{
  return &type metadata for vImage.ConvolutionKernel;
}

uint64_t type metadata instantiation function for vImage.ConvolutionKernel2D()
{
  return swift_allocateGenericValueMetadata();
}

uint64_t initializeBufferWithCopyOfBuffer for vImage.ConvolutionKernel2D(uint64_t a1, uint64_t a2)
{
  *(_OWORD *)a1 = *(_OWORD *)a2;
  *(_QWORD *)(a1 + 16) = *(_QWORD *)(a2 + 16);
  swift_bridgeObjectRetain();
  return a1;
}

uint64_t destroy for vImage.ConvolutionKernel2D()
{
  return swift_bridgeObjectRelease();
}

_QWORD *assignWithCopy for vImage.ConvolutionKernel2D(_QWORD *a1, _QWORD *a2)
{
  *a1 = *a2;
  a1[1] = a2[1];
  a1[2] = a2[2];
  swift_bridgeObjectRetain();
  swift_bridgeObjectRelease();
  return a1;
}

__n128 __swift_memcpy24_8(__n128 *a1, __n128 *a2)
{
  __n128 result;

  result = *a2;
  a1[1].n128_u64[0] = a2[1].n128_u64[0];
  *a1 = result;
  return result;
}

uint64_t assignWithTake for vImage.ConvolutionKernel2D(uint64_t a1, uint64_t a2)
{
  *(_OWORD *)a1 = *(_OWORD *)a2;
  *(_QWORD *)(a1 + 16) = *(_QWORD *)(a2 + 16);
  swift_bridgeObjectRelease();
  return a1;
}

uint64_t getEnumTagSinglePayload for vImage.ConvolutionKernel2D(uint64_t a1, int a2)
{
  unint64_t v2;

  if (!a2)
    return 0;
  if (a2 < 0 && *(_BYTE *)(a1 + 24))
    return *(_DWORD *)a1 + 0x80000000;
  v2 = *(_QWORD *)(a1 + 16);
  if (v2 >= 0xFFFFFFFF)
    LODWORD(v2) = -1;
  return (v2 + 1);
}

uint64_t storeEnumTagSinglePayload for vImage.ConvolutionKernel2D(uint64_t result, int a2, int a3)
{
  if (a2 < 0)
  {
    *(_QWORD *)(result + 8) = 0;
    *(_QWORD *)(result + 16) = 0;
    *(_QWORD *)result = a2 ^ 0x80000000;
    if (a3 < 0)
      *(_BYTE *)(result + 24) = 1;
  }
  else
  {
    if ((a3 & 0x80000000) == 0)
    {
      if (!a2)
        return result;
LABEL_8:
      *(_QWORD *)(result + 16) = (a2 - 1);
      return result;
    }
    *(_BYTE *)(result + 24) = 0;
    if (a2)
      goto LABEL_8;
  }
  return result;
}

uint64_t type metadata accessor for vImage.ConvolutionKernel2D(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return __swift_instantiateGenericMetadata(a1, a2, a3, a4, (uint64_t)&nominal type descriptor for vImage.ConvolutionKernel2D);
}

uint64_t type metadata instantiation function for vImage.EdgeMode()
{
  return swift_allocateGenericValueMetadata();
}

uint64_t type metadata completion function for vImage.EdgeMode()
{
  uint64_t result;
  unint64_t v1;

  result = swift_checkMetadataState();
  if (v1 <= 0x3F)
  {
    swift_initEnumMetadataSinglePayload();
    return 0;
  }
  return result;
}

_QWORD *initializeBufferWithCopyOfBuffer for vImage.EdgeMode(_QWORD *a1, _QWORD *a2, uint64_t a3)
{
  _QWORD *v4;
  uint64_t v5;
  uint64_t v6;
  unsigned int v7;
  size_t v8;
  unint64_t v9;
  uint64_t v10;
  unsigned int v11;
  _BOOL8 v12;
  BOOL v13;
  uint64_t v14;
  uint64_t v17;
  int v18;
  unsigned int v19;
  int v20;
  int v21;
  uint64_t v22;
  unsigned int v23;
  _BOOL8 v24;
  unsigned int v25;

  v4 = a1;
  v5 = *(_QWORD *)(a3 + 16);
  v6 = *(_QWORD *)(v5 - 8);
  v7 = *(_DWORD *)(v6 + 84);
  v8 = *(_QWORD *)(v6 + 64);
  v9 = v8;
  if (v7 <= 2)
  {
    if (v8 <= 3)
    {
      v11 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v11 > 0xFFFE)
      {
        v10 = 4;
      }
      else
      {
        v12 = v11 != 0;
        v13 = v11 >= 0xFF;
        v10 = 2;
        if (!v13)
          v10 = v12;
      }
    }
    else
    {
      v10 = 1;
    }
    v9 = v10 + v8;
  }
  v14 = *(_DWORD *)(v6 + 80);
  if (v14 > 7 || v9 > 0x18 || (*(_DWORD *)(v6 + 80) & 0x100000) != 0)
  {
    v17 = *a2;
    *v4 = *a2;
    v4 = (_QWORD *)(v17 + ((v14 + 16) & ~v14));
    swift_retain();
    return v4;
  }
  if (v7 <= 2)
  {
    if (v8 <= 3)
    {
      v19 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v19 > 0xFFFE)
      {
        v18 = *(_DWORD *)((char *)a2 + v8);
        if (!v18)
          goto LABEL_36;
        goto LABEL_27;
      }
      if (v19 > 0xFE)
      {
        v18 = *(unsigned __int16 *)((char *)a2 + v8);
        if (!*(_WORD *)((char *)a2 + v8))
          goto LABEL_36;
        goto LABEL_27;
      }
      if (!v19)
      {
LABEL_36:
        if (v7)
          goto LABEL_37;
LABEL_50:
        (*(void (**)(_QWORD *, _QWORD *, _QWORD))(v6 + 16))(a1, a2, *(_QWORD *)(a3 + 16));
        if (v8 > 3)
          goto LABEL_54;
        goto LABEL_51;
      }
    }
    v18 = *((unsigned __int8 *)a2 + v8);
    if (!*((_BYTE *)a2 + v8))
      goto LABEL_36;
LABEL_27:
    v20 = (v18 - 1) << (8 * v8);
    if (v8 > 3)
      v20 = 0;
    if ((_DWORD)v8)
    {
      if (v8 <= 3)
        v21 = v8;
      else
        v21 = 4;
      __asm { BR              X11 }
    }
    if (v20 + v7 != -1)
      goto LABEL_44;
    goto LABEL_50;
  }
LABEL_37:
  if ((*(unsigned int (**)(_QWORD *, _QWORD, _QWORD))(v6 + 48))(a2, *(unsigned int *)(v6 + 84), *(_QWORD *)(a3 + 16)))
  {
    if (v7 > 2)
    {
LABEL_57:
      memcpy(v4, a2, v8);
      return v4;
    }
LABEL_44:
    if (v8 <= 3)
    {
      v23 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v23 > 0xFFFE)
      {
        v22 = 4;
      }
      else
      {
        v24 = v23 != 0;
        v13 = v23 >= 0xFF;
        v22 = 2;
        if (!v13)
          v22 = v24;
      }
    }
    else
    {
      v22 = 1;
    }
    v8 += v22;
    goto LABEL_57;
  }
  (*(void (**)(_QWORD *, _QWORD *, uint64_t))(v6 + 16))(v4, a2, v5);
  if (v7 > 2)
    return v4;
  if (v8 > 3)
  {
LABEL_54:
    *((_BYTE *)v4 + v8) = 0;
    return v4;
  }
LABEL_51:
  v25 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
  if (v25 > 0xFFFE)
  {
    *(_DWORD *)((char *)v4 + v8) = 0;
    return v4;
  }
  if (v25 > 0xFE)
  {
    *(_WORD *)((char *)v4 + v8) = 0;
    return v4;
  }
  if (v25)
    goto LABEL_54;
  return v4;
}

uint64_t destroy for vImage.EdgeMode(uint64_t a1, uint64_t a2)
{
  uint64_t v3;
  uint64_t v4;
  unsigned int v5;
  uint64_t v6;
  char v7;
  int v8;
  unsigned int v9;
  int v10;
  uint64_t result;

  v3 = *(_QWORD *)(a2 + 16);
  v4 = *(_QWORD *)(v3 - 8);
  v5 = *(_DWORD *)(v4 + 84);
  if (v5 > 2)
  {
LABEL_18:
    result = (*(uint64_t (**)(uint64_t))(v4 + 48))(a1);
    if ((_DWORD)result)
      return result;
    return (*(uint64_t (**)(uint64_t, uint64_t))(v4 + 8))(a1, v3);
  }
  v6 = *(_QWORD *)(v4 + 64);
  v7 = 8 * v6;
  if (v6 <= 3)
  {
    v9 = (~(-1 << v7) - v5 + 3) >> v7;
    if (v9 > 0xFFFE)
    {
      v8 = *(_DWORD *)(a1 + v6);
      if (!v8)
        goto LABEL_17;
      goto LABEL_10;
    }
    if (v9 > 0xFE)
    {
      v8 = *(unsigned __int16 *)(a1 + v6);
      if (!*(_WORD *)(a1 + v6))
        goto LABEL_17;
      goto LABEL_10;
    }
    if (!v9)
    {
LABEL_17:
      if (!v5)
        return (*(uint64_t (**)(uint64_t, uint64_t))(v4 + 8))(a1, v3);
      goto LABEL_18;
    }
  }
  v8 = *(unsigned __int8 *)(a1 + v6);
  if (!*(_BYTE *)(a1 + v6))
    goto LABEL_17;
LABEL_10:
  v10 = (v8 - 1) << v7;
  if (v6 > 3)
    v10 = 0;
  if ((_DWORD)v6)
  {
    if (v6 > 3)
      LODWORD(v6) = 4;
    __asm { BR              X11 }
  }
  result = v5 + v10 + 1;
  if (v5 + v10 == -1)
    return (*(uint64_t (**)(uint64_t, uint64_t))(v4 + 8))(a1, v3);
  return result;
}

_BYTE *initializeWithCopy for vImage.EdgeMode(_BYTE *a1, _BYTE *a2, uint64_t a3)
{
  uint64_t v5;
  uint64_t v6;
  unsigned int v7;
  size_t v8;
  int v9;
  unsigned int v10;
  int v11;
  int v12;
  uint64_t v13;
  unsigned int v14;
  _BOOL8 v15;
  BOOL v16;
  unsigned int v17;

  v5 = *(_QWORD *)(a3 + 16);
  v6 = *(_QWORD *)(v5 - 8);
  v7 = *(_DWORD *)(v6 + 84);
  v8 = *(_QWORD *)(v6 + 64);
  if (v7 > 2)
    goto LABEL_20;
  if (v8 > 3)
    goto LABEL_3;
  v10 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
  if (v10 > 0xFFFE)
  {
    v9 = *(_DWORD *)&a2[v8];
    if (!v9)
      goto LABEL_19;
    goto LABEL_10;
  }
  if (v10 > 0xFE)
  {
    v9 = *(unsigned __int16 *)&a2[v8];
    if (!*(_WORD *)&a2[v8])
      goto LABEL_19;
    goto LABEL_10;
  }
  if (v10)
  {
LABEL_3:
    v9 = a2[v8];
    if (!a2[v8])
      goto LABEL_19;
LABEL_10:
    v11 = (v9 - 1) << (8 * v8);
    if (v8 > 3)
      v11 = 0;
    if ((_DWORD)v8)
    {
      if (v8 <= 3)
        v12 = *(_QWORD *)(v6 + 64);
      else
        v12 = 4;
      __asm { BR              X11 }
    }
    if (v11 + v7 != -1)
      goto LABEL_27;
LABEL_33:
    (*(void (**)(_BYTE *, _BYTE *, _QWORD))(v6 + 16))(a1, a2, *(_QWORD *)(a3 + 16));
    if (v8 > 3)
    {
LABEL_37:
      a1[v8] = 0;
      return a1;
    }
    goto LABEL_34;
  }
LABEL_19:
  if (!v7)
    goto LABEL_33;
LABEL_20:
  if ((*(unsigned int (**)(_BYTE *, _QWORD, _QWORD))(v6 + 48))(a2, *(unsigned int *)(v6 + 84), *(_QWORD *)(a3 + 16)))
  {
    if (v7 > 2)
    {
LABEL_40:
      memcpy(a1, a2, v8);
      return a1;
    }
LABEL_27:
    if (v8 <= 3)
    {
      v14 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v14 > 0xFFFE)
      {
        v13 = 4;
      }
      else
      {
        v15 = v14 != 0;
        v16 = v14 >= 0xFF;
        v13 = 2;
        if (!v16)
          v13 = v15;
      }
    }
    else
    {
      v13 = 1;
    }
    v8 += v13;
    goto LABEL_40;
  }
  (*(void (**)(_BYTE *, _BYTE *, uint64_t))(v6 + 16))(a1, a2, v5);
  if (v7 > 2)
    return a1;
  if (v8 > 3)
    goto LABEL_37;
LABEL_34:
  v17 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
  if (v17 > 0xFFFE)
  {
    *(_DWORD *)&a1[v8] = 0;
    return a1;
  }
  if (v17 > 0xFE)
  {
    *(_WORD *)&a1[v8] = 0;
    return a1;
  }
  if (v17)
    goto LABEL_37;
  return a1;
}

_BYTE *assignWithCopy for vImage.EdgeMode(_BYTE *a1, _BYTE *a2, uint64_t a3)
{
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  size_t v8;
  int v9;
  unsigned int (*v10)(_BYTE *, uint64_t, uint64_t);
  unsigned int v11;
  unsigned int v12;
  int v13;
  int v14;
  int v15;
  unsigned int v16;
  int v17;
  int v18;
  int v19;
  unsigned int v20;
  int v21;
  int v22;
  uint64_t v23;
  unsigned int v24;
  _BOOL8 v25;
  BOOL v26;
  unsigned int v27;

  v5 = *(_QWORD *)(a3 + 16);
  v6 = *(_QWORD *)(v5 - 8);
  v7 = *(unsigned int *)(v6 + 84);
  v8 = *(_QWORD *)(v6 + 64);
  if (v7 <= 2)
  {
    if (v8 > 3)
      goto LABEL_3;
    v12 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
    if (v12 > 0xFFFE)
    {
      v9 = *(_DWORD *)&a1[v8];
      if (v9)
        goto LABEL_18;
    }
    else
    {
      if (v12 <= 0xFE)
      {
        if (!v12)
          goto LABEL_27;
LABEL_3:
        v9 = a1[v8];
        if (!a1[v8])
          goto LABEL_27;
LABEL_18:
        v13 = (v9 - 1) << (8 * v8);
        if (v8 > 3)
          v13 = 0;
        if ((_DWORD)v8)
        {
          if (v8 <= 3)
            v14 = *(_QWORD *)(v6 + 64);
          else
            v14 = 4;
          __asm { BR              X11 }
        }
        if (v13 + (_DWORD)v7 == -1)
        {
LABEL_29:
          if (v8 > 3)
            goto LABEL_30;
          v16 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
          if (v16 > 0xFFFE)
          {
            v15 = *(_DWORD *)&a2[v8];
            if (v15)
              goto LABEL_37;
          }
          else
          {
            if (v16 <= 0xFE)
            {
              if (!v16)
                goto LABEL_66;
LABEL_30:
              v15 = a2[v8];
              if (!a2[v8])
                goto LABEL_66;
LABEL_37:
              v17 = (v15 - 1) << (8 * v8);
              if (v8 > 3)
                v17 = 0;
              if ((_DWORD)v8)
              {
                if (v8 <= 3)
                  v18 = v8;
                else
                  v18 = 4;
                __asm { BR              X11 }
              }
              if ((_DWORD)v7 + v17 != -1)
              {
LABEL_69:
                (*(void (**)(_BYTE *, uint64_t))(v6 + 8))(a1, v5);
                if (v7 <= 2)
                  goto LABEL_72;
                goto LABEL_85;
              }
LABEL_88:
              (*(void (**)(_BYTE *, _BYTE *, uint64_t))(v6 + 24))(a1, a2, v5);
              return a1;
            }
            v15 = *(unsigned __int16 *)&a2[v8];
            if (*(_WORD *)&a2[v8])
              goto LABEL_37;
          }
LABEL_66:
          if (!(_DWORD)v7)
            goto LABEL_88;
          v10 = *(unsigned int (**)(_BYTE *, uint64_t, uint64_t))(v6 + 48);
          goto LABEL_68;
        }
LABEL_46:
        if (v8 <= 3)
        {
          v20 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
          if (v20 > 0xFFFE)
          {
            v19 = *(_DWORD *)&a2[v8];
            if (!v19)
              goto LABEL_63;
            goto LABEL_54;
          }
          if (v20 > 0xFE)
          {
            v19 = *(unsigned __int16 *)&a2[v8];
            if (!*(_WORD *)&a2[v8])
              goto LABEL_63;
            goto LABEL_54;
          }
          if (!v20)
          {
LABEL_63:
            if ((_DWORD)v7)
            {
              v11 = (*(uint64_t (**)(_BYTE *, uint64_t, uint64_t))(v6 + 48))(a2, v7, v5);
              goto LABEL_7;
            }
LABEL_78:
            (*(void (**)(_BYTE *, _BYTE *, uint64_t))(v6 + 16))(a1, a2, v5);
            if (v8 <= 3)
              goto LABEL_79;
LABEL_82:
            a1[v8] = 0;
            return a1;
          }
        }
        v19 = a2[v8];
        if (!a2[v8])
          goto LABEL_63;
LABEL_54:
        v21 = (v19 - 1) << (8 * v8);
        if (v8 > 3)
          v21 = 0;
        if ((_DWORD)v8)
        {
          if (v8 <= 3)
            v22 = v8;
          else
            v22 = 4;
          __asm { BR              X11 }
        }
        if (v21 + (_DWORD)v7 != -1)
          goto LABEL_72;
        goto LABEL_78;
      }
      v9 = *(unsigned __int16 *)&a1[v8];
      if (*(_WORD *)&a1[v8])
        goto LABEL_18;
    }
LABEL_27:
    if (!(_DWORD)v7
      || !(*(unsigned int (**)(_BYTE *, _QWORD, _QWORD))(v6 + 48))(a1, *(unsigned int *)(v6 + 84), *(_QWORD *)(a3 + 16)))
    {
      goto LABEL_29;
    }
    goto LABEL_46;
  }
  v10 = *(unsigned int (**)(_BYTE *, uint64_t, uint64_t))(v6 + 48);
  if (!v10(a1, *(unsigned int *)(v6 + 84), *(_QWORD *)(a3 + 16)))
  {
LABEL_68:
    if (v10(a2, v7, v5))
      goto LABEL_69;
    goto LABEL_88;
  }
  v11 = v10(a2, v7, v5);
LABEL_7:
  if (v11)
  {
    if (v7 <= 2)
    {
LABEL_72:
      if (v8 <= 3)
      {
        v24 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
        if (v24 > 0xFFFE)
        {
          v23 = 4;
        }
        else
        {
          v25 = v24 != 0;
          v26 = v24 >= 0xFF;
          v23 = 2;
          if (!v26)
            v23 = v25;
        }
      }
      else
      {
        v23 = 1;
      }
      v8 += v23;
    }
LABEL_85:
    memcpy(a1, a2, v8);
    return a1;
  }
  (*(void (**)(_BYTE *, _BYTE *, uint64_t))(v6 + 16))(a1, a2, v5);
  if (v7 > 2)
    return a1;
  if (v8 > 3)
    goto LABEL_82;
LABEL_79:
  v27 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
  if (v27 > 0xFFFE)
  {
    *(_DWORD *)&a1[v8] = 0;
    return a1;
  }
  if (v27 > 0xFE)
  {
    *(_WORD *)&a1[v8] = 0;
    return a1;
  }
  if (v27)
    goto LABEL_82;
  return a1;
}

_BYTE *initializeWithTake for vImage.EdgeMode(_BYTE *a1, _BYTE *a2, uint64_t a3)
{
  uint64_t v5;
  uint64_t v6;
  unsigned int v7;
  size_t v8;
  int v9;
  unsigned int v10;
  int v11;
  int v12;
  uint64_t v13;
  unsigned int v14;
  _BOOL8 v15;
  BOOL v16;
  unsigned int v17;

  v5 = *(_QWORD *)(a3 + 16);
  v6 = *(_QWORD *)(v5 - 8);
  v7 = *(_DWORD *)(v6 + 84);
  v8 = *(_QWORD *)(v6 + 64);
  if (v7 > 2)
    goto LABEL_20;
  if (v8 > 3)
    goto LABEL_3;
  v10 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
  if (v10 > 0xFFFE)
  {
    v9 = *(_DWORD *)&a2[v8];
    if (!v9)
      goto LABEL_19;
    goto LABEL_10;
  }
  if (v10 > 0xFE)
  {
    v9 = *(unsigned __int16 *)&a2[v8];
    if (!*(_WORD *)&a2[v8])
      goto LABEL_19;
    goto LABEL_10;
  }
  if (v10)
  {
LABEL_3:
    v9 = a2[v8];
    if (!a2[v8])
      goto LABEL_19;
LABEL_10:
    v11 = (v9 - 1) << (8 * v8);
    if (v8 > 3)
      v11 = 0;
    if ((_DWORD)v8)
    {
      if (v8 <= 3)
        v12 = *(_QWORD *)(v6 + 64);
      else
        v12 = 4;
      __asm { BR              X11 }
    }
    if (v11 + v7 != -1)
      goto LABEL_27;
LABEL_33:
    (*(void (**)(_BYTE *, _BYTE *, _QWORD))(v6 + 32))(a1, a2, *(_QWORD *)(a3 + 16));
    if (v8 > 3)
    {
LABEL_37:
      a1[v8] = 0;
      return a1;
    }
    goto LABEL_34;
  }
LABEL_19:
  if (!v7)
    goto LABEL_33;
LABEL_20:
  if ((*(unsigned int (**)(_BYTE *, _QWORD, _QWORD))(v6 + 48))(a2, *(unsigned int *)(v6 + 84), *(_QWORD *)(a3 + 16)))
  {
    if (v7 > 2)
    {
LABEL_40:
      memcpy(a1, a2, v8);
      return a1;
    }
LABEL_27:
    if (v8 <= 3)
    {
      v14 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v14 > 0xFFFE)
      {
        v13 = 4;
      }
      else
      {
        v15 = v14 != 0;
        v16 = v14 >= 0xFF;
        v13 = 2;
        if (!v16)
          v13 = v15;
      }
    }
    else
    {
      v13 = 1;
    }
    v8 += v13;
    goto LABEL_40;
  }
  (*(void (**)(_BYTE *, _BYTE *, uint64_t))(v6 + 32))(a1, a2, v5);
  if (v7 > 2)
    return a1;
  if (v8 > 3)
    goto LABEL_37;
LABEL_34:
  v17 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
  if (v17 > 0xFFFE)
  {
    *(_DWORD *)&a1[v8] = 0;
    return a1;
  }
  if (v17 > 0xFE)
  {
    *(_WORD *)&a1[v8] = 0;
    return a1;
  }
  if (v17)
    goto LABEL_37;
  return a1;
}

_BYTE *assignWithTake for vImage.EdgeMode(_BYTE *a1, _BYTE *a2, uint64_t a3)
{
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  size_t v8;
  int v9;
  unsigned int (*v10)(_BYTE *, uint64_t, uint64_t);
  unsigned int v11;
  unsigned int v12;
  int v13;
  int v14;
  int v15;
  unsigned int v16;
  int v17;
  int v18;
  int v19;
  unsigned int v20;
  int v21;
  int v22;
  uint64_t v23;
  unsigned int v24;
  _BOOL8 v25;
  BOOL v26;
  unsigned int v27;

  v5 = *(_QWORD *)(a3 + 16);
  v6 = *(_QWORD *)(v5 - 8);
  v7 = *(unsigned int *)(v6 + 84);
  v8 = *(_QWORD *)(v6 + 64);
  if (v7 <= 2)
  {
    if (v8 > 3)
      goto LABEL_3;
    v12 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
    if (v12 > 0xFFFE)
    {
      v9 = *(_DWORD *)&a1[v8];
      if (v9)
        goto LABEL_18;
    }
    else
    {
      if (v12 <= 0xFE)
      {
        if (!v12)
          goto LABEL_27;
LABEL_3:
        v9 = a1[v8];
        if (!a1[v8])
          goto LABEL_27;
LABEL_18:
        v13 = (v9 - 1) << (8 * v8);
        if (v8 > 3)
          v13 = 0;
        if ((_DWORD)v8)
        {
          if (v8 <= 3)
            v14 = *(_QWORD *)(v6 + 64);
          else
            v14 = 4;
          __asm { BR              X11 }
        }
        if (v13 + (_DWORD)v7 == -1)
        {
LABEL_29:
          if (v8 > 3)
            goto LABEL_30;
          v16 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
          if (v16 > 0xFFFE)
          {
            v15 = *(_DWORD *)&a2[v8];
            if (v15)
              goto LABEL_37;
          }
          else
          {
            if (v16 <= 0xFE)
            {
              if (!v16)
                goto LABEL_66;
LABEL_30:
              v15 = a2[v8];
              if (!a2[v8])
                goto LABEL_66;
LABEL_37:
              v17 = (v15 - 1) << (8 * v8);
              if (v8 > 3)
                v17 = 0;
              if ((_DWORD)v8)
              {
                if (v8 <= 3)
                  v18 = v8;
                else
                  v18 = 4;
                __asm { BR              X11 }
              }
              if ((_DWORD)v7 + v17 != -1)
              {
LABEL_69:
                (*(void (**)(_BYTE *, uint64_t))(v6 + 8))(a1, v5);
                if (v7 <= 2)
                  goto LABEL_72;
                goto LABEL_85;
              }
LABEL_88:
              (*(void (**)(_BYTE *, _BYTE *, uint64_t))(v6 + 40))(a1, a2, v5);
              return a1;
            }
            v15 = *(unsigned __int16 *)&a2[v8];
            if (*(_WORD *)&a2[v8])
              goto LABEL_37;
          }
LABEL_66:
          if (!(_DWORD)v7)
            goto LABEL_88;
          v10 = *(unsigned int (**)(_BYTE *, uint64_t, uint64_t))(v6 + 48);
          goto LABEL_68;
        }
LABEL_46:
        if (v8 <= 3)
        {
          v20 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
          if (v20 > 0xFFFE)
          {
            v19 = *(_DWORD *)&a2[v8];
            if (!v19)
              goto LABEL_63;
            goto LABEL_54;
          }
          if (v20 > 0xFE)
          {
            v19 = *(unsigned __int16 *)&a2[v8];
            if (!*(_WORD *)&a2[v8])
              goto LABEL_63;
            goto LABEL_54;
          }
          if (!v20)
          {
LABEL_63:
            if ((_DWORD)v7)
            {
              v11 = (*(uint64_t (**)(_BYTE *, uint64_t, uint64_t))(v6 + 48))(a2, v7, v5);
              goto LABEL_7;
            }
LABEL_78:
            (*(void (**)(_BYTE *, _BYTE *, uint64_t))(v6 + 32))(a1, a2, v5);
            if (v8 <= 3)
              goto LABEL_79;
LABEL_82:
            a1[v8] = 0;
            return a1;
          }
        }
        v19 = a2[v8];
        if (!a2[v8])
          goto LABEL_63;
LABEL_54:
        v21 = (v19 - 1) << (8 * v8);
        if (v8 > 3)
          v21 = 0;
        if ((_DWORD)v8)
        {
          if (v8 <= 3)
            v22 = v8;
          else
            v22 = 4;
          __asm { BR              X11 }
        }
        if (v21 + (_DWORD)v7 != -1)
          goto LABEL_72;
        goto LABEL_78;
      }
      v9 = *(unsigned __int16 *)&a1[v8];
      if (*(_WORD *)&a1[v8])
        goto LABEL_18;
    }
LABEL_27:
    if (!(_DWORD)v7
      || !(*(unsigned int (**)(_BYTE *, _QWORD, _QWORD))(v6 + 48))(a1, *(unsigned int *)(v6 + 84), *(_QWORD *)(a3 + 16)))
    {
      goto LABEL_29;
    }
    goto LABEL_46;
  }
  v10 = *(unsigned int (**)(_BYTE *, uint64_t, uint64_t))(v6 + 48);
  if (!v10(a1, *(unsigned int *)(v6 + 84), *(_QWORD *)(a3 + 16)))
  {
LABEL_68:
    if (v10(a2, v7, v5))
      goto LABEL_69;
    goto LABEL_88;
  }
  v11 = v10(a2, v7, v5);
LABEL_7:
  if (v11)
  {
    if (v7 <= 2)
    {
LABEL_72:
      if (v8 <= 3)
      {
        v24 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
        if (v24 > 0xFFFE)
        {
          v23 = 4;
        }
        else
        {
          v25 = v24 != 0;
          v26 = v24 >= 0xFF;
          v23 = 2;
          if (!v26)
            v23 = v25;
        }
      }
      else
      {
        v23 = 1;
      }
      v8 += v23;
    }
LABEL_85:
    memcpy(a1, a2, v8);
    return a1;
  }
  (*(void (**)(_BYTE *, _BYTE *, uint64_t))(v6 + 32))(a1, a2, v5);
  if (v7 > 2)
    return a1;
  if (v8 > 3)
    goto LABEL_82;
LABEL_79:
  v27 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
  if (v27 > 0xFFFE)
  {
    *(_DWORD *)&a1[v8] = 0;
    return a1;
  }
  if (v27 > 0xFE)
  {
    *(_WORD *)&a1[v8] = 0;
    return a1;
  }
  if (v27)
    goto LABEL_82;
  return a1;
}

uint64_t getEnumTagSinglePayload for vImage.EdgeMode(uint64_t a1, unsigned int a2, uint64_t a3)
{
  uint64_t v4;
  unsigned int v5;
  unsigned int v6;
  uint64_t v7;
  uint64_t v8;
  unsigned int v9;
  _BOOL8 v10;
  BOOL v11;
  int v12;
  char v13;
  int v14;
  unsigned int v15;
  int v16;
  int v17;
  unsigned int v18;
  uint64_t result;

  v4 = *(_QWORD *)(*(_QWORD *)(a3 + 16) - 8);
  v5 = *(_DWORD *)(v4 + 84);
  v6 = v5 - 3;
  v7 = *(_QWORD *)(v4 + 64);
  if (v5 <= 2)
  {
    v6 = 0;
    if (v7 <= 3)
    {
      v9 = (~(-1 << (8 * v7)) - v5 + 3) >> (8 * v7);
      if (v9 > 0xFFFE)
      {
        v8 = 4;
      }
      else
      {
        v10 = v9 != 0;
        v11 = v9 >= 0xFF;
        v8 = 2;
        if (!v11)
          v8 = v10;
      }
    }
    else
    {
      v8 = 1;
    }
    v7 += v8;
  }
  if (!a2)
    return 0;
  v12 = a2 - v6;
  if (a2 <= v6)
    goto LABEL_29;
  v13 = 8 * v7;
  if (v7 > 3)
    goto LABEL_13;
  v15 = ((v12 + ~(-1 << v13)) >> v13) + 1;
  if (HIWORD(v15))
  {
    v14 = *(_DWORD *)(a1 + v7);
    if (v14)
      goto LABEL_20;
  }
  else
  {
    if (v15 <= 0xFF)
    {
      if (v15 < 2)
        goto LABEL_29;
LABEL_13:
      v14 = *(unsigned __int8 *)(a1 + v7);
      if (!*(_BYTE *)(a1 + v7))
        goto LABEL_29;
LABEL_20:
      v16 = (v14 - 1) << v13;
      if (v7 > 3)
        v16 = 0;
      if ((_DWORD)v7)
      {
        if (v7 <= 3)
          v17 = v7;
        else
          v17 = 4;
        __asm { BR              X12 }
      }
      return v6 + v16 + 1;
    }
    v14 = *(unsigned __int16 *)(a1 + v7);
    if (*(_WORD *)(a1 + v7))
      goto LABEL_20;
  }
LABEL_29:
  if (!v6)
    return 0;
  if (!v5)
    return 0;
  v18 = (*(uint64_t (**)(void))(v4 + 48))();
  v11 = v18 >= 3;
  result = v18 - 3;
  if ((_DWORD)result == 0 || !v11)
    return 0;
  return result;
}

void storeEnumTagSinglePayload for vImage.EdgeMode(_WORD *a1, unsigned int a2, unsigned int a3, uint64_t a4)
{
  uint64_t v6;
  unsigned int v7;
  unsigned int v8;
  size_t v9;
  size_t v10;
  uint64_t v11;
  unsigned int v12;
  _BOOL8 v13;
  BOOL v14;
  unsigned int v15;
  unsigned int v16;
  int v17;
  unsigned int v18;
  int v19;

  v6 = *(_QWORD *)(*(_QWORD *)(a4 + 16) - 8);
  v7 = *(_DWORD *)(v6 + 84);
  v8 = v7 - 3;
  v9 = *(_QWORD *)(v6 + 64);
  v10 = v9;
  if (v7 <= 2)
  {
    v8 = 0;
    if (v9 <= 3)
    {
      v12 = (~(-1 << (8 * v9)) - v7 + 3) >> (8 * v9);
      if (v12 > 0xFFFE)
      {
        v11 = 4;
      }
      else
      {
        v13 = v12 != 0;
        v14 = v12 >= 0xFF;
        v11 = 2;
        if (!v14)
          v11 = v13;
      }
    }
    else
    {
      v11 = 1;
    }
    v10 = v11 + v9;
  }
  v14 = a3 >= v8;
  v15 = a3 - v8;
  if (v15 != 0 && v14)
  {
    if (v10 <= 3)
    {
      v18 = ((v15 + ~(-1 << (8 * v10))) >> (8 * v10)) + 1;
      if (HIWORD(v18))
      {
        v16 = 4u;
      }
      else if (v18 >= 0x100)
      {
        v16 = 2;
      }
      else
      {
        v16 = v18 > 1;
      }
    }
    else
    {
      v16 = 1u;
    }
  }
  else
  {
    v16 = 0u;
  }
  if (v8 < a2)
  {
    v17 = ~v8 + a2;
    if (v10 < 4)
    {
      if ((_DWORD)v10)
      {
        v19 = v17 & ~(-1 << (8 * v10));
        bzero(a1, v10);
        if ((_DWORD)v10 == 3)
        {
          *a1 = v19;
          *((_BYTE *)a1 + 2) = BYTE2(v19);
        }
        else if ((_DWORD)v10 == 2)
        {
          *a1 = v19;
        }
        else
        {
          *(_BYTE *)a1 = v19;
        }
      }
    }
    else
    {
      bzero(a1, v10);
      *(_DWORD *)a1 = v17;
    }
    __asm { BR              X10 }
  }
  __asm { BR              X12 }
}

void sub_1CAB0FFDC()
{
  uint64_t v0;
  int v1;

  *(_WORD *)v0 = v1;
  *(_BYTE *)(v0 + 2) = BYTE2(v1);
  JUMPOUT(0x1CAB0FFC8);
}

void sub_1CAB0FFEC()
{
  _WORD *v0;
  __int16 v1;

  *v0 = v1;
  JUMPOUT(0x1CAB0FFC8);
}

void sub_1CAB0FFF4()
{
  _DWORD *v0;
  int v1;

  *v0 = v1;
  JUMPOUT(0x1CAB0FFC8);
}

uint64_t getEnumTag for vImage.EdgeMode(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  unsigned int v3;
  uint64_t v4;
  char v5;
  int v6;
  unsigned int v7;
  int v8;

  v2 = *(_QWORD *)(*(_QWORD *)(a2 + 16) - 8);
  v3 = *(_DWORD *)(v2 + 84);
  if (v3 > 2)
    return (*(uint64_t (**)(void))(v2 + 48))();
  v4 = *(_QWORD *)(v2 + 64);
  v5 = 8 * v4;
  if (v4 > 3)
    goto LABEL_3;
  v7 = (~(-1 << v5) - v3 + 3) >> v5;
  if (v7 > 0xFFFE)
  {
    v6 = *(_DWORD *)(a1 + v4);
    if (v6)
      goto LABEL_10;
  }
  else
  {
    if (v7 <= 0xFE)
    {
      if (!v7)
        goto LABEL_17;
LABEL_3:
      v6 = *(unsigned __int8 *)(a1 + v4);
      if (!*(_BYTE *)(a1 + v4))
        goto LABEL_17;
LABEL_10:
      v8 = (v6 - 1) << v5;
      if (v4 > 3)
        v8 = 0;
      if (!(_DWORD)v4)
        return v3 + v8 + 1;
      if (v4 > 3)
        LODWORD(v4) = 4;
      return ((uint64_t (*)(void))((char *)&loc_1CAB100A8 + 4 * byte_1CAB60D9E[(v4 - 1)]))();
    }
    v6 = *(unsigned __int16 *)(a1 + v4);
    if (*(_WORD *)(a1 + v4))
      goto LABEL_10;
  }
LABEL_17:
  if (v3)
    return (*(uint64_t (**)(void))(v2 + 48))();
  return 0;
}

void destructiveInjectEnumTag for vImage.EdgeMode(_WORD *a1, unsigned int a2, uint64_t a3)
{
  uint64_t v4;
  unsigned int v5;
  size_t v6;
  unsigned int v7;
  int v8;
  unsigned int v9;
  int v10;

  v4 = *(_QWORD *)(*(_QWORD *)(a3 + 16) - 8);
  v5 = *(_DWORD *)(v4 + 84);
  v6 = *(_QWORD *)(v4 + 64);
  if (v5 > 2)
  {
    v7 = 0u;
  }
  else if (v6 <= 3)
  {
    v9 = (~(-1 << (8 * v6)) - v5 + 3) >> (8 * v6);
    if (v9 > 0xFFFE)
    {
      v7 = 4u;
    }
    else if (v9 >= 0xFF)
    {
      v7 = 2;
    }
    else
    {
      v7 = v9 != 0;
    }
  }
  else
  {
    v7 = 1u;
  }
  if (v5 < a2)
  {
    v8 = ~v5 + a2;
    if (v6 < 4)
    {
      if ((_DWORD)v6)
      {
        v10 = v8 & ~(-1 << (8 * v6));
        bzero(a1, v6);
        if ((_DWORD)v6 == 3)
        {
          *a1 = v10;
          *((_BYTE *)a1 + 2) = BYTE2(v10);
        }
        else if ((_DWORD)v6 == 2)
        {
          *a1 = v10;
        }
        else
        {
          *(_BYTE *)a1 = v10;
        }
      }
    }
    else
    {
      bzero(a1, v6);
      *(_DWORD *)a1 = v8;
    }
    __asm { BR              X10 }
  }
  __asm { BR              X11 }
}

uint64_t type metadata accessor for vImage.EdgeMode(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return __swift_instantiateGenericMetadata(a1, a2, a3, a4, (uint64_t)&nominal type descriptor for vImage.EdgeMode);
}

uint64_t sub_1CAB102B4()
{
  swift_bridgeObjectRelease();
  return swift_deallocObject();
}

uint64_t static vDSP.add<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.add<A>(_:_:)(a1, a2, a3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.add<A>(_:_:));
}

{
  return static vDSP.add<A>(_:_:)(a1, a2, a3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.add<A>(_:_:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A>(_:_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t v2;

  return closure #1 in static vDSP.add<A>(_:_:)(a1, a2, *(_QWORD *)(v2 + 40), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float))static vDSP.add<A, B>(_:_:result:), *(float *)(v2 + 32));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.add<A>(_:_:)(a1, a2, *(_QWORD *)(v2 + 40), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double))static vDSP.add<A, B>(_:_:result:), *(double *)(v2 + 32));
}

uint64_t static vDSP.add<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  return static vDSP.add<A, B>(_:_:result:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B>(_:_:result:));
}

uint64_t static vDSP.add<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  return static vDSP.add<A, B>(_:_:result:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B>(_:_:result:));
}

uint64_t static vDSP.add<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.add<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.add<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.add<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.add<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C>(_:_:result:));
}

{
  return static vDSP.add<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C>(_:_:result:));
}

uint64_t static vDSP.add<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, _QWORD *))
{
  uint64_t v16;
  _QWORD v18[10];

  v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  v18[2] = a3;
  v18[3] = a4;
  v18[4] = a5;
  v18[5] = a6;
  v18[6] = a1;
  v18[7] = a2;
  return a8(v16, a7, v18);
}

uint64_t static vDSP.subtract<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.subtract<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.subtract<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.subtract<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.subtract<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B, C>(_:_:result:));
}

{
  return static vDSP.subtract<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B, C>(_:_:result:));
}

uint64_t static vDSP.subtract<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, _QWORD *))
{
  uint64_t v16;
  _QWORD v18[10];

  v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  v18[2] = a3;
  v18[3] = a4;
  v18[4] = a5;
  v18[5] = a6;
  v18[6] = a1;
  v18[7] = a2;
  return a8(v16, a7, v18);
}

uint64_t closure #1 in static vDSP.subtract<A, B>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, unint64_t *a10, void (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v17;
  uint64_t v18;
  uint64_t result;

  v17 = __swift_instantiateConcreteTypeFromMangledName(a9);
  v18 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a10, a9);
  a11(a3, a4, a1, a5, a6, v17, a7, a8, v18);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
  *a2 = result;
  return result;
}

uint64_t static vDSP.subtract<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  char *v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t (*v27)(uint64_t);
  uint64_t v28;
  uint64_t v29;
  void (*v30)(char *, uint64_t, uint64_t);
  uint64_t v31;
  void (*v32)(char *, uint64_t, uint64_t);
  uint64_t v33;
  uint64_t (*v34)(uint64_t, uint64_t);
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  void (*v38)(char *, uint64_t);
  uint64_t v39;
  uint64_t result;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  void (*v44)(char *, uint64_t);
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;

  v50 = a7;
  v53 = a2;
  v14 = *(_QWORD *)(a4 - 8);
  v15 = MEMORY[0x1E0C80A78](a1);
  v17 = (char *)&v45 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  v18 = MEMORY[0x1E0C80A78](v15);
  v20 = (char *)&v45 - v19;
  v22 = *(_QWORD *)(v21 - 8);
  MEMORY[0x1E0C80A78](v18);
  v24 = (char *)&v45 - ((v23 + 15) & 0xFFFFFFFFFFFFFFF0);
  v47 = v26;
  v48 = v25;
  v27 = *(uint64_t (**)(uint64_t))(*(_QWORD *)(v26 + 8) + 16);
  v49 = v28;
  v29 = v27(v28);
  v51 = v22;
  v30 = *(void (**)(char *, uint64_t, uint64_t))(v22 + 16);
  v31 = v14;
  v45 = a1;
  v30(v24, a1, a5);
  v32 = *(void (**)(char *, uint64_t, uint64_t))(v14 + 16);
  v33 = v29;
  v32(v20, v53, a4);
  v34 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  v52 = a5;
  v46 = a8;
  v35 = v34(a5, a8);
  v32(v17, (uint64_t)v20, a4);
  if (v35 == v33)
  {
    v36 = v50;
    v37 = (*(uint64_t (**)(uint64_t, uint64_t))(v50 + 16))(a4, v50);
    v38 = *(void (**)(char *, uint64_t))(v31 + 8);
    v38(v17, a4);
    v38(v20, a4);
    v39 = v52;
    result = (*(uint64_t (**)(char *, uint64_t))(v51 + 8))(v24, v52);
    if (v37 == v33)
    {
      v41 = MEMORY[0x1E0C80A78](a10);
      *(&v45 - 10) = a4;
      *(&v45 - 9) = v39;
      *(&v45 - 8) = v49;
      *(&v45 - 7) = v36;
      v42 = v47;
      *(&v45 - 6) = v46;
      *(&v45 - 5) = v42;
      v43 = v45;
      *(&v45 - 4) = v53;
      *(&v45 - 3) = v43;
      *(&v45 - 2) = v33;
      return (*(uint64_t (**)(uint64_t))(v42 + 16))(v41);
    }
  }
  else
  {
    v44 = *(void (**)(char *, uint64_t))(v31 + 8);
    v44(v17, a4);
    v44(v20, a4);
    result = (*(uint64_t (**)(char *, uint64_t))(v51 + 8))(v24, v52);
  }
  __break(1u);
  return result;
}

uint64_t static vDSP.multiply<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.add<A>(_:_:)(a1, a2, a3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.multiply<A>(_:_:));
}

{
  return static vDSP.add<A>(_:_:)(a1, a2, a3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.multiply<A>(_:_:));
}

uint64_t static vDSP.multiply<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  return static vDSP.add<A, B>(_:_:result:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B>(_:_:result:));
}

uint64_t static vDSP.add<A, B>(_:_:result:)(uint64_t a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t result;
  _QWORD v22[2];

  v22[0] = a8;
  v14 = *(_QWORD *)(a4 - 8);
  MEMORY[0x1E0C80A78](a1);
  v16 = (char *)v22 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v17 + 8) + 16))(v18);
  (*(void (**)(char *, uint64_t, uint64_t))(v14 + 16))(v16, a1, a4);
  v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  result = (*(uint64_t (**)(char *, uint64_t))(v14 + 8))(v16, a4);
  if (v20 == v19)
  {
    MEMORY[0x1E0C80A78](result);
    v22[-8] = a4;
    v22[-7] = a5;
    v22[-6] = a6;
    v22[-5] = a7;
    v22[-4] = a1;
    *(float *)&v22[-3] = a2;
    v22[-2] = v19;
    return (*(uint64_t (**)(_QWORD))(a7 + 16))(v22[0]);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, _QWORD *a3, uint64_t a4, uint64_t (*a5)(void))
{
  if (!a1)
    goto LABEL_6;
  if (!*a3)
    goto LABEL_7;
  if (a4 < 0)
  {
    __break(1u);
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  return a5();
}

uint64_t static vDSP.multiply<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  return static vDSP.add<A, B>(_:_:result:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B>(_:_:result:));
}

uint64_t static vDSP.add<A, B>(_:_:result:)(uint64_t a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t result;
  _QWORD v22[2];

  v22[0] = a8;
  v14 = *(_QWORD *)(a4 - 8);
  MEMORY[0x1E0C80A78](a1);
  v16 = (char *)v22 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v17 + 8) + 16))(v18);
  (*(void (**)(char *, uint64_t, uint64_t))(v14 + 16))(v16, a1, a4);
  v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  result = (*(uint64_t (**)(char *, uint64_t))(v14 + 8))(v16, a4);
  if (v20 == v19)
  {
    MEMORY[0x1E0C80A78](result);
    v22[-8] = a4;
    v22[-7] = a5;
    v22[-6] = a6;
    v22[-5] = a7;
    v22[-4] = a1;
    *(double *)&v22[-3] = a2;
    v22[-2] = v19;
    return (*(uint64_t (**)(_QWORD))(a7 + 16))(v22[0]);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t static vDSP.multiply<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.add<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.add<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.multiply<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.add<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(_:_:result:));
}

{
  return static vDSP.add<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(_:_:result:));
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, _QWORD *a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, uint64_t, uint64_t))
{
  if (!a3)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (!result)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (*a5)
  {
    if ((a6 & 0x8000000000000000) == 0)
      return a7(a3, 1, result, 1);
    __break(1u);
    goto LABEL_7;
  }
LABEL_9:
  __break(1u);
  return result;
}

uint64_t static vDSP.divide<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;

  v3 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.divide<A>(_:_:));
}

{
  uint64_t v3;

  v3 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.divide<A>(_:_:));
}

{
  return static vDSP.add<A>(_:_:)(a1, a2, a3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.divide<A>(_:_:));
}

{
  return static vDSP.add<A>(_:_:)(a1, a2, a3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.divide<A>(_:_:));
}

uint64_t closure #1 in static vDSP.divide<A>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, float a6)
{
  uint64_t v12;
  uint64_t v13;
  uint64_t result;

  v12 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v13 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.divide<A, B>(_:_:result:)(a3, a6, a1, a4, v12, a5, v13);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.divide<A, B>(_:_:result:)(uint64_t a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v13;
  uint64_t v14;
  _BYTE *v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t result;
  _BYTE v21[16];

  v13 = *(_QWORD *)(a4 - 8);
  MEMORY[0x1E0C80A78](a1);
  v15 = &v21[-((v14 + 15) & 0xFFFFFFFFFFFFFFF0)];
  v18 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v16 + 8) + 16))(v17);
  (*(void (**)(_BYTE *, uint64_t, uint64_t))(v13 + 16))(v15, a1, a4);
  v19 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  result = (*(uint64_t (**)(_BYTE *, uint64_t))(v13 + 8))(v15, a4);
  if (v19 == v18)
  {
    MEMORY[0x1E0C80A78](result);
    *(_QWORD *)&v21[-64] = a4;
    *(_QWORD *)&v21[-56] = a5;
    *(_QWORD *)&v21[-48] = a6;
    *(_QWORD *)&v21[-40] = a7;
    *(_QWORD *)&v21[-32] = a1;
    *(float *)&v21[-24] = a2;
    *(_QWORD *)&v21[-16] = v18;
    return (*(uint64_t (**)(_QWORD))(a7 + 16))(partial apply for closure #1 in static vDSP.divide<A, B>(_:_:result:));
  }
  else
  {
    __break(1u);
  }
  return result;
}

void closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(const float *a1, int a2, float **a3, vDSP_Length __N, float a5)
{
  float v5;

  if (!a1)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  v5 = a5;
  if (*a3)
  {
    if ((__N & 0x8000000000000000) == 0)
    {
      vDSP_vsdiv(a1, 1, &v5, *a3, 1, __N);
      return;
    }
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
}

uint64_t closure #1 in static vDSP.divide<A>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, double a6)
{
  uint64_t v12;
  uint64_t v13;
  uint64_t result;

  v12 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v13 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.divide<A, B>(_:_:result:)(a3, a6, a1, a4, v12, a5, v13);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.divide<A, B>(_:_:result:)(uint64_t a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v13;
  uint64_t v14;
  _BYTE *v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t result;
  _BYTE v21[16];

  v13 = *(_QWORD *)(a4 - 8);
  MEMORY[0x1E0C80A78](a1);
  v15 = &v21[-((v14 + 15) & 0xFFFFFFFFFFFFFFF0)];
  v18 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v16 + 8) + 16))(v17);
  (*(void (**)(_BYTE *, uint64_t, uint64_t))(v13 + 16))(v15, a1, a4);
  v19 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  result = (*(uint64_t (**)(_BYTE *, uint64_t))(v13 + 8))(v15, a4);
  if (v19 == v18)
  {
    MEMORY[0x1E0C80A78](result);
    *(_QWORD *)&v21[-64] = a4;
    *(_QWORD *)&v21[-56] = a5;
    *(_QWORD *)&v21[-48] = a6;
    *(_QWORD *)&v21[-40] = a7;
    *(_QWORD *)&v21[-32] = a1;
    *(double *)&v21[-24] = a2;
    *(_QWORD *)&v21[-16] = v18;
    return (*(uint64_t (**)(_QWORD))(a7 + 16))(partial apply for closure #1 in static vDSP.divide<A, B>(_:_:result:));
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, _QWORD *a3, uint64_t a4, uint64_t (*a5)(uint64_t, uint64_t))
{
  if (!a1)
    goto LABEL_6;
  if (!*a3)
    goto LABEL_7;
  if (a4 < 0)
  {
    __break(1u);
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  return a5(a1, 1);
}

uint64_t static vDSP.add<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(_QWORD *, uint64_t *))
{
  uint64_t v5;

  v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, a4);
}

{
  uint64_t v5;

  v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, a4);
}

uint64_t closure #1 in static vDSP.add<A>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, void (*a6)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float), float a7)
{
  uint64_t v14;
  uint64_t v15;
  uint64_t result;

  v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  a6(a3, a1, a4, v14, a5, v15, a7);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.divide<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  return static vDSP.add<A, B>(_:_:result:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.divide<A, B>(_:_:result:));
}

void closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(const float *__B, int a2, float **a3, vDSP_Length __N, float a5)
{
  float __A;
  uint64_t v6;

  v6 = *MEMORY[0x1E0C80C00];
  __A = a5;
  if (!__B)
    goto LABEL_6;
  if (!*a3)
    goto LABEL_7;
  if ((__N & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  vDSP_svdiv(&__A, __B, 1, *a3, 1, __N);
}

uint64_t closure #1 in static vDSP.add<A>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, void (*a6)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double), double a7)
{
  uint64_t v14;
  uint64_t v15;
  uint64_t result;

  v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  a6(a3, a1, a4, v14, a5, v15, a7);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.divide<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  return static vDSP.add<A, B>(_:_:result:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.divide<A, B>(_:_:result:));
}

void closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(const double *__B, int a2, double **a3, vDSP_Length __N, double a5)
{
  double v5[2];

  v5[1] = *(double *)MEMORY[0x1E0C80C00];
  v5[0] = a5;
  if (!__B)
    goto LABEL_6;
  if (!*a3)
    goto LABEL_7;
  if ((__N & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  vDSP_svdivD(v5, __B, 1, *a3, 1, __N);
}

uint64_t static vDSP.divide<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.add<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.divide<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.add<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.divide<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.divide<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.add<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.divide<A, B, C>(_:_:result:));
}

{
  return static vDSP.add<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.divide<A, B, C>(_:_:result:));
}

uint64_t static vDSP.add<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  char *v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  char *v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t (*v26)(uint64_t);
  uint64_t v27;
  uint64_t v28;
  void (*v29)(char *, uint64_t, uint64_t);
  uint64_t v30;
  void (*v31)(char *, uint64_t, uint64_t);
  uint64_t (*v32)(uint64_t, uint64_t);
  uint64_t v33;
  uint64_t v34;
  void (*v35)(char *, uint64_t);
  uint64_t result;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  void (*v41)(char *, uint64_t);
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;

  v48 = a8;
  v49 = a1;
  v50 = a2;
  v13 = *(_QWORD *)(a5 - 8);
  v14 = MEMORY[0x1E0C80A78](a1);
  v16 = (char *)&v42 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  v17 = MEMORY[0x1E0C80A78](v14);
  v19 = (char *)&v42 - v18;
  v21 = *(_QWORD *)(v20 - 8);
  MEMORY[0x1E0C80A78](v17);
  v23 = (char *)&v42 - ((v22 + 15) & 0xFFFFFFFFFFFFFFF0);
  v44 = v25;
  v45 = v24;
  v26 = *(uint64_t (**)(uint64_t))(*(_QWORD *)(v25 + 8) + 16);
  v46 = v27;
  v28 = v26(v27);
  v47 = v21;
  v29 = *(void (**)(char *, uint64_t, uint64_t))(v21 + 16);
  v30 = v13;
  v29(v23, v49, a4);
  v31 = *(void (**)(char *, uint64_t, uint64_t))(v13 + 16);
  v31(v19, v50, a5);
  v32 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  v43 = a7;
  v33 = v32(a4, a7);
  v31(v16, (uint64_t)v19, a5);
  if (v33 == v28)
  {
    v34 = (*(uint64_t (**)(uint64_t))(v48 + 16))(a5);
    v35 = *(void (**)(char *, uint64_t))(v30 + 8);
    v35(v16, a5);
    v35(v19, a5);
    result = (*(uint64_t (**)(char *, uint64_t))(v47 + 8))(v23, a4);
    if (v34 == v28)
    {
      v37 = MEMORY[0x1E0C80A78](a10);
      *(&v42 - 10) = a4;
      *(&v42 - 9) = a5;
      v39 = v43;
      v38 = v44;
      *(&v42 - 8) = v46;
      *(&v42 - 7) = v39;
      *(&v42 - 6) = v48;
      *(&v42 - 5) = v38;
      v40 = v50;
      *(&v42 - 4) = v49;
      *(&v42 - 3) = v40;
      *(&v42 - 2) = v28;
      return (*(uint64_t (**)(uint64_t))(v38 + 16))(v37);
    }
  }
  else
  {
    v41 = *(void (**)(char *, uint64_t))(v30 + 8);
    v41(v16, a5);
    v41(v19, a5);
    result = (*(uint64_t (**)(char *, uint64_t))(v47 + 8))(v23, a4);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, _QWORD *a5, uint64_t a6, uint64_t (*a7)(void))
{
  if (!result)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (!a3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (*a5)
  {
    if ((a6 & 0x8000000000000000) == 0)
      return a7();
    __break(1u);
    goto LABEL_7;
  }
LABEL_9:
  __break(1u);
  return result;
}

uint64_t static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  return static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:));
}

{
  return static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:));
}

uint64_t static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13)
{
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  char *v19;
  uint64_t v20;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  char *v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t (*v29)(uint64_t);
  uint64_t v30;
  uint64_t v31;
  void (*v32)(char *, uint64_t, uint64_t);
  uint64_t v33;
  uint64_t v34;
  void (*v35)(char *, uint64_t, uint64_t);
  uint64_t (*v36)(uint64_t, uint64_t);
  uint64_t v37;
  uint64_t v38;
  void (*v39)(char *, uint64_t);
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t result;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;
  uint64_t v56;
  uint64_t v57;
  uint64_t v58;

  v54 = a8;
  v55 = a4;
  v58 = a2;
  v16 = *(_QWORD *)(a6 - 8);
  v17 = MEMORY[0x1E0C80A78](a1);
  v19 = (char *)&v48 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  v20 = MEMORY[0x1E0C80A78](v17);
  v22 = (char *)&v48 - v21;
  v24 = *(_QWORD *)(v23 - 8);
  MEMORY[0x1E0C80A78](v20);
  v26 = (char *)&v48 - ((v25 + 15) & 0xFFFFFFFFFFFFFFF0);
  v51 = v28;
  v52 = v27;
  v29 = *(uint64_t (**)(uint64_t))(*(_QWORD *)(v28 + 8) + 16);
  v53 = v30;
  v31 = v29(v30);
  v56 = v24;
  v32 = *(void (**)(char *, uint64_t, uint64_t))(v24 + 16);
  v33 = v16;
  v34 = v31;
  v50 = a1;
  v32(v26, a1, a5);
  v35 = *(void (**)(char *, uint64_t, uint64_t))(v33 + 16);
  v35(v22, v58, a6);
  v36 = *(uint64_t (**)(uint64_t, uint64_t))(a9 + 16);
  v57 = a5;
  v49 = a9;
  v37 = v36(a5, a9);
  v35(v19, (uint64_t)v22, a6);
  if (v37 != v34)
  {
    v39 = *(void (**)(char *, uint64_t))(v33 + 8);
    v39(v19, a6);
    goto LABEL_6;
  }
  v38 = (*(uint64_t (**)(uint64_t, uint64_t))(a10 + 16))(a6, a10);
  v39 = *(void (**)(char *, uint64_t))(v33 + 8);
  v39(v19, a6);
  if (v38 != v34)
  {
LABEL_6:
    v39(v22, a6);
    result = (*(uint64_t (**)(char *, uint64_t))(v56 + 8))(v26, v57);
    goto LABEL_7;
  }
  v40 = v54;
  v41 = v55;
  v42 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a12 + 8) + 16))(v54);
  v39(v22, a6);
  v43 = v57;
  result = (*(uint64_t (**)(char *, uint64_t))(v56 + 8))(v26, v57);
  if (v42 == v34)
  {
    v45 = MEMORY[0x1E0C80A78](a13);
    *(&v48 - 12) = v43;
    *(&v48 - 11) = a6;
    *(&v48 - 10) = v53;
    *(&v48 - 9) = v40;
    *(&v48 - 8) = v49;
    *(&v48 - 7) = a10;
    v47 = v50;
    v46 = v51;
    *(&v48 - 6) = v51;
    *(&v48 - 5) = a12;
    *(&v48 - 4) = v41;
    *(&v48 - 3) = v47;
    *(&v48 - 2) = v58;
    *(&v48 - 1) = v34;
    return (*(uint64_t (**)(uint64_t))(v46 + 16))(v45);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, _QWORD *a5, _QWORD *a6, uint64_t a7, uint64_t (*a8)(uint64_t))
{
  if (!result)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (!a3)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (!*a5)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (*a6)
  {
    if ((a7 & 0x8000000000000000) == 0)
      return a8(result);
    __break(1u);
    goto LABEL_8;
  }
LABEL_11:
  __break(1u);
  return result;
}

uint64_t static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  return static vDSP.multiply<A, B>(addition:_:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:), a7);
}

uint64_t static vDSP.multiply<A, B, C>(addition:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, uint64_t a10)
{
  return static vDSP.multiply<A, B, C>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:));
}

uint64_t static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  return static vDSP.multiply<A, B>(addition:_:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:), a7);
}

uint64_t static vDSP.multiply<A, B, C>(addition:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, uint64_t a10)
{
  return static vDSP.multiply<A, B, C>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:));
}

uint64_t static vDSP.multiply<A, B, C>(addition:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:), (uint64_t (*)(uint64_t, uint64_t, _BYTE *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:), (uint64_t (*)(uint64_t, uint64_t, _BYTE *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.multiply<A, B, C, D>(addition:_:result:)(uint64_t a1, uint64_t a2, char *a3, uint64_t a4, char *a5, char *a6, char *a7, char *a8, uint64_t a9, uint64_t a10, char *a11, uint64_t a12)
{
  return static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

{
  return static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

uint64_t static vDSP.multiply<A, B, C>(addition:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t (*a11)(uint64_t, uint64_t, _BYTE *))
{
  uint64_t TupleTypeMetadata2;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  char *v22;
  void (*v23)(char *, uint64_t, uint64_t);
  void (*v24)(char *, uint64_t, uint64_t);
  uint64_t v25;
  char *v26;
  uint64_t v27;
  void (*v28)(char *, uint64_t);
  uint64_t v30;
  char *v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t (*v39)(uint64_t, uint64_t, _BYTE *);
  _BYTE v40[16];
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  char *v47;
  uint64_t v48;

  v33 = a6;
  v34 = a2;
  v38 = a8;
  v39 = a11;
  v36 = a7;
  v37 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v35 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v16 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v18 = (char *)&v30 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = MEMORY[0x1E0C80A78](v16);
  v21 = (char *)&v30 - v20;
  v31 = (char *)&v30 + *(int *)(v19 + 48) - v20;
  v22 = v31;
  v23 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a4 - 8) + 16);
  v23((char *)&v30 - v20, a1, a4);
  v24 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a5 - 8) + 16);
  v24(v22, v34, a5);
  v32 = a9;
  v25 = v33;
  v34 = (*(uint64_t (**)(uint64_t, uint64_t))(v32 + 16))(v33, v32);
  v26 = &v18[*(int *)(TupleTypeMetadata2 + 48)];
  v23(v18, (uint64_t)v21, a4);
  v24(v26, (uint64_t)v31, a5);
  v41 = a4;
  v42 = a5;
  v43 = v25;
  v44 = v36;
  v45 = v38;
  v46 = v32;
  v47 = v18;
  v48 = a3;
  v27 = v39(v34, v37, v40);
  v28 = *(void (**)(char *, uint64_t))(v35 + 8);
  v28(v21, TupleTypeMetadata2);
  v28(v18, TupleTypeMetadata2);
  return v27;
}

uint64_t closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t *a11, unint64_t *a12, void (*a13)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t TupleTypeMetadata2;
  uint64_t v17;
  uint64_t v18;
  char *v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t result;
  uint64_t v28;
  uint64_t v29;
  unint64_t *v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  void (*v35)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t);
  uint64_t v36;
  uint64_t *v37;

  v33 = a4;
  v34 = a8;
  v36 = a7;
  v37 = a2;
  v35 = a13;
  v31 = a9;
  v32 = a1;
  v30 = a12;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v17 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v19 = (char *)&v28 - v18;
  v20 = *(int *)(v17 + 48);
  v21 = &v19[v20];
  v22 = a3 + v20;
  v29 = *(_QWORD *)(a5 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v29 + 16))(v19, a3, a5);
  v23 = *(_QWORD *)(a6 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v23 + 16))(v21, v22, a6);
  v24 = __swift_instantiateConcreteTypeFromMangledName(a11);
  v25 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(v30, a11);
  v26 = v36;
  v35(v19, v21, v33, v32, a5, a6, v36, v24, v34, v31, a10, v25);
  (*(void (**)(char *, uint64_t))(v23 + 8))(v21, a6);
  (*(void (**)(char *, uint64_t))(v29 + 8))(v19, a5);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a10 + 16))(v26, a10);
  *v37 = result;
  return result;
}

uint64_t static vDSP.multiply<A, B>(subtraction:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  return static vDSP.multiply<A, B>(addition:_:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.multiply<A, B>(subtraction:_:), a7);
}

uint64_t static vDSP.multiply<A, B, C>(subtraction:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, uint64_t a10)
{
  return static vDSP.multiply<A, B, C>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:));
}

uint64_t static vDSP.multiply<A, B, C>(addition:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, uint64_t a10, uint64_t a11)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  char *v23;
  uint64_t v24;
  uint64_t v25;
  char *v26;
  char *v27;
  void (*v28)(char *, uint64_t, uint64_t);
  void (*v29)(char *, uint64_t, uint64_t);
  char *v30;
  char *v31;
  char *v32;
  uint64_t v33;
  char *v34;
  void (*v35)(char *, char *, uint64_t);
  uint64_t v36;
  BOOL v37;
  void (*v38)(char *, uint64_t);
  uint64_t result;
  char *v40;
  char *v41;
  char *v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  char *v47;
  void (*v48)(char *, char *, uint64_t);
  char *v49;
  char *v50;
  uint64_t v51;
  uint64_t v52;
  char *v53;
  uint64_t v54;
  uint64_t v55;
  uint64_t v56;
  uint64_t v57;
  uint64_t v58;

  v55 = a8;
  v56 = a2;
  v57 = a7;
  v58 = a6;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v54 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v17 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v50 = (char *)&v46 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = MEMORY[0x1E0C80A78](v17);
  v53 = (char *)&v46 - v20;
  v21 = MEMORY[0x1E0C80A78](v19);
  v23 = (char *)&v46 - v22;
  v24 = MEMORY[0x1E0C80A78](v21);
  v26 = (char *)&v46 - v25;
  v27 = (char *)&v46 + *(int *)(v24 + 48) - v25;
  v28 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a4 - 8) + 16);
  v28((char *)&v46 - v25, a1, a4);
  v29 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a5 - 8) + 16);
  v29(v27, v56, a5);
  v51 = a10;
  v52 = a3;
  v56 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a10 + 8) + 16))(v58);
  v30 = &v23[*(int *)(TupleTypeMetadata2 + 48)];
  v47 = v26;
  v31 = v26;
  v32 = v53;
  v28(v23, (uint64_t)v31, a4);
  v49 = v27;
  v29(v30, (uint64_t)v27, a5);
  v33 = (*(uint64_t (**)(uint64_t))(v57 + 16))(a4);
  v34 = &v32[*(int *)(TupleTypeMetadata2 + 48)];
  v48 = (void (*)(char *, char *, uint64_t))v28;
  v28(v32, (uint64_t)v23, a4);
  v35 = (void (*)(char *, char *, uint64_t))v29;
  v36 = v56;
  v35(v34, v30, a5);
  v37 = v33 != v36 || (*(uint64_t (**)(uint64_t))(v55 + 16))(a5) != v36;
  v38 = *(void (**)(char *, uint64_t))(v54 + 8);
  v38(v32, TupleTypeMetadata2);
  result = ((uint64_t (*)(char *, uint64_t))v38)(v23, TupleTypeMetadata2);
  if (v37)
  {
    __break(1u);
  }
  else
  {
    v40 = v50;
    v41 = &v50[*(int *)(TupleTypeMetadata2 + 48)];
    v42 = v47;
    v48(v50, v47, a4);
    v43 = ((uint64_t (*)(char *, char *, uint64_t))v35)(v41, v49, a5);
    MEMORY[0x1E0C80A78](v43);
    *(&v46 - 10) = a4;
    *(&v46 - 9) = a5;
    v44 = v57;
    *(&v46 - 8) = v58;
    *(&v46 - 7) = v44;
    v45 = v51;
    *(&v46 - 6) = v55;
    *(&v46 - 5) = v45;
    *(&v46 - 4) = (uint64_t)v40;
    *((float *)&v46 - 6) = a9;
    *(&v46 - 2) = v56;
    (*(void (**)(uint64_t))(v45 + 16))(a11);
    v38(v42, TupleTypeMetadata2);
    return ((uint64_t (*)(char *, uint64_t))v38)(v40, TupleTypeMetadata2);
  }
  return result;
}

uint64_t closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, float a10@<S0>, uint64_t a11, uint64_t a12)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  char *v23;
  uint64_t v24;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  char v33[16];
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  char *v40;
  float v41;
  uint64_t v42;
  uint64_t v43;

  v32 = a9;
  v29 = a8;
  v30 = a3;
  v27 = a6;
  v28 = a1;
  v31 = a12;
  v26 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v18 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v19 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v21 = (char *)&v26 - v20;
  v22 = *(int *)(v19 + 48);
  v23 = &v21[v22];
  v24 = a2 + v22;
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a4 - 8) + 16))(v21, a2, a4);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a5 - 8) + 16))(v23, v24, a5);
  v34 = a4;
  v35 = a5;
  v36 = v27;
  v37 = a7;
  v38 = v29;
  v39 = v26;
  v40 = v21;
  v41 = a10;
  v42 = v28;
  v43 = v30;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(a7 + 24))(v31, v33, MEMORY[0x1E0DEE9C0] + 8, a4, a7);
  return (*(uint64_t (**)(char *, uint64_t))(v18 + 8))(v21, TupleTypeMetadata2);
}

uint64_t static vDSP.multiply<A, B>(subtraction:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  return static vDSP.multiply<A, B>(addition:_:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.multiply<A, B>(subtraction:_:), a7);
}

uint64_t static vDSP.multiply<A, B, C>(subtraction:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, uint64_t a10)
{
  return static vDSP.multiply<A, B, C>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:));
}

uint64_t static vDSP.multiply<A, B, C>(addition:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, uint64_t a10, uint64_t a11)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  char *v23;
  uint64_t v24;
  uint64_t v25;
  char *v26;
  char *v27;
  void (*v28)(char *, uint64_t, uint64_t);
  void (*v29)(char *, uint64_t, uint64_t);
  char *v30;
  char *v31;
  char *v32;
  uint64_t v33;
  char *v34;
  void (*v35)(char *, char *, uint64_t);
  uint64_t v36;
  BOOL v37;
  void (*v38)(char *, uint64_t);
  uint64_t result;
  char *v40;
  char *v41;
  char *v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  char *v47;
  void (*v48)(char *, char *, uint64_t);
  char *v49;
  char *v50;
  uint64_t v51;
  uint64_t v52;
  char *v53;
  uint64_t v54;
  uint64_t v55;
  uint64_t v56;
  uint64_t v57;
  uint64_t v58;

  v55 = a8;
  v56 = a2;
  v57 = a7;
  v58 = a6;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v54 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v17 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v50 = (char *)&v46 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = MEMORY[0x1E0C80A78](v17);
  v53 = (char *)&v46 - v20;
  v21 = MEMORY[0x1E0C80A78](v19);
  v23 = (char *)&v46 - v22;
  v24 = MEMORY[0x1E0C80A78](v21);
  v26 = (char *)&v46 - v25;
  v27 = (char *)&v46 + *(int *)(v24 + 48) - v25;
  v28 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a4 - 8) + 16);
  v28((char *)&v46 - v25, a1, a4);
  v29 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a5 - 8) + 16);
  v29(v27, v56, a5);
  v51 = a10;
  v52 = a3;
  v56 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a10 + 8) + 16))(v58);
  v30 = &v23[*(int *)(TupleTypeMetadata2 + 48)];
  v47 = v26;
  v31 = v26;
  v32 = v53;
  v28(v23, (uint64_t)v31, a4);
  v49 = v27;
  v29(v30, (uint64_t)v27, a5);
  v33 = (*(uint64_t (**)(uint64_t))(v57 + 16))(a4);
  v34 = &v32[*(int *)(TupleTypeMetadata2 + 48)];
  v48 = (void (*)(char *, char *, uint64_t))v28;
  v28(v32, (uint64_t)v23, a4);
  v35 = (void (*)(char *, char *, uint64_t))v29;
  v36 = v56;
  v35(v34, v30, a5);
  v37 = v33 != v36 || (*(uint64_t (**)(uint64_t))(v55 + 16))(a5) != v36;
  v38 = *(void (**)(char *, uint64_t))(v54 + 8);
  v38(v32, TupleTypeMetadata2);
  result = ((uint64_t (*)(char *, uint64_t))v38)(v23, TupleTypeMetadata2);
  if (v37)
  {
    __break(1u);
  }
  else
  {
    v40 = v50;
    v41 = &v50[*(int *)(TupleTypeMetadata2 + 48)];
    v42 = v47;
    v48(v50, v47, a4);
    v43 = ((uint64_t (*)(char *, char *, uint64_t))v35)(v41, v49, a5);
    MEMORY[0x1E0C80A78](v43);
    *(&v46 - 10) = a4;
    *(&v46 - 9) = a5;
    v44 = v57;
    *(&v46 - 8) = v58;
    *(&v46 - 7) = v44;
    v45 = v51;
    *(&v46 - 6) = v55;
    *(&v46 - 5) = v45;
    *(&v46 - 4) = (uint64_t)v40;
    *((double *)&v46 - 3) = a9;
    *(&v46 - 2) = v56;
    (*(void (**)(uint64_t))(v45 + 16))(a11);
    v38(v42, TupleTypeMetadata2);
    return ((uint64_t (*)(char *, uint64_t))v38)(v40, TupleTypeMetadata2);
  }
  return result;
}

uint64_t closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, double a10@<D0>, uint64_t a11, uint64_t a12)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  char *v23;
  uint64_t v24;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  char v33[16];
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  char *v40;
  double v41;
  uint64_t v42;
  uint64_t v43;

  v32 = a9;
  v29 = a8;
  v30 = a3;
  v27 = a6;
  v28 = a1;
  v31 = a12;
  v26 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v18 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v19 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v21 = (char *)&v26 - v20;
  v22 = *(int *)(v19 + 48);
  v23 = &v21[v22];
  v24 = a2 + v22;
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a4 - 8) + 16))(v21, a2, a4);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a5 - 8) + 16))(v23, v24, a5);
  v34 = a4;
  v35 = a5;
  v36 = v27;
  v37 = a7;
  v38 = v29;
  v39 = v26;
  v40 = v21;
  v41 = a10;
  v42 = v28;
  v43 = v30;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(a7 + 24))(v31, v33, MEMORY[0x1E0DEE9C0] + 8, a4, a7);
  return (*(uint64_t (**)(char *, uint64_t))(v18 + 8))(v21, TupleTypeMetadata2);
}

uint64_t static vDSP.multiply<A, B, C>(subtraction:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:), (uint64_t (*)(uint64_t, uint64_t, _BYTE *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:), (uint64_t (*)(uint64_t, uint64_t, _BYTE *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.multiply<A, B, C, D>(subtraction:_:result:)(uint64_t a1, uint64_t a2, char *a3, uint64_t a4, char *a5, char *a6, char *a7, char *a8, uint64_t a9, uint64_t a10, char *a11, uint64_t a12)
{
  return static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:));
}

{
  return static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:));
}

uint64_t static vDSP.add<A, B>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  return static vDSP.multiply<A, B>(addition:_:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:), a7);
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  char *v18;
  void (*v19)(char *, uint64_t, uint64_t);
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  void (*v23)(char *, uint64_t);
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  char *v31;
  uint64_t v32;

  v26 = a5;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v25 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v14 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v16 = (char *)&v25 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1E0C80A78](v14);
  v18 = (char *)&v25 - v17;
  v19 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a3 - 8) + 16);
  v19((char *)&v25 - v17, a1, a3);
  *(float *)&v18[*(int *)(TupleTypeMetadata2 + 48)] = a7;
  v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  v21 = *(int *)(TupleTypeMetadata2 + 48);
  v19(v16, (uint64_t)v18, a3);
  *(float *)&v16[v21] = a7;
  v27 = a3;
  v28 = a4;
  v29 = v26;
  v30 = a6;
  v31 = v16;
  v32 = a2;
  v22 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v20, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:));
  v23 = *(void (**)(char *, uint64_t))(v25 + 8);
  v23(v18, TupleTypeMetadata2);
  v23(v16, TupleTypeMetadata2);
  return v22;
}

uint64_t static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(_QWORD *, uint64_t *), float a8)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  char *v19;
  char *v20;
  void (*v21)(char *, uint64_t, uint64_t);
  void (*v22)(char *, uint64_t, uint64_t);
  uint64_t v23;
  char *v24;
  uint64_t v25;
  void (*v26)(char *, uint64_t);
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t (*v33)(_QWORD *, uint64_t *);
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  char *v38;
  float v39;

  v32 = a6;
  v33 = a7;
  v30 = a5;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v31 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v14 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v16 = (char *)&v28 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  v17 = MEMORY[0x1E0C80A78](v14);
  v19 = (char *)&v28 - v18;
  v20 = (char *)&v28 + *(int *)(v17 + 48) - v18;
  v21 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a3 - 8) + 16);
  v21((char *)&v28 - v18, a1, a3);
  v22 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a4 - 8) + 16);
  v22(v20, a2, a4);
  v23 = v30;
  v29 = (*(uint64_t (**)(uint64_t, uint64_t))(v30 + 16))(a3, v30);
  v24 = &v16[*(int *)(TupleTypeMetadata2 + 48)];
  v21(v16, (uint64_t)v19, a3);
  v22(v24, (uint64_t)v20, a4);
  v34 = a3;
  v35 = a4;
  v36 = v23;
  v37 = v32;
  v38 = v16;
  v39 = a8;
  v25 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v29, v33);
  v26 = *(void (**)(char *, uint64_t))(v31 + 8);
  v26(v19, TupleTypeMetadata2);
  v26(v16, TupleTypeMetadata2);
  return v25;
}

uint64_t closure #1 in static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, void (*a8)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, uint64_t), float a9)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t result;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  void (*v29)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, uint64_t);
  uint64_t *v30;

  v28 = a7;
  v29 = a8;
  v30 = a2;
  v26 = a1;
  v27 = a6;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v14 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v16 = (char *)&v26 - v15;
  v17 = *(int *)(v14 + 48);
  v18 = &v16[v17];
  v19 = a3 + v17;
  v20 = *(_QWORD *)(a4 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v20 + 16))(v16, a3, a4);
  v21 = *(_QWORD *)(a5 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v21 + 16))(v18, v19, a5);
  v22 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v23 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v24 = v27;
  v29(v16, v18, v26, a4, a5, v22, v27, v28, a9, v23);
  (*(void (**)(char *, uint64_t))(v21 + 8))(v18, a5);
  (*(void (**)(char *, uint64_t))(v20 + 8))(v16, a4);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(v24 + 16))(a4, v24);
  *v30 = result;
  return result;
}

uint64_t static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, uint64_t a10)
{
  return static vDSP.multiply<A, B, C>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(uint64_t a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5, _QWORD *a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, float *))
{
  float v9;
  uint64_t v10;

  v10 = *MEMORY[0x1E0C80C00];
  v9 = a2;
  if (!a4)
    goto LABEL_7;
  if (!a1)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (!*a6)
    goto LABEL_9;
  if (a7 < 0)
  {
    __break(1u);
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  return a8(a4, 1, a1, 1, &v9);
}

uint64_t static vDSP.add<A, B>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  return static vDSP.multiply<A, B>(addition:_:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:), a7);
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  char *v18;
  void (*v19)(char *, uint64_t, uint64_t);
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  void (*v23)(char *, uint64_t);
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  char *v31;
  uint64_t v32;

  v26 = a5;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v25 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v14 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v16 = (char *)&v25 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1E0C80A78](v14);
  v18 = (char *)&v25 - v17;
  v19 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a3 - 8) + 16);
  v19((char *)&v25 - v17, a1, a3);
  *(double *)&v18[*(int *)(TupleTypeMetadata2 + 48)] = a7;
  v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  v21 = *(int *)(TupleTypeMetadata2 + 48);
  v19(v16, (uint64_t)v18, a3);
  *(double *)&v16[v21] = a7;
  v27 = a3;
  v28 = a4;
  v29 = v26;
  v30 = a6;
  v31 = v16;
  v32 = a2;
  v22 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v20, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:));
  v23 = *(void (**)(char *, uint64_t))(v25 + 8);
  v23(v18, TupleTypeMetadata2);
  v23(v16, TupleTypeMetadata2);
  return v22;
}

uint64_t static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(_QWORD *, uint64_t *), double a8)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  char *v19;
  char *v20;
  void (*v21)(char *, uint64_t, uint64_t);
  void (*v22)(char *, uint64_t, uint64_t);
  uint64_t v23;
  char *v24;
  uint64_t v25;
  void (*v26)(char *, uint64_t);
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t (*v33)(_QWORD *, uint64_t *);
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  char *v38;
  double v39;

  v32 = a6;
  v33 = a7;
  v30 = a5;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v31 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v14 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v16 = (char *)&v28 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  v17 = MEMORY[0x1E0C80A78](v14);
  v19 = (char *)&v28 - v18;
  v20 = (char *)&v28 + *(int *)(v17 + 48) - v18;
  v21 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a3 - 8) + 16);
  v21((char *)&v28 - v18, a1, a3);
  v22 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a4 - 8) + 16);
  v22(v20, a2, a4);
  v23 = v30;
  v29 = (*(uint64_t (**)(uint64_t, uint64_t))(v30 + 16))(a3, v30);
  v24 = &v16[*(int *)(TupleTypeMetadata2 + 48)];
  v21(v16, (uint64_t)v19, a3);
  v22(v24, (uint64_t)v20, a4);
  v34 = a3;
  v35 = a4;
  v36 = v23;
  v37 = v32;
  v38 = v16;
  v39 = a8;
  v25 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v29, v33);
  v26 = *(void (**)(char *, uint64_t))(v31 + 8);
  v26(v19, TupleTypeMetadata2);
  v26(v16, TupleTypeMetadata2);
  return v25;
}

uint64_t closure #1 in static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, void (*a8)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, uint64_t), double a9)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t result;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  void (*v29)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, uint64_t);
  uint64_t *v30;

  v28 = a7;
  v29 = a8;
  v30 = a2;
  v26 = a1;
  v27 = a6;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v14 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v16 = (char *)&v26 - v15;
  v17 = *(int *)(v14 + 48);
  v18 = &v16[v17];
  v19 = a3 + v17;
  v20 = *(_QWORD *)(a4 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v20 + 16))(v16, a3, a4);
  v21 = *(_QWORD *)(a5 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v21 + 16))(v18, v19, a5);
  v22 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v23 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v24 = v27;
  v29(v16, v18, v26, a4, a5, v22, v27, v28, a9, v23);
  (*(void (**)(char *, uint64_t))(v21 + 8))(v18, a5);
  (*(void (**)(char *, uint64_t))(v20 + 8))(v16, a4);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(v24 + 16))(a4, v24);
  *v30 = result;
  return result;
}

uint64_t static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, uint64_t a10)
{
  return static vDSP.multiply<A, B, C>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(uint64_t a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5, _QWORD *a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, _QWORD *))
{
  _QWORD v9[2];

  v9[1] = *MEMORY[0x1E0C80C00];
  *(double *)v9 = a2;
  if (!a4)
    goto LABEL_7;
  if (!a1)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (!*a6)
    goto LABEL_9;
  if (a7 < 0)
  {
    __break(1u);
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  return a8(a4, 1, a1, 1, v9);
}

uint64_t closure #1 in static vDSP.add<A, B>(multiplication:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  float v20;
  char *v21;
  uint64_t v22;
  uint64_t result;
  uint64_t v24;
  uint64_t v25;
  uint64_t *v26;

  v26 = a2;
  v25 = a7;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v15 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v17 = (char *)&v24 - v16;
  v18 = *(int *)(v15 + 48);
  v19 = *(_QWORD *)(a5 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v19 + 16))((char *)&v24 - v16, a3, a5);
  v20 = *(float *)(a3 + v18);
  *(float *)&v17[v18] = v20;
  v21 = (char *)__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v22 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.add<A, B, C>(multiplication:_:result:)(v17, a4, a1, a5, a6, v21, v25, a8, v20, v22);
  (*(void (**)(char *, uint64_t))(v19 + 8))(v17, a5);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
  *v26 = result;
  return result;
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  double v20;
  char *v21;
  uint64_t v22;
  uint64_t result;
  uint64_t v24;
  uint64_t v25;
  uint64_t *v26;

  v26 = a2;
  v25 = a7;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v15 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v17 = (char *)&v24 - v16;
  v18 = *(int *)(v15 + 48);
  v19 = *(_QWORD *)(a5 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v19 + 16))((char *)&v24 - v16, a3, a5);
  v20 = *(double *)(a3 + v18);
  *(double *)&v17[v18] = v20;
  v21 = (char *)__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v22 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.add<A, B, C>(multiplication:_:result:)(v17, a4, a1, a5, a6, v21, v25, a8, v20, v22);
  (*(void (**)(char *, uint64_t))(v19 + 8))(v17, a5);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
  *v26 = result;
  return result;
}

uint64_t static vDSP.add<A, B, C>(multiplication:_:result:)(char *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, char *a6, uint64_t a7, uint64_t a8, float a9, uint64_t a10)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  char *v23;
  void (*v24)(char *, char *, uint64_t);
  uint64_t (*v25)(char *);
  uint64_t v26;
  uint64_t v27;
  uint64_t (*v28)(uint64_t, uint64_t);
  uint64_t v29;
  uint64_t (*v30)(char *, uint64_t);
  uint64_t result;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  char *v45;
  uint64_t v46;

  v43 = a8;
  v39 = a5;
  v40 = a2;
  v45 = a1;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v46 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v16 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v18 = (char *)&v38 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = MEMORY[0x1E0C80A78](v16);
  v21 = (char *)&v38 - v20;
  MEMORY[0x1E0C80A78](v19);
  v23 = (char *)&v38 - v22;
  v24 = *(void (**)(char *, char *, uint64_t))(*(_QWORD *)(a4 - 8) + 16);
  v24((char *)&v38 - v22, v45, a4);
  *(float *)&v23[*(int *)(TupleTypeMetadata2 + 48)] = a9;
  v41 = a10;
  v25 = *(uint64_t (**)(char *))(*(_QWORD *)(a10 + 8) + 16);
  v44 = a3;
  v45 = a6;
  v26 = v25(a6);
  v27 = *(int *)(TupleTypeMetadata2 + 48);
  v24(v21, v23, a4);
  *(float *)&v21[v27] = a9;
  v28 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  v42 = a7;
  v29 = v28(a4, a7);
  v30 = *(uint64_t (**)(char *, uint64_t))(v46 + 8);
  result = v30(v21, TupleTypeMetadata2);
  if (v29 == v26)
  {
    v32 = *(int *)(TupleTypeMetadata2 + 48);
    v33 = ((uint64_t (*)(char *, char *, uint64_t))v24)(v18, v23, a4);
    *(float *)&v18[v32] = a9;
    MEMORY[0x1E0C80A78](v33);
    v34 = v39;
    *(&v38 - 10) = a4;
    *(&v38 - 9) = v34;
    v35 = v41;
    v36 = v42;
    *(&v38 - 8) = (uint64_t)v45;
    *(&v38 - 7) = v36;
    *(&v38 - 6) = v43;
    *(&v38 - 5) = v35;
    v37 = v40;
    *(&v38 - 4) = (uint64_t)v18;
    *(&v38 - 3) = v37;
    *(&v38 - 2) = v26;
    (*(void (**)(uint64_t (*)(uint64_t)))(v35 + 16))(partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
    v30(v23, TupleTypeMetadata2);
    return v30(v18, TupleTypeMetadata2);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  char v27[16];
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  char *v35;
  uint64_t v36;
  uint64_t v37;

  v21 = a6;
  v22 = a7;
  v24 = a1;
  v25 = a4;
  v23 = a3;
  v26 = a9;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v15 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v16 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v18 = (char *)&v21 - v17;
  v19 = *(int *)(v16 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a5 - 8) + 16))((char *)&v21 - v17, a2, a5);
  *(_DWORD *)&v18[v19] = *(_DWORD *)(a2 + v19);
  v28 = a5;
  v29 = v21;
  v30 = v22;
  v31 = a8;
  v32 = a10;
  v33 = a11;
  v34 = v23;
  v35 = v18;
  v36 = v24;
  v37 = v25;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t, uint64_t))(a8 + 24))(partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:), v27, MEMORY[0x1E0DEE9C0] + 8, a5, a8);
  return (*(uint64_t (**)(char *, uint64_t))(v15 + 8))(v18, TupleTypeMetadata2);
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  char v27[16];
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  char *v35;
  uint64_t v36;
  uint64_t v37;

  v21 = a6;
  v22 = a7;
  v24 = a1;
  v25 = a4;
  v23 = a3;
  v26 = a9;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v15 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v16 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v18 = (char *)&v21 - v17;
  v19 = *(int *)(v16 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a5 - 8) + 16))((char *)&v21 - v17, a2, a5);
  *(_QWORD *)&v18[v19] = *(_QWORD *)(a2 + v19);
  v28 = a5;
  v29 = v21;
  v30 = v22;
  v31 = a8;
  v32 = a10;
  v33 = a11;
  v34 = v23;
  v35 = v18;
  v36 = v24;
  v37 = v25;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t, uint64_t))(a8 + 24))(partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:), v27, MEMORY[0x1E0DEE9C0] + 8, a5, a8);
  return (*(uint64_t (**)(char *, uint64_t))(v15 + 8))(v18, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  char v31[16];
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  char *v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;

  v27 = a5;
  v28 = a6;
  v29 = a9;
  v30 = a3;
  v25 = a1;
  v26 = a2;
  v23 = a11;
  v24 = a13;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v17 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v18 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v20 = (char *)&v23 - v19;
  v21 = *(int *)(v18 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a7 - 8) + 16))((char *)&v23 - v19, a4, a7);
  *(_DWORD *)&v20[v21] = *(_DWORD *)(a4 + v21);
  v32 = a7;
  v33 = a8;
  v34 = a10;
  v35 = v23;
  v36 = a12;
  v37 = v24;
  v38 = v20;
  v39 = v25;
  v40 = v26;
  v41 = v27;
  v42 = v28;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t, uint64_t))(v36 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:), v31, MEMORY[0x1E0DEE9C0] + 8, a8, v36);
  return (*(uint64_t (**)(char *, uint64_t))(v17 + 8))(v20, TupleTypeMetadata2);
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  char v31[16];
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  char *v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;

  v27 = a5;
  v28 = a6;
  v29 = a9;
  v30 = a3;
  v25 = a1;
  v26 = a2;
  v23 = a11;
  v24 = a13;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v17 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v18 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v20 = (char *)&v23 - v19;
  v21 = *(int *)(v18 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a7 - 8) + 16))((char *)&v23 - v19, a4, a7);
  *(_QWORD *)&v20[v21] = *(_QWORD *)(a4 + v21);
  v32 = a7;
  v33 = a8;
  v34 = a10;
  v35 = v23;
  v36 = a12;
  v37 = v24;
  v38 = v20;
  v39 = v25;
  v40 = v26;
  v41 = v27;
  v42 = v28;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t, uint64_t))(v36 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:), v31, MEMORY[0x1E0DEE9C0] + 8, a8, v36);
  return (*(uint64_t (**)(char *, uint64_t))(v17 + 8))(v20, TupleTypeMetadata2);
}

void closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(const float *a1, uint64_t a2, uint64_t a3, const float *a4, uint64_t a5, float **a6, vDSP_Length a7)
{
  float __B;
  uint64_t v12;

  v12 = *MEMORY[0x1E0C80C00];
  __B = *(float *)(a3 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  if (!a4)
    goto LABEL_7;
  if (!a1)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (!*a6)
    goto LABEL_9;
  if ((a7 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  vDSP_vsma(a4, 1, &__B, a1, 1, *a6, 1, a7);
}

uint64_t static vDSP.add<A, B, C>(multiplication:_:result:)(char *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, char *a6, uint64_t a7, uint64_t a8, double a9, uint64_t a10)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  char *v23;
  void (*v24)(char *, char *, uint64_t);
  uint64_t (*v25)(char *);
  uint64_t v26;
  uint64_t v27;
  uint64_t (*v28)(uint64_t, uint64_t);
  uint64_t v29;
  uint64_t (*v30)(char *, uint64_t);
  uint64_t result;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  char *v45;
  uint64_t v46;

  v43 = a8;
  v39 = a5;
  v40 = a2;
  v45 = a1;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v46 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v16 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v18 = (char *)&v38 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = MEMORY[0x1E0C80A78](v16);
  v21 = (char *)&v38 - v20;
  MEMORY[0x1E0C80A78](v19);
  v23 = (char *)&v38 - v22;
  v24 = *(void (**)(char *, char *, uint64_t))(*(_QWORD *)(a4 - 8) + 16);
  v24((char *)&v38 - v22, v45, a4);
  *(double *)&v23[*(int *)(TupleTypeMetadata2 + 48)] = a9;
  v41 = a10;
  v25 = *(uint64_t (**)(char *))(*(_QWORD *)(a10 + 8) + 16);
  v44 = a3;
  v45 = a6;
  v26 = v25(a6);
  v27 = *(int *)(TupleTypeMetadata2 + 48);
  v24(v21, v23, a4);
  *(double *)&v21[v27] = a9;
  v28 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  v42 = a7;
  v29 = v28(a4, a7);
  v30 = *(uint64_t (**)(char *, uint64_t))(v46 + 8);
  result = v30(v21, TupleTypeMetadata2);
  if (v29 == v26)
  {
    v32 = *(int *)(TupleTypeMetadata2 + 48);
    v33 = ((uint64_t (*)(char *, char *, uint64_t))v24)(v18, v23, a4);
    *(double *)&v18[v32] = a9;
    MEMORY[0x1E0C80A78](v33);
    v34 = v39;
    *(&v38 - 10) = a4;
    *(&v38 - 9) = v34;
    v35 = v41;
    v36 = v42;
    *(&v38 - 8) = (uint64_t)v45;
    *(&v38 - 7) = v36;
    *(&v38 - 6) = v43;
    *(&v38 - 5) = v35;
    v37 = v40;
    *(&v38 - 4) = (uint64_t)v18;
    *(&v38 - 3) = v37;
    *(&v38 - 2) = v26;
    (*(void (**)(uint64_t (*)(uint64_t)))(v35 + 16))(partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
    v30(v23, TupleTypeMetadata2);
    return v30(v18, TupleTypeMetadata2);
  }
  else
  {
    __break(1u);
  }
  return result;
}

void closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(const double *a1, uint64_t a2, uint64_t a3, const double *a4, uint64_t a5, double **a6, vDSP_Length a7)
{
  double v11[2];

  v11[1] = *(double *)MEMORY[0x1E0C80C00];
  v11[0] = *(double *)(a3 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  if (!a4)
    goto LABEL_7;
  if (!a1)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (!*a6)
    goto LABEL_9;
  if ((a7 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  vDSP_vsmaD(a4, 1, v11, a1, 1, *a6, 1, a7);
}

uint64_t static vDSP.add<A, B, C>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:), (uint64_t (*)(uint64_t, uint64_t, _BYTE *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:), (uint64_t (*)(uint64_t, uint64_t, _BYTE *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.add<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2, char *a3, uint64_t a4, char *a5, char *a6, char *a7, char *a8, uint64_t a9, uint64_t a10, char *a11, uint64_t a12)
{
  return static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:));
}

{
  return static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:));
}

uint64_t static vDSP.multiply<A, B, C, D>(addition:_:result:)(uint64_t a1, uint64_t a2, char *a3, uint64_t a4, char *a5, char *a6, char *a7, char *a8, uint64_t a9, uint64_t a10, char *a11, uint64_t a12, uint64_t a13)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  char *v24;
  uint64_t v25;
  uint64_t v26;
  char *v27;
  char *v28;
  void (*v29)(char *, uint64_t, char *);
  void (*v30)(char *, uint64_t, char *);
  char *v31;
  char *v32;
  char *v33;
  uint64_t v34;
  char *v35;
  void (*v36)(char *, char *, char *);
  uint64_t v37;
  BOOL v38;
  void (*v39)(char *, uint64_t);
  uint64_t result;
  char *v41;
  char *v42;
  char *v43;
  uint64_t v44;
  char *v45;
  uint64_t v46;
  char *v47;
  uint64_t v48;
  char *v49;
  char *v50;
  void (*v51)(char *, char *, char *);
  char *v52;
  char *v53;
  char *v54;
  char *v55;
  uint64_t v56;
  uint64_t v57;
  char *v58;
  uint64_t v59;
  uint64_t v60;
  uint64_t v61;
  uint64_t v62;
  char *v63;

  v54 = a7;
  v55 = a3;
  v60 = a10;
  v61 = a2;
  v62 = a9;
  v63 = a8;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v59 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v18 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v53 = (char *)&v50 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  v20 = MEMORY[0x1E0C80A78](v18);
  v58 = (char *)&v50 - v21;
  v22 = MEMORY[0x1E0C80A78](v20);
  v24 = (char *)&v50 - v23;
  v25 = MEMORY[0x1E0C80A78](v22);
  v27 = (char *)&v50 - v26;
  v28 = (char *)&v50 + *(int *)(v25 + 48) - v26;
  v29 = *(void (**)(char *, uint64_t, char *))(*((_QWORD *)a5 - 1) + 16);
  v29((char *)&v50 - v26, a1, a5);
  v30 = *(void (**)(char *, uint64_t, char *))(*((_QWORD *)a6 - 1) + 16);
  v30(v28, v61, a6);
  v56 = a12;
  v57 = a4;
  v61 = (*(uint64_t (**)(char *))(*(_QWORD *)(a12 + 8) + 16))(v63);
  v31 = &v24[*(int *)(TupleTypeMetadata2 + 48)];
  v50 = v27;
  v32 = v27;
  v33 = v58;
  v29(v24, (uint64_t)v32, a5);
  v52 = v28;
  v30(v31, (uint64_t)v28, a6);
  v34 = (*(uint64_t (**)(char *))(v62 + 16))(a5);
  v35 = &v33[*(int *)(TupleTypeMetadata2 + 48)];
  v51 = (void (*)(char *, char *, char *))v29;
  v29(v33, (uint64_t)v24, a5);
  v36 = (void (*)(char *, char *, char *))v30;
  v37 = v61;
  v36(v35, v31, a6);
  v38 = v34 != v37 || (*(uint64_t (**)(char *))(v60 + 16))(a6) != v37;
  v39 = *(void (**)(char *, uint64_t))(v59 + 8);
  v39(v33, TupleTypeMetadata2);
  result = ((uint64_t (*)(char *, uint64_t))v39)(v24, TupleTypeMetadata2);
  if (v38)
  {
    __break(1u);
  }
  else
  {
    v59 = a13;
    v41 = v53;
    v42 = &v53[*(int *)(TupleTypeMetadata2 + 48)];
    v43 = v50;
    v51(v53, v50, a5);
    v44 = ((uint64_t (*)(char *, char *, char *))v36)(v42, v52, a6);
    MEMORY[0x1E0C80A78](v44);
    *(&v50 - 12) = a5;
    *(&v50 - 11) = a6;
    v45 = v63;
    *(&v50 - 10) = v54;
    *(&v50 - 9) = v45;
    v46 = v59;
    v47 = (char *)v60;
    *(&v50 - 8) = (char *)v62;
    *(&v50 - 7) = v47;
    v49 = v55;
    v48 = v56;
    *(&v50 - 6) = a11;
    *(&v50 - 5) = (char *)v48;
    *(&v50 - 4) = v41;
    *(&v50 - 3) = v49;
    *(&v50 - 2) = (char *)v61;
    (*(void (**)(uint64_t))(v48 + 16))(v46);
    v39(v43, TupleTypeMetadata2);
    return ((uint64_t (*)(char *, uint64_t))v39)(v41, TupleTypeMetadata2);
  }
  return result;
}

uint64_t closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, __int128 a11, uint64_t a12, uint64_t a13)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v17;
  uint64_t v18;
  char *v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  __int128 v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  char v33[16];
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  __int128 v39;
  uint64_t v40;
  char *v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;

  v28 = a8;
  v25 = a7;
  v29 = a3;
  v30 = a4;
  v26 = a1;
  v32 = a9;
  v31 = a13;
  v24 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v27 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v17 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v19 = (char *)&v24 - v18;
  v20 = *(int *)(v17 + 48);
  v21 = &v19[v20];
  v22 = a2 + v20;
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a5 - 8) + 16))(v19, a2, a5);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a6 - 8) + 16))(v21, v22, a6);
  v34 = a5;
  v35 = a6;
  v36 = v25;
  v37 = v28;
  v38 = a10;
  v39 = v24;
  v40 = a12;
  v41 = v19;
  v42 = v29;
  v43 = v26;
  v44 = v30;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(v38 + 24))(v31, v33, MEMORY[0x1E0DEE9C0] + 8, a5, v38);
  return (*(uint64_t (**)(char *, uint64_t))(v27 + 8))(v19, TupleTypeMetadata2);
}

uint64_t static vDSP.subtract<A, B, C>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.subtract<A, B, C>(multiplication:_:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:), (uint64_t (*)(uint64_t, uint64_t, _BYTE *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.subtract<A, B, C>(multiplication:_:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:), (uint64_t (*)(uint64_t, uint64_t, _BYTE *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2, char *a3, uint64_t a4, char *a5, char *a6, char *a7, char *a8, char *a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  return static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:));
}

{
  return static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:));
}

uint64_t static vDSP.subtract<A, B, C>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t (*a11)(uint64_t, uint64_t, _BYTE *))
{
  uint64_t TupleTypeMetadata2;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  char *v22;
  void (*v23)(char *, uint64_t, uint64_t);
  void (*v24)(char *, uint64_t, uint64_t);
  uint64_t v25;
  uint64_t v26;
  char *v27;
  uint64_t v28;
  void (*v29)(char *, uint64_t);
  uint64_t v31;
  char *v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t (*v39)(uint64_t, uint64_t, _BYTE *);
  uint64_t v40;
  _BYTE v41[16];
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  char *v48;
  uint64_t v49;

  v40 = a7;
  v33 = a4;
  v34 = a2;
  v38 = a8;
  v39 = a11;
  v36 = a9;
  v37 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v35 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v16 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v18 = (char *)&v31 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = MEMORY[0x1E0C80A78](v16);
  v21 = (char *)&v31 - v20;
  v32 = (char *)&v31 + *(int *)(v19 + 48) - v20;
  v22 = v32;
  v23 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a5 - 8) + 16);
  v23((char *)&v31 - v20, a1, a5);
  v24 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a6 - 8) + 16);
  v24(v22, v34, a6);
  v25 = a3;
  v26 = v33;
  v34 = (*(uint64_t (**)(uint64_t))(v40 + 16))(v33);
  v27 = &v18[*(int *)(TupleTypeMetadata2 + 48)];
  v23(v18, (uint64_t)v21, a5);
  v24(v27, (uint64_t)v32, a6);
  v42 = v26;
  v43 = a5;
  v44 = a6;
  v45 = v40;
  v46 = v38;
  v47 = v36;
  v48 = v18;
  v49 = v25;
  v28 = v39(v34, v37, v41);
  v29 = *(void (**)(char *, uint64_t))(v35 + 8);
  v29(v21, TupleTypeMetadata2);
  v29(v18, TupleTypeMetadata2);
  return v28;
}

uint64_t closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t *a11, unint64_t *a12, void (*a13)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t TupleTypeMetadata2;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t result;
  uint64_t v29;
  uint64_t v30;
  unint64_t *v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  void (*v36)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t);
  uint64_t v37;
  uint64_t *v38;

  v37 = a5;
  v38 = a2;
  v34 = a1;
  v35 = a4;
  v36 = a13;
  v32 = a10;
  v33 = a9;
  v31 = a12;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v18 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v20 = (char *)&v29 - v19;
  v21 = *(int *)(v18 + 48);
  v22 = &v20[v21];
  v23 = a3 + v21;
  v30 = *(_QWORD *)(a6 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v30 + 16))(v20, a3, a6);
  v24 = *(_QWORD *)(a7 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v24 + 16))(v22, v23, a7);
  v25 = __swift_instantiateConcreteTypeFromMangledName(a11);
  v26 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(v31, a11);
  v27 = v37;
  v36(v20, v22, v35, v34, v37, a6, a7, v25, a8, v33, v32, v26);
  (*(void (**)(char *, uint64_t))(v24 + 8))(v22, a7);
  (*(void (**)(char *, uint64_t))(v30 + 8))(v20, a6);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(v27, a8);
  *v38 = result;
  return result;
}

uint64_t static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2, char *a3, uint64_t a4, char *a5, char *a6, char *a7, char *a8, char *a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  char *v24;
  uint64_t v25;
  uint64_t v26;
  char *v27;
  char *v28;
  void (*v29)(char *, uint64_t, char *);
  void (*v30)(char *, uint64_t, char *);
  char *v31;
  char *v32;
  char *v33;
  uint64_t v34;
  char *v35;
  void (*v36)(char *, char *, char *);
  uint64_t v37;
  BOOL v38;
  void (*v39)(char *, uint64_t);
  uint64_t result;
  char *v41;
  char *v42;
  char *v43;
  uint64_t v44;
  char *v45;
  char *v46;
  uint64_t v47;
  char *v48;
  char *v49;
  void (*v50)(char *, char *, char *);
  char *v51;
  char *v52;
  char *v53;
  char *v54;
  uint64_t v55;
  uint64_t v56;
  char *v57;
  uint64_t v58;
  uint64_t v59;
  uint64_t v60;
  uint64_t v61;
  char *v62;

  v52 = a5;
  v54 = a3;
  v59 = a11;
  v60 = a2;
  v61 = a10;
  v62 = a8;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v58 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v18 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v53 = (char *)&v49 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  v20 = MEMORY[0x1E0C80A78](v18);
  v57 = (char *)&v49 - v21;
  v22 = MEMORY[0x1E0C80A78](v20);
  v24 = (char *)&v49 - v23;
  v25 = MEMORY[0x1E0C80A78](v22);
  v27 = (char *)&v49 - v26;
  v28 = (char *)&v49 + *(int *)(v25 + 48) - v26;
  v29 = *(void (**)(char *, uint64_t, char *))(*((_QWORD *)a6 - 1) + 16);
  v29((char *)&v49 - v26, a1, a6);
  v30 = *(void (**)(char *, uint64_t, char *))(*((_QWORD *)a7 - 1) + 16);
  v30(v28, v60, a7);
  v55 = a12;
  v56 = a4;
  v60 = (*(uint64_t (**)(char *))(*(_QWORD *)(a12 + 8) + 16))(v62);
  v31 = &v24[*(int *)(TupleTypeMetadata2 + 48)];
  v49 = v27;
  v32 = v27;
  v33 = v57;
  v29(v24, (uint64_t)v32, a6);
  v51 = v28;
  v30(v31, (uint64_t)v28, a7);
  v34 = (*(uint64_t (**)(char *))(v61 + 16))(a6);
  v35 = &v33[*(int *)(TupleTypeMetadata2 + 48)];
  v50 = (void (*)(char *, char *, char *))v29;
  v29(v33, (uint64_t)v24, a6);
  v36 = (void (*)(char *, char *, char *))v30;
  v37 = v60;
  v36(v35, v31, a7);
  v38 = v34 != v37 || (*(uint64_t (**)(char *))(v59 + 16))(a7) != v37;
  v39 = *(void (**)(char *, uint64_t))(v58 + 8);
  v39(v33, TupleTypeMetadata2);
  result = ((uint64_t (*)(char *, uint64_t))v39)(v24, TupleTypeMetadata2);
  if (v38)
  {
    __break(1u);
  }
  else
  {
    v41 = v53;
    v42 = &v53[*(int *)(TupleTypeMetadata2 + 48)];
    v43 = v49;
    v50(v53, v49, a6);
    v44 = ((uint64_t (*)(char *, char *, char *))v36)(v42, v51, a7);
    MEMORY[0x1E0C80A78](v44);
    *(&v49 - 12) = v52;
    *(&v49 - 11) = a6;
    v46 = (char *)v61;
    v45 = v62;
    *(&v49 - 10) = a7;
    *(&v49 - 9) = v45;
    *(&v49 - 8) = a9;
    *(&v49 - 7) = v46;
    v47 = v55;
    *(&v49 - 6) = (char *)v59;
    *(&v49 - 5) = (char *)v47;
    v48 = v54;
    *(&v49 - 4) = v41;
    *(&v49 - 3) = v48;
    *(&v49 - 2) = (char *)v60;
    (*(void (**)(uint64_t))(v47 + 16))(a13);
    v39(v43, TupleTypeMetadata2);
    return ((uint64_t (*)(char *, uint64_t))v39)(v41, TupleTypeMetadata2);
  }
  return result;
}

uint64_t closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  char *v23;
  uint64_t v24;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  char v36[16];
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  char *v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;

  v30 = a8;
  v29 = a5;
  v32 = a3;
  v33 = a4;
  v31 = a1;
  v35 = a9;
  v34 = a14;
  v28 = a13;
  v27 = a12;
  v26 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v18 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v19 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v21 = (char *)&v26 - v20;
  v22 = *(int *)(v19 + 48);
  v23 = &v21[v22];
  v24 = a2 + v22;
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a6 - 8) + 16))(v21, a2, a6);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a7 - 8) + 16))(v23, v24, a7);
  v37 = v29;
  v38 = a6;
  v39 = a7;
  v40 = v30;
  v41 = v26;
  v42 = a11;
  v43 = v27;
  v44 = v28;
  v45 = v21;
  v46 = v32;
  v47 = v31;
  v48 = v33;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(v42 + 24))(v34, v36, MEMORY[0x1E0DEE9C0] + 8, a6, v42);
  return (*(uint64_t (**)(char *, uint64_t))(v18 + 8))(v21, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, _QWORD *a7, uint64_t a8, uint64_t (*a9)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  if (!a3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (!a5)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (!result)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (*a7)
  {
    if ((a8 & 0x8000000000000000) == 0)
      return a9(a3, 1, a5, 1, result, 1);
    __break(1u);
    goto LABEL_8;
  }
LABEL_11:
  __break(1u);
  return result;
}

uint64_t static vDSP.add<A, B>(multiplication:multiplication:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7, float a8)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  uint64_t v24;
  char *v25;
  uint64_t v26;
  void (*v27)(char *, uint64_t, uint64_t);
  void (*v28)(char *, uint64_t, uint64_t);
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  char *v32;
  char *v33;
  uint64_t v34;
  void (*v35)(char *, uint64_t);
  void (*v36)(char *, uint64_t);
  uint64_t v38;
  char *v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  char *v48;
  char *v49;

  v42 = a6;
  v43 = a5;
  v38 = a1;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v41 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v14 = TupleTypeMetadata2;
  v15 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v39 = (char *)&v38 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1E0C80A78](v15);
  v18 = (char *)&v38 - v17;
  v19 = swift_getTupleTypeMetadata2();
  v40 = *(_QWORD *)(v19 - 8);
  v20 = MEMORY[0x1E0C80A78](v19);
  v22 = (char *)&v38 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  v23 = MEMORY[0x1E0C80A78](v20);
  v25 = (char *)&v38 - v24;
  v26 = *(int *)(v23 + 48);
  v27 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a3 - 8) + 16);
  v27((char *)&v38 - v24, v38, a3);
  *(float *)&v25[v26] = a7;
  v28 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a4 - 8) + 16);
  v28(v18, a2, a4);
  v29 = v14;
  *(float *)&v18[*(int *)(v14 + 48)] = a8;
  v38 = (*(uint64_t (**)(uint64_t))(v43 + 16))(a3);
  v30 = *(int *)(v19 + 48);
  v27(v22, (uint64_t)v25, a3);
  *(float *)&v22[v30] = a7;
  v31 = *(int *)(v29 + 48);
  v32 = v39;
  v28(v39, (uint64_t)v18, a4);
  *(float *)&v32[v31] = a8;
  v44 = a3;
  v45 = a4;
  v46 = v43;
  v47 = v42;
  v48 = v22;
  v49 = v32;
  v33 = v32;
  v34 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v38, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.add<A, B>(multiplication:multiplication:));
  v35 = *(void (**)(char *, uint64_t))(v41 + 8);
  v35(v18, v29);
  v36 = *(void (**)(char *, uint64_t))(v40 + 8);
  v36(v25, v19);
  v36(v22, v19);
  v35(v33, v29);
  return v34;
}

uint64_t closure #1 in static vDSP.add<A, B>(multiplication:multiplication:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v13;
  char *v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  float v21;
  uint64_t v22;
  uint64_t v23;
  float v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t result;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t *v32;

  v30 = a7;
  v31 = a8;
  v32 = a2;
  v29 = a1;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v14 = (char *)&v29 - v13;
  v15 = swift_getTupleTypeMetadata2();
  v16 = MEMORY[0x1E0C80A78](v15);
  v18 = (char *)&v29 - v17;
  v19 = *(int *)(v16 + 48);
  v20 = *(_QWORD *)(a5 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v20 + 16))((char *)&v29 - v17, a3, a5);
  v21 = *(float *)(a3 + v19);
  *(float *)&v18[v19] = v21;
  v22 = *(int *)(TupleTypeMetadata2 + 48);
  v23 = *(_QWORD *)(a6 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v23 + 16))(v14, a4, a6);
  v24 = *(float *)(a4 + v22);
  *(float *)&v14[v22] = v24;
  v25 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v26 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v27 = v30;
  static vDSP.add<A, B, C>(multiplication:multiplication:result:)((uint64_t)v18, v14, v29, a5, a6, v25, v30, v31, v21, v24, v26);
  (*(void (**)(char *, uint64_t))(v23 + 8))(v14, a6);
  (*(void (**)(char *, uint64_t))(v20 + 8))(v18, a5);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(v27 + 16))(a5, v27);
  *v32 = result;
  return result;
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v13;
  char *v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  double v21;
  uint64_t v22;
  uint64_t v23;
  double v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t result;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t *v32;

  v30 = a7;
  v31 = a8;
  v32 = a2;
  v29 = a1;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v14 = (char *)&v29 - v13;
  v15 = swift_getTupleTypeMetadata2();
  v16 = MEMORY[0x1E0C80A78](v15);
  v18 = (char *)&v29 - v17;
  v19 = *(int *)(v16 + 48);
  v20 = *(_QWORD *)(a5 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v20 + 16))((char *)&v29 - v17, a3, a5);
  v21 = *(double *)(a3 + v19);
  *(double *)&v18[v19] = v21;
  v22 = *(int *)(TupleTypeMetadata2 + 48);
  v23 = *(_QWORD *)(a6 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v23 + 16))(v14, a4, a6);
  v24 = *(double *)(a4 + v22);
  *(double *)&v14[v22] = v24;
  v25 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v26 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v27 = v30;
  static vDSP.add<A, B, C>(multiplication:multiplication:result:)((uint64_t)v18, v14, v29, a5, a6, v25, v30, v31, v21, v24, v26);
  (*(void (**)(char *, uint64_t))(v23 + 8))(v14, a6);
  (*(void (**)(char *, uint64_t))(v20 + 8))(v18, a5);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(v27 + 16))(a5, v27);
  *v32 = result;
  return result;
}

uint64_t static vDSP.add<A, B, C>(multiplication:multiplication:result:)(uint64_t a1, char *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, float a10, uint64_t a11)
{
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  char *v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  char *v30;
  uint64_t v31;
  char *v32;
  void (*v33)(char *, uint64_t, uint64_t);
  void (*v34)(char *, char *, uint64_t);
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  char *v38;
  char *v39;
  uint64_t v40;
  uint64_t v41;
  char *v42;
  uint64_t (*v43)(uint64_t);
  uint64_t v44;
  uint64_t v45;
  BOOL v46;
  void (*v47)(char *, uint64_t);
  uint64_t (*v48)(char *, uint64_t);
  uint64_t v49;
  uint64_t result;
  uint64_t v51;
  char *v52;
  char *v53;
  uint64_t v54;
  uint64_t v55;
  char *v56;
  char *v57;
  uint64_t v58;
  uint64_t v59;
  uint64_t v60;
  uint64_t v61;
  uint64_t v62;
  uint64_t v63;
  uint64_t (*v64)(char *, char *, uint64_t);
  void (*v65)(char *, char *, uint64_t);
  uint64_t v66;
  char *v67;
  char *v68;
  char *v69;
  uint64_t v70;
  char *v71;
  uint64_t v72;
  uint64_t v73;
  char *v74;
  uint64_t TupleTypeMetadata2;
  char *v76;
  char *v77;
  uint64_t v78;
  uint64_t *v79;
  uint64_t v80;
  uint64_t v81;
  uint64_t v82;
  uint64_t v83;
  uint64_t v84;

  v80 = a8;
  v83 = a7;
  v84 = a3;
  v77 = a2;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v79 = *(uint64_t **)(TupleTypeMetadata2 - 8);
  v17 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v71 = (char *)&v63 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = MEMORY[0x1E0C80A78](v17);
  v76 = (char *)&v63 - v20;
  v21 = MEMORY[0x1E0C80A78](v19);
  v74 = (char *)&v63 - v22;
  MEMORY[0x1E0C80A78](v21);
  v24 = (char *)&v63 - v23;
  v25 = swift_getTupleTypeMetadata2();
  v78 = *(_QWORD *)(v25 - 8);
  v26 = MEMORY[0x1E0C80A78](v25);
  v68 = (char *)&v63 - ((v27 + 15) & 0xFFFFFFFFFFFFFFF0);
  v28 = MEMORY[0x1E0C80A78](v26);
  v30 = (char *)&v63 - v29;
  MEMORY[0x1E0C80A78](v28);
  v32 = (char *)&v63 - v31;
  v33 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a4 - 8) + 16);
  v33((char *)&v63 - v31, a1, a4);
  *(float *)&v32[*(int *)(v25 + 48)] = a9;
  v81 = a5;
  v34 = *(void (**)(char *, char *, uint64_t))(*(_QWORD *)(a5 - 8) + 16);
  v34(v24, v77, a5);
  v35 = TupleTypeMetadata2;
  *(float *)&v24[*(int *)(TupleTypeMetadata2 + 48)] = a10;
  v72 = a11;
  v73 = a6;
  v36 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a11 + 8) + 16))(a6);
  v82 = v25;
  v37 = *(int *)(v25 + 48);
  v67 = v32;
  v38 = v32;
  v39 = v74;
  v65 = (void (*)(char *, char *, uint64_t))v33;
  v33(v30, (uint64_t)v38, a4);
  *(float *)&v30[v37] = a9;
  v40 = *(int *)(v35 + 48);
  v69 = v24;
  v41 = v81;
  v42 = v76;
  v34(v39, v24, v81);
  *(float *)&v39[v40] = a10;
  v43 = *(uint64_t (**)(uint64_t))(v83 + 16);
  v77 = v30;
  v70 = a4;
  v44 = v43(a4);
  v45 = *(int *)(v35 + 48);
  v64 = (uint64_t (*)(char *, char *, uint64_t))v34;
  v34(v42, v39, v41);
  *(float *)&v42[v45] = a10;
  v66 = v36;
  v46 = v44 != v36 || (*(uint64_t (**)(uint64_t))(v80 + 16))(v41) != v36;
  v47 = (void (*)(char *, uint64_t))v79[1];
  v47(v42, v35);
  v47(v39, v35);
  v48 = *(uint64_t (**)(char *, uint64_t))(v78 + 8);
  v49 = v82;
  result = v48(v77, v82);
  if (v46)
  {
    __break(1u);
  }
  else
  {
    v51 = *(int *)(v49 + 48);
    v52 = v67;
    v53 = v68;
    v54 = v70;
    v65(v68, v67, v70);
    *(float *)&v53[v51] = a9;
    v55 = *(int *)(v35 + 48);
    v56 = v71;
    v57 = v69;
    v58 = v81;
    v59 = v64(v71, v69, v81);
    v79 = &v63;
    *(float *)&v56[v55] = a10;
    MEMORY[0x1E0C80A78](v59);
    *(&v63 - 10) = v54;
    *(&v63 - 9) = v58;
    v60 = v72;
    v61 = v83;
    *(&v63 - 8) = v73;
    *(&v63 - 7) = v61;
    *(&v63 - 6) = v80;
    *(&v63 - 5) = v60;
    *(&v63 - 4) = (uint64_t)v53;
    *(&v63 - 3) = (uint64_t)v56;
    *(&v63 - 2) = v66;
    (*(void (**)(uint64_t (*)(uint64_t)))(v60 + 16))(partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:));
    v47(v57, v35);
    v62 = v82;
    v48(v52, v82);
    v48(v53, v62);
    return ((uint64_t (*)(char *, uint64_t))v47)(v56, v35);
  }
  return result;
}

uint64_t closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  char v37[16];
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  char *v44;
  char *v45;
  uint64_t v46;
  uint64_t v47;

  v33 = a8;
  v34 = a4;
  v31 = a7;
  v28 = a2;
  v32 = a1;
  v36 = a9;
  v29 = a10;
  v30 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v35 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v16 = (char *)&v27 - v15;
  v17 = swift_getTupleTypeMetadata2();
  v18 = *(_QWORD *)(v17 - 8);
  v19 = MEMORY[0x1E0C80A78](v17);
  v21 = (char *)&v27 - v20;
  v22 = *(int *)(v19 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a6 - 8) + 16))((char *)&v27 - v20, a3, a6);
  *(_DWORD *)&v21[v22] = *(_DWORD *)(a3 + v22);
  v23 = *(int *)(TupleTypeMetadata2 + 48);
  v24 = TupleTypeMetadata2;
  v25 = v28;
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a5 - 8) + 16))(v16, v28, a5);
  *(_DWORD *)&v16[v23] = *(_DWORD *)(v25 + v23);
  v38 = a5;
  v39 = a6;
  v40 = v31;
  v41 = v33;
  v42 = v29;
  v43 = v30;
  v44 = v21;
  v45 = v16;
  v46 = v32;
  v47 = v34;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t))(v33 + 24))(partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:), v37, MEMORY[0x1E0DEE9C0] + 8, a5);
  (*(void (**)(char *, uint64_t))(v18 + 8))(v21, v17);
  return (*(uint64_t (**)(char *, uint64_t))(v35 + 8))(v16, v24);
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  char v37[16];
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  char *v44;
  char *v45;
  uint64_t v46;
  uint64_t v47;

  v33 = a8;
  v34 = a4;
  v31 = a7;
  v28 = a2;
  v32 = a1;
  v36 = a9;
  v29 = a10;
  v30 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v35 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v16 = (char *)&v27 - v15;
  v17 = swift_getTupleTypeMetadata2();
  v18 = *(_QWORD *)(v17 - 8);
  v19 = MEMORY[0x1E0C80A78](v17);
  v21 = (char *)&v27 - v20;
  v22 = *(int *)(v19 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a6 - 8) + 16))((char *)&v27 - v20, a3, a6);
  *(_QWORD *)&v21[v22] = *(_QWORD *)(a3 + v22);
  v23 = *(int *)(TupleTypeMetadata2 + 48);
  v24 = TupleTypeMetadata2;
  v25 = v28;
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a5 - 8) + 16))(v16, v28, a5);
  *(_QWORD *)&v16[v23] = *(_QWORD *)(v25 + v23);
  v38 = a5;
  v39 = a6;
  v40 = v31;
  v41 = v33;
  v42 = v29;
  v43 = v30;
  v44 = v21;
  v45 = v16;
  v46 = v32;
  v47 = v34;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t))(v33 + 24))(partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:), v37, MEMORY[0x1E0DEE9C0] + 8, a5);
  (*(void (**)(char *, uint64_t))(v18 + 8))(v21, v17);
  return (*(uint64_t (**)(char *, uint64_t))(v35 + 8))(v16, v24);
}

uint64_t closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  char *v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  char v40[16];
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  char *v47;
  char *v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;

  v36 = a5;
  v37 = a6;
  v29 = a3;
  v34 = a1;
  v35 = a2;
  v39 = a9;
  v33 = a13;
  v32 = a12;
  v31 = a11;
  v30 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v38 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v18 = (char *)&v28 - v17;
  v19 = swift_getTupleTypeMetadata2();
  v20 = *(_QWORD *)(v19 - 8);
  v21 = MEMORY[0x1E0C80A78](v19);
  v23 = (char *)&v28 - v22;
  v24 = *(int *)(v21 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a7 - 8) + 16))((char *)&v28 - v22, a4, a7);
  *(_DWORD *)&v23[v24] = *(_DWORD *)(a4 + v24);
  v25 = *(int *)(TupleTypeMetadata2 + 48);
  v26 = v29;
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a8 - 8) + 16))(v18, v29, a8);
  *(_DWORD *)&v18[v25] = *(_DWORD *)(v26 + v25);
  v41 = a7;
  v42 = a8;
  v43 = v30;
  v44 = v31;
  v45 = v32;
  v46 = v33;
  v47 = v23;
  v48 = v18;
  v49 = v34;
  v50 = v35;
  v51 = v36;
  v52 = v37;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t))(v32 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:), v40, MEMORY[0x1E0DEE9C0] + 8, a8);
  (*(void (**)(char *, uint64_t))(v20 + 8))(v23, v19);
  return (*(uint64_t (**)(char *, uint64_t))(v38 + 8))(v18, TupleTypeMetadata2);
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  char *v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  char v40[16];
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  char *v47;
  char *v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;

  v36 = a5;
  v37 = a6;
  v29 = a3;
  v34 = a1;
  v35 = a2;
  v39 = a9;
  v33 = a13;
  v32 = a12;
  v31 = a11;
  v30 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v38 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v18 = (char *)&v28 - v17;
  v19 = swift_getTupleTypeMetadata2();
  v20 = *(_QWORD *)(v19 - 8);
  v21 = MEMORY[0x1E0C80A78](v19);
  v23 = (char *)&v28 - v22;
  v24 = *(int *)(v21 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a7 - 8) + 16))((char *)&v28 - v22, a4, a7);
  *(_QWORD *)&v23[v24] = *(_QWORD *)(a4 + v24);
  v25 = *(int *)(TupleTypeMetadata2 + 48);
  v26 = v29;
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a8 - 8) + 16))(v18, v29, a8);
  *(_QWORD *)&v18[v25] = *(_QWORD *)(v26 + v25);
  v41 = a7;
  v42 = a8;
  v43 = v30;
  v44 = v31;
  v45 = v32;
  v46 = v33;
  v47 = v23;
  v48 = v18;
  v49 = v34;
  v50 = v35;
  v51 = v36;
  v52 = v37;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t))(v32 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:), v40, MEMORY[0x1E0DEE9C0] + 8, a8);
  (*(void (**)(char *, uint64_t))(v20 + 8))(v23, v19);
  return (*(uint64_t (**)(char *, uint64_t))(v38 + 8))(v18, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)@<X0>(const float *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, const float *a5@<X4>, uint64_t a6@<X5>, float **a7@<X6>, vDSP_Length a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, __int128 a12, uint64_t a13)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  int v19;
  uint64_t v20;
  _QWORD v22[2];
  __int128 v23;
  uint64_t v24;
  const float *v25;
  uint64_t v26;
  const float *v27;
  uint64_t v28;
  float **v29;
  vDSP_Length v30;
  int v31;
  uint64_t v32;

  v30 = a8;
  v28 = a6;
  v29 = a7;
  v26 = a2;
  v27 = a5;
  v24 = a9;
  v25 = a1;
  v23 = a12;
  v22[1] = a13;
  v32 = *MEMORY[0x1E0C80C00];
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v16 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v18 = (char *)v22 - v17;
  v19 = *(_DWORD *)(a3 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  v20 = *(int *)(TupleTypeMetadata2 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a11 - 8) + 16))(v18, a4, a11);
  *(_DWORD *)&v18[v20] = *(_DWORD *)(a4 + v20);
  v31 = v19;
  closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)((const float *)&v31, (uint64_t)v18, v27, v28, v25, v26, v29, v30);
  return (*(uint64_t (**)(char *, uint64_t))(v16 + 8))(v18, TupleTypeMetadata2);
}

void closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(const float *a1, uint64_t a2, const float *a3, uint64_t a4, const float *a5, uint64_t a6, float **a7, vDSP_Length a8)
{
  float __D;
  uint64_t v14;

  v14 = *MEMORY[0x1E0C80C00];
  __D = *(float *)(a2 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  if (!a3)
    goto LABEL_7;
  if (!a5)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (!*a7)
    goto LABEL_9;
  if ((a8 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  vDSP_vsmsma(a3, 1, a1, a5, 1, &__D, *a7, 1, a8);
}

uint64_t static vDSP.add<A, B>(multiplication:multiplication:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7, double a8)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  uint64_t v24;
  char *v25;
  uint64_t v26;
  void (*v27)(char *, uint64_t, uint64_t);
  void (*v28)(char *, uint64_t, uint64_t);
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  char *v32;
  char *v33;
  uint64_t v34;
  void (*v35)(char *, uint64_t);
  void (*v36)(char *, uint64_t);
  uint64_t v38;
  char *v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  char *v48;
  char *v49;

  v42 = a6;
  v43 = a5;
  v38 = a1;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v41 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v14 = TupleTypeMetadata2;
  v15 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v39 = (char *)&v38 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1E0C80A78](v15);
  v18 = (char *)&v38 - v17;
  v19 = swift_getTupleTypeMetadata2();
  v40 = *(_QWORD *)(v19 - 8);
  v20 = MEMORY[0x1E0C80A78](v19);
  v22 = (char *)&v38 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  v23 = MEMORY[0x1E0C80A78](v20);
  v25 = (char *)&v38 - v24;
  v26 = *(int *)(v23 + 48);
  v27 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a3 - 8) + 16);
  v27((char *)&v38 - v24, v38, a3);
  *(double *)&v25[v26] = a7;
  v28 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a4 - 8) + 16);
  v28(v18, a2, a4);
  v29 = v14;
  *(double *)&v18[*(int *)(v14 + 48)] = a8;
  v38 = (*(uint64_t (**)(uint64_t))(v43 + 16))(a3);
  v30 = *(int *)(v19 + 48);
  v27(v22, (uint64_t)v25, a3);
  *(double *)&v22[v30] = a7;
  v31 = *(int *)(v29 + 48);
  v32 = v39;
  v28(v39, (uint64_t)v18, a4);
  *(double *)&v32[v31] = a8;
  v44 = a3;
  v45 = a4;
  v46 = v43;
  v47 = v42;
  v48 = v22;
  v49 = v32;
  v33 = v32;
  v34 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v38, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.add<A, B>(multiplication:multiplication:));
  v35 = *(void (**)(char *, uint64_t))(v41 + 8);
  v35(v18, v29);
  v36 = *(void (**)(char *, uint64_t))(v40 + 8);
  v36(v25, v19);
  v36(v22, v19);
  v35(v33, v29);
  return v34;
}

uint64_t static vDSP.add<A, B, C>(multiplication:multiplication:result:)(uint64_t a1, char *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, double a10, uint64_t a11)
{
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  char *v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  char *v30;
  uint64_t v31;
  char *v32;
  void (*v33)(char *, uint64_t, uint64_t);
  void (*v34)(char *, char *, uint64_t);
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  char *v38;
  char *v39;
  uint64_t v40;
  uint64_t v41;
  char *v42;
  uint64_t (*v43)(uint64_t);
  uint64_t v44;
  uint64_t v45;
  BOOL v46;
  void (*v47)(char *, uint64_t);
  uint64_t (*v48)(char *, uint64_t);
  uint64_t v49;
  uint64_t result;
  uint64_t v51;
  char *v52;
  char *v53;
  uint64_t v54;
  uint64_t v55;
  char *v56;
  char *v57;
  uint64_t v58;
  uint64_t v59;
  uint64_t v60;
  uint64_t v61;
  uint64_t v62;
  uint64_t v63;
  uint64_t (*v64)(char *, char *, uint64_t);
  void (*v65)(char *, char *, uint64_t);
  uint64_t v66;
  char *v67;
  char *v68;
  char *v69;
  uint64_t v70;
  char *v71;
  uint64_t v72;
  uint64_t v73;
  char *v74;
  uint64_t TupleTypeMetadata2;
  char *v76;
  char *v77;
  uint64_t v78;
  uint64_t *v79;
  uint64_t v80;
  uint64_t v81;
  uint64_t v82;
  uint64_t v83;
  uint64_t v84;

  v80 = a8;
  v83 = a7;
  v84 = a3;
  v77 = a2;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v79 = *(uint64_t **)(TupleTypeMetadata2 - 8);
  v17 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v71 = (char *)&v63 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = MEMORY[0x1E0C80A78](v17);
  v76 = (char *)&v63 - v20;
  v21 = MEMORY[0x1E0C80A78](v19);
  v74 = (char *)&v63 - v22;
  MEMORY[0x1E0C80A78](v21);
  v24 = (char *)&v63 - v23;
  v25 = swift_getTupleTypeMetadata2();
  v78 = *(_QWORD *)(v25 - 8);
  v26 = MEMORY[0x1E0C80A78](v25);
  v68 = (char *)&v63 - ((v27 + 15) & 0xFFFFFFFFFFFFFFF0);
  v28 = MEMORY[0x1E0C80A78](v26);
  v30 = (char *)&v63 - v29;
  MEMORY[0x1E0C80A78](v28);
  v32 = (char *)&v63 - v31;
  v33 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a4 - 8) + 16);
  v33((char *)&v63 - v31, a1, a4);
  *(double *)&v32[*(int *)(v25 + 48)] = a9;
  v81 = a5;
  v34 = *(void (**)(char *, char *, uint64_t))(*(_QWORD *)(a5 - 8) + 16);
  v34(v24, v77, a5);
  v35 = TupleTypeMetadata2;
  *(double *)&v24[*(int *)(TupleTypeMetadata2 + 48)] = a10;
  v72 = a11;
  v73 = a6;
  v36 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a11 + 8) + 16))(a6);
  v82 = v25;
  v37 = *(int *)(v25 + 48);
  v67 = v32;
  v38 = v32;
  v39 = v74;
  v65 = (void (*)(char *, char *, uint64_t))v33;
  v33(v30, (uint64_t)v38, a4);
  *(double *)&v30[v37] = a9;
  v40 = *(int *)(v35 + 48);
  v69 = v24;
  v41 = v81;
  v42 = v76;
  v34(v39, v24, v81);
  *(double *)&v39[v40] = a10;
  v43 = *(uint64_t (**)(uint64_t))(v83 + 16);
  v77 = v30;
  v70 = a4;
  v44 = v43(a4);
  v45 = *(int *)(v35 + 48);
  v64 = (uint64_t (*)(char *, char *, uint64_t))v34;
  v34(v42, v39, v41);
  *(double *)&v42[v45] = a10;
  v66 = v36;
  v46 = v44 != v36 || (*(uint64_t (**)(uint64_t))(v80 + 16))(v41) != v36;
  v47 = (void (*)(char *, uint64_t))v79[1];
  v47(v42, v35);
  v47(v39, v35);
  v48 = *(uint64_t (**)(char *, uint64_t))(v78 + 8);
  v49 = v82;
  result = v48(v77, v82);
  if (v46)
  {
    __break(1u);
  }
  else
  {
    v51 = *(int *)(v49 + 48);
    v52 = v67;
    v53 = v68;
    v54 = v70;
    v65(v68, v67, v70);
    *(double *)&v53[v51] = a9;
    v55 = *(int *)(v35 + 48);
    v56 = v71;
    v57 = v69;
    v58 = v81;
    v59 = v64(v71, v69, v81);
    v79 = &v63;
    *(double *)&v56[v55] = a10;
    MEMORY[0x1E0C80A78](v59);
    *(&v63 - 10) = v54;
    *(&v63 - 9) = v58;
    v60 = v72;
    v61 = v83;
    *(&v63 - 8) = v73;
    *(&v63 - 7) = v61;
    *(&v63 - 6) = v80;
    *(&v63 - 5) = v60;
    *(&v63 - 4) = (uint64_t)v53;
    *(&v63 - 3) = (uint64_t)v56;
    *(&v63 - 2) = v66;
    (*(void (**)(uint64_t (*)(uint64_t)))(v60 + 16))(partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:));
    v47(v57, v35);
    v62 = v82;
    v48(v52, v82);
    v48(v53, v62);
    return ((uint64_t (*)(char *, uint64_t))v47)(v56, v35);
  }
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)@<X0>(const double *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, const double *a5@<X4>, uint64_t a6@<X5>, double **a7@<X6>, vDSP_Length a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, __int128 a12, uint64_t a13)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  double v19;
  uint64_t v20;
  _QWORD v22[2];
  __int128 v23;
  uint64_t v24;
  const double *v25;
  uint64_t v26;
  const double *v27;
  uint64_t v28;
  double **v29;
  vDSP_Length v30;
  double v31[2];

  v30 = a8;
  v28 = a6;
  v29 = a7;
  v26 = a2;
  v27 = a5;
  v24 = a9;
  v25 = a1;
  v23 = a12;
  v22[1] = a13;
  v31[1] = *(double *)MEMORY[0x1E0C80C00];
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v16 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v18 = (char *)v22 - v17;
  v19 = *(double *)(a3 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  v20 = *(int *)(TupleTypeMetadata2 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a11 - 8) + 16))(v18, a4, a11);
  *(_QWORD *)&v18[v20] = *(_QWORD *)(a4 + v20);
  v31[0] = v19;
  closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(v31, (uint64_t)v18, v27, v28, v25, v26, v29, v30);
  return (*(uint64_t (**)(char *, uint64_t))(v16 + 8))(v18, TupleTypeMetadata2);
}

void closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(const double *a1, uint64_t a2, const double *a3, uint64_t a4, const double *a5, uint64_t a6, double **a7, vDSP_Length a8)
{
  double __D[2];

  __D[1] = *(double *)MEMORY[0x1E0C80C00];
  __D[0] = *(double *)(a2 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  if (!a3)
    goto LABEL_7;
  if (!a5)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (!*a7)
    goto LABEL_9;
  if ((a8 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  vDSP_vsmsmaD(a3, 1, a1, a5, 1, __D, *a7, 1, a8);
}

uint64_t static vDSP.add<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, char *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, unint64_t a10, unint64_t a11, uint64_t a12)
{
  return static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, __PAIR128__(a11, a10), a12, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, __PAIR128__(a11, a10), a12, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, void (*a2)(char *, char *, uint64_t), void (*a3)(char *, char *, uint64_t), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15)
{
  return static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, (char *)partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, (char *)partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, void (*a2)(char *, char *, uint64_t), void (*a3)(char *, char *, uint64_t), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15, char *a16)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  char *v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  char *v38;
  uint64_t v39;
  uint64_t v40;
  char *v41;
  char *v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  char *v46;
  char *v47;
  void (*v48)(char *, char *, uint64_t);
  void (*v49)(char *, uint64_t, uint64_t);
  char *v50;
  char *v51;
  void (*v52)(char *, char *, uint64_t);
  char *v53;
  char *v54;
  char *v55;
  uint64_t v56;
  char *v57;
  uint64_t v58;
  char *v59;
  uint64_t v60;
  _BOOL4 v61;
  void (*v62)(char *, uint64_t);
  uint64_t v63;
  char *v64;
  char *v65;
  char *v66;
  char *v67;
  uint64_t v68;
  uint64_t v69;
  char *v70;
  char *v71;
  uint64_t v72;
  uint64_t v73;
  char *v74;
  uint64_t v75;
  void (*v76)(char *, uint64_t);
  uint64_t result;
  uint64_t v78;
  char *v79;
  char *v80;
  uint64_t v81;
  uint64_t v82;
  char *v83;
  char *v84;
  uint64_t v85;
  char *v86;
  uint64_t v87;
  uint64_t v88;
  uint64_t v89;
  uint64_t v90;
  uint64_t v91;
  uint64_t v92;
  uint64_t (*v93)(char *, uint64_t);
  uint64_t v94;
  void (*v95)(char *, uint64_t);
  char *v96;
  uint64_t v97;
  void (*v98)(char *, uint64_t);
  uint64_t v99;
  char *v100;
  char *v101;
  char *v102;
  char *v103;
  uint64_t v104;
  uint64_t v105;
  char *v106;
  char *v107;
  uint64_t v108;
  char *v109;
  char *v110;
  char *v111;
  char *v112;
  uint64_t v113;
  uint64_t v114;
  uint64_t v115;
  void (*v116)(char *, uint64_t, uint64_t);
  char *v117;
  uint64_t v118;
  uint64_t v119;
  uint64_t v120;
  uint64_t v121;
  uint64_t v122;
  char *v123;
  uint64_t v124;
  void (*v125)(char *, char *, uint64_t);
  void (*v126)(char *, char *, uint64_t);
  void (*v127)(char *, char *, uint64_t);
  uint64_t v128;
  uint64_t v129;
  uint64_t v130;

  v128 = a4;
  v129 = a8;
  v125 = a2;
  v126 = a3;
  v120 = a15;
  v121 = a5;
  v113 = a12;
  v114 = a13;
  v118 = a11;
  v119 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v20 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v130 = TupleTypeMetadata2;
  v115 = v20;
  v21 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v111 = (char *)&v99 - ((v22 + 15) & 0xFFFFFFFFFFFFFFF0);
  v23 = MEMORY[0x1E0C80A78](v21);
  v110 = (char *)&v99 - v24;
  v25 = MEMORY[0x1E0C80A78](v23);
  v103 = (char *)&v99 - v26;
  v27 = MEMORY[0x1E0C80A78](v25);
  v123 = (char *)&v99 - v28;
  MEMORY[0x1E0C80A78](v27);
  v30 = (char *)&v99 - v29;
  v31 = swift_getTupleTypeMetadata2();
  v108 = *(_QWORD *)(v31 - 8);
  v32 = MEMORY[0x1E0C80A78](v31);
  v102 = (char *)&v99 - ((v33 + 15) & 0xFFFFFFFFFFFFFFF0);
  v34 = MEMORY[0x1E0C80A78](v32);
  v107 = (char *)&v99 - v35;
  v36 = MEMORY[0x1E0C80A78](v34);
  v38 = (char *)&v99 - v37;
  v39 = MEMORY[0x1E0C80A78](v36);
  v41 = (char *)&v99 - v40;
  v42 = (char *)&v99 + *(int *)(v39 + 48) - v40;
  v116 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a6 - 8) + 16);
  v43 = a1;
  v44 = a6;
  v116((char *)&v99 - v40, v43, a6);
  v122 = a7;
  v127 = *(void (**)(char *, char *, uint64_t))(*(_QWORD *)(a7 - 8) + 16);
  v127(v42, (char *)v125, a7);
  v45 = *(int *)(TupleTypeMetadata2 + 48);
  v46 = v30;
  v117 = v30;
  v47 = &v30[v45];
  v48 = *(void (**)(char *, char *, uint64_t))(*(_QWORD *)(v129 - 8) + 16);
  ((void (*)(char *, void (*)(char *, char *, uint64_t)))v48)(v46, v126);
  v49 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a9 - 8) + 16);
  v49(v47, v128, a9);
  v105 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v120 + 8) + 16))(v119);
  v106 = &v38[*(int *)(v31 + 48)];
  v50 = v106;
  v112 = v41;
  v51 = v41;
  v52 = (void (*)(char *, char *, uint64_t))v116;
  v116(v38, (uint64_t)v51, v44);
  v101 = v42;
  v127(v50, v42, v122);
  v53 = &v123[*(int *)(v130 + 48)];
  v126 = v48;
  ((void (*)(void))v48)();
  v54 = v53;
  v100 = v47;
  v55 = v47;
  v56 = v105;
  v124 = a9;
  v57 = v107;
  v125 = (void (*)(char *, char *, uint64_t))v49;
  v49(v53, (uint64_t)v55, a9);
  v58 = (*(uint64_t (**)(uint64_t))(v118 + 16))(v44);
  v128 = v31;
  v59 = &v57[*(int *)(v31 + 48)];
  v109 = v38;
  v104 = v44;
  v52(v57, v38, v44);
  v60 = v122;
  v127(v59, v106, v122);
  v61 = v58 == v56 && (*(uint64_t (**)(uint64_t))(v113 + 16))(v60) == v56;
  v62 = *(void (**)(char *, uint64_t))(v108 + 8);
  v62(v57, v128);
  v63 = v129;
  v64 = v110;
  v65 = &v110[*(int *)(v130 + 48)];
  v66 = v123;
  v126(v110, v123, v129);
  v125(v65, v54, v124);
  if (v61)
    v61 = (*(uint64_t (**)(uint64_t))(v114 + 16))(v63) == v56;
  v67 = v64;
  v68 = v130;
  v115 = *(_QWORD *)(v115 + 8);
  ((void (*)(char *, uint64_t))v115)(v67, v130);
  v69 = *(int *)(v68 + 48);
  v70 = v111;
  v71 = &v111[v69];
  v126(v111, v66, v63);
  v72 = v124;
  v125(v71, v54, v124);
  if (v61)
  {
    v73 = (*(uint64_t (**)(uint64_t, uint64_t))(a14 + 16))(v72, a14);
    v74 = v70;
    v75 = v130;
    v76 = (void (*)(char *, uint64_t))v115;
    ((void (*)(char *, uint64_t))v115)(v74, v130);
    v76(v66, v75);
    result = ((uint64_t (*)(char *, uint64_t))v62)(v109, v128);
    if (v73 == v56)
    {
      v111 = a16;
      v78 = *(int *)(v128 + 48);
      v123 = (char *)v62;
      v79 = v102;
      v80 = &v102[v78];
      v81 = v104;
      v116(v102, (uint64_t)v112, v104);
      v127(v80, v101, v60);
      v82 = v129;
      v83 = v103;
      v84 = &v103[*(int *)(v130 + 48)];
      v85 = v56;
      v86 = v117;
      v126(v103, v117, v129);
      v87 = ((uint64_t (*)(char *, char *, uint64_t))v125)(v84, v100, v72);
      MEMORY[0x1E0C80A78](v87);
      *(&v99 - 14) = v81;
      *(&v99 - 13) = v60;
      *(&v99 - 12) = v82;
      *(&v99 - 11) = v88;
      v89 = v118;
      *(&v99 - 10) = v119;
      *(&v99 - 9) = v89;
      v90 = v114;
      *(&v99 - 8) = v113;
      *(&v99 - 7) = v90;
      v91 = v120;
      *(&v99 - 6) = a14;
      *(&v99 - 5) = v91;
      *(&v99 - 4) = (uint64_t)v79;
      *(&v99 - 3) = (uint64_t)v83;
      *(&v99 - 2) = v85;
      (*(void (**)(char *))(v91 + 16))(v111);
      v92 = v130;
      v93 = (uint64_t (*)(char *, uint64_t))v115;
      ((void (*)(char *, uint64_t))v115)(v86, v130);
      v94 = v128;
      v95 = (void (*)(char *, uint64_t))v123;
      ((void (*)(char *, uint64_t))v123)(v112, v128);
      v95(v79, v94);
      return v93(v83, v92);
    }
  }
  else
  {
    v96 = v70;
    v97 = v130;
    v98 = (void (*)(char *, uint64_t))v115;
    ((void (*)(char *, uint64_t))v115)(v96, v130);
    v98(v66, v97);
    result = ((uint64_t (*)(char *, uint64_t))v62)(v109, v128);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, __int128 a12, uint64_t a13, uint64_t a14, uint64_t a15)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  char *v26;
  uint64_t v27;
  char *v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  char *v32;
  uint64_t v33;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  __int128 v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  char v49[16];
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;
  __int128 v56;
  uint64_t v57;
  uint64_t v58;
  char *v59;
  char *v60;
  uint64_t v61;
  uint64_t v62;

  v45 = a4;
  v37 = a3;
  v44 = a1;
  v48 = a9;
  v47 = a15;
  v43 = a14;
  v42 = a13;
  v41 = a12;
  v40 = a11;
  v39 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v46 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v22 = (char *)&v35 - v21;
  v23 = swift_getTupleTypeMetadata2();
  v38 = *(_QWORD *)(v23 - 8);
  v36 = v23;
  v24 = MEMORY[0x1E0C80A78](v23);
  v26 = (char *)&v35 - v25;
  v27 = *(int *)(v24 + 48);
  v28 = &v26[v27];
  v29 = a2;
  v35 = a2;
  v30 = a2 + v27;
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a5 - 8) + 16))(v26, v29, a5);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a6 - 8) + 16))(v28, v30, a6);
  v31 = *(int *)(TupleTypeMetadata2 + 48);
  v32 = &v22[v31];
  v33 = v37 + v31;
  (*(void (**)(char *))(*(_QWORD *)(a7 - 8) + 16))(v22);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a8 - 8) + 16))(v32, v33, a8);
  v50 = a5;
  v51 = a6;
  v52 = a7;
  v53 = a8;
  v54 = v39;
  v55 = v40;
  v56 = v41;
  v57 = v42;
  v58 = v43;
  v59 = v26;
  v60 = v22;
  v61 = v44;
  v62 = v45;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t))(v40 + 24))(v47, v49, MEMORY[0x1E0DEE9C0] + 8, a5);
  (*(void (**)(char *, uint64_t))(v38 + 8))(v26, v36);
  return (*(uint64_t (**)(char *, uint64_t))(v46 + 8))(v22, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, __int128 a15, uint64_t a16, uint64_t a17)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  char *v24;
  uint64_t v25;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  __int128 v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  char v40[16];
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  __int128 v48;
  uint64_t v49;
  char *v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;

  v36 = a5;
  v37 = a6;
  v27 = a3;
  v35 = a2;
  v34 = a1;
  v39 = a9;
  v38 = a17;
  v32 = a16;
  v31 = a15;
  v30 = a14;
  v29 = a13;
  v28 = a12;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v33 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v22 = (char *)&v27 - v21;
  v27 += *(int *)(swift_getTupleTypeMetadata2() + 48);
  v23 = *(int *)(TupleTypeMetadata2 + 48);
  v24 = &v22[v23];
  v25 = a4 + v23;
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a10 - 8) + 16))(v22, a4, a10);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a11 - 8) + 16))(v24, v25, a11);
  v41 = a7;
  v42 = a8;
  v43 = a10;
  v44 = a11;
  v45 = v28;
  v46 = v29;
  v47 = v30;
  v48 = v31;
  v49 = v32;
  v50 = v22;
  v51 = v34;
  v52 = v35;
  v53 = v36;
  v54 = v37;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t))(v30 + 24))(v38, v40, MEMORY[0x1E0DEE9C0] + 8, a8);
  return (*(uint64_t (**)(char *, uint64_t))(v33 + 8))(v22, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, uint64_t a12, __int128 a13, uint64_t a14, uint64_t a15, uint64_t a16, uint64_t a17, uint64_t a18)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  char *v23;
  uint64_t v24;
  char *v25;
  uint64_t v26;
  uint64_t v28;
  uint64_t v29;
  __int128 v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  char v43[16];
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  __int128 v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  char *v53;
  uint64_t v54;
  uint64_t v55;
  uint64_t v56;
  uint64_t v57;
  uint64_t v58;
  uint64_t v59;

  v34 = a8;
  v40 = a7;
  v39 = a6;
  v38 = a5;
  v36 = a4;
  v37 = a2;
  v35 = a1;
  v42 = a9;
  v41 = a18;
  v33 = a17;
  v32 = a16;
  v31 = a14;
  v30 = a13;
  v29 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v20 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v21 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v23 = (char *)&v28 - v22;
  v24 = *(int *)(v21 + 48);
  v25 = &v23[v24];
  v26 = a3 + v24;
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a11 - 8) + 16))(v23, a3, a11);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a12 - 8) + 16))(v25, v26, a12);
  v44 = v34;
  v45 = v29;
  v46 = a11;
  v47 = a12;
  v48 = v30;
  v49 = v31;
  v50 = a15;
  v51 = v32;
  v52 = v33;
  v53 = v23;
  v54 = v36;
  v55 = v38;
  v56 = v35;
  v57 = v37;
  v58 = v39;
  v59 = v40;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(v50 + 24))(v41, v43, MEMORY[0x1E0DEE9C0] + 8, a11, v50);
  return (*(uint64_t (**)(char *, uint64_t))(v20 + 8))(v23, TupleTypeMetadata2);
}

uint64_t static vDSP.multiply<A, B, C>(addition:addition:)(uint64_t a1, char *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return static vDSP.multiply<A, B, C>(addition:addition:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:addition:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.multiply<A, B, C>(addition:addition:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:addition:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.multiply<A, B, C, D>(addition:addition:result:)(uint64_t a1, char *a2, uint64_t a3, char *a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13)
{
  return static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, (char *)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:));
}

{
  return static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, (char *)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:));
}

uint64_t static vDSP.multiply<A, B, C>(addition:addition:)(uint64_t a1, char *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t (*a12)(uint64_t, uint64_t, char *))
{
  uint64_t TupleTypeMetadata2;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  char *v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  char *v23;
  uint64_t v24;
  uint64_t v25;
  char *v26;
  uint64_t v27;
  char *v28;
  void (*v29)(char *, uint64_t, uint64_t);
  uint64_t v30;
  void (*v31)(char *, char *, uint64_t);
  uint64_t v32;
  char *v33;
  char *v34;
  void (*v35)(char *, uint64_t, uint64_t);
  char *v36;
  uint64_t v37;
  uint64_t v38;
  char *v39;
  char *v40;
  uint64_t v41;
  void (*v42)(char *, uint64_t);
  uint64_t v43;
  void (*v44)(char *, uint64_t);
  char *v45;
  uint64_t v46;
  uint64_t v48;
  uint64_t v49;
  char *v50;
  uint64_t v51;
  uint64_t v52;
  char *v53;
  uint64_t v54;
  char *v55;
  uint64_t v56;
  uint64_t v57;
  uint64_t v58;
  uint64_t v59;
  uint64_t v60;
  uint64_t v61;
  uint64_t (*v62)(uint64_t, uint64_t, char *);
  uint64_t v63;
  char v64[16];
  uint64_t v65;
  uint64_t v66;
  uint64_t v67;
  uint64_t v68;
  uint64_t v69;
  uint64_t v70;
  char *v71;
  char *v72;

  v63 = a8;
  v56 = a4;
  v54 = a3;
  v53 = a2;
  v51 = a1;
  v61 = a11;
  v62 = a12;
  v59 = a9;
  v60 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v58 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v49 = TupleTypeMetadata2;
  v16 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v55 = (char *)&v48 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1E0C80A78](v16);
  v19 = (char *)&v48 - v18;
  v20 = swift_getTupleTypeMetadata2();
  v57 = *(_QWORD *)(v20 - 8);
  v21 = MEMORY[0x1E0C80A78](v20);
  v23 = (char *)&v48 - ((v22 + 15) & 0xFFFFFFFFFFFFFFF0);
  v24 = MEMORY[0x1E0C80A78](v21);
  v26 = (char *)&v48 - v25;
  v27 = *(int *)(v24 + 48);
  v52 = v24;
  v28 = (char *)&v48 + v27 - v25;
  v50 = v28;
  v29 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a5 - 8) + 16);
  v29((char *)&v48 - v25, v51, a5);
  v30 = a6;
  v48 = a6;
  v31 = *(void (**)(char *, char *, uint64_t))(*(_QWORD *)(a6 - 8) + 16);
  v31(v28, v53, v30);
  v32 = *(int *)(TupleTypeMetadata2 + 48);
  v33 = v19;
  v34 = &v19[v32];
  v53 = &v19[v32];
  v35 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a7 - 8) + 16);
  v35(v33, v54, a7);
  v35(v34, v56, a7);
  v56 = (*(uint64_t (**)(uint64_t))(v63 + 16))(a5);
  v36 = &v23[*(int *)(v20 + 48)];
  v29(v23, (uint64_t)v26, a5);
  v37 = v48;
  v31(v36, v50, v48);
  v38 = v49;
  v39 = v55;
  v40 = &v55[*(int *)(v49 + 48)];
  v35(v55, (uint64_t)v33, a7);
  v35(v40, (uint64_t)v53, a7);
  v65 = a5;
  v66 = v37;
  v67 = a7;
  v68 = v63;
  v69 = v59;
  v70 = v60;
  v71 = v23;
  v72 = v39;
  v41 = v62(v56, v61, v64);
  v42 = *(void (**)(char *, uint64_t))(v58 + 8);
  v43 = v38;
  v42(v33, v38);
  v44 = *(void (**)(char *, uint64_t))(v57 + 8);
  v45 = v26;
  v46 = v52;
  v44(v45, v52);
  v44(v23, v46);
  v42(v39, v43);
  return v41;
}

uint64_t closure #1 in static vDSP.multiply<A, B, C>(addition:addition:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t *a11, unint64_t *a12, void (*a13)(char *, char *, char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t TupleTypeMetadata2;
  uint64_t v18;
  char *v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  char *v23;
  uint64_t v24;
  char *v25;
  uint64_t v26;
  char *v27;
  char *v28;
  uint64_t v29;
  uint64_t v30;
  char *v31;
  uint64_t v32;
  void (*v33)(char *);
  uint64_t *v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  void (*v39)(char *, uint64_t);
  uint64_t result;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t *v46;
  uint64_t v47;
  unint64_t *v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  void (*v53)(char *, char *, char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t);
  uint64_t *v54;

  v51 = a1;
  v52 = a8;
  v44 = a4;
  v53 = a13;
  v54 = a2;
  v49 = a10;
  v50 = a9;
  v48 = a12;
  v46 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v19 = (char *)&v41 - v18;
  v20 = swift_getTupleTypeMetadata2();
  v21 = MEMORY[0x1E0C80A78](v20);
  v23 = (char *)&v41 - v22;
  v24 = *(int *)(v21 + 48);
  v25 = &v23[v24];
  v47 = a3;
  v26 = a3 + v24;
  v42 = a5;
  v45 = *(_QWORD *)(a5 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v45 + 16))(v23, a3, a5);
  v43 = *(_QWORD *)(a6 - 8);
  v27 = v25;
  v28 = v25;
  v29 = a6;
  (*(void (**)(char *, uint64_t, uint64_t))(v43 + 16))(v27, v26, a6);
  v30 = *(int *)(TupleTypeMetadata2 + 48);
  v31 = &v19[v30];
  v32 = v44 + v30;
  v41 = *(_QWORD *)(a7 - 8);
  v33 = *(void (**)(char *))(v41 + 16);
  v33(v19);
  ((void (*)(char *, uint64_t, uint64_t))v33)(v31, v32, a7);
  v34 = v46;
  v35 = __swift_instantiateConcreteTypeFromMangledName(v46);
  v36 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(v48, v34);
  v37 = v52;
  v38 = v42;
  v53(v23, v28, v19, v31, v51, v42, v29, a7, v35, v52, v50, v49, v36);
  v39 = *(void (**)(char *, uint64_t))(v41 + 8);
  v39(v31, a7);
  v39(v19, a7);
  (*(void (**)(char *, uint64_t))(v43 + 8))(v28, v29);
  (*(void (**)(char *, uint64_t))(v45 + 8))(v23, v38);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(v37 + 16))(v38, v37);
  *v54 = result;
  return result;
}

uint64_t static vDSP.multiply<A, B, C, D>(addition:addition:result:)(uint64_t a1, char *a2, uint64_t a3, char *a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, char *a14)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  char *v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  char *v37;
  uint64_t v38;
  uint64_t v39;
  char *v40;
  char *v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  char *v45;
  char *v46;
  uint64_t v47;
  void (*v48)(void);
  uint64_t v49;
  char *v50;
  char *v51;
  uint64_t v52;
  char *v53;
  uint64_t v54;
  char *v55;
  char *v56;
  char *v57;
  uint64_t v58;
  _BOOL4 v59;
  uint64_t v60;
  uint64_t v61;
  char *v62;
  char *v63;
  uint64_t v64;
  char *v65;
  uint64_t v66;
  char *v67;
  uint64_t v68;
  char *v69;
  void (*v70)(void);
  char *v71;
  uint64_t v72;
  uint64_t result;
  uint64_t v74;
  char *v75;
  char *v76;
  uint64_t v77;
  uint64_t v78;
  uint64_t v79;
  uint64_t v80;
  char *v81;
  char *v82;
  uint64_t v83;
  char *v84;
  uint64_t v85;
  uint64_t v86;
  uint64_t v87;
  uint64_t v88;
  void (*v89)(void);
  uint64_t v90;
  void (*v91)(char *, uint64_t);
  uint64_t v92;
  char *v93;
  char *v94;
  char *v95;
  char *v96;
  uint64_t v97;
  uint64_t v98;
  char *v99;
  char *v100;
  char *v101;
  uint64_t v102;
  char *v103;
  char *v104;
  uint64_t v105;
  uint64_t v106;
  uint64_t v107;
  char *v108;
  uint64_t v109;
  uint64_t v110;
  uint64_t v111;
  uint64_t v112;
  char *v113;
  uint64_t v114;
  char *v115;
  void (*v116)(char *, char *, uint64_t);
  void (*v117)(void);
  uint64_t v118;
  uint64_t v119;

  v112 = a5;
  v113 = a4;
  v107 = a3;
  v104 = a2;
  v105 = a11;
  v109 = a10;
  v110 = a12;
  v111 = a9;
  v114 = a8;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v18 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v19 = TupleTypeMetadata2;
  v118 = TupleTypeMetadata2;
  v102 = v18;
  v20 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v103 = (char *)&v92 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  v22 = MEMORY[0x1E0C80A78](v20);
  v101 = (char *)&v92 - v23;
  v24 = MEMORY[0x1E0C80A78](v22);
  v96 = (char *)&v92 - v25;
  v26 = MEMORY[0x1E0C80A78](v24);
  v115 = (char *)&v92 - v27;
  MEMORY[0x1E0C80A78](v26);
  v29 = (char *)&v92 - v28;
  v30 = swift_getTupleTypeMetadata2();
  v106 = *(_QWORD *)(v30 - 8);
  v31 = MEMORY[0x1E0C80A78](v30);
  v95 = (char *)&v92 - ((v32 + 15) & 0xFFFFFFFFFFFFFFF0);
  v33 = MEMORY[0x1E0C80A78](v31);
  v99 = (char *)&v92 - v34;
  v35 = MEMORY[0x1E0C80A78](v33);
  v37 = (char *)&v92 - v36;
  v38 = MEMORY[0x1E0C80A78](v35);
  v40 = (char *)&v92 - v39;
  v41 = (char *)&v92 + *(int *)(v38 + 48) - v39;
  v117 = *(void (**)(void))(*(_QWORD *)(a6 - 8) + 16);
  v42 = a1;
  v43 = a6;
  ((void (*)(char *, uint64_t, uint64_t))v117)((char *)&v92 - v39, v42, a6);
  v119 = a7;
  v116 = *(void (**)(char *, char *, uint64_t))(*(_QWORD *)(a7 - 8) + 16);
  v116(v41, v104, a7);
  v44 = *(int *)(v19 + 48);
  v45 = v29;
  v108 = v29;
  v46 = &v29[v44];
  v47 = v114;
  v48 = *(void (**)(void))(*(_QWORD *)(v114 - 8) + 16);
  ((void (*)(char *, uint64_t, uint64_t))v48)(v45, v107, v114);
  ((void (*)(char *, char *, uint64_t))v48)(v46, v113, v47);
  v98 = a13;
  v49 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a13 + 8) + 16))(v111);
  v50 = &v37[*(int *)(v30 + 48)];
  v104 = v40;
  ((void (*)(char *, char *, uint64_t))v117)(v37, v40, v43);
  v94 = v41;
  v51 = v41;
  v52 = v49;
  v116(v50, v51, v119);
  v53 = &v115[*(int *)(v118 + 48)];
  v48();
  v113 = v53;
  v93 = v46;
  ((void (*)(char *, char *, uint64_t))v48)(v53, v46, v47);
  v54 = (*(uint64_t (**)(uint64_t))(v109 + 16))(v43);
  v55 = &v99[*(int *)(v30 + 48)];
  v56 = v99;
  v100 = v37;
  v97 = v43;
  v117();
  v57 = v50;
  v58 = v119;
  v116(v55, v57, v119);
  v107 = v52;
  v59 = v54 == v52 && (*(uint64_t (**)(uint64_t))(v105 + 16))(v58) == v52;
  v60 = v30;
  v106 = *(_QWORD *)(v106 + 8);
  ((void (*)(char *, uint64_t))v106)(v56, v30);
  v61 = v118;
  v62 = v101;
  v63 = &v101[*(int *)(v118 + 48)];
  v64 = v114;
  v65 = v115;
  ((void (*)(char *, char *, uint64_t))v48)(v101, v115, v114);
  ((void (*)(char *, char *, uint64_t))v48)(v63, v113, v64);
  v66 = v110;
  v67 = v103;
  if (v59)
  {
    v68 = (*(uint64_t (**)(uint64_t, uint64_t))(v110 + 16))(v64, v110);
    v59 = v68 == v107;
  }
  v69 = v62;
  v70 = *(void (**)(void))(v102 + 8);
  ((void (*)(char *, uint64_t))v70)(v69, v61);
  v71 = &v67[*(int *)(v61 + 48)];
  ((void (*)(char *, char *, uint64_t))v48)(v67, v65, v64);
  ((void (*)(char *, char *, uint64_t))v48)(v71, v113, v64);
  if (v59)
  {
    v72 = (*(uint64_t (**)(uint64_t, uint64_t))(v66 + 16))(v64, v66);
    ((void (*)(char *, uint64_t))v70)(v67, v61);
    ((void (*)(char *, uint64_t))v70)(v65, v61);
    result = ((uint64_t (*)(char *, uint64_t))v106)(v100, v60);
    v74 = v107;
    if (v72 == v107)
    {
      v115 = a14;
      v75 = v95;
      v76 = &v95[*(int *)(v60 + 48)];
      v77 = v97;
      ((void (*)(char *, char *, uint64_t))v117)(v95, v104, v97);
      v117 = v70;
      v78 = v119;
      v116(v76, v94, v119);
      v79 = *(int *)(v61 + 48);
      v80 = v60;
      v81 = v96;
      v82 = &v96[v79];
      v83 = v74;
      v84 = v108;
      ((void (*)(char *, char *, uint64_t))v48)(v96, v108, v64);
      v85 = ((uint64_t (*)(char *, char *, uint64_t))v48)(v82, v93, v64);
      MEMORY[0x1E0C80A78](v85);
      *(&v92 - 12) = v77;
      *(&v92 - 11) = v78;
      v86 = v111;
      *(&v92 - 10) = v64;
      *(&v92 - 9) = v86;
      v87 = v105;
      *(&v92 - 8) = v109;
      *(&v92 - 7) = v87;
      v88 = v98;
      *(&v92 - 6) = v110;
      *(&v92 - 5) = v88;
      *(&v92 - 4) = (uint64_t)v75;
      *(&v92 - 3) = (uint64_t)v81;
      *(&v92 - 2) = v83;
      (*(void (**)(char *))(v88 + 16))(v115);
      v89 = v117;
      v90 = v118;
      ((void (*)(char *, uint64_t))v117)(v84, v118);
      v91 = (void (*)(char *, uint64_t))v106;
      ((void (*)(char *, uint64_t))v106)(v104, v80);
      v91(v75, v80);
      return ((uint64_t (*)(char *, uint64_t))v89)(v81, v90);
    }
  }
  else
  {
    ((void (*)(char *, uint64_t))v70)(v67, v61);
    ((void (*)(char *, uint64_t))v70)(v65, v61);
    result = ((uint64_t (*)(char *, uint64_t))v106)(v100, v60);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, __int128 a11, uint64_t a12, uint64_t a13)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  char *v24;
  uint64_t v25;
  char *v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  char *v30;
  uint64_t v31;
  void (*v32)(char *);
  _QWORD v34[2];
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  __int128 v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  char v47[16];
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  __int128 v53;
  uint64_t v54;
  char *v55;
  char *v56;
  uint64_t v57;
  uint64_t v58;

  v42 = a8;
  v43 = a4;
  v36 = a3;
  v41 = a1;
  v46 = a9;
  v45 = a13;
  v40 = a12;
  v39 = a11;
  v38 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v44 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v18 = TupleTypeMetadata2;
  v35 = TupleTypeMetadata2;
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v20 = (char *)v34 - v19;
  v21 = swift_getTupleTypeMetadata2();
  v37 = *(_QWORD *)(v21 - 8);
  v22 = MEMORY[0x1E0C80A78](v21);
  v24 = (char *)v34 - v23;
  v25 = *(int *)(v22 + 48);
  v26 = &v24[v25];
  v27 = a2;
  v34[1] = a2;
  v28 = a2 + v25;
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a5 - 8) + 16))(v24, v27, a5);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a6 - 8) + 16))(v26, v28, a6);
  v29 = *(int *)(v18 + 48);
  v30 = &v20[v29];
  v31 = v36 + v29;
  v32 = *(void (**)(char *))(*(_QWORD *)(a7 - 8) + 16);
  v32(v20);
  ((void (*)(char *, uint64_t, uint64_t))v32)(v30, v31, a7);
  v48 = a5;
  v49 = a6;
  v50 = a7;
  v51 = v42;
  v52 = v38;
  v53 = v39;
  v54 = v40;
  v55 = v24;
  v56 = v20;
  v57 = v41;
  v58 = v43;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t))(v38 + 24))(v45, v47, MEMORY[0x1E0DEE9C0] + 8, a5);
  (*(void (**)(char *, uint64_t))(v37 + 8))(v24, v21);
  return (*(uint64_t (**)(char *, uint64_t))(v44 + 8))(v20, v35);
}

uint64_t closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  char *v24;
  uint64_t v25;
  void (*v26)(char *, uint64_t, uint64_t);
  _QWORD v28[2];
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  char v41[16];
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  char *v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;

  v37 = a5;
  v38 = a6;
  v35 = a1;
  v36 = a2;
  v40 = a9;
  v39 = a16;
  v33 = a15;
  v32 = a14;
  v31 = a13;
  v30 = a12;
  v29 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v34 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v22 = (char *)v28 - v21;
  v28[1] = a3 + *(int *)(swift_getTupleTypeMetadata2() + 48);
  v23 = *(int *)(TupleTypeMetadata2 + 48);
  v24 = &v22[v23];
  v25 = a4 + v23;
  v26 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a10 - 8) + 16);
  v26(v22, a4, a10);
  v26(v24, v25, a10);
  v42 = a7;
  v43 = a8;
  v44 = a10;
  v45 = v29;
  v46 = v30;
  v47 = v31;
  v48 = v32;
  v49 = v33;
  v50 = v22;
  v51 = v35;
  v52 = v36;
  v53 = v37;
  v54 = v38;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t))(v31 + 24))(v39, v41, MEMORY[0x1E0DEE9C0] + 8, a8);
  return (*(uint64_t (**)(char *, uint64_t))(v34 + 8))(v22, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, __int128 a12, uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  char *v23;
  uint64_t v24;
  void (*v25)(char *, uint64_t, uint64_t);
  uint64_t v27;
  uint64_t v28;
  __int128 v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  char v41[16];
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  __int128 v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  char *v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;

  v32 = a8;
  v37 = a6;
  v38 = a7;
  v36 = a5;
  v34 = a4;
  v35 = a2;
  v33 = a1;
  v40 = a9;
  v39 = a16;
  v31 = a15;
  v30 = a13;
  v29 = a12;
  v28 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v18 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v19 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v21 = (char *)&v27 - v20;
  v22 = *(int *)(v19 + 48);
  v23 = &v21[v22];
  v24 = a3 + v22;
  v25 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a11 - 8) + 16);
  v25(v21, a3, a11);
  v25(v23, v24, a11);
  v42 = v32;
  v43 = v28;
  v44 = a11;
  v45 = v29;
  v46 = v30;
  v47 = a14;
  v48 = v31;
  v49 = v21;
  v50 = v34;
  v51 = v36;
  v52 = v33;
  v53 = v35;
  v54 = v37;
  v55 = v38;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(v47 + 24))(v39, v41, MEMORY[0x1E0DEE9C0] + 8, a11, v47);
  return (*(uint64_t (**)(char *, uint64_t))(v18 + 8))(v21, TupleTypeMetadata2);
}

uint64_t static vDSP.subtract<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, char *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  return static vDSP.subtract<A, B, C, D>(multiplication:multiplication:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:multiplication:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.subtract<A, B, C, D>(multiplication:multiplication:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:multiplication:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, void (*a2)(char *, char *, uint64_t), void (*a3)(char *, char *, uint64_t), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15)
{
  return static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, (char *)partial apply for closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, (char *)partial apply for closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t static vDSP.subtract<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, char *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t (*a14)(uint64_t, uint64_t, char *))
{
  uint64_t TupleTypeMetadata2;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  char *v25;
  uint64_t v26;
  uint64_t v27;
  char *v28;
  uint64_t v29;
  char *v30;
  void (*v31)(char *, uint64_t, uint64_t);
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  void (*v35)(char *, char *, uint64_t);
  char *v36;
  char *v37;
  void (*v38)(char *, uint64_t);
  void (*v39)(char *, uint64_t);
  char *v40;
  uint64_t v41;
  uint64_t v42;
  char *v43;
  uint64_t v44;
  uint64_t v45;
  char *v46;
  char *v47;
  char *v48;
  uint64_t v49;
  char *v50;
  uint64_t v51;
  void (*v52)(char *, uint64_t);
  uint64_t v53;
  void (*v54)(char *, uint64_t);
  char *v55;
  uint64_t v56;
  char *v58;
  char *v59;
  uint64_t v60;
  uint64_t v61;
  char *v62;
  uint64_t v63;
  uint64_t v64;
  char *v65;
  uint64_t v66;
  char *v67;
  uint64_t v68;
  uint64_t v69;
  uint64_t v70;
  uint64_t v71;
  uint64_t v72;
  uint64_t v73;
  uint64_t v74;
  uint64_t (*v75)(uint64_t, uint64_t, char *);
  uint64_t v76;
  uint64_t v77;
  char v78[16];
  uint64_t v79;
  uint64_t v80;
  uint64_t v81;
  uint64_t v82;
  uint64_t v83;
  uint64_t v84;
  uint64_t v85;
  uint64_t v86;
  char *v87;
  char *v88;

  v64 = a6;
  v76 = a5;
  v68 = a4;
  v66 = a3;
  v65 = a2;
  v74 = a13;
  v75 = a14;
  v71 = a9;
  v72 = a10;
  v73 = a12;
  v77 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v70 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v61 = TupleTypeMetadata2;
  v18 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v67 = (char *)&v58 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1E0C80A78](v18);
  v21 = (char *)&v58 - v20;
  v22 = swift_getTupleTypeMetadata2();
  v69 = *(_QWORD *)(v22 - 8);
  v23 = MEMORY[0x1E0C80A78](v22);
  v25 = (char *)&v58 - ((v24 + 15) & 0xFFFFFFFFFFFFFFF0);
  v26 = MEMORY[0x1E0C80A78](v23);
  v28 = (char *)&v58 - v27;
  v29 = *(int *)(v26 + 48);
  v63 = v26;
  v30 = (char *)&v58 + v29 - v27;
  v62 = v30;
  v31 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a7 - 8) + 16);
  v32 = a1;
  v33 = a7;
  v31((char *)&v58 - v27, v32, a7);
  v34 = a8;
  v60 = a8;
  v35 = *(void (**)(char *, char *, uint64_t))(*(_QWORD *)(a8 - 8) + 16);
  v35(v30, v65, v34);
  v36 = v21;
  v65 = &v21[*(int *)(TupleTypeMetadata2 + 48)];
  v37 = v65;
  v38 = *(void (**)(char *, uint64_t))(*(_QWORD *)(v76 - 8) + 16);
  v58 = v21;
  v38(v21, v66);
  v39 = *(void (**)(char *, uint64_t))(*(_QWORD *)(v64 - 8) + 16);
  v40 = v37;
  v41 = v64;
  v39(v40, v68);
  v68 = (*(uint64_t (**)(uint64_t))(v77 + 16))(v33);
  v42 = *(int *)(v22 + 48);
  v59 = v25;
  v43 = &v25[v42];
  v31(v25, (uint64_t)v28, v33);
  v44 = v60;
  v35(v43, v62, v60);
  v45 = v61;
  v46 = v67;
  v47 = &v67[*(int *)(v61 + 48)];
  v48 = v36;
  v49 = v76;
  ((void (*)(char *, char *, uint64_t))v38)(v67, v48, v76);
  ((void (*)(char *, char *, uint64_t))v39)(v47, v65, v41);
  v79 = v49;
  v80 = v41;
  v81 = v33;
  v82 = v44;
  v83 = v71;
  v84 = v72;
  v85 = v77;
  v86 = v73;
  v50 = v59;
  v87 = v59;
  v88 = v46;
  v51 = v75(v68, v74, v78);
  v52 = *(void (**)(char *, uint64_t))(v70 + 8);
  v53 = v45;
  v52(v58, v45);
  v54 = *(void (**)(char *, uint64_t))(v69 + 8);
  v55 = v28;
  v56 = v63;
  v54(v55, v63);
  v54(v50, v56);
  v52(v46, v53);
  return v51;
}

uint64_t closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t *a13, unint64_t *a14, void (*a15)(char *, char *, char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t TupleTypeMetadata2;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  char *v25;
  uint64_t v26;
  char *v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  char *v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t *v36;
  uint64_t v37;
  uint64_t v38;
  char *v39;
  uint64_t v40;
  uint64_t result;
  uint64_t v42;
  char *v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t *v49;
  uint64_t v50;
  uint64_t v51;
  unint64_t *v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;
  uint64_t v56;
  void (*v57)(char *, char *, char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t);
  uint64_t *v58;
  uint64_t v59;

  v58 = a2;
  v44 = a6;
  v47 = a4;
  v56 = a1;
  v57 = a15;
  v54 = a10;
  v55 = a9;
  v59 = a11;
  v52 = a14;
  v53 = a12;
  v49 = a13;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v21 = (char *)&v43 - v20;
  v22 = swift_getTupleTypeMetadata2();
  v23 = MEMORY[0x1E0C80A78](v22);
  v25 = (char *)&v43 - v24;
  v26 = *(int *)(v23 + 48);
  v27 = &v25[v26];
  v43 = &v25[v26];
  v28 = a3;
  v51 = a3;
  v29 = a3 + v26;
  v46 = a7;
  v50 = *(_QWORD *)(a7 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v50 + 16))(v25, v28, a7);
  v48 = *(_QWORD *)(a8 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v48 + 16))(v27, v29, a8);
  v30 = *(int *)(TupleTypeMetadata2 + 48);
  v31 = &v21[v30];
  v32 = v47 + v30;
  v33 = a5;
  v45 = *(_QWORD *)(a5 - 8);
  (*(void (**)(char *))(v45 + 16))(v21);
  v34 = v44;
  v35 = *(_QWORD *)(v44 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v35 + 16))(v31, v32, v44);
  v36 = v49;
  v37 = __swift_instantiateConcreteTypeFromMangledName(v49);
  v38 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(v52, v36);
  v42 = v37;
  v39 = v43;
  v40 = v46;
  v57(v25, v43, v21, v31, v56, v33, v34, v46, a8, v42, v55, v54, v59, v53, v38);
  (*(void (**)(char *, uint64_t))(v35 + 8))(v31, v34);
  (*(void (**)(char *, uint64_t))(v45 + 8))(v21, v33);
  (*(void (**)(char *, uint64_t))(v48 + 8))(v39, a8);
  (*(void (**)(char *, uint64_t))(v50 + 8))(v25, v40);
  result = (*(uint64_t (**)(uint64_t))(v59 + 16))(v40);
  *v58 = result;
  return result;
}

uint64_t static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, void (*a2)(char *, char *, uint64_t), void (*a3)(char *, char *, uint64_t), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15, char *a16)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  char *v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  char *v38;
  uint64_t v39;
  uint64_t v40;
  char *v41;
  char *v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  char *v46;
  char *v47;
  void (*v48)(char *, char *, uint64_t);
  uint64_t v49;
  void (*v50)(char *, uint64_t, uint64_t);
  char *v51;
  char *v52;
  void (*v53)(char *, char *, uint64_t);
  char *v54;
  uint64_t v55;
  char *v56;
  uint64_t v57;
  uint64_t v58;
  char *v59;
  uint64_t v60;
  uint64_t v61;
  _BOOL4 v62;
  uint64_t v63;
  uint64_t v64;
  char *v65;
  char *v66;
  char *v67;
  uint64_t v68;
  uint64_t v69;
  char *v70;
  uint64_t v71;
  char *v72;
  uint64_t v73;
  char *v74;
  char *v75;
  uint64_t v76;
  uint64_t result;
  char *v78;
  char *v79;
  uint64_t v80;
  uint64_t v81;
  char *v82;
  char *v83;
  char *v84;
  uint64_t v85;
  uint64_t v86;
  uint64_t v87;
  uint64_t v88;
  uint64_t v89;
  char *v90;
  void (*v91)(char *, void (*)(char *, uint64_t, uint64_t));
  void (*v92)(char *, uint64_t, uint64_t);
  uint64_t v93;
  char *v94;
  char *v95;
  char *v96;
  char *v97;
  uint64_t v98;
  char *v99;
  char *v100;
  char *v101;
  char *v102;
  uint64_t v103;
  char *v104;
  char *v105;
  uint64_t v106;
  uint64_t v107;
  uint64_t v108;
  void (*v109)(char *, uint64_t, uint64_t);
  char *v110;
  uint64_t v111;
  uint64_t v112;
  uint64_t v113;
  uint64_t v114;
  char *v115;
  char *v116;
  uint64_t v117;
  uint64_t v118;
  void (*v119)(char *, char *, uint64_t);
  void (*v120)(char *, char *, uint64_t);
  uint64_t *v121;
  uint64_t v122;
  uint64_t v123;
  uint64_t v124;

  v118 = a7;
  v119 = a2;
  v124 = a4;
  v120 = a3;
  v113 = a15;
  v114 = a5;
  v106 = a11;
  v107 = a14;
  v111 = a13;
  v112 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v20 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v117 = TupleTypeMetadata2;
  v103 = v20;
  v21 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v104 = (char *)&v93 - ((v22 + 15) & 0xFFFFFFFFFFFFFFF0);
  v23 = MEMORY[0x1E0C80A78](v21);
  v102 = (char *)&v93 - v24;
  v25 = MEMORY[0x1E0C80A78](v23);
  v97 = (char *)&v93 - v26;
  v27 = MEMORY[0x1E0C80A78](v25);
  v116 = (char *)&v93 - v28;
  MEMORY[0x1E0C80A78](v27);
  v30 = (char *)&v93 - v29;
  v31 = swift_getTupleTypeMetadata2();
  v108 = *(_QWORD *)(v31 - 8);
  v32 = MEMORY[0x1E0C80A78](v31);
  v96 = (char *)&v93 - ((v33 + 15) & 0xFFFFFFFFFFFFFFF0);
  v34 = MEMORY[0x1E0C80A78](v32);
  v100 = (char *)&v93 - v35;
  v36 = MEMORY[0x1E0C80A78](v34);
  v38 = (char *)&v93 - v37;
  v39 = MEMORY[0x1E0C80A78](v36);
  v41 = (char *)&v93 - v40;
  v42 = (char *)&v93 + *(int *)(v39 + 48) - v40;
  v109 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a8 - 8) + 16);
  v43 = a1;
  v44 = a8;
  v109((char *)&v93 - v40, v43, a8);
  v121 = *(uint64_t **)(*(_QWORD *)(a9 - 8) + 16);
  v122 = a9;
  ((void (*)(char *, _QWORD, uint64_t))v121)(v42, v119, a9);
  v45 = *(int *)(TupleTypeMetadata2 + 48);
  v46 = v30;
  v110 = v30;
  v47 = &v30[v45];
  v123 = a6;
  v48 = *(void (**)(char *, char *, uint64_t))(*(_QWORD *)(a6 - 8) + 16);
  v48(v46, (char *)v120, a6);
  v49 = v118;
  v50 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(v118 - 8) + 16);
  v50(v47, v124, v118);
  v124 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v113 + 8) + 16))(v112);
  v99 = &v38[*(int *)(v31 + 48)];
  v51 = v99;
  v105 = v41;
  v52 = v41;
  v53 = (void (*)(char *, char *, uint64_t))v109;
  v109(v38, (uint64_t)v52, v44);
  v95 = v42;
  ((void (*)(char *, char *, uint64_t))v121)(v51, v42, v122);
  v54 = &v116[*(int *)(v117 + 48)];
  v120 = v48;
  ((void (*)(void))v48)();
  v115 = v54;
  v94 = v47;
  v55 = v49;
  v56 = v100;
  v119 = (void (*)(char *, char *, uint64_t))v50;
  v50(v54, (uint64_t)v47, v55);
  v57 = (*(uint64_t (**)(uint64_t))(v111 + 16))(v44);
  v58 = v31;
  v59 = &v56[*(int *)(v31 + 48)];
  v101 = v38;
  v98 = v44;
  v53(v56, v38, v44);
  v60 = v122;
  ((void (*)(char *, char *, uint64_t))v121)(v59, v99, v122);
  v61 = v124;
  v62 = v57 == v124 && (*(uint64_t (**)(uint64_t, uint64_t))(v107 + 16))(v60, v107) == v61;
  v63 = v58;
  v108 = *(_QWORD *)(v108 + 8);
  ((void (*)(char *, uint64_t))v108)(v56, v58);
  v65 = v116;
  v64 = v117;
  v66 = v102;
  v67 = &v102[*(int *)(v117 + 48)];
  v68 = v123;
  v120(v102, v116, v123);
  v69 = v118;
  v119(v67, v115, v118);
  v70 = v104;
  if (v62)
  {
    v71 = (*(uint64_t (**)(uint64_t))(v106 + 16))(v68);
    v62 = v71 == v124;
  }
  v72 = v66;
  v73 = v68;
  v74 = *(char **)(v103 + 8);
  ((void (*)(char *, uint64_t))v74)(v72, v64);
  v75 = &v70[*(int *)(v64 + 48)];
  v120(v70, v65, v73);
  v119(v75, v115, v69);
  if (v62)
  {
    v76 = (*(uint64_t (**)(uint64_t, uint64_t))(a12 + 16))(v69, a12);
    ((void (*)(char *, uint64_t))v74)(v70, v64);
    ((void (*)(char *, uint64_t))v74)(v65, v64);
    result = ((uint64_t (*)(char *, uint64_t))v108)(v101, v63);
    if (v76 == v124)
    {
      v115 = v74;
      v116 = a16;
      v78 = v96;
      v79 = &v96[*(int *)(v63 + 48)];
      v80 = v98;
      v109(v96, (uint64_t)v105, v98);
      v109 = (void (*)(char *, uint64_t, uint64_t))v63;
      v81 = v122;
      ((void (*)(char *, char *, uint64_t))v121)(v79, v95, v122);
      v82 = v97;
      v83 = &v97[*(int *)(v64 + 48)];
      v84 = v110;
      v85 = v123;
      v120(v97, v110, v123);
      v86 = ((uint64_t (*)(char *, char *, uint64_t))v119)(v83, v94, v69);
      v121 = &v93;
      MEMORY[0x1E0C80A78](v86);
      *(&v93 - 14) = v85;
      *(&v93 - 13) = v69;
      *(&v93 - 12) = v80;
      *(&v93 - 11) = v81;
      v87 = v113;
      v88 = v106;
      *(&v93 - 10) = v112;
      *(&v93 - 9) = v88;
      v89 = v111;
      *(&v93 - 8) = a12;
      *(&v93 - 7) = v89;
      *(&v93 - 6) = v107;
      *(&v93 - 5) = v87;
      *(&v93 - 4) = (uint64_t)v78;
      *(&v93 - 3) = (uint64_t)v82;
      *(&v93 - 2) = v124;
      (*(void (**)(char *))(v87 + 16))(v116);
      v90 = v115;
      ((void (*)(char *, uint64_t))v115)(v84, v64);
      v91 = (void (*)(char *, void (*)(char *, uint64_t, uint64_t)))v108;
      v92 = v109;
      ((void (*)(char *, void (*)(char *, uint64_t, uint64_t)))v108)(v105, v109);
      v91(v78, v92);
      return ((uint64_t (*)(char *, uint64_t))v90)(v82, v64);
    }
  }
  else
  {
    ((void (*)(char *, uint64_t))v74)(v70, v64);
    ((void (*)(char *, uint64_t))v74)(v65, v64);
    result = ((uint64_t (*)(char *, uint64_t))v108)(v101, v63);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, __int128 a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  char *v26;
  uint64_t v27;
  char *v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  char *v32;
  uint64_t v33;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  __int128 v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  char v49[16];
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  __int128 v54;
  uint64_t v55;
  uint64_t v56;
  uint64_t v57;
  uint64_t v58;
  char *v59;
  char *v60;
  uint64_t v61;
  uint64_t v62;

  v45 = a4;
  v37 = a3;
  v44 = a1;
  v48 = a9;
  v47 = a15;
  v43 = a14;
  v42 = a13;
  v41 = a12;
  v40 = a11;
  v39 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v46 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v22 = (char *)&v35 - v21;
  v23 = swift_getTupleTypeMetadata2();
  v38 = *(_QWORD *)(v23 - 8);
  v36 = v23;
  v24 = MEMORY[0x1E0C80A78](v23);
  v26 = (char *)&v35 - v25;
  v27 = *(int *)(v24 + 48);
  v28 = &v26[v27];
  v29 = a2;
  v35 = a2;
  v30 = a2 + v27;
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a7 - 8) + 16))(v26, v29, a7);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a8 - 8) + 16))(v28, v30, a8);
  v31 = *(int *)(TupleTypeMetadata2 + 48);
  v32 = &v22[v31];
  v33 = v37 + v31;
  (*(void (**)(char *))(*(_QWORD *)(a5 - 8) + 16))(v22);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a6 - 8) + 16))(v32, v33, a6);
  v50 = a5;
  v51 = a6;
  v52 = a7;
  v53 = a8;
  v54 = v39;
  v55 = v40;
  v56 = v41;
  v57 = v42;
  v58 = v43;
  v59 = v26;
  v60 = v22;
  v61 = v44;
  v62 = v45;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t))(v41 + 24))(v47, v49, MEMORY[0x1E0DEE9C0] + 8, a7);
  (*(void (**)(char *, uint64_t))(v38 + 8))(v26, v36);
  return (*(uint64_t (**)(char *, uint64_t))(v46 + 8))(v22, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, __int128 a12, uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16, uint64_t a17)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v22;
  char *v23;
  uint64_t v24;
  char *v25;
  uint64_t v26;
  _QWORD v28[2];
  __int128 v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  char v41[16];
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  __int128 v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  char *v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;

  v37 = a5;
  v38 = a6;
  v36 = a2;
  v35 = a1;
  v40 = a9;
  v39 = a17;
  v33 = a16;
  v32 = a15;
  v31 = a14;
  v30 = a13;
  v29 = a12;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v34 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v23 = (char *)v28 - v22;
  v28[1] = a3 + *(int *)(swift_getTupleTypeMetadata2() + 48);
  v24 = *(int *)(TupleTypeMetadata2 + 48);
  v25 = &v23[v24];
  v26 = a4 + v24;
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a7 - 8) + 16))(v23, a4, a7);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a8 - 8) + 16))(v25, v26, a8);
  v42 = a7;
  v43 = a8;
  v44 = a10;
  v45 = a11;
  v46 = v29;
  v47 = v30;
  v48 = v31;
  v49 = v32;
  v50 = v33;
  v51 = v23;
  v52 = v35;
  v53 = v36;
  v54 = v37;
  v55 = v38;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t))(v32 + 24))(v39, v41, MEMORY[0x1E0DEE9C0] + 8, a11);
  return (*(uint64_t (**)(char *, uint64_t))(v34 + 8))(v23, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, __int128 a11, uint64_t a12, uint64_t a13, __int128 a14, uint64_t a15, uint64_t a16, uint64_t a17)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  char *v23;
  uint64_t v24;
  char *v25;
  uint64_t v26;
  __int128 v28;
  uint64_t v29;
  __int128 v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  char v41[16];
  uint64_t v42;
  uint64_t v43;
  __int128 v44;
  uint64_t v45;
  uint64_t v46;
  __int128 v47;
  uint64_t v48;
  uint64_t v49;
  char *v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;
  uint64_t v56;

  v38 = a7;
  v37 = a6;
  v36 = a5;
  v34 = a4;
  v35 = a2;
  v33 = a1;
  v40 = a9;
  v39 = a17;
  v32 = a16;
  v31 = a15;
  v30 = a14;
  v29 = a12;
  v28 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v20 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v21 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v23 = (char *)&v28 - v22;
  v24 = *(int *)(v21 + 48);
  v25 = &v23[v24];
  v26 = a3 + v24;
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a8 - 8) + 16))(v23, a3, a8);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a10 - 8) + 16))(v25, v26, a10);
  v42 = a8;
  v43 = a10;
  v44 = v28;
  v45 = v29;
  v46 = a13;
  v47 = v30;
  v48 = v31;
  v49 = v32;
  v50 = v23;
  v51 = v34;
  v52 = v36;
  v53 = v33;
  v54 = v35;
  v55 = v37;
  v56 = v38;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(v46 + 24))(v39, v41, MEMORY[0x1E0DEE9C0] + 8, a8, v46);
  return (*(uint64_t (**)(char *, uint64_t))(v20 + 8))(v23, TupleTypeMetadata2);
}

uint64_t static vDSP.multiply<A, B, C, D>(subtraction:subtraction:)(uint64_t a1, char *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, unint64_t a10, unint64_t a11, uint64_t a12)
{
  return static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, __PAIR128__(a11, a10), a12, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:subtraction:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, __PAIR128__(a11, a10), a12, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:subtraction:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:)(uint64_t a1, void (*a2)(char *, char *, uint64_t), void (*a3)(char *, char *, uint64_t), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15)
{
  return static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, (char *)partial apply for closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:));
}

{
  return static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, (char *)partial apply for closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:));
}

uint64_t static vDSP.multiply<A, B, C, D>(addition:subtraction:)(uint64_t a1, char *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, unint64_t a10, unint64_t a11, uint64_t a12)
{
  return static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, __PAIR128__(a11, a10), a12, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:subtraction:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, __PAIR128__(a11, a10), a12, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:subtraction:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, void (*a2)(char *, char *, uint64_t), void (*a3)(char *, char *, uint64_t), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15)
{
  return static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, (char *)partial apply for closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:));
}

{
  return static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, (char *)partial apply for closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:));
}

uint64_t static vDSP.add<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, char *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, __int128 a10, uint64_t a11, uint64_t a12, uint64_t (*a13)(uint64_t, uint64_t, char *))
{
  uint64_t TupleTypeMetadata2;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  char *v24;
  uint64_t v25;
  uint64_t v26;
  char *v27;
  uint64_t v28;
  char *v29;
  void (*v30)(char *, uint64_t, uint64_t);
  void (*v31)(char *, char *, uint64_t);
  char *v32;
  char *v33;
  void (*v34)(char *, uint64_t);
  void (*v35)(char *, uint64_t);
  char *v36;
  uint64_t v37;
  uint64_t v38;
  char *v39;
  uint64_t v40;
  uint64_t v41;
  char *v42;
  char *v43;
  char *v44;
  uint64_t v45;
  char *v46;
  uint64_t v47;
  void (*v48)(char *, uint64_t);
  uint64_t v49;
  void (*v50)(char *, uint64_t);
  char *v51;
  uint64_t v52;
  char *v54;
  char *v55;
  uint64_t v56;
  uint64_t v57;
  char *v58;
  uint64_t v59;
  uint64_t v60;
  uint64_t v61;
  char *v62;
  uint64_t v63;
  char *v64;
  uint64_t v65;
  uint64_t v66;
  uint64_t v67;
  __int128 v68;
  uint64_t v69;
  uint64_t v70;
  uint64_t (*v71)(uint64_t, uint64_t, char *);
  uint64_t v72;
  uint64_t v73;
  char v74[16];
  uint64_t v75;
  uint64_t v76;
  uint64_t v77;
  uint64_t v78;
  uint64_t v79;
  __int128 v80;
  uint64_t v81;
  char *v82;
  char *v83;

  v61 = a8;
  v72 = a7;
  v65 = a4;
  v63 = a3;
  v62 = a2;
  v60 = a1;
  v70 = a12;
  v71 = a13;
  v69 = a11;
  v68 = a10;
  v73 = a9;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v67 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v16 = TupleTypeMetadata2;
  v57 = TupleTypeMetadata2;
  v17 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v64 = (char *)&v54 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1E0C80A78](v17);
  v20 = (char *)&v54 - v19;
  v21 = swift_getTupleTypeMetadata2();
  v66 = *(_QWORD *)(v21 - 8);
  v22 = MEMORY[0x1E0C80A78](v21);
  v24 = (char *)&v54 - ((v23 + 15) & 0xFFFFFFFFFFFFFFF0);
  v25 = MEMORY[0x1E0C80A78](v22);
  v27 = (char *)&v54 - v26;
  v28 = *(int *)(v25 + 48);
  v59 = v25;
  v29 = (char *)&v54 + v28 - v26;
  v58 = v29;
  v30 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a5 - 8) + 16);
  v30((char *)&v54 - v26, v60, a5);
  v56 = a6;
  v31 = *(void (**)(char *, char *, uint64_t))(*(_QWORD *)(a6 - 8) + 16);
  v31(v29, v62, a6);
  v32 = v20;
  v33 = &v20[*(int *)(v16 + 48)];
  v62 = v33;
  v34 = *(void (**)(char *, uint64_t))(*(_QWORD *)(v72 - 8) + 16);
  v54 = v20;
  v34(v20, v63);
  v35 = *(void (**)(char *, uint64_t))(*(_QWORD *)(v61 - 8) + 16);
  v36 = v33;
  v37 = v61;
  v35(v36, v65);
  v65 = (*(uint64_t (**)(uint64_t))(v73 + 16))(a5);
  v38 = *(int *)(v21 + 48);
  v55 = v24;
  v39 = &v24[v38];
  v30(v24, (uint64_t)v27, a5);
  v40 = v56;
  v31(v39, v58, v56);
  v41 = v57;
  v42 = v64;
  v43 = &v64[*(int *)(v57 + 48)];
  v44 = v32;
  v45 = v72;
  ((void (*)(char *, char *, uint64_t))v34)(v64, v44, v72);
  ((void (*)(char *, char *, uint64_t))v35)(v43, v62, v37);
  v75 = a5;
  v76 = v40;
  v77 = v45;
  v78 = v37;
  v79 = v73;
  v80 = v68;
  v46 = v55;
  v81 = v69;
  v82 = v55;
  v83 = v42;
  v47 = v71(v65, v70, v74);
  v48 = *(void (**)(char *, uint64_t))(v67 + 8);
  v49 = v41;
  v48(v54, v41);
  v50 = *(void (**)(char *, uint64_t))(v66 + 8);
  v51 = v27;
  v52 = v59;
  v50(v51, v59);
  v50(v46, v52);
  v48(v42, v49);
  return v47;
}

uint64_t closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, __int128 a10, uint64_t a11, uint64_t *a12, unint64_t *a13, void (*a14)(char *, char *, char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t))
{
  uint64_t TupleTypeMetadata2;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  char *v25;
  uint64_t v26;
  char *v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  char *v31;
  uint64_t v32;
  char *v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t *v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  char *v40;
  char *v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t result;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t *v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;
  unint64_t *v56;
  uint64_t v57;
  __int128 v58;
  uint64_t v59;
  void (*v60)(char *, char *, char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t);
  uint64_t *v61;

  v50 = a4;
  v59 = a1;
  v58 = a10;
  v60 = a14;
  v61 = a2;
  v55 = a9;
  v56 = a13;
  v57 = a11;
  v52 = a12;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v21 = (char *)&v46 - v20;
  v22 = swift_getTupleTypeMetadata2();
  v23 = MEMORY[0x1E0C80A78](v22);
  v25 = (char *)&v46 - v24;
  v26 = *(int *)(v23 + 48);
  v27 = &v25[v26];
  v28 = a3;
  v54 = a3;
  v29 = a3 + v26;
  v49 = a5;
  v53 = *(_QWORD *)(a5 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v53 + 16))(v25, v28, a5);
  v30 = a6;
  v47 = a6;
  v51 = *(_QWORD *)(a6 - 8);
  v31 = v27;
  (*(void (**)(char *, uint64_t, uint64_t))(v51 + 16))(v27, v29, v30);
  v32 = *(int *)(TupleTypeMetadata2 + 48);
  v33 = &v21[v32];
  v34 = v50 + v32;
  v48 = *(_QWORD *)(a7 - 8);
  (*(void (**)(char *))(v48 + 16))(v21);
  v35 = a8;
  v50 = *(_QWORD *)(a8 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v50 + 16))(v33, v34, a8);
  v36 = v52;
  v37 = __swift_instantiateConcreteTypeFromMangledName(v52);
  v38 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(v56, v36);
  v39 = v55;
  v45 = v37;
  v40 = v31;
  v41 = v31;
  v42 = v49;
  v43 = v47;
  v60(v25, v41, v21, v33, v59, v49, v47, a7, v35, v45, v55, v58, *((_QWORD *)&v58 + 1), v57, v38);
  (*(void (**)(char *, uint64_t))(v50 + 8))(v33, v35);
  (*(void (**)(char *, uint64_t))(v48 + 8))(v21, a7);
  (*(void (**)(char *, uint64_t))(v51 + 8))(v40, v43);
  (*(void (**)(char *, uint64_t))(v53 + 8))(v25, v42);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(v39 + 16))(v42, v39);
  *v61 = result;
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, _QWORD *a9, uint64_t a10, uint64_t (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, _QWORD, uint64_t, uint64_t))
{
  if (!a3)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (!a5)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (!a7)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  if (!result)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  if (*a9)
  {
    if ((a10 & 0x8000000000000000) == 0)
      return a11(a3, 1, a5, 1, a7, 1, result, 1, *a9, 1, a10);
    __break(1u);
    goto LABEL_9;
  }
LABEL_13:
  __break(1u);
  return result;
}

uint64_t static vDSP.add<A>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, float a4, float a5)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  _BYTE *v14;
  uint64_t v15;
  uint64_t v16;
  _BYTE *v17;
  uint64_t v18;
  void (*v19)(_BYTE *, uint64_t, uint64_t);
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  void (*v23)(_BYTE *, uint64_t);
  _BYTE v25[16];
  uint64_t v26;
  uint64_t v27;
  _BYTE *v28;
  float v29;

  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v11 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v12 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v14 = &v25[-((v13 + 15) & 0xFFFFFFFFFFFFFFF0)];
  v15 = MEMORY[0x1E0C80A78](v12);
  v17 = &v25[-v16];
  v18 = *(int *)(v15 + 48);
  v19 = *(void (**)(_BYTE *, uint64_t, uint64_t))(*(_QWORD *)(a2 - 8) + 16);
  v19(&v25[-v16], a1, a2);
  *(float *)&v17[v18] = a4;
  v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  v21 = *(int *)(TupleTypeMetadata2 + 48);
  v19(v14, (uint64_t)v17, a2);
  *(float *)&v14[v21] = a4;
  v26 = a2;
  v27 = a3;
  v28 = v14;
  v29 = a5;
  v22 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v20, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.add<A>(multiplication:_:));
  v23 = *(void (**)(_BYTE *, uint64_t))(v11 + 8);
  v23(v17, TupleTypeMetadata2);
  v23(v14, TupleTypeMetadata2);
  return v22;
}

uint64_t closure #1 in static vDSP.add<A>(multiplication:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, float a6)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v13;
  uint64_t v14;
  char *v15;
  uint64_t v16;
  uint64_t v17;
  float v18;
  char *v19;
  uint64_t v20;
  uint64_t result;
  uint64_t v22;

  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v13 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v15 = (char *)&v22 - v14;
  v16 = *(int *)(v13 + 48);
  v17 = *(_QWORD *)(a4 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v17 + 16))((char *)&v22 - v14, a3, a4);
  v18 = *(float *)(a3 + v16);
  *(float *)&v15[v16] = v18;
  v19 = (char *)__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v20 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.add<A, B>(multiplication:_:result:)(v15, a1, a4, v19, a5, v20, v18, a6);
  (*(void (**)(char *, uint64_t))(v17 + 8))(v15, a4);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.add<A, B>(multiplication:_:result:)(char *a1, uint64_t a2, uint64_t a3, char *a4, uint64_t a5, uint64_t a6, float a7, float a8)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  char *v23;
  void (*v24)(char *, char *, uint64_t);
  uint64_t (*v25)(char *);
  uint64_t v26;
  uint64_t v27;
  uint64_t (*v28)(uint64_t, uint64_t);
  uint64_t v29;
  uint64_t (*v30)(char *, uint64_t);
  uint64_t result;
  uint64_t v32;
  uint64_t v33;
  char *v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  char *v39;
  uint64_t v40;

  v39 = a1;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v40 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v16 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v18 = (char *)&v36 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = MEMORY[0x1E0C80A78](v16);
  v21 = (char *)&v36 - v20;
  MEMORY[0x1E0C80A78](v19);
  v23 = (char *)&v36 - v22;
  v24 = *(void (**)(char *, char *, uint64_t))(*(_QWORD *)(a3 - 8) + 16);
  v24((char *)&v36 - v22, v39, a3);
  *(float *)&v23[*(int *)(TupleTypeMetadata2 + 48)] = a7;
  v36 = a6;
  v25 = *(uint64_t (**)(char *))(*(_QWORD *)(a6 + 8) + 16);
  v38 = a2;
  v39 = a4;
  v26 = v25(a4);
  v27 = *(int *)(TupleTypeMetadata2 + 48);
  v24(v21, v23, a3);
  *(float *)&v21[v27] = a7;
  v28 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  v37 = a5;
  v29 = v28(a3, a5);
  v30 = *(uint64_t (**)(char *, uint64_t))(v40 + 8);
  result = v30(v21, TupleTypeMetadata2);
  if (v29 == v26)
  {
    v32 = *(int *)(TupleTypeMetadata2 + 48);
    v33 = ((uint64_t (*)(char *, char *, uint64_t))v24)(v18, v23, a3);
    *(float *)&v18[v32] = a7;
    MEMORY[0x1E0C80A78](v33);
    v34 = v39;
    *(&v36 - 8) = a3;
    *(&v36 - 7) = (uint64_t)v34;
    v35 = v36;
    *(&v36 - 6) = v37;
    *(&v36 - 5) = v35;
    *(&v36 - 4) = (uint64_t)v18;
    *((float *)&v36 - 6) = a8;
    *(&v36 - 2) = v26;
    (*(void (**)(uint64_t (*)@<X0>(uint64_t@<X0>, uint64_t@<X8>)))(v35 + 16))(partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:result:));
    v30(v23, TupleTypeMetadata2);
    return v30(v18, TupleTypeMetadata2);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in static vDSP.add<A, B>(multiplication:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X8>, float a9@<S0>)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  char *v19;
  uint64_t v20;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  char v25[16];
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  char *v30;
  float v31;
  uint64_t v32;
  uint64_t v33;

  v24 = a8;
  v22 = a7;
  v23 = a3;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v16 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v17 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v19 = (char *)&v22 - v18;
  v20 = *(int *)(v17 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a4 - 8) + 16))((char *)&v22 - v18, a2, a4);
  *(_DWORD *)&v19[v20] = *(_DWORD *)(a2 + v20);
  v26 = a4;
  v27 = a5;
  v28 = a6;
  v29 = v22;
  v30 = v19;
  v31 = a9;
  v32 = a1;
  v33 = v23;
  (*(void (**)(void (*)(const float *, uint64_t), char *, uint64_t, uint64_t, uint64_t))(a6 + 24))(partial apply for closure #1 in closure #1 in static vDSP.add<A, B>(multiplication:_:result:), v25, MEMORY[0x1E0DEE9C0] + 8, a4, a6);
  return (*(uint64_t (**)(char *, uint64_t))(v16 + 8))(v19, TupleTypeMetadata2);
}

void closure #1 in closure #1 in static vDSP.add<A, B>(multiplication:_:result:)(const float *a1, float a2, uint64_t a3, uint64_t a4, float **a5, vDSP_Length a6)
{
  float v10;
  float v11;
  float __B;
  uint64_t v13;

  v13 = *MEMORY[0x1E0C80C00];
  v10 = *(float *)(a4 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  v11 = a2;
  __B = v10;
  if (!a1)
    goto LABEL_6;
  if (!*a5)
    goto LABEL_7;
  if ((a6 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  vDSP_vsmsa(a1, 1, &__B, &v11, *a5, 1, a6);
}

uint64_t static vDSP.add<A>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, double a4, double a5)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  _BYTE *v14;
  uint64_t v15;
  uint64_t v16;
  _BYTE *v17;
  uint64_t v18;
  void (*v19)(_BYTE *, uint64_t, uint64_t);
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  void (*v23)(_BYTE *, uint64_t);
  _BYTE v25[16];
  uint64_t v26;
  uint64_t v27;
  _BYTE *v28;
  double v29;

  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v11 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v12 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v14 = &v25[-((v13 + 15) & 0xFFFFFFFFFFFFFFF0)];
  v15 = MEMORY[0x1E0C80A78](v12);
  v17 = &v25[-v16];
  v18 = *(int *)(v15 + 48);
  v19 = *(void (**)(_BYTE *, uint64_t, uint64_t))(*(_QWORD *)(a2 - 8) + 16);
  v19(&v25[-v16], a1, a2);
  *(double *)&v17[v18] = a4;
  v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  v21 = *(int *)(TupleTypeMetadata2 + 48);
  v19(v14, (uint64_t)v17, a2);
  *(double *)&v14[v21] = a4;
  v26 = a2;
  v27 = a3;
  v28 = v14;
  v29 = a5;
  v22 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v20, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.add<A>(multiplication:_:));
  v23 = *(void (**)(_BYTE *, uint64_t))(v11 + 8);
  v23(v17, TupleTypeMetadata2);
  v23(v14, TupleTypeMetadata2);
  return v22;
}

uint64_t closure #1 in static vDSP.add<A>(multiplication:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, double a6)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v13;
  uint64_t v14;
  char *v15;
  uint64_t v16;
  uint64_t v17;
  double v18;
  char *v19;
  uint64_t v20;
  uint64_t result;
  uint64_t v22;

  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v13 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v15 = (char *)&v22 - v14;
  v16 = *(int *)(v13 + 48);
  v17 = *(_QWORD *)(a4 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v17 + 16))((char *)&v22 - v14, a3, a4);
  v18 = *(double *)(a3 + v16);
  *(double *)&v15[v16] = v18;
  v19 = (char *)__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v20 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.add<A, B>(multiplication:_:result:)(v15, a1, a4, v19, a5, v20, v18, a6);
  (*(void (**)(char *, uint64_t))(v17 + 8))(v15, a4);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.add<A, B>(multiplication:_:result:)(char *a1, uint64_t a2, uint64_t a3, char *a4, uint64_t a5, uint64_t a6, double a7, double a8)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  char *v23;
  void (*v24)(char *, char *, uint64_t);
  uint64_t (*v25)(char *);
  uint64_t v26;
  uint64_t v27;
  uint64_t (*v28)(uint64_t, uint64_t);
  uint64_t v29;
  uint64_t (*v30)(char *, uint64_t);
  uint64_t result;
  uint64_t v32;
  uint64_t v33;
  char *v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  char *v39;
  uint64_t v40;

  v39 = a1;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v40 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v16 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v18 = (char *)&v36 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = MEMORY[0x1E0C80A78](v16);
  v21 = (char *)&v36 - v20;
  MEMORY[0x1E0C80A78](v19);
  v23 = (char *)&v36 - v22;
  v24 = *(void (**)(char *, char *, uint64_t))(*(_QWORD *)(a3 - 8) + 16);
  v24((char *)&v36 - v22, v39, a3);
  *(double *)&v23[*(int *)(TupleTypeMetadata2 + 48)] = a7;
  v36 = a6;
  v25 = *(uint64_t (**)(char *))(*(_QWORD *)(a6 + 8) + 16);
  v38 = a2;
  v39 = a4;
  v26 = v25(a4);
  v27 = *(int *)(TupleTypeMetadata2 + 48);
  v24(v21, v23, a3);
  *(double *)&v21[v27] = a7;
  v28 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  v37 = a5;
  v29 = v28(a3, a5);
  v30 = *(uint64_t (**)(char *, uint64_t))(v40 + 8);
  result = v30(v21, TupleTypeMetadata2);
  if (v29 == v26)
  {
    v32 = *(int *)(TupleTypeMetadata2 + 48);
    v33 = ((uint64_t (*)(char *, char *, uint64_t))v24)(v18, v23, a3);
    *(double *)&v18[v32] = a7;
    MEMORY[0x1E0C80A78](v33);
    v34 = v39;
    *(&v36 - 8) = a3;
    *(&v36 - 7) = (uint64_t)v34;
    v35 = v36;
    *(&v36 - 6) = v37;
    *(&v36 - 5) = v35;
    *(&v36 - 4) = (uint64_t)v18;
    *((double *)&v36 - 3) = a8;
    *(&v36 - 2) = v26;
    (*(void (**)(uint64_t (*)@<X0>(uint64_t@<X0>, uint64_t@<X8>)))(v35 + 16))(partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:result:));
    v30(v23, TupleTypeMetadata2);
    return v30(v18, TupleTypeMetadata2);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in static vDSP.add<A, B>(multiplication:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X8>, double a9@<D0>)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  char *v19;
  uint64_t v20;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  char v25[16];
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  char *v30;
  double v31;
  uint64_t v32;
  uint64_t v33;

  v24 = a8;
  v22 = a7;
  v23 = a3;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v16 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v17 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v19 = (char *)&v22 - v18;
  v20 = *(int *)(v17 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a4 - 8) + 16))((char *)&v22 - v18, a2, a4);
  *(_QWORD *)&v19[v20] = *(_QWORD *)(a2 + v20);
  v26 = a4;
  v27 = a5;
  v28 = a6;
  v29 = v22;
  v30 = v19;
  v31 = a9;
  v32 = a1;
  v33 = v23;
  (*(void (**)(void (*)(const double *, uint64_t), char *, uint64_t, uint64_t, uint64_t))(a6 + 24))(partial apply for closure #1 in closure #1 in static vDSP.add<A, B>(multiplication:_:result:), v25, MEMORY[0x1E0DEE9C0] + 8, a4, a6);
  return (*(uint64_t (**)(char *, uint64_t))(v16 + 8))(v19, TupleTypeMetadata2);
}

void closure #1 in closure #1 in static vDSP.add<A, B>(multiplication:_:result:)(const double *a1, double a2, uint64_t a3, uint64_t a4, double **a5, vDSP_Length a6)
{
  double v10;
  double __C;
  double __B[2];

  __B[1] = *(double *)MEMORY[0x1E0C80C00];
  v10 = *(double *)(a4 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  __C = a2;
  __B[0] = v10;
  if (!a1)
    goto LABEL_6;
  if (!*a5)
    goto LABEL_7;
  if ((a6 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  vDSP_vsmsaD(a1, 1, __B, &__C, *a5, 1, a6);
}

uint64_t static vDSP.subtract<A, B>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  char *v18;
  void (*v19)(char *, uint64_t, uint64_t);
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  void (*v23)(char *, uint64_t);
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  char *v31;
  uint64_t v32;

  v26 = a6;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v25 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v14 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v16 = (char *)&v25 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1E0C80A78](v14);
  v18 = (char *)&v25 - v17;
  v19 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a4 - 8) + 16);
  v19((char *)&v25 - v17, a1, a4);
  *(float *)&v18[*(int *)(TupleTypeMetadata2 + 48)] = a7;
  v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  v21 = *(int *)(TupleTypeMetadata2 + 48);
  v19(v16, (uint64_t)v18, a4);
  *(float *)&v16[v21] = a7;
  v27 = a3;
  v28 = a4;
  v29 = a5;
  v30 = v26;
  v31 = v16;
  v32 = a2;
  v22 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v20, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.subtract<A, B>(multiplication:_:));
  v23 = *(void (**)(char *, uint64_t))(v25 + 8);
  v23(v18, TupleTypeMetadata2);
  v23(v16, TupleTypeMetadata2);
  return v22;
}

uint64_t closure #1 in static vDSP.subtract<A, B>(multiplication:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t (*a4)(char *, char *, _QWORD *), uint64_t (*a5)(_QWORD, _QWORD, _QWORD), _QWORD *a6, uint64_t a7, uint64_t a8)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  float v20;
  char *v21;
  uint64_t v22;
  uint64_t result;
  uint64_t v24;
  uint64_t v25;
  uint64_t *v26;

  v26 = a2;
  v25 = a8;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v15 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v17 = (char *)&v24 - v16;
  v18 = *(int *)(v15 + 48);
  v19 = *(a6 - 1);
  (*(void (**)(char *, uint64_t, _QWORD *))(v19 + 16))((char *)&v24 - v16, a3, a6);
  v20 = *(float *)(a3 + v18);
  *(float *)&v17[v18] = v20;
  v21 = (char *)__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v22 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.subtract<A, B, C>(multiplication:_:result:)(v17, a4, a1, a5, a6, v21, a7, v25, v20, v22);
  (*(void (**)(char *, _QWORD *))(v19 + 8))(v17, a6);
  result = (*(uint64_t (**)(uint64_t (*)(_QWORD, _QWORD, _QWORD), uint64_t))(a7 + 16))(a5, a7);
  *v26 = result;
  return result;
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  double v20;
  char *v21;
  uint64_t v22;
  uint64_t result;
  uint64_t v24;
  uint64_t v25;
  uint64_t *v26;

  v26 = a2;
  v25 = a8;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v15 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v17 = (char *)&v24 - v16;
  v18 = *(int *)(v15 + 48);
  v19 = *(a6 - 1);
  (*(void (**)(char *, uint64_t, _QWORD *))(v19 + 16))((char *)&v24 - v16, a3, a6);
  v20 = *(double *)(a3 + v18);
  *(double *)&v17[v18] = v20;
  v21 = (char *)__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v22 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.subtract<A, B, C>(multiplication:_:result:)(v17, a4, a1, a5, a6, v21, a7, v25, v20, v22);
  (*(void (**)(char *, _QWORD *))(v19 + 8))(v17, a6);
  result = (*(uint64_t (**)(uint64_t (*)(_QWORD, _QWORD, _QWORD), uint64_t))(a7 + 16))(a5, a7);
  *v26 = result;
  return result;
}

uint64_t static vDSP.subtract<A, B, C>(multiplication:_:result:)(char *a1, uint64_t (*a2)(char *, char *, _QWORD *), uint64_t a3, uint64_t (*a4)(_QWORD, _QWORD, _QWORD), _QWORD *a5, char *a6, uint64_t a7, uint64_t a8, float a9, uint64_t a10)
{
  uint64_t v15;
  char *v16;
  uint64_t TupleTypeMetadata2;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  char *v24;
  uint64_t (*v25)(char *, char *, _QWORD *);
  uint64_t (*v26)(char *);
  uint64_t v27;
  uint64_t v28;
  uint64_t (*v29)(_QWORD *, uint64_t);
  uint64_t v30;
  uint64_t (*v31)(char *, uint64_t);
  uint64_t result;
  uint64_t (*v33)(char *, char *, _QWORD *);
  uint64_t v34;
  uint64_t (*v35)(char *, char *, _QWORD *);
  uint64_t (*v36)(_QWORD, _QWORD, _QWORD);
  uint64_t (*v37)(char *, char *, _QWORD *);
  uint64_t v38;
  uint64_t v39;
  char *v40;
  uint64_t v41;
  uint64_t v42;
  char *v43;
  uint64_t (*v44)(char *, uint64_t);
  uint64_t (*v45)(char *, char *, _QWORD *);
  char *v46;
  uint64_t (*v47)(char *, char *, _QWORD *);
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t (*v51)(char *, char *, _QWORD *);
  uint64_t v52;
  char *v53;
  uint64_t (*v54)(char *, uint64_t);
  uint64_t (*v55)(_QWORD, _QWORD, _QWORD);

  v51 = a2;
  v52 = a7;
  v53 = a1;
  v55 = a4;
  v50 = *((_QWORD *)a4 - 1);
  MEMORY[0x1E0C80A78](a1);
  v16 = (char *)&v45 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v54 = *(uint64_t (**)(char *, uint64_t))(TupleTypeMetadata2 - 8);
  v18 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v46 = (char *)&v45 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  v20 = MEMORY[0x1E0C80A78](v18);
  v22 = (char *)&v45 - v21;
  MEMORY[0x1E0C80A78](v20);
  v24 = (char *)&v45 - v23;
  v25 = *(uint64_t (**)(char *, char *, _QWORD *))(*(a5 - 1) + 16);
  v25((char *)&v45 - v23, v53, a5);
  *(float *)&v24[*(int *)(TupleTypeMetadata2 + 48)] = a9;
  v48 = a10;
  v49 = a3;
  v26 = *(uint64_t (**)(char *))(*(_QWORD *)(a10 + 8) + 16);
  v53 = a6;
  v27 = v26(a6);
  v28 = *(int *)(TupleTypeMetadata2 + 48);
  v45 = v25;
  v25(v22, v24, a5);
  *(float *)&v22[v28] = a9;
  v29 = *(uint64_t (**)(_QWORD *, uint64_t))(a8 + 16);
  v47 = (uint64_t (*)(char *, char *, _QWORD *))a8;
  v30 = v29(a5, a8);
  v31 = (uint64_t (*)(char *, uint64_t))*((_QWORD *)v54 + 1);
  result = v31(v22, TupleTypeMetadata2);
  if (v30 == v27)
  {
    v33 = (uint64_t (*)(char *, char *, _QWORD *))v27;
    v54 = v31;
    v34 = v50;
    v35 = v51;
    v36 = v55;
    (*(void (**)(char *, uint64_t (*)(char *, char *, _QWORD *), uint64_t (*)(_QWORD, _QWORD, _QWORD)))(v50 + 16))(v16, v51, v55);
    v37 = (uint64_t (*)(char *, char *, _QWORD *))v52;
    v38 = (*(uint64_t (**)(uint64_t (*)(_QWORD, _QWORD, _QWORD), uint64_t))(v52 + 16))(v36, v52);
    result = (*(uint64_t (**)(char *, uint64_t (*)(_QWORD, _QWORD, _QWORD)))(v34 + 8))(v16, v36);
    if ((uint64_t (*)(char *, char *, _QWORD *))v38 == v33)
    {
      v39 = *(int *)(TupleTypeMetadata2 + 48);
      v40 = v46;
      v41 = v45(v46, v24, a5);
      *(float *)&v40[v39] = a9;
      MEMORY[0x1E0C80A78](v41);
      *(&v45 - 10) = (uint64_t (*)(char *, char *, _QWORD *))v55;
      *(&v45 - 9) = (uint64_t (*)(char *, char *, _QWORD *))a5;
      *(&v45 - 8) = (uint64_t (*)(char *, char *, _QWORD *))v53;
      *(&v45 - 7) = v37;
      v42 = v48;
      *(&v45 - 6) = v47;
      *(&v45 - 5) = (uint64_t (*)(char *, char *, _QWORD *))v42;
      *(&v45 - 4) = (uint64_t (*)(char *, char *, _QWORD *))v40;
      *(&v45 - 3) = v35;
      *(&v45 - 2) = v33;
      (*(void (**)(uint64_t (*)(uint64_t)))(v42 + 16))(partial apply for closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:));
      v43 = v24;
      v44 = v54;
      v54(v43, TupleTypeMetadata2);
      return v44(v40, TupleTypeMetadata2);
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  char v27[16];
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  char *v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;

  v24 = a8;
  v25 = a4;
  v21 = a7;
  v22 = a1;
  v23 = a3;
  v26 = a9;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v15 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v16 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v18 = (char *)&v21 - v17;
  v19 = *(int *)(v16 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a6 - 8) + 16))((char *)&v21 - v17, a2, a6);
  *(_DWORD *)&v18[v19] = *(_DWORD *)(a2 + v19);
  v28 = a5;
  v29 = a6;
  v30 = v21;
  v31 = v24;
  v32 = a10;
  v33 = a11;
  v34 = v18;
  v35 = v23;
  v36 = v22;
  v37 = v25;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t, uint64_t))(v32 + 24))(partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:), v27, MEMORY[0x1E0DEE9C0] + 8, a6, v32);
  return (*(uint64_t (**)(char *, uint64_t))(v15 + 8))(v18, TupleTypeMetadata2);
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  char v27[16];
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  char *v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;

  v24 = a8;
  v25 = a4;
  v21 = a7;
  v22 = a1;
  v23 = a3;
  v26 = a9;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v15 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v16 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v18 = (char *)&v21 - v17;
  v19 = *(int *)(v16 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a6 - 8) + 16))((char *)&v21 - v17, a2, a6);
  *(_QWORD *)&v18[v19] = *(_QWORD *)(a2 + v19);
  v28 = a5;
  v29 = a6;
  v30 = v21;
  v31 = v24;
  v32 = a10;
  v33 = a11;
  v34 = v18;
  v35 = v23;
  v36 = v22;
  v37 = v25;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t, uint64_t))(v32 + 24))(partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:), v27, MEMORY[0x1E0DEE9C0] + 8, a6, v32);
  return (*(uint64_t (**)(char *, uint64_t))(v15 + 8))(v18, TupleTypeMetadata2);
}

uint64_t static vDSP.subtract<A, B>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  uint64_t TupleTypeMetadata2;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  char *v18;
  void (*v19)(char *, uint64_t, uint64_t);
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  void (*v23)(char *, uint64_t);
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  char *v31;
  uint64_t v32;

  v26 = a6;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v25 = *(_QWORD *)(TupleTypeMetadata2 - 8);
  v14 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v16 = (char *)&v25 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1E0C80A78](v14);
  v18 = (char *)&v25 - v17;
  v19 = *(void (**)(char *, uint64_t, uint64_t))(*(_QWORD *)(a4 - 8) + 16);
  v19((char *)&v25 - v17, a1, a4);
  *(double *)&v18[*(int *)(TupleTypeMetadata2 + 48)] = a7;
  v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  v21 = *(int *)(TupleTypeMetadata2 + 48);
  v19(v16, (uint64_t)v18, a4);
  *(double *)&v16[v21] = a7;
  v27 = a3;
  v28 = a4;
  v29 = a5;
  v30 = v26;
  v31 = v16;
  v32 = a2;
  v22 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v20, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.subtract<A, B>(multiplication:_:));
  v23 = *(void (**)(char *, uint64_t))(v25 + 8);
  v23(v18, TupleTypeMetadata2);
  v23(v16, TupleTypeMetadata2);
  return v22;
}

uint64_t static vDSP.subtract<A, B, C>(multiplication:_:result:)(char *a1, uint64_t (*a2)(char *, char *, _QWORD *), uint64_t a3, uint64_t (*a4)(_QWORD, _QWORD, _QWORD), _QWORD *a5, char *a6, uint64_t a7, uint64_t a8, double a9, uint64_t a10)
{
  uint64_t v15;
  char *v16;
  uint64_t TupleTypeMetadata2;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  char *v24;
  uint64_t (*v25)(char *, char *, _QWORD *);
  uint64_t (*v26)(char *);
  uint64_t v27;
  uint64_t v28;
  uint64_t (*v29)(_QWORD *, uint64_t);
  uint64_t v30;
  uint64_t (*v31)(char *, uint64_t);
  uint64_t result;
  uint64_t (*v33)(char *, char *, _QWORD *);
  uint64_t v34;
  uint64_t (*v35)(char *, char *, _QWORD *);
  uint64_t (*v36)(_QWORD, _QWORD, _QWORD);
  uint64_t (*v37)(char *, char *, _QWORD *);
  uint64_t v38;
  uint64_t v39;
  char *v40;
  uint64_t v41;
  uint64_t v42;
  char *v43;
  uint64_t (*v44)(char *, uint64_t);
  uint64_t (*v45)(char *, char *, _QWORD *);
  char *v46;
  uint64_t (*v47)(char *, char *, _QWORD *);
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t (*v51)(char *, char *, _QWORD *);
  uint64_t v52;
  char *v53;
  uint64_t (*v54)(char *, uint64_t);
  uint64_t (*v55)(_QWORD, _QWORD, _QWORD);

  v51 = a2;
  v52 = a7;
  v53 = a1;
  v55 = a4;
  v50 = *((_QWORD *)a4 - 1);
  MEMORY[0x1E0C80A78](a1);
  v16 = (char *)&v45 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  v54 = *(uint64_t (**)(char *, uint64_t))(TupleTypeMetadata2 - 8);
  v18 = MEMORY[0x1E0C80A78](TupleTypeMetadata2);
  v46 = (char *)&v45 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  v20 = MEMORY[0x1E0C80A78](v18);
  v22 = (char *)&v45 - v21;
  MEMORY[0x1E0C80A78](v20);
  v24 = (char *)&v45 - v23;
  v25 = *(uint64_t (**)(char *, char *, _QWORD *))(*(a5 - 1) + 16);
  v25((char *)&v45 - v23, v53, a5);
  *(double *)&v24[*(int *)(TupleTypeMetadata2 + 48)] = a9;
  v48 = a10;
  v49 = a3;
  v26 = *(uint64_t (**)(char *))(*(_QWORD *)(a10 + 8) + 16);
  v53 = a6;
  v27 = v26(a6);
  v28 = *(int *)(TupleTypeMetadata2 + 48);
  v45 = v25;
  v25(v22, v24, a5);
  *(double *)&v22[v28] = a9;
  v29 = *(uint64_t (**)(_QWORD *, uint64_t))(a8 + 16);
  v47 = (uint64_t (*)(char *, char *, _QWORD *))a8;
  v30 = v29(a5, a8);
  v31 = (uint64_t (*)(char *, uint64_t))*((_QWORD *)v54 + 1);
  result = v31(v22, TupleTypeMetadata2);
  if (v30 == v27)
  {
    v33 = (uint64_t (*)(char *, char *, _QWORD *))v27;
    v54 = v31;
    v34 = v50;
    v35 = v51;
    v36 = v55;
    (*(void (**)(char *, uint64_t (*)(char *, char *, _QWORD *), uint64_t (*)(_QWORD, _QWORD, _QWORD)))(v50 + 16))(v16, v51, v55);
    v37 = (uint64_t (*)(char *, char *, _QWORD *))v52;
    v38 = (*(uint64_t (**)(uint64_t (*)(_QWORD, _QWORD, _QWORD), uint64_t))(v52 + 16))(v36, v52);
    result = (*(uint64_t (**)(char *, uint64_t (*)(_QWORD, _QWORD, _QWORD)))(v34 + 8))(v16, v36);
    if ((uint64_t (*)(char *, char *, _QWORD *))v38 == v33)
    {
      v39 = *(int *)(TupleTypeMetadata2 + 48);
      v40 = v46;
      v41 = v45(v46, v24, a5);
      *(double *)&v40[v39] = a9;
      MEMORY[0x1E0C80A78](v41);
      *(&v45 - 10) = (uint64_t (*)(char *, char *, _QWORD *))v55;
      *(&v45 - 9) = (uint64_t (*)(char *, char *, _QWORD *))a5;
      *(&v45 - 8) = (uint64_t (*)(char *, char *, _QWORD *))v53;
      *(&v45 - 7) = v37;
      v42 = v48;
      *(&v45 - 6) = v47;
      *(&v45 - 5) = (uint64_t (*)(char *, char *, _QWORD *))v42;
      *(&v45 - 4) = (uint64_t (*)(char *, char *, _QWORD *))v40;
      *(&v45 - 3) = v35;
      *(&v45 - 2) = v33;
      (*(void (**)(uint64_t (*)(uint64_t)))(v42 + 16))(partial apply for closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:));
      v43 = v24;
      v44 = v54;
      v54(v43, TupleTypeMetadata2);
      return v44(v40, TupleTypeMetadata2);
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, _QWORD *a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  if (!a3)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (!result)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (*a6)
  {
    if ((a7 & 0x8000000000000000) == 0)
      return a8(a3, 1, a5, result, 1);
    __break(1u);
    goto LABEL_7;
  }
LABEL_9:
  __break(1u);
  return result;
}

uint64_t static vDSP.normalize<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.normalize<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.normalize<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t closure #1 in static vDSP.normalize<A>(_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v10;
  uint64_t v11;
  double v12;
  uint64_t result;

  v10 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v11 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v12 = static vDSP.normalize<A, B>(_:result:)(a3, a1, a4, v10, a5, v11);
  result = (*(uint64_t (**)(uint64_t, uint64_t, double))(a5 + 16))(a4, a5, v12);
  *a2 = result;
  return result;
}

{
  uint64_t v10;
  uint64_t v11;
  float v12;
  uint64_t result;

  v10 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v11 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v12 = static vDSP.normalize<A, B>(_:result:)(a3, a1, a4, v10, a5, v11);
  result = (*(uint64_t (**)(uint64_t, uint64_t, float))(a5 + 16))(a4, a5, v12);
  *a2 = result;
  return result;
}

double static vDSP.normalize<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v7;
  double v8;
  _BYTE v9[16];
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  double *v15;
  uint64_t *v16;
  uint64_t v17;
  uint64_t v18;

  v18 = *MEMORY[0x1E0C80C00];
  v7 = 0;
  v8 = 0.0;
  v10 = a3;
  v11 = a4;
  v12 = a5;
  v13 = a6;
  v14 = a2;
  v15 = &v8;
  v16 = &v7;
  v17 = a1;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), _BYTE *, uint64_t, uint64_t))(a5 + 24))(partial apply for closure #1 in static vDSP.normalize<A, B>(_:result:), v9, MEMORY[0x1E0DEE9C0] + 8, a3);
  return v8;
}

void closure #1 in closure #1 in static vDSP.normalize<A, B>(_:result:)(double **a1, const double *a2, uint64_t a3, double *a4, double *a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  double *v9;
  vDSP_Length v13;

  if (!a2)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  v9 = *a1;
  if (*a1)
  {
    v13 = (*(uint64_t (**)(uint64_t))(a9 + 16))(a7);
    if ((v13 & 0x8000000000000000) == 0)
    {
      vDSP_normalizeD(a2, 1, v9, 1, a4, a5, v13);
      return;
    }
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
}

double static vDSP.standardDeviation<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  double v4;
  uint64_t v5;
  _BYTE v6[16];
  uint64_t v7;
  uint64_t v8;
  uint64_t *v9;
  double *v10;
  uint64_t v11;
  uint64_t v12;

  v12 = *MEMORY[0x1E0C80C00];
  v4 = 0.0;
  v5 = 0;
  v7 = a2;
  v8 = a3;
  v9 = &v5;
  v10 = &v4;
  v11 = a1;
  (*(void (**)(void (*)(const double *, uint64_t), _BYTE *, uint64_t, uint64_t))(*(_QWORD *)(a3 + 8)
                                                                                                  + 24))(partial apply for closure #1 in static vDSP.standardDeviation<A>(_:), v6, MEMORY[0x1E0DEE9C0] + 8, a2);
  return v4;
}

void closure #1 in static vDSP.standardDeviation<A>(_:)(const double *a1, uint64_t a2, double *a3, double *a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  vDSP_Length v10;

  if (a1)
  {
    v10 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a7 + 8) + 16))(a6);
    if ((v10 & 0x8000000000000000) == 0)
    {
      vDSP_normalizeD(a1, 1, 0, 1, a3, a4, v10);
      return;
    }
    __break(1u);
  }
  __break(1u);
}

float static vDSP.normalize<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v7;
  _BYTE v8[16];
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  char *v14;
  uint64_t *v15;
  uint64_t v16;
  uint64_t v17;

  v17 = *MEMORY[0x1E0C80C00];
  v7 = 0;
  v9 = a3;
  v10 = a4;
  v11 = a5;
  v12 = a6;
  v13 = a2;
  v14 = (char *)&v7 + 4;
  v15 = &v7;
  v16 = a1;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), _BYTE *, uint64_t, uint64_t))(a5 + 24))(partial apply for closure #1 in static vDSP.normalize<A, B>(_:result:), v8, MEMORY[0x1E0DEE9C0] + 8, a3);
  return *((float *)&v7 + 1);
}

void closure #1 in closure #1 in static vDSP.normalize<A, B>(_:result:)(float **a1, const float *a2, uint64_t a3, float *a4, float *a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  float *v9;
  vDSP_Length v13;

  if (!a2)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  v9 = *a1;
  if (*a1)
  {
    v13 = (*(uint64_t (**)(uint64_t))(a9 + 16))(a7);
    if ((v13 & 0x8000000000000000) == 0)
    {
      vDSP_normalize(a2, 1, v9, 1, a4, a5, v13);
      return;
    }
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
}

float static vDSP.standardDeviation<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4;
  _BYTE v5[16];
  uint64_t v6;
  uint64_t v7;
  char *v8;
  uint64_t *v9;
  uint64_t v10;
  uint64_t v11;

  v11 = *MEMORY[0x1E0C80C00];
  v4 = 0;
  v6 = a2;
  v7 = a3;
  v8 = (char *)&v4 + 4;
  v9 = &v4;
  v10 = a1;
  (*(void (**)(void (*)(const float *, uint64_t), _BYTE *, uint64_t, uint64_t))(*(_QWORD *)(a3 + 8)
                                                                                                 + 24))(partial apply for closure #1 in static vDSP.standardDeviation<A>(_:), v5, MEMORY[0x1E0DEE9C0] + 8, a2);
  return *(float *)&v4;
}

void closure #1 in static vDSP.standardDeviation<A>(_:)(const float *a1, uint64_t a2, float *a3, float *a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  vDSP_Length v10;

  if (a1)
  {
    v10 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a7 + 8) + 16))(a6);
    if ((v10 & 0x8000000000000000) == 0)
    {
      vDSP_normalize(a1, 1, 0, 1, a3, a4, v10);
      return;
    }
    __break(1u);
  }
  __break(1u);
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.add<A, B, C>(_:_:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.hypot<A, B>(_:_:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.add<A, B, C>(_:_:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.hypot<A, B>(_:_:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.subtract<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.subtract<A, B, C>(_:_:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.subtract<A, B>(_:_:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.subtract<A, B, C>(_:_:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.subtract<A, B>(_:_:));
}

uint64_t partial apply for closure #1 in static vDSP.subtract<A, B, C>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A>(_:_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t v2;

  return closure #1 in static vDSP.add<A>(_:_:)(a1, a2, *(_QWORD *)(v2 + 40), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float))static vDSP.multiply<A, B>(_:_:result:), *(float *)(v2 + 32));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.add<A>(_:_:)(a1, a2, *(_QWORD *)(v2 + 40), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double))static vDSP.multiply<A, B>(_:_:result:), *(double *)(v2 + 32));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C>(_:_:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.hypot<A, B>(_:_:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C>(_:_:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.hypot<A, B>(_:_:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.divide<A>(_:_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t v2;

  return closure #1 in static vDSP.divide<A>(_:_:)(a1, a2, *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(float *)(v2 + 40));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.divide<A>(_:_:)(a1, a2, *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(double *)(v2 + 40));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.add<A>(_:_:)(a1, a2, *(_QWORD *)(v2 + 40), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float))static vDSP.divide<A, B>(_:_:result:), *(float *)(v2 + 32));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.add<A>(_:_:)(a1, a2, *(_QWORD *)(v2 + 40), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double))static vDSP.divide<A, B>(_:_:result:), *(double *)(v2 + 32));
}

uint64_t partial apply for closure #1 in static vDSP.divide<A, B>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.divide<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.divide<A, B, C>(_:_:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.hypot<A, B>(_:_:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.divide<A, B, C>(_:_:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.hypot<A, B>(_:_:));
}

uint64_t partial apply for closure #1 in static vDSP.divide<A, B, C>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))
{
  _QWORD *v6;

  return a6(a1, a2, v6[6], v6[7], v6[2], v6[3], v6[4], v6[5], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:));
}

{
  return partial apply for closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:));
}

uint64_t partial apply for closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  _OWORD v6[2];
  uint64_t v7;
  __int128 v8;
  __int128 v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v12;

  v3 = *(_QWORD *)(v2 + 32);
  v4 = *(_QWORD *)(v2 + 72);
  v6[1] = *(_OWORD *)(v2 + 16);
  v7 = v3;
  v8 = *(_OWORD *)(v2 + 40);
  v9 = *(_OWORD *)(v2 + 56);
  v10 = v4;
  v11 = *(_OWORD *)(v2 + 88);
  v12 = a1;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, _QWORD))(v4 + 16))(a2, v6, MEMORY[0x1E0DEE9C0] + 8, v8);
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(a1, a2, (void (*)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, uint64_t))static vDSP.multiply<A, B, C>(addition:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(a1, a2, (void (*)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, uint64_t))static vDSP.multiply<A, B, C>(addition:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:), a2);
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:), a2);
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D>(addition:_:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.multiply<A, B, C>(addition:_:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D>(addition:_:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.multiply<A, B, C>(addition:_:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:), (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:), (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))
{
  _QWORD *v3;

  return a3(a1, v3[10], v3[11], v3[12], v3[2], v3[3], v3[4], v3[5], v3[6], v3[7], v3[8], v3[9], a2);
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B>(subtraction:_:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(a1, a2, (void (*)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, uint64_t))static vDSP.multiply<A, B, C>(subtraction:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(a1, a2, (void (*)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, uint64_t))static vDSP.multiply<A, B, C>(subtraction:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t *a2, void (*a3)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, uint64_t))
{
  uint64_t v3;

  return closure #1 in static vDSP.multiply<A, B>(addition:_:)(a1, a2, *(_QWORD *)(v3 + 48), *(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24), *(_QWORD *)(v3 + 32), *(_QWORD *)(v3 + 40), a3, *(float *)(v3 + 56));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:), a2);
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:), a2);
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t *a2, void (*a3)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, uint64_t))
{
  uint64_t v3;

  return closure #1 in static vDSP.multiply<A, B>(addition:_:)(a1, a2, *(_QWORD *)(v3 + 48), *(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24), *(_QWORD *)(v3 + 32), *(_QWORD *)(v3 + 40), a3, *(double *)(v3 + 56));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D>(subtraction:_:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.multiply<A, B, C>(addition:_:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D>(subtraction:_:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.multiply<A, B, C>(addition:_:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:), (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:), (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(a1, a2, (void (*)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, uint64_t))static vDSP.add<A, B, C>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(a1, a2, (void (*)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, uint64_t))static vDSP.add<A, B, C>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:), a2);
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:), a2);
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  uint64_t v3;

  return closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, *(_QWORD *)(v3 + 64), *(_QWORD *)(v3 + 80), *(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24), *(_QWORD *)(v3 + 32), *(_QWORD *)(v3 + 40), *(_QWORD *)(v3 + 48), a3, *(float *)(v3 + 72), *(_QWORD *)(v3 + 56), a2);
}

{
  uint64_t v3;

  return closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, *(_QWORD *)(v3 + 64), *(_QWORD *)(v3 + 80), *(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24), *(_QWORD *)(v3 + 32), *(_QWORD *)(v3 + 40), *(_QWORD *)(v3 + 48), a3, *(double *)(v3 + 72), *(_QWORD *)(v3 + 56), a2);
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in static vDSP.add<A, B>(multiplication:_:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in static vDSP.add<A, B>(multiplication:_:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.add<A, B, C, D>(multiplication:_:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.multiply<A, B, C>(addition:_:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.add<A, B, C, D>(multiplication:_:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.multiply<A, B, C>(addition:_:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:), (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:), (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.subtract<A, B, C, D>(multiplication:_:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.subtract<A, B, C, D>(multiplication:_:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:));
}

uint64_t partial apply for closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:), (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:), (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))
{
  _QWORD *v6;

  return a6(a1, a2, v6[8], v6[9], v6[2], v6[3], v6[4], v6[5], v6[6], v6[7], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B>(multiplication:multiplication:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in static vDSP.add<A, B>(multiplication:multiplication:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in static vDSP.add<A, B>(multiplication:multiplication:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))
{
  _QWORD *v6;

  return a6(a1, a2, v6[10], v6[11], v6[2], v6[3], v6[4], v6[5], v6[6], v6[7], v6[8], v6[9], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:addition:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:addition:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(char *, char *, char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.multiply<A, B, C, D>(addition:addition:result:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:addition:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(char *, char *, char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.multiply<A, B, C, D>(addition:addition:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:), a2);
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:), a2);
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  uint64_t *v3;
  _BYTE v5[24];

  *(_QWORD *)&v5[16] = v3[8];
  *(_OWORD *)v5 = *((_OWORD *)v3 + 3);
  return closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, v3[10], v3[11], v3[12], v3[2], v3[3], v3[4], v3[5], a3, v3[6], *(__int128 *)&v5[8], v3[9], a2);
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:addition:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(char *, char *, char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t *v5;

  return closure #1 in static vDSP.multiply<A, B, C>(addition:addition:)(a1, a2, v5[8], v5[9], v5[2], v5[3], v5[4], v5[5], v5[6], v5[7], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:multiplication:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:multiplication:));
}

uint64_t partial apply for closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:subtraction:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:), (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:), (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:subtraction:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:), (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:), (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))
{
  _QWORD *v3;

  return a3(a1, v3[12], v3[13], v3[14], v3[2], v3[3], v3[4], v3[5], v3[6], v3[7], v3[8], v3[9], v3[10], v3[11], a2);
}

uint64_t partial apply for closure #1 in static vDSP.add<A>(multiplication:_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t v2;

  return closure #1 in static vDSP.add<A>(multiplication:_:)(a1, a2, *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(float *)(v2 + 40));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.add<A>(multiplication:_:)(a1, a2, *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(double *)(v2 + 40));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  uint64_t v2;

  return closure #1 in static vDSP.add<A, B>(multiplication:_:result:)(a1, *(_QWORD *)(v2 + 48), *(_QWORD *)(v2 + 64), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), a2, *(float *)(v2 + 56));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.add<A, B>(multiplication:_:result:)(a1, *(_QWORD *)(v2 + 48), *(_QWORD *)(v2 + 64), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), a2, *(double *)(v2 + 56));
}

uint64_t partial apply for closure #1 in static vDSP.subtract<A, B>(multiplication:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in static vDSP.subtract<A, B>(multiplication:_:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in static vDSP.subtract<A, B>(multiplication:_:));
}

uint64_t partial apply for closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))
{
  _QWORD *v3;

  return a3(a1, a2, v3[6], v3[7], v3[2], v3[3], v3[4], v3[5]);
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t (*a2)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))
{
  _QWORD *v2;

  return a2(a1, v2[8], v2[9], v2[10], v2[2], v2[3], v2[4], v2[5], v2[6], v2[7]);
}

uint64_t partial apply for closure #1 in static vDSP.normalize<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.normalize<A>(_:)(a1, a2, v2[4], v2[2], v2[3]);
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.normalize<A>(_:)(a1, a2, v2[4], v2[2], v2[3]);
}

uint64_t partial apply for closure #1 in static vDSP.normalize<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.normalize<A, B>(_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.normalize<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.normalize<A, B>(_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.normalize<A, B>(_:result:));
}

void partial apply for closure #1 in static vDSP.standardDeviation<A>(_:)(const double *a1, uint64_t a2)
{
  uint64_t v2;

  closure #1 in static vDSP.standardDeviation<A>(_:)(a1, a2, *(double **)(v2 + 32), *(double **)(v2 + 40), *(_QWORD *)(v2 + 48), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24));
}

uint64_t partial apply for closure #1 in static vDSP.normalize<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  _QWORD v7[3];
  __int128 v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  __int128 v12;
  uint64_t v13;

  v4 = *(_QWORD *)(v3 + 40);
  v5 = *(_QWORD *)(v3 + 72);
  v7[2] = *(_QWORD *)(v3 + 16);
  v8 = *(_OWORD *)(v3 + 24);
  v9 = v4;
  v10 = a1;
  v11 = a2;
  v12 = *(_OWORD *)(v3 + 56);
  v13 = v5;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, _QWORD))(v4 + 16))(a3, v7, MEMORY[0x1E0DEE9C0] + 8, v8);
}

void partial apply for closure #1 in static vDSP.standardDeviation<A>(_:)(const float *a1, uint64_t a2)
{
  uint64_t v2;

  closure #1 in static vDSP.standardDeviation<A>(_:)(a1, a2, *(float **)(v2 + 32), *(float **)(v2 + 40), *(_QWORD *)(v2 + 48), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24));
}

void partial apply for closure #1 in closure #1 in static vDSP.normalize<A, B>(_:result:)(float **a1)
{
  uint64_t v1;

  closure #1 in closure #1 in static vDSP.normalize<A, B>(_:result:)(a1, *(const float **)(v1 + 48), *(_QWORD *)(v1 + 56), *(float **)(v1 + 64), *(float **)(v1 + 72), *(_QWORD *)(v1 + 80), *(_QWORD *)(v1 + 16), *(_QWORD *)(v1 + 24), *(_QWORD *)(v1 + 32));
}

void partial apply for closure #1 in closure #1 in static vDSP.normalize<A, B>(_:result:)(double **a1)
{
  uint64_t v1;

  closure #1 in closure #1 in static vDSP.normalize<A, B>(_:result:)(a1, *(const double **)(v1 + 48), *(_QWORD *)(v1 + 56), *(double **)(v1 + 64), *(double **)(v1 + 72), *(_QWORD *)(v1 + 80), *(_QWORD *)(v1 + 16), *(_QWORD *)(v1 + 24), *(_QWORD *)(v1 + 32));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  _QWORD *v2;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  _BYTE v11[16];
  uint64_t v12;
  uint64_t v13;
  uint64_t *v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;

  v5 = v2[2];
  v6 = v2[5];
  v7 = v2[8];
  v8 = v2[10];
  v9 = v2[11];
  v17 = *(_QWORD *)(v7 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  v12 = a1;
  v13 = a2;
  v14 = &v17;
  v15 = v8;
  v16 = v9;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t), _BYTE *, uint64_t, uint64_t, uint64_t))(v6 + 24))(partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:), v11, MEMORY[0x1E0DEE9C0] + 8, v5, v6);
}

{
  _QWORD *v2;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  _BYTE v11[16];
  uint64_t v12;
  uint64_t v13;
  int *v14;
  uint64_t v15;
  uint64_t v16;
  int v17;

  v5 = v2[2];
  v6 = v2[5];
  v7 = v2[8];
  v8 = v2[10];
  v9 = v2[11];
  v17 = *(_DWORD *)(v7 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  v12 = a1;
  v13 = a2;
  v14 = &v17;
  v15 = v8;
  v16 = v9;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t), _BYTE *, uint64_t, uint64_t, uint64_t))(v6 + 24))(partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:), v11, MEMORY[0x1E0DEE9C0] + 8, v5, v6);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)(a1, a2, MEMORY[0x1E0C8C8B8]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)(a1, a2, MEMORY[0x1E0C8C8B0]);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v3;

  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)(a1, a2, *(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24), *(_QWORD *)(v3 + 32), *(_QWORD **)(v3 + 40), *(_QWORD *)(v3 + 48), a3);
}

void partial apply for closure #1 in closure #1 in static vDSP.add<A, B>(multiplication:_:result:)(const double *a1, uint64_t a2)
{
  uint64_t v2;

  closure #1 in closure #1 in static vDSP.add<A, B>(multiplication:_:result:)(a1, *(double *)(v2 + 56), a2, *(_QWORD *)(v2 + 48), *(double ***)(v2 + 64), *(_QWORD *)(v2 + 72));
}

void partial apply for closure #1 in closure #1 in static vDSP.add<A, B>(multiplication:_:result:)(const float *a1, uint64_t a2)
{
  uint64_t v2;

  closure #1 in closure #1 in static vDSP.add<A, B>(multiplication:_:result:)(a1, *(float *)(v2 + 56), a2, *(_QWORD *)(v2 + 48), *(float ***)(v2 + 64), *(_QWORD *)(v2 + 72));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E0C8C3B0]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E0C8C3A8]);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, _QWORD, uint64_t, uint64_t))
{
  uint64_t v3;

  return closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, *(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24), *(_QWORD *)(v3 + 32), *(_QWORD *)(v3 + 40), *(_QWORD *)(v3 + 48), *(_QWORD *)(v3 + 56), *(_QWORD **)(v3 + 64), *(_QWORD *)(v3 + 72), a3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E0C8C840]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E0C8C838]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E0C8C710]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E0C8C708]);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  __int128 v12;
  _BYTE v15[16];
  __int128 v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;

  v5 = *(_QWORD *)(v3 + 24);
  v6 = *(_QWORD *)(v3 + 64);
  v12 = *(_OWORD *)(v3 + 104);
  v8 = *(_QWORD *)(v3 + 120);
  v7 = *(_QWORD *)(v3 + 128);
  v10 = *(_QWORD *)(v3 + 136);
  v9 = *(_QWORD *)(v3 + 144);
  swift_getTupleTypeMetadata2();
  v16 = v12;
  v17 = v8;
  v18 = v7;
  v19 = a1;
  v20 = a2;
  v21 = v10;
  v22 = v9;
  return (*(uint64_t (**)(uint64_t, _BYTE *, uint64_t, uint64_t, uint64_t))(v6 + 24))(a3, v15, MEMORY[0x1E0DEE9C0] + 8, v5, v6);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:), a3);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:), a3);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:), a3);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:), a3);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E0C8C348]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E0C8C340]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X8>)
{
  uint64_t *v4;

  return closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, v4[10], v4[11], v4[12], v4[13], v4[2], v4[3], a4, v4[4], v4[5], v4[6], v4[7], v4[8], v4[9], a3);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X8>)
{
  uint64_t v4;

  return closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, *(_QWORD *)(v4 + 80), *(_QWORD *)(v4 + 88), *(_QWORD *)(v4 + 96), *(_QWORD *)(v4 + 104), *(_QWORD *)(v4 + 112), *(_QWORD *)(v4 + 16), a4, *(_QWORD *)(v4 + 24), *(_QWORD *)(v4 + 32), *(_OWORD *)(v4 + 40), *(_QWORD *)(v4 + 56), *(_QWORD *)(v4 + 64), *(_QWORD *)(v4 + 72), a3);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  __int128 v12;
  _BYTE v15[16];
  __int128 v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;

  v5 = *(_QWORD *)(v3 + 32);
  v6 = *(_QWORD *)(v3 + 64);
  v12 = *(_OWORD *)(v3 + 88);
  v8 = *(_QWORD *)(v3 + 104);
  v7 = *(_QWORD *)(v3 + 112);
  v10 = *(_QWORD *)(v3 + 120);
  v9 = *(_QWORD *)(v3 + 128);
  swift_getTupleTypeMetadata2();
  v16 = v12;
  v17 = v8;
  v18 = v7;
  v19 = a1;
  v20 = a2;
  v21 = v10;
  v22 = v9;
  return (*(uint64_t (**)(uint64_t, _BYTE *, uint64_t, uint64_t, uint64_t))(v6 + 24))(a3, v15, MEMORY[0x1E0DEE9C0] + 8, v5, v6);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E0C8C700]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E0C8C6F8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))
{
  _QWORD *v4;

  return a4(a1, a2, v4[12], v4[13], v4[14], v4[15], v4[2], v4[3], v4[4], v4[5], v4[6], v4[7], v4[8], v4[9], v4[10], v4[11], a3);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t))
{
  _QWORD *v4;

  return a4(a1, a2, v4[12], v4[13], v4[14], v4[15], v4[16], v4[2], v4[3], v4[4], v4[5], v4[6], v4[7], v4[8], v4[9], v4[10], v4[11], a3);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  __int128 v12;
  _BYTE v15[16];
  __int128 v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;

  v5 = *(_QWORD *)(v3 + 40);
  v6 = *(_QWORD *)(v3 + 80);
  v12 = *(_OWORD *)(v3 + 104);
  v8 = *(_QWORD *)(v3 + 120);
  v7 = *(_QWORD *)(v3 + 128);
  v10 = *(_QWORD *)(v3 + 136);
  v9 = *(_QWORD *)(v3 + 144);
  swift_getTupleTypeMetadata2();
  v16 = v12;
  v17 = v8;
  v18 = v7;
  v19 = a1;
  v20 = a2;
  v21 = v10;
  v22 = v9;
  return (*(uint64_t (**)(uint64_t, _BYTE *, uint64_t, uint64_t, uint64_t))(v6 + 24))(a3, v15, MEMORY[0x1E0DEE9C0] + 8, v5, v6);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))
{
  _QWORD *v3;

  return a3(a1, a2, v3[8], v3[9], v3[10], v3[11], v3[12], v3[13], v3[2], v3[3], v3[4], v3[5], v3[6], v3[7]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, MEMORY[0x1E0C8C730]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, MEMORY[0x1E0C8C728]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  __int128 v11;
  __int128 v12;
  __int128 v14;
  _BYTE v17[16];
  uint64_t v18;
  uint64_t v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;

  v6 = *(_QWORD *)(v3 + 16);
  v5 = *(_QWORD *)(v3 + 24);
  v7 = *(_QWORD *)(v3 + 88);
  v9 = *(_QWORD *)(v3 + 96);
  v8 = *(_QWORD *)(v3 + 104);
  v14 = *(_OWORD *)(v3 + 32);
  v10 = *(_QWORD *)(v3 + 32);
  swift_getTupleTypeMetadata2();
  v11 = *(_OWORD *)(v3 + 48);
  v12 = *(_OWORD *)(v3 + 64);
  v20 = v14;
  v21 = v11;
  v18 = v6;
  v19 = v5;
  v22 = v12;
  v23 = v7;
  v24 = a1;
  v25 = a2;
  v26 = v9;
  v27 = v8;
  return (*(uint64_t (**)(uint64_t, _BYTE *, uint64_t, uint64_t))(v12 + 24))(a3, v17, MEMORY[0x1E0DEE9C0] + 8, v10);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _OWORD v8[2];
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;

  v4 = *(_QWORD *)(v3 + 16);
  v5 = *(_QWORD *)(v3 + 48);
  v6 = *(_QWORD *)(v3 + 104);
  v8[1] = *(_OWORD *)(v3 + 88);
  v9 = a1;
  v10 = a2;
  v11 = v6;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E0DEE9C0] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v3;

  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, a2, *(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24), *(_QWORD *)(v3 + 32), *(_QWORD *)(v3 + 40), *(_QWORD **)(v3 + 48), *(_QWORD *)(v3 + 56), a3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, MEMORY[0x1E0C8C6B0]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, MEMORY[0x1E0C8C6A0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  __int128 v12;
  _BYTE v15[16];
  uint64_t v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;

  v5 = *(_QWORD *)(v3 + 16);
  v6 = *(_QWORD *)(v3 + 72);
  v7 = *(_QWORD *)(v3 + 88);
  v8 = *(_QWORD *)(v3 + 96);
  v9 = *(_QWORD *)(v3 + 104);
  v12 = *(_OWORD *)(v3 + 24);
  v10 = *(_QWORD *)(v3 + 24);
  swift_getTupleTypeMetadata2();
  v16 = v5;
  v17 = v12;
  v18 = *(_OWORD *)(v3 + 40);
  v19 = *(_OWORD *)(v3 + 56);
  v20 = v6;
  v21 = v7;
  v22 = a1;
  v23 = a2;
  v24 = v8;
  v25 = v9;
  return (*(uint64_t (**)(uint64_t, _BYTE *, uint64_t, uint64_t))(v19 + 24))(a3, v15, MEMORY[0x1E0DEE9C0] + 8, v10);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, MEMORY[0x1E0C8C720]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, MEMORY[0x1E0C8C718]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))
{
  _QWORD *v3;

  return a3(a1, a2, v3[8], v3[9], v3[10], v3[11], v3[2], v3[3], v3[4], v3[5], v3[6], v3[7]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, MEMORY[0x1E0C8C830]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, MEMORY[0x1E0C8C828]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, MEMORY[0x1E0C8C850]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, MEMORY[0x1E0C8C848]);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, _QWORD *))
{
  uint64_t v3;

  return closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, *(double *)(v3 + 16), a2, *(_QWORD *)(v3 + 24), *(_QWORD *)(v3 + 32), *(_QWORD **)(v3 + 40), *(_QWORD *)(v3 + 48), a3);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, float *))
{
  uint64_t v3;

  return closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, *(float *)(v3 + 16), a2, *(_QWORD *)(v3 + 24), *(_QWORD *)(v3 + 32), *(_QWORD **)(v3 + 40), *(_QWORD *)(v3 + 48), a3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, MEMORY[0x1E0C8C3A0]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, MEMORY[0x1E0C8C398]);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _OWORD v8[2];
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;

  v4 = *(_QWORD *)(v3 + 32);
  v5 = *(_QWORD *)(v3 + 64);
  v6 = *(_QWORD *)(v3 + 104);
  v8[1] = *(_OWORD *)(v3 + 88);
  v9 = a1;
  v10 = a2;
  v11 = v6;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E0DEE9C0] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  _QWORD *v3;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  _BYTE v13[16];
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;

  v6 = v3[3];
  v7 = v3[6];
  v8 = v3[9];
  v10 = v3[10];
  v9 = v3[11];
  swift_getTupleTypeMetadata2();
  v14 = v8;
  v15 = a1;
  v16 = a2;
  v17 = v10;
  v18 = v9;
  return (*(uint64_t (**)(uint64_t, _BYTE *, uint64_t, uint64_t, uint64_t))(v7 + 24))(a3, v13, MEMORY[0x1E0DEE9C0] + 8, v6, v7);
}

{
  uint64_t v3;
  uint64_t v6;
  uint64_t v7;
  int v8;
  uint64_t v9;
  uint64_t v10;
  _BYTE v13[16];
  int v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;

  v6 = *(_QWORD *)(v3 + 24);
  v7 = *(_QWORD *)(v3 + 48);
  v8 = *(_DWORD *)(v3 + 72);
  v10 = *(_QWORD *)(v3 + 80);
  v9 = *(_QWORD *)(v3 + 88);
  swift_getTupleTypeMetadata2();
  v14 = v8;
  v15 = a1;
  v16 = a2;
  v17 = v10;
  v18 = v9;
  return (*(uint64_t (**)(uint64_t, _BYTE *, uint64_t, uint64_t, uint64_t))(v7 + 24))(a3, v13, MEMORY[0x1E0DEE9C0] + 8, v6, v7);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, MEMORY[0x1E0C8C3C8]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, MEMORY[0x1E0C8C3C0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, a2, MEMORY[0x1E0C8C390]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, a2, MEMORY[0x1E0C8C388]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1, uint64_t a2)
{
  _OWORD *v2;
  __int128 v3;
  __int128 v4;
  uint64_t v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  uint64_t v12;

  v3 = v2[2];
  v7 = v2[1];
  v8 = v3;
  v4 = v2[4];
  v9 = v2[3];
  v10 = v4;
  v11 = *(_OWORD *)((char *)v2 + 88);
  v12 = a1;
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t, _QWORD))(v9 + 24))(a2, &v6, MEMORY[0x1E0DEE9C0] + 8, v7);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD v8[4];
  __int128 v9;
  uint64_t v10;

  v4 = *(_QWORD *)(v3 + 24);
  v5 = *(_QWORD *)(v3 + 56);
  v6 = *(_QWORD *)(v3 + 104);
  v8[2] = a1;
  v8[3] = a2;
  v9 = *(_OWORD *)(v3 + 88);
  v10 = v6;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E0DEE9C0] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t))
{
  uint64_t v3;

  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, a2, *(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24), *(_QWORD **)(v3 + 32), *(_QWORD **)(v3 + 40), *(_QWORD *)(v3 + 48), a3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C450]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C448]);
}

void partial apply for closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(const double *a1, int a2)
{
  uint64_t v2;

  closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(a1, a2, *(double ***)(v2 + 24), *(_QWORD *)(v2 + 32), *(double *)(v2 + 16));
}

void partial apply for closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(const float *a1, int a2)
{
  uint64_t v2;

  closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(a1, a2, *(float ***)(v2 + 24), *(_QWORD *)(v2 + 32), *(float *)(v2 + 16));
}

{
  uint64_t v2;

  closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(a1, a2, *(float ***)(v2 + 24), *(_QWORD *)(v2 + 32), *(float *)(v2 + 16));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:)(a1, a2, *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C868]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C748]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C740]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:)(a1, a2, *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), (uint64_t (*)(uint64_t, uint64_t))MEMORY[0x1E0C8C8E8]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:)(a1, a2, *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), (uint64_t (*)(void))MEMORY[0x1E0C8C8E0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C950]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C940]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C378]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C368]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:)(a1, a2, *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C810]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:)(a1, a2, *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C800]);
}

uint64_t BNNS.ReductionFunction.bnnsReduceFunction.getter()
{
  int *v0;

  if (*((_BYTE *)v0 + 4) == 1)
    return dword_1CAB60F90[*v0];
  else
    return 8;
}

uint64_t static BNNS.applyReduction(_:input:output:weights:filterParameters:)(unsigned int *a1, _OWORD *a2, __int128 *a3, uint64_t a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v11;
  int v12;
  unsigned int v13;
  uint64_t v14;
  unsigned int v15;
  uint64_t v16;
  float v17;
  unint64_t v18;
  int *v19;
  uint64_t result;
  _BYTE *v21;
  _BYTE v26[136];
  int v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  _BYTE v31[136];
  _BYTE v32[136];
  uint64_t v33;

  v33 = *MEMORY[0x1E0C80C00];
  v11 = *a1;
  v12 = *((unsigned __int8 *)a1 + 4);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v27);
  outlined init with take of BNNS.Shape((uint64_t)&v27, (uint64_t)v31);
  v13 = _s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v31);
  if (v13 > 0x12)
    v14 = 8;
  else
    v14 = qword_1CAB60EF8[v13];
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v26);
  outlined init with take of BNNS.Shape((uint64_t)v26, (uint64_t)v32);
  v15 = _s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v32);
  if (v15 > 0x12)
    v16 = 8;
  else
    v16 = qword_1CAB60EF8[v15];
  if (v14 != v16)
    __break(1u);
  v17 = *(float *)&v11;
  if (v12)
    v17 = 0.0;
  if (a7 == 1)
  {
    LOBYTE(v27) = v12;
    v18 = v11 | ((unint64_t)v12 << 32);
    v19 = 0;
  }
  else
  {
    v27 = a5;
    v28 = a6;
    v29 = a7;
    v30 = a8;
    v26[0] = v12;
    v18 = v11 | ((unint64_t)v12 << 32);
    v19 = &v27;
  }
  result = closure #1 in static BNNS.applyReduction(_:input:output:weights:filterParameters:)((uint64_t)v19, a2, a3, a4, v18, v17);
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v21 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t closure #1 in static BNNS.applyReduction(_:input:output:weights:filterParameters:)(uint64_t a1, _OWORD *a2, __int128 *a3, uint64_t a4, uint64_t a5, float a6)
{
  int v6;
  uint64_t v13;
  uint64_t v14;
  int v15;
  uint64_t v16;
  unsigned int v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  uint64_t v34;
  uint64_t v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  _OWORD __src[22];
  _BYTE __dst[352];
  uint64_t v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  uint64_t v55;
  uint64_t v56;
  uint64_t v57;
  int v58;
  uint64_t v59;
  int v60;
  unsigned int v61;
  float v62;
  _BYTE v63[184];
  _BYTE v64[184];
  _BYTE v65[184];
  uint64_t v66;

  v66 = *MEMORY[0x1E0C80C00];
  outlined init with take of BNNSNDArrayDescriptor?(a4, (uint64_t)v63);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v63, (uint64_t)v65);
  outlined init with take of BNNSNDArrayDescriptor?(a4, (uint64_t)v64);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v64) != 1)
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v65, (uint64_t)__src);
    v35 = *(_QWORD *)&__src[0];
    v42 = *(_OWORD *)((char *)&__src[1] + 8);
    v43 = *(_OWORD *)((char *)__src + 8);
    v40 = *(_OWORD *)((char *)&__src[3] + 8);
    v41 = *(_OWORD *)((char *)&__src[2] + 8);
    v38 = *(_OWORD *)((char *)&__src[5] + 8);
    v39 = *(_OWORD *)((char *)&__src[4] + 8);
    v36 = *(_OWORD *)((char *)&__src[7] + 8);
    v37 = *(_OWORD *)((char *)&__src[6] + 8);
    v13 = *((_QWORD *)&__src[8] + 1);
    v14 = *((_QWORD *)&__src[9] + 1);
    v34 = *(_QWORD *)&__src[9];
    v15 = __src[10];
    v16 = *(_QWORD *)((char *)&__src[10] + 4);
    v6 = HIDWORD(__src[10]);
    if ((a5 & 0x100000000) != 0)
      goto LABEL_3;
LABEL_5:
    v17 = 8;
    goto LABEL_6;
  }
  v13 = 0;
  v14 = 0;
  v15 = 0;
  v42 = 0u;
  v43 = 0u;
  v16 = 0;
  v40 = 0u;
  v41 = 0u;
  v38 = 0u;
  v39 = 0u;
  v36 = 0u;
  v37 = 0u;
  v34 = 0;
  v35 = 0;
  if ((a5 & 0x100000000) == 0)
    goto LABEL_5;
LABEL_3:
  v17 = dword_1CAB60F90[(int)a5];
LABEL_6:
  v18 = a2[9];
  __src[8] = a2[8];
  __src[9] = v18;
  v19 = a2[5];
  __src[4] = a2[4];
  __src[5] = v19;
  v20 = a2[7];
  __src[6] = a2[6];
  __src[7] = v20;
  v21 = a2[1];
  __src[0] = *a2;
  __src[1] = v21;
  v22 = a2[3];
  __src[2] = a2[2];
  __src[3] = v22;
  v23 = a3[8];
  v24 = a3[9];
  v25 = a3[6];
  __src[18] = a3[7];
  __src[19] = v23;
  v26 = a3[10];
  __src[20] = v24;
  __src[21] = v26;
  v27 = a3[4];
  v28 = a3[5];
  v29 = a3[2];
  __src[14] = a3[3];
  __src[15] = v27;
  v30 = a2[10];
  __src[16] = v28;
  __src[17] = v25;
  v31 = *a3;
  v32 = a3[1];
  __src[10] = v30;
  __src[11] = v31;
  __src[12] = v32;
  __src[13] = v29;
  memcpy(__dst, __src, sizeof(__dst));
  v46 = v35;
  v47 = v43;
  v48 = v42;
  v49 = v41;
  v50 = v40;
  v51 = v39;
  v52 = v38;
  v53 = v37;
  v54 = v36;
  v55 = v13;
  v56 = v34;
  v57 = v14;
  v58 = v15;
  v59 = v16;
  v60 = v6;
  v61 = v17;
  v62 = a6;
  return MEMORY[0x1D1794540](__dst, a1);
}

uint64_t BNNS.ReductionLayer.__allocating_init(function:input:output:weights:filterParameters:)(int *a1, _OWORD *a2, __int128 *a3, uint64_t a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  int v8;
  uint64_t v14;
  int v15;
  float v16;
  uint64_t v17;
  uint64_t v18;
  int v19;
  uint64_t v20;
  unsigned int v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  int *v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v45;
  uint64_t v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  int v55;
  uint64_t v56;
  uint64_t v57;
  uint64_t v58;
  _OWORD __src[22];
  _BYTE __dst[352];
  uint64_t v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  __int128 v65;
  __int128 v66;
  __int128 v67;
  __int128 v68;
  __int128 v69;
  uint64_t v70;
  uint64_t v71;
  uint64_t v72;
  int v73;
  uint64_t v74;
  int v75;
  unsigned int v76;
  float v77;
  _BYTE v78[184];
  _BYTE v79[184];
  _BYTE v80[184];
  uint64_t v81;

  v81 = *MEMORY[0x1E0C80C00];
  outlined init with take of BNNSNDArrayDescriptor?(a4, (uint64_t)v78);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v78, (uint64_t)v80);
  v14 = *a1;
  v15 = *((unsigned __int8 *)a1 + 4);
  if (*((_BYTE *)a1 + 4))
    v16 = 0.0;
  else
    v16 = *(float *)a1;
  outlined init with take of BNNSNDArrayDescriptor?(a4, (uint64_t)v79);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v79) == 1)
  {
    v17 = 0;
    v18 = 0;
    v19 = 0;
    v53 = 0u;
    v54 = 0u;
    v20 = 0;
    v51 = 0u;
    v52 = 0u;
    v49 = 0u;
    v50 = 0u;
    v47 = 0u;
    v48 = 0u;
    v45 = 0;
    v46 = 0;
    if (!v15)
    {
LABEL_6:
      v21 = 8;
      goto LABEL_9;
    }
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v80, (uint64_t)__src);
    v46 = *(_QWORD *)&__src[0];
    v53 = *(_OWORD *)((char *)&__src[1] + 8);
    v54 = *(_OWORD *)((char *)__src + 8);
    v51 = *(_OWORD *)((char *)&__src[3] + 8);
    v52 = *(_OWORD *)((char *)&__src[2] + 8);
    v49 = *(_OWORD *)((char *)&__src[5] + 8);
    v50 = *(_OWORD *)((char *)&__src[4] + 8);
    v47 = *(_OWORD *)((char *)&__src[7] + 8);
    v48 = *(_OWORD *)((char *)&__src[6] + 8);
    v17 = *((_QWORD *)&__src[8] + 1);
    v18 = *((_QWORD *)&__src[9] + 1);
    v45 = *(_QWORD *)&__src[9];
    v20 = *(_QWORD *)((char *)&__src[10] + 4);
    v19 = __src[10];
    v8 = HIDWORD(__src[10]);
    if (!v15)
      goto LABEL_6;
  }
  v21 = dword_1CAB60F90[v14];
LABEL_9:
  v22 = a2[9];
  __src[8] = a2[8];
  __src[9] = v22;
  v23 = a2[5];
  __src[4] = a2[4];
  __src[5] = v23;
  v24 = a2[7];
  __src[6] = a2[6];
  __src[7] = v24;
  v25 = a2[1];
  __src[0] = *a2;
  __src[1] = v25;
  v26 = a2[3];
  __src[2] = a2[2];
  __src[3] = v26;
  v27 = a3[8];
  v28 = a3[9];
  v29 = a3[6];
  __src[18] = a3[7];
  __src[19] = v27;
  v30 = a3[10];
  __src[20] = v28;
  __src[21] = v30;
  v31 = a3[4];
  v32 = a3[5];
  v33 = a3[2];
  __src[14] = a3[3];
  __src[15] = v31;
  v34 = a2[10];
  __src[16] = v32;
  __src[17] = v29;
  v35 = *a3;
  v36 = a3[1];
  __src[10] = v34;
  __src[11] = v35;
  __src[12] = v36;
  __src[13] = v33;
  memcpy(__dst, __src, sizeof(__dst));
  v61 = v46;
  v62 = v54;
  v63 = v53;
  v64 = v52;
  v65 = v51;
  v66 = v50;
  v67 = v49;
  v68 = v48;
  v69 = v47;
  v70 = v17;
  v71 = v45;
  v72 = v18;
  v73 = v19;
  v74 = v20;
  v75 = v8;
  v76 = v21;
  v77 = v16;
  if (a7 == 1)
  {
    v37 = 0;
  }
  else
  {
    v55 = a5;
    v56 = a6;
    v57 = a7;
    v58 = a8;
    v37 = &v55;
  }
  v38 = MEMORY[0x1D1794624](__dst, v37);
  type metadata accessor for BNNS.ReductionLayer();
  v39 = swift_allocObject();
  v40 = v39;
  if (v38)
  {
    *(_QWORD *)(v39 + 16) = v38;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v40;
}

uint64_t BNNS.ReductionLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:)(size_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, _OWORD *a5, uint64_t a6)
{
  uint64_t v6;
  uint64_t v7;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  uint64_t result;
  _BYTE *v24;
  BNNSNDArrayDescriptor v25;
  BNNSNDArrayDescriptor v26;
  BNNSNDArrayDescriptor v27;
  BNNSNDArrayDescriptor v28;
  uint64_t v29;

  v7 = v6;
  v29 = *MEMORY[0x1E0C80C00];
  v13 = a4[9];
  *(_OWORD *)&v27.stride[7] = a4[8];
  *(_OWORD *)&v27.data_type = v13;
  *(_OWORD *)&v27.table_data_type = a4[10];
  v14 = a4[5];
  *(_OWORD *)&v27.size[7] = a4[4];
  *(_OWORD *)&v27.stride[1] = v14;
  v15 = a4[7];
  *(_OWORD *)&v27.stride[3] = a4[6];
  *(_OWORD *)&v27.stride[5] = v15;
  v16 = a4[1];
  *(_OWORD *)&v27.flags = *a4;
  *(_OWORD *)&v27.size[1] = v16;
  v17 = a4[3];
  *(_OWORD *)&v27.size[3] = a4[2];
  *(_OWORD *)&v27.size[5] = v17;
  v18 = a5[9];
  *(_OWORD *)&v26.stride[7] = a5[8];
  *(_OWORD *)&v26.data_type = v18;
  *(_OWORD *)&v26.table_data_type = a5[10];
  v19 = a5[5];
  *(_OWORD *)&v26.size[7] = a5[4];
  *(_OWORD *)&v26.stride[1] = v19;
  v20 = a5[7];
  *(_OWORD *)&v26.stride[3] = a5[6];
  *(_OWORD *)&v26.stride[5] = v20;
  v21 = a5[1];
  *(_OWORD *)&v26.flags = *a5;
  *(_OWORD *)&v26.size[1] = v21;
  v22 = a5[3];
  *(_OWORD *)&v26.size[3] = a5[2];
  *(_OWORD *)&v26.size[5] = v22;
  outlined init with take of BNNSNDArrayDescriptor?(a6, (uint64_t)&v28);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)&v28) == 1)
  {
    result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(0, v7, a1, a2, &v26, (uint64_t)a5, a3, &v27, (uint64_t)a4, 0);
  }
  else
  {
    v25 = v28;
    result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(0, v7, a1, a2, &v26, (uint64_t)a5, a3, &v27, (uint64_t)a4, &v25);
  }
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v24 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t type metadata accessor for BNNS.ReductionLayer()
{
  return objc_opt_self();
}

uint64_t BNNS.ReductionLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.ReductionLayer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

uint64_t method lookup function for BNNS.ReductionLayer()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of BNNS.ReductionLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5, uint64_t a6)
{
  uint64_t v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  uint64_t v35;
  int v36;
  uint64_t v37;
  int v38;
  uint64_t v39;
  int v40;
  uint64_t v41;
  int v42;
  uint64_t v43;
  int v44;
  uint64_t v45;
  int v46;
  uint64_t v47;
  int v48;
  uint64_t v49;
  int v50;
  char v51;
  uint64_t (*v52)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, _OWORD *);
  _OWORD v54[11];
  char v55;
  uint64_t v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  __int128 v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  uint64_t v65;
  int v66;
  uint64_t v67;
  int v68;
  uint64_t v69;
  uint64_t v70;
  __int128 v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  __int128 v76;
  __int128 v77;
  __int128 v78;
  uint64_t v79;
  int v80;
  uint64_t v81;
  int v82;
  uint64_t v83;
  uint64_t v84;
  __int128 v85;
  __int128 v86;
  __int128 v87;
  __int128 v88;
  __int128 v89;
  __int128 v90;
  __int128 v91;
  __int128 v92;
  uint64_t v93;
  int v94;
  uint64_t v95;
  int v96;
  uint64_t v97;
  uint64_t v98;
  __int128 v99;
  __int128 v100;
  __int128 v101;
  __int128 v102;
  __int128 v103;
  __int128 v104;
  __int128 v105;
  __int128 v106;
  uint64_t v107;
  int v108;
  uint64_t v109;
  int v110;
  uint64_t v111;

  v98 = *a2;
  v99 = *(_OWORD *)(a2 + 1);
  v100 = *(_OWORD *)(a2 + 3);
  v101 = *(_OWORD *)(a2 + 5);
  v102 = *(_OWORD *)(a2 + 7);
  v103 = *(_OWORD *)(a2 + 9);
  v104 = *(_OWORD *)(a2 + 11);
  v105 = *(_OWORD *)(a2 + 13);
  v106 = *(_OWORD *)(a2 + 15);
  v111 = *(uint64_t *)((char *)a2 + 164);
  v84 = *a3;
  v7 = *(_OWORD *)(a3 + 3);
  v85 = *(_OWORD *)(a3 + 1);
  v8 = *(_OWORD *)(a3 + 5);
  v86 = v7;
  v9 = *(_OWORD *)(a3 + 7);
  v87 = v8;
  v10 = *(_OWORD *)(a3 + 9);
  v88 = v9;
  v11 = *(_OWORD *)(a3 + 11);
  v89 = v10;
  v12 = *(_OWORD *)(a3 + 13);
  v90 = v11;
  v13 = *(_OWORD *)(a3 + 15);
  v91 = v12;
  v14 = *(_OWORD *)(a4 + 1);
  v92 = v13;
  v97 = *(uint64_t *)((char *)a3 + 164);
  v70 = *a4;
  v15 = *(_OWORD *)(a4 + 3);
  v71 = v14;
  v16 = *(_OWORD *)(a4 + 5);
  v72 = v15;
  v17 = *(_OWORD *)(a4 + 7);
  v73 = v16;
  v18 = *(_OWORD *)(a4 + 9);
  v74 = v17;
  v19 = *(_OWORD *)(a4 + 11);
  v75 = v18;
  v20 = *(_OWORD *)(a4 + 13);
  v76 = v19;
  v21 = *(_OWORD *)(a4 + 15);
  v77 = v20;
  v22 = *(_OWORD *)(a5 + 1);
  v78 = v21;
  v83 = *(uint64_t *)((char *)a4 + 164);
  v56 = *a5;
  v23 = *(_OWORD *)(a5 + 3);
  v57 = v22;
  v24 = *(_OWORD *)(a5 + 5);
  v58 = v23;
  v25 = *(_OWORD *)(a5 + 7);
  v59 = v24;
  v26 = *(_OWORD *)(a5 + 9);
  v60 = v25;
  v27 = *(_OWORD *)(a5 + 11);
  v61 = v26;
  v28 = *(_OWORD *)(a5 + 13);
  v62 = v27;
  v29 = *(_OWORD *)(a5 + 15);
  v63 = v28;
  *(_QWORD *)&v28 = *(uint64_t *)((char *)a5 + 164);
  v64 = v29;
  v69 = v28;
  v30 = *(_OWORD *)(a6 + 16);
  v54[0] = *(_OWORD *)a6;
  v54[1] = v30;
  v31 = *(_OWORD *)(a6 + 48);
  v54[2] = *(_OWORD *)(a6 + 32);
  v54[3] = v31;
  v32 = *(_OWORD *)(a6 + 80);
  v54[4] = *(_OWORD *)(a6 + 64);
  v54[5] = v32;
  v33 = *(_OWORD *)(a6 + 112);
  v54[6] = *(_OWORD *)(a6 + 96);
  v54[7] = v33;
  v34 = *(_OWORD *)(a6 + 144);
  v54[8] = *(_OWORD *)(a6 + 128);
  v54[9] = v34;
  v54[10] = *(_OWORD *)(a6 + 160);
  v35 = a2[17];
  v36 = *((_DWORD *)a2 + 36);
  v37 = a2[19];
  v38 = *((_DWORD *)a2 + 40);
  v39 = a3[17];
  v40 = *((_DWORD *)a3 + 36);
  v41 = a3[19];
  v42 = *((_DWORD *)a3 + 40);
  v43 = a4[17];
  v44 = *((_DWORD *)a4 + 36);
  v45 = a4[19];
  v46 = *((_DWORD *)a4 + 40);
  v47 = a5[17];
  v48 = *((_DWORD *)a5 + 36);
  v49 = a5[19];
  v50 = *((_DWORD *)a5 + 40);
  v51 = *(_BYTE *)(a6 + 176);
  v52 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, _OWORD *))(*(_QWORD *)v6 + 112);
  v107 = v35;
  v108 = v36;
  v109 = v37;
  v110 = v38;
  v93 = v39;
  v94 = v40;
  v95 = v41;
  v96 = v42;
  v79 = v43;
  v80 = v44;
  v81 = v45;
  v82 = v46;
  v65 = v47;
  v66 = v48;
  v67 = v49;
  v68 = v50;
  v55 = v51;
  return v52(a1, &v98, &v84, &v70, &v56, v54);
}

uint64_t getEnumTagSinglePayload for BNNS.ReductionFunction(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 5))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for BNNS.ReductionFunction(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_BYTE *)(result + 4) = 0;
    *(_DWORD *)result = a2 - 1;
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 5) = v3;
  return result;
}

uint64_t getEnumTag for BNNS.ReductionFunction(uint64_t a1)
{
  if (*(_BYTE *)(a1 + 4))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t destructiveInjectEnumTag for BNNS.ReductionFunction(uint64_t result, int a2)
{
  if (a2)
  {
    *(_DWORD *)result = a2 - 1;
    *(_BYTE *)(result + 4) = 1;
  }
  else
  {
    *(_BYTE *)(result + 4) = 0;
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.ReductionFunction()
{
  return &type metadata for BNNS.ReductionFunction;
}

vImage_Error vImage_Buffer.copy(destinationBuffer:pixelSize:flags:)(vImage_Buffer *dest, size_t pixelSize, vImage_Flags *a3, void *a4, vImagePixelCount a5, vImagePixelCount a6, size_t a7)
{
  vImage_Flags v7;
  vImage_Error result;
  uint64_t v9;
  char *v10;
  char *v11;
  char data;
  _BYTE *v13;
  vImage_Buffer src;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  if ((a6 & 0x8000000000000000) != 0)
    __break(1u);
  if (a6)
  {
    v7 = *a3;
    src.data = a4;
    src.height = a5;
    src.width = a6;
    src.rowBytes = a7;
    result = vImageCopyBuffer(&src, dest, pixelSize, v7);
    if (!result)
      return result;
    v9 = result;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v11 = v10;
    vImage.Error.init(rawValue:)(v9, (char *)&src);
    data = (char)src.data;
    if (LOBYTE(src.data) == 20)
      data = 11;
    *v11 = data;
  }
  else
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    *v13 = 8;
  }
  return swift_willThrow();
}

double vImage_Buffer.size.getter(void *a1, vImagePixelCount a2, vImagePixelCount a3, size_t a4)
{
  double result;
  vImage_Buffer buf;
  uint64_t v6;

  v6 = *MEMORY[0x1E0C80C00];
  buf.data = a1;
  buf.height = a2;
  buf.width = a3;
  buf.rowBytes = a4;
  *(_QWORD *)&result = *(_OWORD *)&vImageBuffer_GetSize(&buf);
  return result;
}

void *vImage_Buffer.init(size:bitsPerPixel:)(uint32_t a1, double a2, double a3)
{
  uint64_t v3;
  void *result;
  uint64_t v5;

  result = specialized vImage_Buffer.init(size:bitsPerPixel:)(a1, a2, a3);
  if (v3)
    return (void *)v5;
  return result;
}

void *vImage_Buffer.init(width:height:bitsPerPixel:)(vImagePixelCount a1, vImagePixelCount a2, uint32_t a3)
{
  uint64_t v3;
  void *result;
  uint64_t v5;

  result = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(a1, a2, a3);
  if (v3)
    return (void *)v5;
  return result;
}

vImage_Error static vImage_Buffer.preferredAlignmentAndRowBytes(width:height:bitsPerPixel:)(vImagePixelCount width, vImagePixelCount a2, uint32_t pixelBits)
{
  vImage_Error v3;
  _BYTE *v4;
  char *v5;
  char *v6;
  char v7;
  char v9;
  vImage_Buffer buf;
  uint64_t v11;

  v11 = *MEMORY[0x1E0C80C00];
  if ((width & 0x8000000000000000) != 0 || (a2 & 0x8000000000000000) != 0)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    *v4 = 8;
  }
  else
  {
    memset(&buf, 0, sizeof(buf));
    v3 = vImageBuffer_Init(&buf, a2, width, pixelBits, 0x200u);
    if ((v3 & 0x8000000000000000) == 0)
      return v3;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v6 = v5;
    vImage.Error.init(rawValue:)(v3, &v9);
    v7 = v9;
    if (v9 == 20)
      v7 = 11;
    *v6 = v7;
  }
  swift_willThrow();
  return v3;
}

void vImage_Buffer.init(cgImage:flags:)(void *a1, vImage_Flags *a2)
{
  vImage_Flags v2;
  CGImage *v3;
  _BYTE *v4;
  vImage_Error v5;
  uint64_t v6;
  char *v7;
  char *v8;
  char v9;
  char v10;
  vImage_CGImageFormat format;
  vImage_Buffer buf;
  size_t v13[5];
  vImage_CGImageFormat v14;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v2 = *a2;
  memset(&buf, 0, sizeof(buf));
  v3 = a1;
  specialized vImage_CGImageFormat.init(cgImage:)(v3, v13);
  outlined init with take of vImage_CGImageFormat?((uint64_t)v13, (uint64_t)&v14);
  if (v14.colorSpace == (CGColorSpaceRef)1)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    *v4 = 13;
LABEL_7:
    swift_willThrow();

    return;
  }
  format = v14;
  v5 = vImageBuffer_InitWithCGImage(&buf, &format, 0, v3, v2);
  if (v5)
  {
    v6 = v5;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v8 = v7;
    vImage.Error.init(rawValue:)(v6, &v10);
    v9 = v10;
    if (v10 == 20)
      v9 = 11;
    *v8 = v9;
    goto LABEL_7;
  }

}

void vImage_Buffer.init(cgImage:format:flags:)(CGImageRef image, uint64_t a2, vImage_Flags *a3)
{
  vImage_Flags v4;
  __int128 v5;
  vImage_Error v6;
  uint64_t v7;
  char *v8;
  char *v9;
  char v10;
  char v11;
  vImage_CGImageFormat format;
  vImage_Buffer buf;
  uint64_t v14;

  v14 = *MEMORY[0x1E0C80C00];
  v4 = *a3;
  memset(&buf, 0, sizeof(buf));
  v5 = *(_OWORD *)(a2 + 16);
  *(_OWORD *)&format.bitsPerComponent = *(_OWORD *)a2;
  *(_OWORD *)&format.bitmapInfo = v5;
  *(_QWORD *)&format.renderingIntent = *(_QWORD *)(a2 + 32);
  v6 = vImageBuffer_InitWithCGImage(&buf, &format, 0, image, v4);
  if (v6)
  {
    v7 = v6;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v9 = v8;
    vImage.Error.init(rawValue:)(v7, &v11);
    v10 = v11;
    if (v11 == 20)
      v10 = 11;
    *v9 = v10;
    swift_willThrow();

  }
  else
  {

  }
}

CGImageRef vImage_Buffer.createCGImage(format:flags:)(uint64_t a1, vImage_Flags *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, size_t a6)
{
  vImage_Flags v6;
  __int128 v7;
  CGImageRef v8;
  CGImageRef v9;
  vImage_Error v10;
  char *v11;
  char *v12;
  char data;
  vImage_Error error;
  vImage_Buffer buf;
  vImage_CGImageFormat format;
  uint64_t v18;

  v18 = *MEMORY[0x1E0C80C00];
  v6 = *a2;
  v7 = *(_OWORD *)(a1 + 16);
  *(_OWORD *)&format.bitsPerComponent = *(_OWORD *)a1;
  *(_OWORD *)&format.bitmapInfo = v7;
  *(_QWORD *)&format.renderingIntent = *(_QWORD *)(a1 + 32);
  error = 0;
  buf.data = a3;
  buf.height = a4;
  buf.width = a5;
  buf.rowBytes = a6;
  v8 = vImageCreateCGImageFromBuffer(&buf, &format, 0, 0, v6, &error);
  if (!v8)
    __break(1u);
  v9 = v8;
  v10 = error;
  if (error)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v12 = v11;
    vImage.Error.init(rawValue:)(v10, (char *)&buf);
    data = (char)buf.data;
    if (LOBYTE(buf.data) == 20)
      data = 11;
    *v12 = data;
    swift_willThrow();

  }
  return v9;
}

void *specialized vImage_Buffer.init(width:height:bitsPerPixel:)(vImagePixelCount width, vImagePixelCount a2, uint32_t pixelBits)
{
  vImage_Error v3;
  _BYTE *v5;
  uint64_t v6;
  char *v7;
  char *v8;
  char v9;
  char v10;
  vImage_Buffer buf;
  uint64_t v12;

  v12 = *MEMORY[0x1E0C80C00];
  if ((width & 0x8000000000000000) != 0 || (a2 & 0x8000000000000000) != 0)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    *v5 = 8;
  }
  else
  {
    memset(&buf, 0, sizeof(buf));
    v3 = vImageBuffer_Init(&buf, a2, width, pixelBits, 0);
    if ((v3 & 0x8000000000000000) == 0)
      return buf.data;
    v6 = v3;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v8 = v7;
    vImage.Error.init(rawValue:)(v6, &v10);
    v9 = v10;
    if (v10 == 20)
      v9 = 11;
    *v8 = v9;
  }
  return (void *)swift_willThrow();
}

void *specialized vImage_Buffer.init(size:bitsPerPixel:)(uint32_t a1, double a2, double a3)
{
  vImagePixelCount v5;
  vImage_Error v6;
  _BYTE *v8;
  uint64_t v9;
  char *v10;
  char *v11;
  char v12;
  vImagePixelCount height;
  char v14;
  vImage_Buffer buf;
  uint64_t v16;

  v16 = *MEMORY[0x1E0C80C00];
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&height, a2)
    || v14 == 1
    || (v5 = height, (uint64_t)height < 1)
    || !specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&height, a3)
    || v14 == 1
    || (uint64_t)height < 1)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    *v8 = 8;
  }
  else
  {
    memset(&buf, 0, sizeof(buf));
    v6 = vImageBuffer_Init(&buf, height, v5, a1, 0);
    if ((v6 & 0x8000000000000000) == 0)
      return buf.data;
    v9 = v6;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v11 = v10;
    vImage.Error.init(rawValue:)(v9, (char *)&height);
    v12 = height;
    if (height == 20)
      v12 = 11;
    *v11 = v12;
  }
  return (void *)swift_willThrow();
}

uint64_t outlined init with take of vImage_CGImageFormat?(uint64_t a1, uint64_t a2)
{
  uint64_t v4;

  v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for vImage_CGImageFormat?);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(_QWORD *)(v4 - 8) + 32))(a2, a1, v4);
  return a2;
}

uint64_t vImage_PerpsectiveTransform.Interpolation.vImageWarpInterpolation.getter()
{
  unsigned __int8 *v0;

  return *v0;
}

BOOL static vImage_PerpsectiveTransform.Interpolation.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void vImage_PerpsectiveTransform.Interpolation.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

Swift::Int vImage_PerpsectiveTransform.Interpolation.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

void __swiftcall vImage_PerpsectiveTransform.init(source:destination:)(__C::vImage_PerpsectiveTransform_optional *__return_ptr retstr, Swift::tuple_CGPoint_CGPoint_CGPoint_CGPoint *source, Swift::tuple_CGPoint_CGPoint_CGPoint_CGPoint *destination)
{
  double v3;
  double v4;
  double v5;
  double v6;
  double v7;
  double v8;
  double v9;
  double v10;
  float v12;
  float v13;
  float v14;
  float v15;
  float v16;
  float v17;
  float v18;
  float v19;
  float v20;
  float v21;
  float v22;
  float v23;
  float v24;
  float v25;
  float v26;
  float v27;
  vImage_Error PerspectiveWarp;
  int64x2_t v29;
  int64x2_t v30;
  int8x16_t v31;
  int8x16_t v32;
  int8x16_t v33;
  Swift::Float v34;
  _DWORD v35[42];
  uint64_t v36;
  double v37;
  double v38;
  double v39;
  double v40;
  double v41;
  double v42;
  double v43;
  double v44;

  v36 = *MEMORY[0x1E0C80C00];
  v12 = v3;
  memset(v35, 0, 36);
  v13 = v4;
  *(float *)&v35[34] = v12;
  *(float *)&v35[35] = v13;
  v14 = v5;
  v15 = v6;
  *(float *)&v35[36] = v14;
  *(float *)&v35[37] = v15;
  v16 = v7;
  v17 = v8;
  *(float *)&v35[38] = v16;
  *(float *)&v35[39] = v17;
  v18 = v9;
  v19 = v10;
  *(float *)&v35[40] = v18;
  *(float *)&v35[41] = v19;
  v20 = v37;
  v21 = v38;
  *(float *)&v35[18] = v20;
  *(float *)&v35[19] = v21;
  v22 = v39;
  v23 = v40;
  *(float *)&v35[20] = v22;
  *(float *)&v35[21] = v23;
  v24 = v41;
  v25 = v42;
  *(float *)&v35[22] = v24;
  *(float *)&v35[23] = v25;
  v26 = v43;
  v27 = v44;
  *(float *)&v35[24] = v26;
  *(float *)&v35[25] = v27;
  PerspectiveWarp = vImageGetPerspectiveWarp((const float (*)[2])&v35[34], (const float (*)[2])&v35[18], (vImage_PerpsectiveTransform *)v35, 0);
  v29.i64[0] = 0;
  v30.i64[0] = PerspectiveWarp;
  v31 = (int8x16_t)vdupq_lane_s64(vceqq_s64(v30, v29).i64[0], 0);
  v32 = vandq_s8(*(int8x16_t *)&v35[4], v31);
  v33 = vandq_s8(*(int8x16_t *)v35, v31);
  v34 = *(float *)&v35[8];
  if (PerspectiveWarp)
    v34 = 0.0;
  *(int8x16_t *)&retstr->value.a = v33;
  *(int8x16_t *)&retstr->value.tx = v32;
  retstr->value.v = v34;
  retstr->is_nil = PerspectiveWarp != 0;
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance vImage_PerpsectiveTransform(uint64_t a1, uint64_t a2)
{
  __int128 v2;
  __int128 v3;
  _OWORD v5[2];
  int v6;
  _OWORD v7[2];
  int v8;

  v2 = *(_OWORD *)(a1 + 16);
  v5[0] = *(_OWORD *)a1;
  v5[1] = v2;
  v6 = *(_DWORD *)(a1 + 32);
  v3 = *(_OWORD *)(a2 + 16);
  v7[0] = *(_OWORD *)a2;
  v7[1] = v3;
  v8 = *(_DWORD *)(a2 + 32);
  return specialized static vImage_PerpsectiveTransform.== infix(_:_:)((float *)v5, (float *)v7);
}

vImage_Error vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(vImage_Error result, unsigned __int8 *a2, Pixel_8 a3, uint64_t *a4)
{
  uint64_t *v4;
  uint64_t v5;
  uint64_t v6;
  void *v7;
  void *v8;
  BOOL v9;
  size_t v10;
  size_t v11;
  __int128 v12;
  vImage_PerpsectiveTransform v13;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v16;

  v16 = *MEMORY[0x1E0C80C00];
  v5 = *v4;
  if (!*(_QWORD *)(*v4 + 16))
  {
    __break(1u);
LABEL_13:
    __break(1u);
  }
  v6 = *a4;
  if (!*(_QWORD *)(*a4 + 16))
    goto LABEL_13;
  v7 = *(void **)(v5 + 32);
  v8 = *(void **)(v6 + 32);
  if (v7)
  {
    if (v8)
      v9 = v7 == v8;
    else
      v9 = 0;
    if (!v9)
      goto LABEL_11;
    __break(1u);
  }
  if (v8)
  {
LABEL_11:
    v10 = *(_QWORD *)(v5 + 56);
    src.data = v7;
    *(_OWORD *)&src.height = *(_OWORD *)(v5 + 40);
    src.rowBytes = v10;
    v11 = *(_QWORD *)(v6 + 56);
    dest.data = v8;
    *(_OWORD *)&dest.height = *(_OWORD *)(v6 + 40);
    dest.rowBytes = v11;
    v12 = *(_OWORD *)(result + 16);
    *(_OWORD *)&v13.a = *(_OWORD *)result;
    *(_OWORD *)&v13.tx = v12;
    v13.v = *(float *)(result + 32);
    return vImagePerspectiveWarp_Planar8(&src, &dest, 0, &v13, *a2, a3, 4u);
  }
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(uint64_t a1, unsigned __int8 *a2, unsigned __int16 a3, uint64_t *a4)
{
  return vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(a1, a2, a3, a4, MEMORY[0x1E0C8D558]);
}

{
  return vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(a1, a2, a3, a4, MEMORY[0x1E0C8D560]);
}

uint64_t vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(uint64_t result, unsigned __int8 *a2, unsigned __int16 a3, uint64_t *a4, uint64_t (*a5)(uint64_t *, uint64_t *, _QWORD, _OWORD *, _QWORD, _QWORD, uint64_t))
{
  uint64_t *v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  BOOL v10;
  uint64_t v11;
  uint64_t v12;
  __int128 v13;
  _OWORD v14[2];
  int v15;
  uint64_t v16;
  __int128 v17;
  uint64_t v18;
  uint64_t v19;
  __int128 v20;
  uint64_t v21;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  v6 = *v5;
  if (!*(_QWORD *)(*v5 + 16))
  {
    __break(1u);
LABEL_13:
    __break(1u);
  }
  v7 = *a4;
  if (!*(_QWORD *)(*a4 + 16))
    goto LABEL_13;
  v8 = *(_QWORD *)(v6 + 32);
  v9 = *(_QWORD *)(v7 + 32);
  if (v8)
  {
    if (v9)
      v10 = v8 == v9;
    else
      v10 = 0;
    if (!v10)
      goto LABEL_11;
    __break(1u);
  }
  if (v9)
  {
LABEL_11:
    v11 = *(_QWORD *)(v6 + 56);
    v19 = v8;
    v20 = *(_OWORD *)(v6 + 40);
    v21 = v11;
    v12 = *(_QWORD *)(v7 + 56);
    v16 = v9;
    v17 = *(_OWORD *)(v7 + 40);
    v18 = v12;
    v13 = *(_OWORD *)(result + 16);
    v14[0] = *(_OWORD *)result;
    v14[1] = v13;
    v15 = *(_DWORD *)(result + 32);
    return a5(&v19, &v16, 0, v14, *a2, a3, 4);
  }
  __break(1u);
  return result;
}

vImage_Error vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(vImage_Error result, unsigned __int8 *a2, uint8_t a3, uint8_t a4, uint8_t a5, uint8_t a6, uint64_t *a7)
{
  uint64_t *v7;
  uint64_t v8;
  uint64_t v9;
  void *v10;
  void *v11;
  BOOL v12;
  size_t v13;
  size_t v14;
  __int128 v15;
  vImage_PerpsectiveTransform v16;
  uint8_t v17[8];
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v20;

  v20 = *MEMORY[0x1E0C80C00];
  v8 = *v7;
  if (!*(_QWORD *)(*v7 + 16))
  {
    __break(1u);
LABEL_13:
    __break(1u);
  }
  v9 = *a7;
  if (!*(_QWORD *)(*a7 + 16))
    goto LABEL_13;
  v10 = *(void **)(v8 + 32);
  v11 = *(void **)(v9 + 32);
  if (v10)
  {
    if (v11)
      v12 = v10 == v11;
    else
      v12 = 0;
    if (!v12)
      goto LABEL_11;
    __break(1u);
  }
  if (v11)
  {
LABEL_11:
    v13 = *(_QWORD *)(v8 + 56);
    *(_OWORD *)&src.height = *(_OWORD *)(v8 + 40);
    src.rowBytes = v13;
    v14 = *(_QWORD *)(v9 + 56);
    dest.data = v11;
    *(_OWORD *)&dest.height = *(_OWORD *)(v9 + 40);
    dest.rowBytes = v14;
    src.data = v10;
    v15 = *(_OWORD *)(result + 16);
    *(_OWORD *)&v16.a = *(_OWORD *)result;
    *(_OWORD *)&v16.tx = v15;
    v16.v = *(float *)(result + 32);
    v17[0] = a3;
    v17[1] = a4;
    v17[2] = a5;
    v17[3] = a6;
    return vImagePerspectiveWarp_ARGB8888(&src, &dest, 0, &v16, *a2, v17, 4u);
  }
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(uint64_t a1, unsigned __int8 *a2, __int16 a3, __int16 a4, __int16 a5, __int16 a6, uint64_t *a7)
{
  return vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(a1, a2, a3, a4, a5, a6, a7, MEMORY[0x1E0C8D540]);
}

{
  return vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(a1, a2, a3, a4, a5, a6, a7, MEMORY[0x1E0C8D548]);
}

uint64_t vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(uint64_t result, unsigned __int8 *a2, __int16 a3, __int16 a4, __int16 a5, __int16 a6, uint64_t *a7, uint64_t (*a8)(uint64_t *, uint64_t *, _QWORD, _OWORD *, _QWORD, _WORD *, uint64_t))
{
  uint64_t *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  BOOL v13;
  uint64_t v14;
  uint64_t v15;
  __int128 v16;
  _OWORD v17[2];
  int v18;
  _WORD v19[4];
  uint64_t v20;
  __int128 v21;
  uint64_t v22;
  uint64_t v23;
  __int128 v24;
  uint64_t v25;
  uint64_t v26;

  v26 = *MEMORY[0x1E0C80C00];
  v9 = *v8;
  if (!*(_QWORD *)(*v8 + 16))
  {
    __break(1u);
LABEL_13:
    __break(1u);
  }
  v10 = *a7;
  if (!*(_QWORD *)(*a7 + 16))
    goto LABEL_13;
  v11 = *(_QWORD *)(v9 + 32);
  v12 = *(_QWORD *)(v10 + 32);
  if (v11)
  {
    if (v12)
      v13 = v11 == v12;
    else
      v13 = 0;
    if (!v13)
      goto LABEL_11;
    __break(1u);
  }
  if (v12)
  {
LABEL_11:
    v14 = *(_QWORD *)(v9 + 56);
    v24 = *(_OWORD *)(v9 + 40);
    v25 = v14;
    v15 = *(_QWORD *)(v10 + 56);
    v20 = v12;
    v21 = *(_OWORD *)(v10 + 40);
    v22 = v15;
    v23 = v11;
    v16 = *(_OWORD *)(result + 16);
    v17[0] = *(_OWORD *)result;
    v17[1] = v16;
    v18 = *(_DWORD *)(result + 32);
    v19[0] = a3;
    v19[1] = a4;
    v19[2] = a5;
    v19[3] = a6;
    return a8(&v23, &v20, 0, v17, *a2, v19, 4);
  }
  __break(1u);
  return result;
}

BOOL specialized static vImage_PerpsectiveTransform.== infix(_:_:)(float *a1, float *a2)
{
  return *a1 == *a2
      && a1[1] == a2[1]
      && a1[2] == a2[2]
      && a1[3] == a2[3]
      && a1[4] == a2[4]
      && a1[5] == a2[5]
      && a1[8] == a2[8]
      && a1[6] == a2[6]
      && a1[7] == a2[7];
}

unint64_t lazy protocol witness table accessor for type vImage_PerpsectiveTransform.Interpolation and conformance vImage_PerpsectiveTransform.Interpolation()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImage_PerpsectiveTransform.Interpolation and conformance vImage_PerpsectiveTransform.Interpolation;
  if (!lazy protocol witness table cache variable for type vImage_PerpsectiveTransform.Interpolation and conformance vImage_PerpsectiveTransform.Interpolation)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImage_PerpsectiveTransform.Interpolation, &type metadata for vImage_PerpsectiveTransform.Interpolation);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage_PerpsectiveTransform.Interpolation and conformance vImage_PerpsectiveTransform.Interpolation);
  }
  return result;
}

uint64_t storeEnumTagSinglePayload for vImage_PerpsectiveTransform.Interpolation(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 1 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 1) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFF)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFE)
    return ((uint64_t (*)(void))((char *)&loc_1CAB219AC + 4 * byte_1CAB60FE1[v4]))();
  *a1 = a2 + 1;
  return ((uint64_t (*)(void))((char *)sub_1CAB219E0 + 4 * byte_1CAB60FDC[v4]))();
}

uint64_t sub_1CAB219E0(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB219E8(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB219F0);
  return result;
}

uint64_t sub_1CAB219FC(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB21A04);
  *(_BYTE *)result = a2 + 1;
  return result;
}

uint64_t sub_1CAB21A08(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB21A10(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vImage_PerpsectiveTransform.Interpolation()
{
  return &type metadata for vImage_PerpsectiveTransform.Interpolation;
}

uint64_t vImage.PixelBuffer<>.init(lumaSource:chromaSource:conversionInfo:)@<X0>(uint64_t *a1@<X0>, uint64_t *a2@<X1>, _OWORD *a3@<X2>, uint64_t *a4@<X8>)
{
  vImagePixelCount v4;
  uint64_t v6;
  vImagePixelCount v7;
  uint64_t v9;
  uint64_t v10;
  void *v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  _QWORD *v18;
  uint64_t result;
  uint64_t v20;
  _QWORD v21[3];

  v6 = *a1;
  if (!*(_QWORD *)(*a1 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  v7 = *(_QWORD *)(v6 + 48);
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v4 = *(_QWORD *)(v6 + 40);
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (!v7)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (v4)
  {
    v9 = *a2;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    v10 = swift_allocObject();
    *(_OWORD *)(v10 + 16) = xmmword_1CAB5E430;
    v11 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v7, v4, 0x20u);
    v13 = v12;
    v15 = v14;
    v17 = v16;
    type metadata accessor for vImage.BufferReference();
    v18 = (_QWORD *)swift_allocObject();
    v18[2] = v11;
    v18[3] = v13;
    v18[4] = v15;
    v18[5] = v17;
    *(_QWORD *)(v10 + 32) = v11;
    *(_QWORD *)(v10 + 40) = v13;
    *(_QWORD *)(v10 + 48) = v15;
    *(_QWORD *)(v10 + 56) = v17;
    *(_QWORD *)(v10 + 64) = v18;
    v21[0] = v6;
    v21[1] = v10;
    v20 = v9;
    vImage.PixelBuffer<>.convert(lumaSource:chromaSource:conversionInfo:)((uint64_t)v21, &v20, a3);
    swift_bridgeObjectRelease();
    result = swift_bridgeObjectRelease();
    *a4 = v10;
    return result;
  }
LABEL_11:
  __break(1u);

  result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

vImage_Error vImage.PixelBuffer<>.convert(lumaSource:chromaSource:conversionInfo:)(uint64_t a1, uint64_t *a2, _OWORD *a3)
{
  uint64_t v3;
  _QWORD *v4;
  vImagePixelCount v5;
  _QWORD *v6;
  vImagePixelCount v7;
  vImagePixelCount v8;
  vImagePixelCount v9;
  uint64_t v10;
  vImagePixelCount v11;
  vImagePixelCount v12;
  void *v13;
  size_t v14;
  size_t v15;
  void *v16;
  size_t v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  vImage_YpCbCrToARGB v23;
  vImage_Buffer srcYp;
  vImage_Buffer srcCbCr;
  vImage_Buffer dest;
  uint64_t v27;

  v27 = *MEMORY[0x1E0C80C00];
  v4 = *(_QWORD **)a1;
  if (!*(_QWORD *)(*(_QWORD *)a1 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v6 = *(_QWORD **)v3;
  if (!*(_QWORD *)(*(_QWORD *)v3 + 16))
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v7 = v6[6];
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (v7 < v5)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v8 = v4[5];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v9 = v6[5];
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (v9 < v8)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v10 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v11 = *(_QWORD *)(v10 + 48);
  if ((v11 & 0x8000000000000000) != 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (v7 < v11)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  v12 = *(_QWORD *)(v10 + 40);
  if ((v12 & 0x8000000000000000) != 0)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v9 < v12)
    goto LABEL_27;
  v13 = (void *)v6[4];
  v14 = v6[7];
  dest.data = v13;
  dest.height = v9;
  dest.width = v7;
  dest.rowBytes = v14;
  v15 = *(_QWORD *)(v10 + 56);
  srcCbCr.data = *(void **)(v10 + 32);
  srcCbCr.height = v12;
  srcCbCr.width = v11;
  srcCbCr.rowBytes = v15;
  v16 = (void *)v4[4];
  v17 = v4[7];
  srcYp.data = v16;
  srcYp.height = v8;
  srcYp.width = v5;
  srcYp.rowBytes = v17;
  v18 = a3[5];
  *(_OWORD *)&v23.opaque[64] = a3[4];
  *(_OWORD *)&v23.opaque[80] = v18;
  v19 = a3[7];
  *(_OWORD *)&v23.opaque[96] = a3[6];
  *(_OWORD *)&v23.opaque[112] = v19;
  v20 = a3[1];
  *(_OWORD *)v23.opaque = *a3;
  *(_OWORD *)&v23.opaque[16] = v20;
  v21 = a3[3];
  *(_OWORD *)&v23.opaque[32] = a3[2];
  *(_OWORD *)&v23.opaque[48] = v21;
  return vImageConvert_420Yp8_CbCr8ToARGB8888(&srcYp, &srcCbCr, &dest, &v23, 0, 0xFFu, 0);
}

void vImage.PixelBuffer<>.withCVPixelBuffer(readOnly:body:)(char a1, void (*a2)(void))
{
  uint64_t v2;
  _QWORD *v3;
  size_t v5;
  size_t v6;
  void *v7;
  CVPixelBufferRef v9;
  CVPixelBufferLockFlags v10;
  __CVBuffer *v11;
  CVPixelBufferRef pixelBuffer[2];

  pixelBuffer[1] = *(CVPixelBufferRef *)MEMORY[0x1E0C80C00];
  pixelBuffer[0] = 0;
  v3 = *(_QWORD **)v2;
  if (!*(_QWORD *)(*(_QWORD *)v2 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  v5 = v3[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v6 = v3[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  v7 = (void *)v3[4];
  if (!v7)
  {
LABEL_10:
    __break(1u);
LABEL_11:
    __break(1u);
  }
  CVPixelBufferCreateWithBytes(0, v5, v6, 0x42475241u, v7, v3[7], 0, 0, 0, pixelBuffer);
  v9 = pixelBuffer[0];
  if (!pixelBuffer[0])
    goto LABEL_11;
  v10 = a1 & 1;
  CVPixelBufferLockBaseAddress(pixelBuffer[0], v10);
  v11 = v9;
  a2();

  CVPixelBufferUnlockBaseAddress(v11, v10);
}

uint64_t vImage.PixelBuffer<>.rotate(_:backgroundColor:destination:)(unsigned int *a1, __int16 a2, uint64_t *a3)
{
  uint64_t *v3;

  return specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a3, a2 & 0x1FF, 0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, char *, unint64_t, __n128))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UInt8, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, unint64_t, char *, unint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UInt8, @unowned UInt32) -> (@unowned Int), 0, 0, *v3);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UInt8, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, Pixel_8 *a4, vImage_Flags a5, float a6)
{
  return vImageRotate_Planar8(a1, a2, a3, a6, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UInt8, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, uint8_t a3, Pixel_8 *a4, vImage_Flags a5)
{
  return vImageRotate90_Planar8(a1, a2, a3, *a4, a5);
}

uint64_t specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, __int16 a3, char a4, uint64_t (*a5)(uint64_t, uint64_t, _QWORD, char *, unint64_t, __n128), uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, unint64_t, char *, unint64_t), uint64_t a8, char a9, uint64_t a10)
{
  uint64_t v10;
  uint64_t v11;
  unint64_t v12;
  uint64_t v13;
  uint64_t v14;
  unint64_t v15;
  uint64_t v16;
  __n128 v17;
  uint64_t v18;
  uint64_t v19;
  __int128 v20;
  uint64_t v21;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a10 + 16))
  {
    __break(1u);
LABEL_12:
    __break(1u);
  }
  if (!*(_QWORD *)(a2 + 16))
    goto LABEL_12;
  v10 = *(_QWORD *)(a10 + 32);
  v11 = *(_QWORD *)(a2 + 32);
  if (v10)
  {
    if (!v11 || v10 != v11)
      goto LABEL_8;
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  v12 = 4;
  if ((a3 & 0x100) != 0)
    v12 = 8;
  v15 = v12;
  v16 = v11;
  v13 = *(_QWORD *)(a10 + 56);
  v19 = v10;
  v20 = *(_OWORD *)(a10 + 40);
  v21 = v13;
  v14 = *(_QWORD *)(a2 + 56);
  v17 = *(__n128 *)(a2 + 40);
  v18 = v14;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)((uint64_t)&v16, result & 0xFFFFFFFFFFLL, a7, v17, a8, (uint64_t)&v19, a3 & 0x1FF, a4, &v15, a9 & 1, a5);
}

uint64_t specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(uint64_t, uint64_t, _QWORD, uint64_t *, unint64_t, __n128), uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t), uint64_t a8, char a9, uint64_t a10)
{
  uint64_t v10;
  uint64_t v11;
  unint64_t v12;
  uint64_t v13;
  uint64_t v14;
  unint64_t v15;
  uint64_t v16;
  __n128 v17;
  uint64_t v18;
  uint64_t v19;
  __int128 v20;
  uint64_t v21;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a10 + 16))
  {
    __break(1u);
LABEL_12:
    __break(1u);
  }
  if (!*(_QWORD *)(a2 + 16))
    goto LABEL_12;
  v10 = *(_QWORD *)(a10 + 32);
  v11 = *(_QWORD *)(a2 + 32);
  if (v10)
  {
    if (!v11 || v10 != v11)
      goto LABEL_8;
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  v12 = 4;
  if (!a3)
    v12 = 8;
  v15 = v12;
  v16 = v11;
  v13 = *(_QWORD *)(a10 + 56);
  v19 = v10;
  v20 = *(_OWORD *)(a10 + 40);
  v21 = v13;
  v14 = *(_QWORD *)(a2 + 56);
  v17 = *(__n128 *)(a2 + 40);
  v18 = v14;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)((uint64_t)&v16, result & 0xFFFFFFFFFFLL, a7, v17, a8, (uint64_t)&v19, a3, a4, &v15, a9 & 1, a5);
}

uint64_t specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, unint64_t a3, uint64_t (*a4)(uint64_t, uint64_t, _QWORD, float *, unint64_t, float), float a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, unint64_t, float *, unint64_t), uint64_t a8, char a9, uint64_t a10)
{
  uint64_t v10;
  uint64_t v11;
  unint64_t v12;
  uint64_t v13;
  uint64_t v14;
  unint64_t v15;
  uint64_t v16;
  __int128 v17;
  uint64_t v18;
  uint64_t v19;
  __int128 v20;
  uint64_t v21;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a10 + 16))
  {
    __break(1u);
LABEL_12:
    __break(1u);
  }
  if (!*(_QWORD *)(a2 + 16))
    goto LABEL_12;
  v10 = *(_QWORD *)(a10 + 32);
  v11 = *(_QWORD *)(a2 + 32);
  if (v10)
  {
    if (!v11 || v10 != v11)
      goto LABEL_8;
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  v12 = 4;
  if ((a3 & 0x100000000) != 0)
    v12 = 8;
  v15 = v12;
  v16 = v11;
  v13 = *(_QWORD *)(a10 + 56);
  v19 = v10;
  v20 = *(_OWORD *)(a10 + 40);
  v21 = v13;
  v14 = *(_QWORD *)(a2 + 56);
  v17 = *(_OWORD *)(a2 + 40);
  v18 = v14;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)((uint64_t)&v16, result & 0xFFFFFFFFFFLL, a7, a5, a8, (uint64_t)&v19, a3 | ((HIDWORD(a3) & 1) << 32), &v15, a9 & 1, a4);
}

uint64_t specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, int a3, __int16 a4, uint64_t (*a5)(uint64_t, uint64_t, _QWORD, __int16 *, unint64_t, __n128), uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, unint64_t, __int16 *, unint64_t), uint64_t a8, char a9, uint64_t a10)
{
  uint64_t v10;
  uint64_t v11;
  unint64_t v12;
  uint64_t v13;
  uint64_t v14;
  unint64_t v15;
  uint64_t v16;
  __n128 v17;
  uint64_t v18;
  uint64_t v19;
  __int128 v20;
  uint64_t v21;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a10 + 16))
  {
    __break(1u);
LABEL_12:
    __break(1u);
  }
  if (!*(_QWORD *)(a2 + 16))
    goto LABEL_12;
  v10 = *(_QWORD *)(a10 + 32);
  v11 = *(_QWORD *)(a2 + 32);
  if (v10)
  {
    if (!v11 || v10 != v11)
      goto LABEL_8;
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  v12 = 4;
  if ((a3 & 0x10000) != 0)
    v12 = 8;
  v15 = v12;
  v16 = v11;
  v13 = *(_QWORD *)(a10 + 56);
  v19 = v10;
  v20 = *(_OWORD *)(a10 + 40);
  v21 = v13;
  v14 = *(_QWORD *)(a2 + 56);
  v17 = *(__n128 *)(a2 + 40);
  v18 = v14;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)((uint64_t)&v16, result & 0xFFFFFFFFFFLL, a7, v17, a8, (uint64_t)&v19, a3 & 0x1FFFF, a4, &v15, a9 & 1, a5);
}

uint64_t specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t a3, char a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t, _QWORD, uint64_t *, unint64_t, __n128), uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t), uint64_t a9, char a10, uint64_t a11)
{
  uint64_t v11;
  uint64_t v12;
  unint64_t v13;
  uint64_t v14;
  uint64_t v15;
  unint64_t v16;
  uint64_t v17;
  __n128 v18;
  uint64_t v19;
  uint64_t v20;
  __int128 v21;
  uint64_t v22;
  uint64_t v23;

  v23 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a11 + 16))
  {
    __break(1u);
LABEL_12:
    __break(1u);
  }
  if (!*(_QWORD *)(a2 + 16))
    goto LABEL_12;
  v11 = *(_QWORD *)(a11 + 32);
  v12 = *(_QWORD *)(a2 + 32);
  if (v11)
  {
    if (!v12 || v11 != v12)
      goto LABEL_8;
    __break(1u);
  }
  if (!v12)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  v13 = 8;
  if ((a4 & 1) == 0)
    v13 = 4;
  v16 = v13;
  v17 = v12;
  v14 = *(_QWORD *)(a11 + 56);
  v20 = v11;
  v21 = *(_OWORD *)(a11 + 40);
  v22 = v14;
  v15 = *(_QWORD *)(a2 + 56);
  v18 = *(__n128 *)(a2 + 40);
  v19 = v15;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)((uint64_t)&v17, result & 0xFFFFFFFFFFLL, a8, v18, a9, (uint64_t)&v20, a3, a4 & 1, a5, &v16, a10 & 1, a6);
}

uint64_t vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(unsigned int *a1, uint64_t *a2, uint64_t a3, uint64_t a4, void (*a5)(uint64_t, uint64_t, _QWORD, char *, float), uint64_t a6, void (*a7)(uint64_t, uint64_t, unint64_t, char *), uint64_t a8, char a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14)
{
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t result;
  BOOL v20;
  int v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v32;
  _QWORD v33[6];

  v33[4] = *MEMORY[0x1E0C80C00];
  v27 = *a1;
  v26 = *((unsigned __int8 *)a1 + 4);
  v16 = *a2;
  v17 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v33[0] = v16;
  type metadata accessor for vImage.PixelBuffer(0, a11, *(_QWORD *)(*(_QWORD *)(a14 + 8) + 8), v18);
  result = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (v17)
  {
    if (result)
      v20 = v17 == result;
    else
      v20 = 0;
    if (!v20)
      goto LABEL_9;
    __break(1u);
  }
  if (!result)
  {
    __break(1u);
    return result;
  }
LABEL_9:
  v21 = (*(uint64_t (**)(uint64_t, uint64_t, uint64_t))(*(_QWORD *)(a12 - 8) + 48))(a3, 1, a12);
  v22 = 4;
  if (v21 == 1)
    v22 = 8;
  v32 = v22;
  v33[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v33[1] = v23;
  v33[2] = v24;
  v33[3] = v25;
  return closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)((uint64_t)v33, v16, v27 | (v26 << 32), a7, a8, a3, a4, &v32, a9 & 1, a5, a6, *(_QWORD *)(a10 + 16), a11, a12, a13, a14);
}

uint64_t vImage.PixelBuffer<>.rotate(_:backgroundColor:destination:)(unsigned int *a1, unint64_t a2, uint64_t *a3)
{
  uint64_t *v3;
  char v6;
  unint64_t v7;
  unint64_t v8;
  unint64_t v9;
  uint64_t v10;
  uint64_t v11;

  if ((a2 & 0x100000000) != 0)
  {
    v11 = 0;
  }
  else
  {
    v6 = a2;
    v7 = a2 >> 8;
    v8 = a2 >> 16;
    v9 = a2 >> 24;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    v10 = swift_allocObject();
    *(_OWORD *)(v10 + 16) = xmmword_1CAB5F170;
    *(_BYTE *)(v10 + 32) = v6;
    v11 = v10 + 32;
    *(_BYTE *)(v10 + 33) = v7;
    *(_BYTE *)(v10 + 34) = v8;
    *(_BYTE *)(v10 + 35) = v9;
  }
  specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a3, v11, (uint64_t)&unk_1E84EDBB0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, uint64_t *, unint64_t, __n128))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<UInt8>, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<UInt8>, @unowned UInt32) -> (@unowned Int), 0, 0, *v3);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

{
  uint64_t *v3;

  return specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a3, a2 | ((HIDWORD(a2) & 1) << 32), (uint64_t (*)(uint64_t, uint64_t, _QWORD, float *, unint64_t, float))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned Float, @unowned UInt32) -> (@unowned Int), 0.0, 0, (uint64_t (*)(uint64_t, uint64_t, unint64_t, float *, unint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned Float, @unowned UInt32) -> (@unowned Int), 0, 0, *v3);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<UInt8>, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, const uint8_t *__attribute__((__org_typedef(Pixel_8888))) *a4, vImage_Flags a5, float a6)
{
  return vImageRotate_ARGB8888(a1, a2, a3, a6, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<UInt8>, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, uint8_t a3, const uint8_t *__attribute__((__org_typedef(Pixel_8888))) *a4, vImage_Flags a5)
{
  return vImageRotate90_ARGB8888(a1, a2, a3, *a4, a5);
}

uint64_t vImage.PixelBuffer<>.rotate(_:backgroundColor:destination:)(unsigned int *a1, unint64_t a2, char a3, uint64_t *a4)
{
  uint64_t *v4;
  __int16 v7;
  unint64_t v8;
  unint64_t v9;
  unint64_t v10;
  uint64_t v11;
  uint64_t v12;

  if ((a3 & 1) != 0)
  {
    v12 = 0;
  }
  else
  {
    v7 = a2;
    v8 = a2 >> 16;
    v9 = HIDWORD(a2);
    v10 = HIWORD(a2);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    v11 = swift_allocObject();
    *(_OWORD *)(v11 + 16) = xmmword_1CAB5F170;
    *(_WORD *)(v11 + 32) = v7;
    v12 = v11 + 32;
    *(_WORD *)(v11 + 34) = v8;
    *(_WORD *)(v11 + 36) = v9;
    *(_WORD *)(v11 + 38) = v10;
  }
  specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a4, v12, (uint64_t)&unk_1E84EDED8, (uint64_t (*)(uint64_t, uint64_t, _QWORD, uint64_t *, unint64_t, __n128))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<UInt16>, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<UInt16>, @unowned UInt32) -> (@unowned Int), 0, 0, *v4);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<UInt16>, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, const uint16_t *__attribute__((__org_typedef(Pixel_ARGB_16U))) *a4, vImage_Flags a5, float a6)
{
  return vImageRotate_ARGB16U(a1, a2, a3, a6, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<UInt16>, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, uint8_t a3, const uint16_t *__attribute__((__org_typedef(Pixel_ARGB_16U))) *a4, vImage_Flags a5)
{
  return vImageRotate90_ARGB16U(a1, a2, a3, *a4, a5);
}

uint64_t vImage.PixelBuffer<>.rotate(_:backgroundColor:destination:)(unsigned int *a1, unint64_t a2, unint64_t a3, char a4, uint64_t *a5)
{
  uint64_t *v5;
  int v8;
  int v9;
  unint64_t v10;
  unint64_t v11;
  uint64_t v12;
  uint64_t inited;
  uint64_t v14;

  if ((a4 & 1) != 0)
  {
    v12 = 0;
  }
  else
  {
    v8 = a3;
    v9 = a2;
    v10 = HIDWORD(a2);
    v11 = HIDWORD(a3);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
    v12 = swift_allocObject();
    *(_OWORD *)(v12 + 16) = xmmword_1CAB5F170;
    *(_DWORD *)(v12 + 32) = v9;
    *(_DWORD *)(v12 + 36) = v10;
    *(_DWORD *)(v12 + 40) = v8;
    *(_DWORD *)(v12 + 44) = v11;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5E430;
  *(_DWORD *)(inited + 32) = 0;
  if (v12)
    v14 = v12 + 32;
  else
    v14 = 0;
  specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a5, v14, inited + 32, (uint64_t (*)(uint64_t, uint64_t, _QWORD, uint64_t *, unint64_t, __n128))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<Float>, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<Float>, @unowned UInt32) -> (@unowned Int), 0, 0, *v5);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<Float>, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, const float *__attribute__((__org_typedef(Pixel_FFFF))) *a4, vImage_Flags a5, float a6)
{
  return vImageRotate_ARGBFFFF(a1, a2, a3, a6, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<Float>, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, uint8_t a3, const float *__attribute__((__org_typedef(Pixel_FFFF))) *a4, vImage_Flags a5)
{
  return vImageRotate90_ARGBFFFF(a1, a2, a3, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned Float, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, Pixel_F *a4, vImage_Flags flags, float a6)
{
  return vImageRotate_PlanarF(a1, a2, a3, a6, *a4, flags);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned Float, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, uint8_t a3, Pixel_F *a4, vImage_Flags flags)
{
  return vImageRotate90_PlanarF(a1, a2, a3, *a4, flags);
}

uint64_t vImage.PixelBuffer<>.rotate(_:backgroundColor:useFloat16Accumulator:destination:)(unsigned int *a1, int a2, char a3, uint64_t *a4)
{
  uint64_t *v4;

  return specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a4, a2 & 0x1FFFF, 0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, __int16 *, unint64_t, __n128))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UInt16, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, unint64_t, __int16 *, unint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UInt16, @unowned UInt32) -> (@unowned Int), 0, a3, *v4);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UInt16, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, const Pixel_16F *a4, vImage_Flags a5, float a6)
{
  return vImageRotate_Planar16F(a1, a2, a3, a6, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UInt16, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, uint8_t a3, const Pixel_16F *a4, vImage_Flags a5)
{
  return vImageRotate90_Planar16F(a1, a2, a3, *a4, a5);
}

uint64_t vImage.PixelBuffer<>.rotate(_:backgroundColor:useFloat16Accumulator:destination:)(unsigned int *a1, unint64_t a2, char a3, uint64_t *a4)
{
  uint64_t *v4;
  char v8;
  __int16 v9;
  unint64_t v10;
  uint64_t v11;
  uint64_t v12;

  v8 = BYTE4(a2) & 1;
  if ((a2 & 0x100000000) != 0)
  {
    v12 = 0;
  }
  else
  {
    v9 = a2;
    v10 = a2 >> 16;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    v11 = swift_allocObject();
    *(_OWORD *)(v11 + 16) = xmmword_1CAB5E440;
    *(_WORD *)(v11 + 32) = v9;
    v12 = v11 + 32;
    *(_WORD *)(v11 + 34) = v10;
  }
  specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a4, v12, v8, (uint64_t)&unk_1E84EDDC0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, uint64_t *, unint64_t, __n128))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, a3 & 1, *v4);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, const uint16_t *__attribute__((__org_typedef(Pixel_16F16F))) *a4, vImage_Flags a5, float a6)
{
  return vImageRotate_CbCr16F(a1, a2, a3, a6, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, const uint16_t *__attribute__((__org_typedef(Pixel_ARGB_16F))) *a4, vImage_Flags a5, float a6)
{
  return vImageRotate_ARGB16F(a1, a2, a3, a6, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, uint8_t a3, const uint16_t *__attribute__((__org_typedef(Pixel_16F16F))) *a4, vImage_Flags a5)
{
  return vImageRotate90_CbCr16F(a1, a2, a3, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, uint8_t a3, const uint16_t *__attribute__((__org_typedef(Pixel_ARGB_16F))) *a4, vImage_Flags a5)
{
  return vImageRotate90_ARGB16F(a1, a2, a3, *a4, a5);
}

uint64_t vImage.PixelBuffer<>.rotate(_:backgroundColor:useFloat16Accumulator:destination:)(unsigned int *a1, unint64_t a2, char a3, char a4, uint64_t *a5)
{
  uint64_t *v5;
  char v9;
  __int16 v10;
  unint64_t v11;
  unint64_t v12;
  unint64_t v13;
  uint64_t v14;
  uint64_t v15;

  v9 = a3 & 1;
  if ((a3 & 1) != 0)
  {
    v15 = 0;
  }
  else
  {
    v10 = a2;
    v11 = a2 >> 16;
    v12 = HIDWORD(a2);
    v13 = HIWORD(a2);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    v14 = swift_allocObject();
    *(_OWORD *)(v14 + 16) = xmmword_1CAB5F170;
    *(_WORD *)(v14 + 32) = v10;
    v15 = v14 + 32;
    *(_WORD *)(v14 + 34) = v11;
    *(_WORD *)(v14 + 36) = v12;
    *(_WORD *)(v14 + 38) = v13;
  }
  specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a5, v15, v9, (uint64_t)&unk_1E84EDE38, (uint64_t (*)(uint64_t, uint64_t, _QWORD, uint64_t *, unint64_t, __n128))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, a4 & 1, *v5);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

uint64_t closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t a1, uint64_t a2, uint64_t a3, void (*a4)(uint64_t, uint64_t, unint64_t, char *), uint64_t a5, uint64_t a6, uint64_t a7, _QWORD *a8, char a9, void (*a10)(uint64_t, uint64_t, _QWORD, char *, float), uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16)
{
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  _QWORD v27[6];

  v27[4] = *MEMORY[0x1E0C80C00];
  v27[0] = a2;
  type metadata accessor for vImage.PixelBuffer(0, a13, *(_QWORD *)(*(_QWORD *)(a16 + 8) + 8), (uint64_t)a4);
  v27[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v27[1] = v17;
  v27[2] = v18;
  v27[3] = v19;
  return closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)((uint64_t)v27, a3 & 0xFFFFFFFFFFLL, a4, a5, a1, a6, a7, a8, a9, a10, a11, a12, a13, a14);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, unint64_t, char *, unint64_t), __n128 a4, uint64_t a5, uint64_t a6, __int16 a7, char a8, unint64_t *a9, char a10, uint64_t (*a11)(uint64_t, uint64_t, _QWORD, char *, unint64_t, __n128))
{
  uint64_t v12;
  unint64_t v13;
  char v14;
  char v15;

  if (BYTE4(a2) >= 2u)
  {
    if ((a7 & 0x100) == 0)
      a8 = a7;
    v14 = a8;
    if ((*a9 & 0x8000000000000000) == 0)
    {
      if (!HIDWORD(*a9))
        return a3(a6, result, 0x302010001020300uLL >> (8 * a2), &v14, *a9);
      goto LABEL_21;
    }
    __break(1u);
LABEL_20:
    __break(1u);
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v12 = 4096;
  if ((a10 & 1) == 0)
    v12 = 0;
  v13 = *a9 | v12;
  *a9 = v13;
  if (BYTE4(a2) == 1)
  {
    a4.n128_f32[0] = (float)(*(float *)&a2 * 3.1416) / 180.0;
  }
  else
  {
    if (BYTE4(a2))
    {
LABEL_23:
      __break(1u);
      return result;
    }
    a4.n128_u32[0] = a2;
  }
  if ((a7 & 0x100) == 0)
    a8 = a7;
  v15 = a8;
  if ((v13 & 0x8000000000000000) != 0)
    goto LABEL_20;
  if (HIDWORD(v13))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  return a11(a6, result, 0, &v15, v13, a4);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t), __n128 a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, unint64_t *a9, char a10, uint64_t (*a11)(uint64_t, uint64_t, _QWORD, uint64_t *, unint64_t, __n128))
{
  uint64_t v11;
  unint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;

  if (BYTE4(a2) >= 2u)
  {
    if (a7)
      v13 = a7;
    else
      v13 = a8;
    v15 = v13;
    if ((*a9 & 0x8000000000000000) == 0)
    {
      if (!HIDWORD(*a9))
        return a3(a6, result, 0x302010001020300uLL >> (8 * a2), &v15, *a9);
      goto LABEL_23;
    }
    __break(1u);
LABEL_22:
    __break(1u);
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v11 = 4096;
  if ((a10 & 1) == 0)
    v11 = 0;
  v12 = *a9 | v11;
  *a9 = v12;
  if (BYTE4(a2) == 1)
  {
    a4.n128_f32[0] = (float)(*(float *)&a2 * 3.1416) / 180.0;
  }
  else
  {
    if (BYTE4(a2))
    {
LABEL_25:
      __break(1u);
      return result;
    }
    a4.n128_u32[0] = a2;
  }
  if (a7)
    v14 = a7;
  else
    v14 = a8;
  v15 = v14;
  if ((v12 & 0x8000000000000000) != 0)
    goto LABEL_22;
  if (HIDWORD(v12))
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  return a11(a6, result, 0, &v15, v12, a4);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, unint64_t, float *, unint64_t), float a4, uint64_t a5, uint64_t a6, uint64_t a7, unint64_t *a8, char a9, uint64_t (*a10)(uint64_t, uint64_t, _QWORD, float *, unint64_t, float))
{
  uint64_t v10;
  unint64_t v11;
  float v12;
  float v13;
  float v14;
  float v15;
  float v16;

  if (BYTE4(a2) >= 2u)
  {
    v13 = a4;
    if ((a7 & 0x100000000) == 0)
      v13 = *(float *)&a7;
    v15 = v13;
    if ((*a8 & 0x8000000000000000) == 0)
    {
      if (!HIDWORD(*a8))
        return a3(a6, result, 0x302010001020300uLL >> (8 * a2), &v15, *a8);
      goto LABEL_21;
    }
    __break(1u);
LABEL_20:
    __break(1u);
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v10 = 4096;
  if ((a9 & 1) == 0)
    v10 = 0;
  v11 = *a8 | v10;
  *a8 = v11;
  if (BYTE4(a2) == 1)
  {
    v12 = (float)(*(float *)&a2 * 3.1416) / 180.0;
  }
  else
  {
    if (BYTE4(a2))
    {
LABEL_23:
      __break(1u);
      return result;
    }
    v12 = *(float *)&a2;
  }
  v14 = a4;
  if ((a7 & 0x100000000) == 0)
    v14 = *(float *)&a7;
  v16 = v14;
  if ((v11 & 0x8000000000000000) != 0)
    goto LABEL_20;
  if (HIDWORD(v11))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  return a10(a6, result, 0, &v16, v11, v12);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, unint64_t, __int16 *, unint64_t), __n128 a4, uint64_t a5, uint64_t a6, int a7, __int16 a8, unint64_t *a9, char a10, uint64_t (*a11)(uint64_t, uint64_t, _QWORD, __int16 *, unint64_t, __n128))
{
  uint64_t v11;
  unint64_t v12;
  __int16 v13;
  __int16 v14;
  __int16 v15;
  __int16 v16;

  if (BYTE4(a2) >= 2u)
  {
    if ((a7 & 0x10000) != 0)
      v13 = a8;
    else
      v13 = a7;
    v15 = v13;
    if ((*a9 & 0x8000000000000000) == 0)
    {
      if (!HIDWORD(*a9))
        return a3(a6, result, 0x302010001020300uLL >> (8 * a2), &v15, *a9);
      goto LABEL_23;
    }
    __break(1u);
LABEL_22:
    __break(1u);
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v11 = 4096;
  if ((a10 & 1) == 0)
    v11 = 0;
  v12 = *a9 | v11;
  *a9 = v12;
  if (BYTE4(a2) == 1)
  {
    a4.n128_f32[0] = (float)(*(float *)&a2 * 3.1416) / 180.0;
  }
  else
  {
    if (BYTE4(a2))
    {
LABEL_25:
      __break(1u);
      return result;
    }
    a4.n128_u32[0] = a2;
  }
  if ((a7 & 0x10000) != 0)
    v14 = a8;
  else
    v14 = a7;
  v16 = v14;
  if ((v12 & 0x8000000000000000) != 0)
    goto LABEL_22;
  if (HIDWORD(v12))
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  return a11(a6, result, 0, &v16, v12, a4);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t), __n128 a4, uint64_t a5, uint64_t a6, uint64_t a7, char a8, uint64_t a9, unint64_t *a10, char a11, uint64_t (*a12)(uint64_t, uint64_t, _QWORD, uint64_t *, unint64_t, __n128))
{
  uint64_t v12;
  unint64_t v13;
  uint64_t v14;
  unint64_t v15;
  uint64_t v16;
  uint64_t v17;

  if (BYTE4(a2) >= 2u)
  {
    if ((a8 & 1) != 0)
      v14 = a9;
    else
      v14 = a7;
    v17 = v14;
    v15 = *a10;
    if ((*a10 & 0x8000000000000000) == 0)
    {
      if (!HIDWORD(v15))
        return a3(a6, result, 0x302010001020300uLL >> (8 * a2), &v17, v15);
      goto LABEL_23;
    }
    __break(1u);
LABEL_22:
    __break(1u);
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v12 = 4096;
  if ((a11 & 1) == 0)
    v12 = 0;
  v13 = *a10 | v12;
  *a10 = v13;
  if (BYTE4(a2) == 1)
  {
    a4.n128_f32[0] = (float)(*(float *)&a2 * 3.1416) / 180.0;
  }
  else
  {
    if (BYTE4(a2))
    {
LABEL_25:
      __break(1u);
      return result;
    }
    a4.n128_u32[0] = a2;
  }
  if ((a8 & 1) != 0)
    v16 = a9;
  else
    v16 = a7;
  v17 = v16;
  if ((v13 & 0x8000000000000000) != 0)
    goto LABEL_22;
  if (HIDWORD(v13))
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  return a12(a6, result, 0, &v17, v13, a4);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t a1, uint64_t a2, void (*a3)(uint64_t, uint64_t, unint64_t, char *), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, _QWORD *a8, char a9, void (*a10)(uint64_t, uint64_t, _QWORD, char *, float), uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14)
{
  unsigned int v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  char *v19;
  uint64_t v20;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  char *v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  char *v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  char *v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  float v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t result;
  uint64_t v43;
  uint64_t v44;
  int v45;
  char *v46;
  _QWORD v47[2];
  void (*v48)(uint64_t, uint64_t, unint64_t, char *);
  char *v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;

  v51 = a5;
  v53 = a6;
  v54 = a7;
  v47[1] = a4;
  v48 = a3;
  v50 = a1;
  v52 = a2;
  v15 = BYTE4(a2);
  v16 = type metadata accessor for Optional();
  v17 = MEMORY[0x1E0C80A78](v16);
  v19 = (char *)v47 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  v20 = MEMORY[0x1E0C80A78](v17);
  v22 = (char *)v47 - v21;
  v23 = *(_QWORD *)(a14 - 8);
  v24 = MEMORY[0x1E0C80A78](v20);
  v26 = (char *)v47 - ((v25 + 15) & 0xFFFFFFFFFFFFFFF0);
  v27 = MEMORY[0x1E0C80A78](v24);
  v49 = (char *)v47 - v28;
  v29 = MEMORY[0x1E0C80A78](v27);
  v31 = (char *)v47 - v30;
  MEMORY[0x1E0C80A78](v29);
  v35 = (char *)v47 - v34;
  if (v15 >= 2)
  {
    v40 = v32;
    v41 = v33;
    (*(void (**)(char *, uint64_t, uint64_t))(v33 + 16))(v22, v53, v32);
    (*(void (**)(char *, uint64_t, uint64_t))(v23 + 16))(v31, v54, a14);
    if ((*(unsigned int (**)(char *, uint64_t, uint64_t))(v23 + 48))(v22, 1, a14) == 1)
    {
      (*(void (**)(char *, char *, uint64_t))(v23 + 32))(v35, v31, a14);
      result = (*(uint64_t (**)(char *, uint64_t))(v41 + 8))(v22, v40);
    }
    else
    {
      (*(void (**)(char *, uint64_t))(v23 + 8))(v31, a14);
      result = (*(uint64_t (**)(char *, char *, uint64_t))(v23 + 32))(v35, v22, a14);
    }
    if ((*a8 & 0x8000000000000000) == 0)
    {
      if (!HIDWORD(*a8))
      {
        v48(v51, v50, 0x302010001020300uLL >> (8 * v52), v35);
        return (*(uint64_t (**)(char *, uint64_t))(v23 + 8))(v35, a14);
      }
      goto LABEL_22;
    }
    __break(1u);
LABEL_21:
    __break(1u);
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v36 = v53;
  v37 = v54;
  v38 = 4096;
  if ((a9 & 1) == 0)
    v38 = 0;
  *a8 |= v38;
  if (v15 == 1)
    v39 = (float)(*(float *)&v52 * 3.1416) / 180.0;
  else
    v39 = *(float *)&v52;
  v43 = v32;
  v44 = v33;
  (*(void (**)(char *, uint64_t, uint64_t))(v33 + 16))(v19, v36, v32);
  (*(void (**)(char *, uint64_t, uint64_t))(v23 + 16))(v26, v37, a14);
  v45 = (*(uint64_t (**)(char *, uint64_t, uint64_t))(v23 + 48))(v19, 1, a14);
  v46 = v49;
  if (v45 == 1)
  {
    (*(void (**)(char *, char *, uint64_t))(v23 + 32))(v49, v26, a14);
    result = (*(uint64_t (**)(char *, uint64_t))(v44 + 8))(v19, v43);
  }
  else
  {
    (*(void (**)(char *, uint64_t))(v23 + 8))(v26, a14);
    result = (*(uint64_t (**)(char *, char *, uint64_t))(v23 + 32))(v46, v19, a14);
  }
  if ((*a8 & 0x8000000000000000) != 0)
    goto LABEL_21;
  if (!HIDWORD(*a8))
  {
    a10(v51, v50, 0, v46, v39);
    v35 = v46;
    return (*(uint64_t (**)(char *, uint64_t))(v23 + 8))(v35, a14);
  }
LABEL_23:
  __break(1u);
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.shear(direction:translate:slope:resamplingFilter:backgroundColor:destination:)(unsigned __int8 *a1, uint64_t a2, unsigned int a3, uint64_t *a4)
{
  uint64_t *v4;
  char v8;
  char v9;
  unsigned int v10;
  uint64_t v11;
  uint64_t v12;

  v8 = BYTE2(a3) & 1;
  if ((a3 & 0x10000) != 0)
  {
    v12 = 0;
  }
  else
  {
    v9 = a3;
    v10 = a3 >> 8;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    v11 = swift_allocObject();
    *(_OWORD *)(v11 + 16) = xmmword_1CAB5E440;
    *(_BYTE *)(v11 + 32) = v9;
    v12 = v11 + 32;
    *(_BYTE *)(v11 + 33) = v10;
  }
  specialized vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*a1, a2, *a4, v12, v8, (uint64_t)&unk_1E84EDAA0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Float, @unowned Float, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Float, @unowned Float, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int), 0, 0, *v4);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Float, @unowned Float, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, const uint8_t *__attribute__((__org_typedef(Pixel_88))) *a6, vImage_Flags a7, float a8, float a9)
{
  return vImageVerticalShear_CbCr8(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShear_CbCr8(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

uint64_t specialized vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, char a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *), uint64_t a8, uint64_t (*a9)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *), uint64_t a10, char a11, uint64_t a12)
{
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  __int128 v20;
  uint64_t v21;
  uint64_t v22;
  __int128 v23;
  uint64_t v24;
  uint64_t v25;

  v25 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a12 + 16))
  {
    __break(1u);
LABEL_15:
    __break(1u);
  }
  if (!*(_QWORD *)(a3 + 16))
    goto LABEL_15;
  v12 = *(_QWORD *)(a12 + 32);
  v13 = *(_QWORD *)(a3 + 32);
  if (v12)
  {
    if (!v13 || v12 != v13)
      goto LABEL_8;
    __break(1u);
  }
  if (!v13)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  if ((a11 & 1) != 0)
    v14 = 4096;
  else
    v14 = 0;
  v15 = 8;
  if ((a5 & 1) == 0)
    v15 = 4;
  v18 = v14 | v15;
  v19 = v13;
  v16 = *(_QWORD *)(a12 + 56);
  v22 = v12;
  v23 = *(_OWORD *)(a12 + 40);
  v24 = v16;
  v17 = *(_QWORD *)(a3 + 56);
  v20 = *(_OWORD *)(a3 + 40);
  v21 = v17;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)&v19, result & 1, a9, a10, (uint64_t)&v22, a2, a4, a5 & 1, a6, &v18, a7);
}

uint64_t specialized vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t a3, int a4, __int16 a5, uint64_t (*a6)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, __int16 *), uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, __int16 *), uint64_t a9, char a10, uint64_t a11)
{
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  __int128 v19;
  uint64_t v20;
  uint64_t v21;
  __int128 v22;
  uint64_t v23;
  uint64_t v24;

  v24 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a11 + 16))
  {
    __break(1u);
LABEL_14:
    __break(1u);
  }
  if (!*(_QWORD *)(a3 + 16))
    goto LABEL_14;
  v11 = *(_QWORD *)(a11 + 32);
  v12 = *(_QWORD *)(a3 + 32);
  if (v11)
  {
    if (!v12 || v11 != v12)
      goto LABEL_8;
    __break(1u);
  }
  if (!v12)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  v13 = 4096;
  if ((a10 & 1) == 0)
    v13 = 0;
  v14 = 4;
  if ((a4 & 0x10000) != 0)
    v14 = 8;
  v17 = v14 | v13;
  v18 = v12;
  v15 = *(_QWORD *)(a11 + 56);
  v21 = v11;
  v22 = *(_OWORD *)(a11 + 40);
  v23 = v15;
  v16 = *(_QWORD *)(a3 + 56);
  v19 = *(_OWORD *)(a3 + 40);
  v20 = v16;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)&v18, result & 1, a8, a9, (uint64_t)&v21, a2, a4 & 0x1FFFF, a5, &v17, a6);
}

uint64_t vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(char *a1, uint64_t a2, uint64_t *a3, uint64_t a4, uint64_t a5, void (*a6)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *, float, float), uint64_t a7, void (*a8)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *, float, float), float a9, float a10, uint64_t a11, char a12, uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16, uint64_t a17)
{
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t result;
  BOOL v24;
  int v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  char v32;
  uint64_t v37;
  _QWORD v38[6];

  v38[4] = *MEMORY[0x1E0C80C00];
  v32 = *a1;
  v20 = *a3;
  v21 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v38[0] = v20;
  type metadata accessor for vImage.PixelBuffer(0, a14, *(_QWORD *)(*(_QWORD *)(a17 + 8) + 8), v22);
  result = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (v21)
  {
    if (result)
      v24 = v21 == result;
    else
      v24 = 0;
    if (!v24)
      goto LABEL_9;
    __break(1u);
  }
  if (!result)
  {
    __break(1u);
    return result;
  }
LABEL_9:
  v25 = (*(uint64_t (**)(uint64_t, uint64_t, uint64_t))(*(_QWORD *)(a15 - 8) + 48))(a4, 1, a15);
  v26 = 4;
  if (v25 == 1)
    v26 = 8;
  v27 = 4096;
  if ((a12 & 1) == 0)
    v27 = 0;
  v37 = v26 | v27;
  v38[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v38[1] = v28;
  v38[2] = v29;
  v38[3] = v30;
  return closure #1 in vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)v38, v20, v32, a8, a11, a2, a4, a5, a9, a10, &v37, a6, a7, *(_QWORD *)(a13 + 16), a14, a15, a16, a17);
}

uint64_t vImage.PixelBuffer<>.shear(direction:translate:slope:resamplingFilter:backgroundColor:destination:)(unsigned __int8 *a1, uint64_t a2, int a3, uint64_t *a4)
{
  uint64_t *v4;

  return specialized vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*a1, a2, *a4, a3 & 0x1FFFF, 0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, __int16 *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Float, @unowned Float, @unowned UnsafeMutableRawPointer, @unowned UInt16, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, __int16 *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Float, @unowned Float, @unowned UnsafeMutableRawPointer, @unowned UInt16, @unowned UInt32) -> (@unowned Int), 0, 0, *v4);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Float, @unowned Float, @unowned UnsafeMutableRawPointer, @unowned UInt16, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, Pixel_16U *a6, vImage_Flags a7, float a8, float a9)
{
  return vImageVerticalShear_Planar16U(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShear_Planar16U(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

uint64_t closure #1 in vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t a1, uint64_t a2, char a3, void (*a4)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *, float, float), uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, float a10, _QWORD *a11, void (*a12)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *, float, float), uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16, uint64_t a17, uint64_t a18)
{
  int v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  _QWORD v31[6];

  v31[4] = *MEMORY[0x1E0C80C00];
  v20 = a3 & 1;
  v31[0] = a2;
  type metadata accessor for vImage.PixelBuffer(0, a15, *(_QWORD *)(*(_QWORD *)(a18 + 8) + 8), (uint64_t)a4);
  v31[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v31[1] = v21;
  v31[2] = v22;
  v31[3] = v23;
  return closure #1 in closure #1 in vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)v31, v20, a4, a5, a1, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, a16);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, char a2, uint64_t (*a3)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, char a8, uint64_t a9, _QWORD *a10, uint64_t (*a11)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *))
{
  uint64_t v12;

  if ((a8 & 1) == 0)
    a9 = a7;
  if ((a2 & 1) == 0)
  {
    v12 = a9;
    if ((*a10 & 0x8000000000000000) != 0)
    {
      __break(1u);
    }
    else if (!HIDWORD(*a10))
    {
      return a11(a5, result, 0, 0, a6, &v12);
    }
    __break(1u);
    goto LABEL_13;
  }
  v12 = a9;
  if ((*a10 & 0x8000000000000000) != 0)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  if (HIDWORD(*a10))
  {
LABEL_14:
    __break(1u);
    return result;
  }
  return a3(a5, result, 0, 0, a6, &v12);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, char a2, uint64_t (*a3)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, __int16 *), uint64_t a4, uint64_t a5, uint64_t a6, int a7, __int16 a8, _QWORD *a9, uint64_t (*a10)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, __int16 *))
{
  __int16 v10;
  __int16 v11;
  __int16 v12;

  if ((a7 & 0x10000) != 0)
    v10 = a8;
  else
    v10 = a7;
  if ((a2 & 1) == 0)
  {
    v12 = v10;
    if ((*a9 & 0x8000000000000000) != 0)
    {
      __break(1u);
    }
    else if (!HIDWORD(*a9))
    {
      return a10(a5, result, 0, 0, a6, &v12);
    }
    __break(1u);
    goto LABEL_14;
  }
  v11 = v10;
  if ((*a9 & 0x8000000000000000) != 0)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  if (HIDWORD(*a9))
  {
LABEL_15:
    __break(1u);
    return result;
  }
  return a3(a5, result, 0, 0, a6, &v11);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t a1, int a2, void (*a3)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *, float, float), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, float a10, _QWORD *a11, void (*a12)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *, float, float), uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16)
{
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  uint64_t v24;
  char *v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  char *v29;
  uint64_t v30;
  uint64_t v31;
  char *v32;
  uint64_t v33;
  uint64_t v34;
  char *v35;
  uint64_t v36;
  uint64_t v37;
  char *v38;
  void (*v39)(char *, uint64_t, uint64_t);
  uint64_t v40;
  uint64_t result;
  uint64_t v42;
  _QWORD v43[2];
  void (*v44)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *, float, float);
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  int v48;
  uint64_t v49;
  uint64_t v50;
  _QWORD *v51;

  v47 = a6;
  v49 = a7;
  v50 = a8;
  v45 = a1;
  v46 = a5;
  v43[1] = a4;
  v44 = a3;
  v48 = a2;
  v51 = a11;
  v18 = type metadata accessor for Optional();
  v19 = *(_QWORD *)(v18 - 8);
  v20 = MEMORY[0x1E0C80A78](v18);
  v22 = (char *)v43 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  v23 = MEMORY[0x1E0C80A78](v20);
  v25 = (char *)v43 - v24;
  v26 = *(_QWORD *)(a16 - 8);
  v27 = MEMORY[0x1E0C80A78](v23);
  v29 = (char *)v43 - ((v28 + 15) & 0xFFFFFFFFFFFFFFF0);
  v30 = MEMORY[0x1E0C80A78](v27);
  v32 = (char *)v43 - v31;
  v33 = MEMORY[0x1E0C80A78](v30);
  v35 = (char *)v43 - v34;
  MEMORY[0x1E0C80A78](v33);
  v38 = (char *)v43 - v37;
  v39 = *(void (**)(char *, uint64_t, uint64_t))(v19 + 16);
  if ((v48 & 1) == 0)
  {
    v40 = v36;
    v39(v22, v49, v36);
    (*(void (**)(char *, uint64_t, uint64_t))(v26 + 16))(v29, v50, a16);
    if ((*(unsigned int (**)(char *, uint64_t, uint64_t))(v26 + 48))(v22, 1, a16) == 1)
    {
      (*(void (**)(char *, char *, uint64_t))(v26 + 32))(v32, v29, a16);
      result = (*(uint64_t (**)(char *, uint64_t))(v19 + 8))(v22, v40);
    }
    else
    {
      (*(void (**)(char *, uint64_t))(v26 + 8))(v29, a16);
      result = (*(uint64_t (**)(char *, char *, uint64_t))(v26 + 32))(v32, v22, a16);
    }
    if ((*v51 & 0x8000000000000000) != 0)
    {
      __break(1u);
    }
    else if (!HIDWORD(*v51))
    {
      a12(v46, v45, 0, 0, v47, v32, a9, a10);
      v38 = v32;
      return (*(uint64_t (**)(char *, uint64_t))(v26 + 8))(v38, a16);
    }
    __break(1u);
    goto LABEL_17;
  }
  v42 = v36;
  v39(v25, v49, v36);
  (*(void (**)(char *, uint64_t, uint64_t))(v26 + 16))(v35, v50, a16);
  if ((*(unsigned int (**)(char *, uint64_t, uint64_t))(v26 + 48))(v25, 1, a16) == 1)
  {
    (*(void (**)(char *, char *, uint64_t))(v26 + 32))(v38, v35, a16);
    result = (*(uint64_t (**)(char *, uint64_t))(v19 + 8))(v25, v42);
  }
  else
  {
    (*(void (**)(char *, uint64_t))(v26 + 8))(v35, a16);
    result = (*(uint64_t (**)(char *, char *, uint64_t))(v26 + 32))(v38, v25, a16);
  }
  if ((*v51 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!HIDWORD(*v51))
  {
    v44(v46, v45, 0, 0, v47, v38, a9, a10);
    return (*(uint64_t (**)(char *, uint64_t))(v26 + 8))(v38, a16);
  }
LABEL_18:
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.shear<A>(direction:translate:slope:resamplingFilter:backgroundColor:destination:)(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4, int a5, uint64_t *a6, uint64_t a7)
{
  uint64_t *v7;
  uint64_t *v8;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  void (*v19)(char *, uint64_t, uint64_t);
  uint64_t v20;
  uint64_t v22;
  uint64_t *v23;
  unsigned __int8 *v24;
  uint64_t *v25;

  v8 = v7;
  v24 = a1;
  v25 = a6;
  v14 = *(_QWORD *)(a7 - 8);
  MEMORY[0x1E0C80A78](a1);
  v16 = (char *)&v22 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  if ((v17 & 0x100000000) != 0)
  {
    v18 = 0;
  }
  else
  {
    v23 = v8;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    v18 = swift_allocObject();
    *(_OWORD *)(v18 + 16) = xmmword_1CAB5E440;
    *(_DWORD *)(v18 + 32) = a5;
    v8 = v23;
  }
  v19 = *(void (**)(char *, uint64_t, uint64_t))(v14 + 16);
  v19(v16, a2, a7);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  v19(v16, a3, a7);
  BinaryFloatingPoint.init<A>(_:)();
  if (v18)
    v20 = v18 + 32;
  else
    v20 = 0;
  specialized vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*v24, a4, *v25, v20, v18 == 0, (uint64_t)&unk_1E84EDE88, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, 0, *v8);
  return swift_bridgeObjectRelease();
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, const uint16_t *__attribute__((__org_typedef(Pixel_16U16U))) *a6, vImage_Flags a7, double a8, double a9)
{
  return vImageVerticalShearD_CbCr16U(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShearD_CbCr16U(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, const uint16_t *__attribute__((__org_typedef(Pixel_16F16F))) *a6, vImage_Flags a7, double a8, double a9)
{
  return vImageVerticalShearD_CbCr16F(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShearD_CbCr16F(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, const uint16_t *__attribute__((__org_typedef(Pixel_ARGB_16F))) *a6, vImage_Flags a7, double a8, double a9)
{
  return vImageVerticalShearD_ARGB16F(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShearD_ARGB16F(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, const uint16_t *__attribute__((__org_typedef(Pixel_ARGB_16U))) *a6, vImage_Flags a7, double a8, double a9)
{
  return vImageVerticalShearD_ARGB16U(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShearD_ARGB16U(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

uint64_t specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t a3, __int16 a4, char a5, uint64_t (*a6)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *), uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *), uint64_t a9, char a10, uint64_t a11)
{
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  __int128 v19;
  uint64_t v20;
  uint64_t v21;
  __int128 v22;
  uint64_t v23;
  uint64_t v24;

  v24 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a11 + 16))
  {
    __break(1u);
LABEL_14:
    __break(1u);
  }
  if (!*(_QWORD *)(a3 + 16))
    goto LABEL_14;
  v11 = *(_QWORD *)(a11 + 32);
  v12 = *(_QWORD *)(a3 + 32);
  if (v11)
  {
    if (!v12 || v11 != v12)
      goto LABEL_8;
    __break(1u);
  }
  if (!v12)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  v13 = 4096;
  if ((a10 & 1) == 0)
    v13 = 0;
  v14 = 4;
  if ((a4 & 0x100) != 0)
    v14 = 8;
  v17 = v14 | v13;
  v18 = v12;
  v15 = *(_QWORD *)(a11 + 56);
  v21 = v11;
  v22 = *(_OWORD *)(a11 + 40);
  v23 = v15;
  v16 = *(_QWORD *)(a3 + 56);
  v19 = *(_OWORD *)(a3 + 40);
  v20 = v16;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)&v18, result & 1, a8, a9, (uint64_t)&v21, a2, a4 & 0x1FF, a5, &v17, a6);
}

uint64_t specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t a3, unint64_t a4, uint64_t (*a5)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, float *), double a6, double a7, float a8, uint64_t a9, uint64_t (*a10)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, float *), uint64_t a11, char a12, uint64_t a13)
{
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  __int128 v21;
  uint64_t v22;
  uint64_t v23;
  __int128 v24;
  uint64_t v25;
  uint64_t v26;

  v26 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a13 + 16))
  {
    __break(1u);
LABEL_14:
    __break(1u);
  }
  if (!*(_QWORD *)(a3 + 16))
    goto LABEL_14;
  v13 = *(_QWORD *)(a13 + 32);
  v14 = *(_QWORD *)(a3 + 32);
  if (v13)
  {
    if (!v14 || v13 != v14)
      goto LABEL_8;
    __break(1u);
  }
  if (!v14)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  v15 = 4096;
  if ((a12 & 1) == 0)
    v15 = 0;
  v16 = 4;
  if ((a4 & 0x100000000) != 0)
    v16 = 8;
  v19 = v16 | v15;
  v20 = v14;
  v17 = *(_QWORD *)(a13 + 56);
  v23 = v13;
  v24 = *(_OWORD *)(a13 + 40);
  v25 = v17;
  v18 = *(_QWORD *)(a3 + 56);
  v21 = *(_OWORD *)(a3 + 40);
  v22 = v18;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)&v20, result & 1, a10, a6, a7, a8, a11, (uint64_t)&v23, a2, a4 | ((HIDWORD(a4) & 1) << 32), &v19, a5);
}

uint64_t vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(char *a1, uint64_t a2, uint64_t *a3, uint64_t a4, uint64_t a5, void (*a6)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *, double, double), uint64_t a7, void (*a8)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *, double, double), double a9, double a10, uint64_t a11, char a12, uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16, uint64_t a17)
{
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t result;
  BOOL v24;
  int v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  char v32;
  uint64_t v37;
  _QWORD v38[6];

  v38[4] = *MEMORY[0x1E0C80C00];
  v32 = *a1;
  v20 = *a3;
  v21 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v38[0] = v20;
  type metadata accessor for vImage.PixelBuffer(0, a14, *(_QWORD *)(*(_QWORD *)(a17 + 8) + 8), v22);
  result = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (v21)
  {
    if (result)
      v24 = v21 == result;
    else
      v24 = 0;
    if (!v24)
      goto LABEL_9;
    __break(1u);
  }
  if (!result)
  {
    __break(1u);
    return result;
  }
LABEL_9:
  v25 = (*(uint64_t (**)(uint64_t, uint64_t, uint64_t))(*(_QWORD *)(a15 - 8) + 48))(a4, 1, a15);
  v26 = 4;
  if (v25 == 1)
    v26 = 8;
  v27 = 4096;
  if ((a12 & 1) == 0)
    v27 = 0;
  v37 = v26 | v27;
  v38[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v38[1] = v28;
  v38[2] = v29;
  v38[3] = v30;
  return closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)v38, v20, v32, a8, a11, a2, a4, a5, a9, a10, &v37, a6, a7, *(_QWORD *)(a13 + 16), a14, a15, a16, a17);
}

uint64_t vImage.PixelBuffer<>.shear<A>(direction:translate:slope:resamplingFilter:backgroundColor:destination:)(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4, __int16 a5, uint64_t *a6, uint64_t a7)
{
  uint64_t v12;
  char *v13;
  uint64_t v14;
  void (*v15)(char *);
  _QWORD v17[2];
  uint64_t v18;

  v18 = a4;
  MEMORY[0x1E0C80A78](a1);
  v13 = (char *)v17 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  v15 = *(void (**)(char *))(v14 + 16);
  v15(v13);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  ((void (*)(char *, uint64_t, uint64_t))v15)(v13, a3, a7);
  BinaryFloatingPoint.init<A>(_:)();
  return specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*a1, v18, *a6, a5 & 0x1FF, 0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UInt8, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UInt8, @unowned UInt32) -> (@unowned Int), 0, 0, *(_QWORD *)v17[1]);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UInt8, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, Pixel_8 *a6, vImage_Flags a7, double a8, double a9)
{
  return vImageVerticalShearD_Planar8(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShearD_Planar8(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

uint64_t vImage.PixelBuffer<>.shear<A>(direction:translate:slope:resamplingFilter:backgroundColor:useFloat16Accumulator:destination:)(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4, int a5, int a6, uint64_t *a7, uint64_t a8)
{
  uint64_t v13;
  char *v14;
  uint64_t v15;
  void (*v16)(char *);
  uint64_t *v18;
  int v19;
  uint64_t v20;

  v19 = a6;
  v20 = a4;
  MEMORY[0x1E0C80A78](a1);
  v14 = (char *)&v18 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  v16 = *(void (**)(char *))(v15 + 16);
  v16(v14);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  ((void (*)(char *, uint64_t, uint64_t))v16)(v14, a3, a8);
  BinaryFloatingPoint.init<A>(_:)();
  return specialized vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*a1, v20, *a7, a5 & 0x1FFFF, 0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, __int16 *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UInt16, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, __int16 *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UInt16, @unowned UInt32) -> (@unowned Int), 0, v19, *v18);
}

{
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  void (*v19)(char *, uint64_t, uint64_t);
  uint64_t v20;
  uint64_t v22;
  unsigned __int8 *v23;
  uint64_t *v24;
  uint64_t *v25;

  v24 = a7;
  v23 = a1;
  v14 = *(_QWORD *)(a8 - 8);
  MEMORY[0x1E0C80A78](a1);
  v16 = (char *)&v22 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  if ((v17 & 0x100000000) != 0)
  {
    v18 = 0;
  }
  else
  {
    HIDWORD(v22) = a6;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    v18 = swift_allocObject();
    *(_OWORD *)(v18 + 16) = xmmword_1CAB5E440;
    *(_DWORD *)(v18 + 32) = a5;
    LOBYTE(a6) = BYTE4(v22);
  }
  v19 = *(void (**)(char *, uint64_t, uint64_t))(v14 + 16);
  v19(v16, a2, a8);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  v19(v16, a3, a8);
  BinaryFloatingPoint.init<A>(_:)();
  if (v18)
    v20 = v18 + 32;
  else
    v20 = 0;
  specialized vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*v23, a4, *v24, v20, v18 == 0, (uint64_t)&unk_1E84EDD98, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, a6 & 1, *v25);
  return swift_bridgeObjectRelease();
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UInt16, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, const Pixel_16F *a6, vImage_Flags a7, double a8, double a9)
{
  return vImageVerticalShearD_Planar16F(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShearD_Planar16F(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

uint64_t vImage.PixelBuffer<>.shear<A>(direction:translate:slope:resamplingFilter:backgroundColor:destination:)(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4, unint64_t a5, uint64_t *a6, uint64_t a7)
{
  uint64_t *v7;
  uint64_t *v8;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  void (*v19)(char *, uint64_t, uint64_t);
  uint64_t v20;
  uint64_t v22;
  unint64_t v23;
  uint64_t *v24;
  uint64_t *v25;
  uint64_t v26;
  unsigned __int8 *v27;

  v8 = v7;
  v26 = a3;
  v27 = a1;
  v14 = *(_QWORD *)(a7 - 8);
  MEMORY[0x1E0C80A78](a1);
  v16 = (char *)&v22 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  if ((v17 & 0x100000000) != 0)
  {
    v18 = 0;
  }
  else
  {
    v24 = a6;
    v25 = v8;
    v23 = a5 >> 24;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    v18 = swift_allocObject();
    *(_OWORD *)(v18 + 16) = xmmword_1CAB5F170;
    *(_WORD *)(v18 + 32) = a5;
    *(_BYTE *)(v18 + 34) = BYTE2(a5);
    a6 = v24;
    v8 = v25;
    *(_BYTE *)(v18 + 35) = v23;
  }
  v19 = *(void (**)(char *, uint64_t, uint64_t))(v14 + 16);
  v19(v16, a2, a7);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  v19(v16, v26, a7);
  BinaryFloatingPoint.init<A>(_:)();
  if (v18)
    v20 = v18 + 32;
  else
    v20 = 0;
  specialized vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*v27, a4, *a6, v20, v18 == 0, (uint64_t)&unk_1E84EDB88, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int), 0, 0, *v8);
  return swift_bridgeObjectRelease();
}

{
  uint64_t v12;
  char *v13;
  uint64_t v14;
  void (*v15)(char *);
  double v16;
  _QWORD v18[2];
  uint64_t v19;
  double v20;
  double v21;

  v19 = a4;
  MEMORY[0x1E0C80A78](a1);
  v13 = (char *)v18 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  v15 = *(void (**)(char *))(v14 + 16);
  v15(v13);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  v16 = v21;
  ((void (*)(char *, uint64_t, uint64_t))v15)(v13, a3, a7);
  BinaryFloatingPoint.init<A>(_:)();
  return specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*a1, v19, *a6, a5 | ((HIDWORD(a5) & 1) << 32), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, float *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned Float, @unowned UInt32) -> (@unowned Int), v16, v20, 0.0, 0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, float *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned Float, @unowned UInt32) -> (@unowned Int), 0, 0, *(_QWORD *)v18[1]);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, const uint8_t *__attribute__((__org_typedef(Pixel_8888))) *a6, vImage_Flags a7, double a8, double a9)
{
  return vImageVerticalShearD_ARGB8888(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShearD_ARGB8888(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

uint64_t vImage.PixelBuffer<>.shear<A>(direction:translate:slope:resamplingFilter:backgroundColor:useFloat16Accumulator:destination:)(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4, unint64_t a5, uint64_t a6, int a7, uint64_t *a8, uint64_t a9)
{
  uint64_t *v9;
  uint64_t *v10;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  char v18;
  uint64_t v19;
  __int16 v20;
  void (*v21)(char *, uint64_t, uint64_t);
  uint64_t v22;
  unint64_t v24;
  uint64_t *v25;
  int v26;
  uint64_t v27;
  unsigned __int8 *v28;
  uint64_t *v29;

  v10 = v9;
  v28 = a1;
  v29 = a8;
  v27 = a3;
  v15 = *(_QWORD *)(a9 - 8);
  MEMORY[0x1E0C80A78](a1);
  v17 = (char *)&v24 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  if ((v18 & 1) != 0)
  {
    v19 = 0;
  }
  else
  {
    v25 = v10;
    v26 = a7;
    v24 = HIWORD(a5);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    v19 = swift_allocObject();
    *(_OWORD *)(v19 + 16) = xmmword_1CAB5F170;
    *(_DWORD *)(v19 + 32) = a5;
    v20 = v24;
    v10 = v25;
    *(_WORD *)(v19 + 36) = WORD2(a5);
    LOBYTE(a7) = v26;
    *(_WORD *)(v19 + 38) = v20;
  }
  v21 = *(void (**)(char *, uint64_t, uint64_t))(v15 + 16);
  v21(v17, a2, a9);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  v21(v17, v27, a9);
  BinaryFloatingPoint.init<A>(_:)();
  if (v19)
    v22 = v19 + 32;
  else
    v22 = 0;
  specialized vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*v28, a4, *v29, v22, v19 == 0, (uint64_t)&unk_1E84EDE10, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, a7 & 1, *v10);
  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.shear<A>(direction:translate:slope:resamplingFilter:backgroundColor:destination:)(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4, unint64_t a5, uint64_t a6, uint64_t *a7, uint64_t a8)
{
  uint64_t *v8;
  uint64_t *v9;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  char v18;
  uint64_t v19;
  void (*v20)(char *, uint64_t, uint64_t);
  uint64_t v21;
  uint64_t v23;
  unint64_t v24;
  uint64_t *v25;
  uint64_t *v26;
  uint64_t v27;
  unsigned __int8 *v28;

  v9 = v8;
  v27 = a3;
  v28 = a1;
  v15 = *(_QWORD *)(a8 - 8);
  MEMORY[0x1E0C80A78](a1);
  v17 = (char *)&v23 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  if ((v18 & 1) != 0)
  {
    v19 = 0;
  }
  else
  {
    v25 = a7;
    v26 = v9;
    v24 = HIWORD(a5);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    v19 = swift_allocObject();
    *(_OWORD *)(v19 + 16) = xmmword_1CAB5F170;
    *(_DWORD *)(v19 + 32) = a5;
    *(_WORD *)(v19 + 36) = WORD2(a5);
    a7 = v25;
    v9 = v26;
    *(_WORD *)(v19 + 38) = v24;
  }
  v20 = *(void (**)(char *, uint64_t, uint64_t))(v15 + 16);
  v20(v17, a2, a8);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  v20(v17, v27, a8);
  BinaryFloatingPoint.init<A>(_:)();
  if (v19)
    v21 = v19 + 32;
  else
    v21 = 0;
  specialized vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*v28, a4, *a7, v21, v19 == 0, (uint64_t)&unk_1E84EDEB0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, 0, *v9);
  return swift_bridgeObjectRelease();
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned Float, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, Pixel_F *a6, vImage_Flags flags, double a8, double a9)
{
  return vImageVerticalShearD_PlanarF(a1, a2, a3, a4, a8, a9, a5, *a6, flags);
}

{
  return vImageHorizontalShearD_PlanarF(a1, a2, a3, a4, a8, a9, a5, *a6, flags);
}

uint64_t vImage.PixelBuffer<>.shear<A>(direction:translate:slope:resamplingFilter:backgroundColor:destination:)(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, unint64_t a6, uint64_t a7, uint64_t *a8, uint64_t a9)
{
  uint64_t *v9;
  uint64_t *v10;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  char v18;
  uint64_t v19;
  int v20;
  void (*v21)(char *, uint64_t, uint64_t);
  uint64_t inited;
  uint64_t v23;
  unint64_t v25;
  uint64_t *v26;
  uint64_t v27;
  unsigned __int8 *v28;
  uint64_t *v29;

  v10 = v9;
  v28 = a1;
  v29 = a8;
  v27 = a3;
  v15 = *(_QWORD *)(a9 - 8);
  MEMORY[0x1E0C80A78](a1);
  v17 = (char *)&v25 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  if ((v18 & 1) != 0)
  {
    v19 = 0;
  }
  else
  {
    v26 = v10;
    v25 = HIDWORD(a6);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
    v19 = swift_allocObject();
    *(_OWORD *)(v19 + 16) = xmmword_1CAB5F170;
    *(_QWORD *)(v19 + 32) = a5;
    v20 = v25;
    v10 = v26;
    *(_DWORD *)(v19 + 40) = a6;
    *(_DWORD *)(v19 + 44) = v20;
  }
  v21 = *(void (**)(char *, uint64_t, uint64_t))(v15 + 16);
  v21(v17, a2, a9);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  v21(v17, v27, a9);
  BinaryFloatingPoint.init<A>(_:)();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5E430;
  *(_DWORD *)(inited + 32) = 0;
  if (v19)
    v23 = v19 + 32;
  else
    v23 = 0;
  specialized vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*v28, a4, *v29, v23, v19 == 0, inited + 32, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<Float>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<Float>?, @unowned UInt32) -> (@unowned Int), 0, 0, *v10);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<Float>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, const float *__attribute__((__org_typedef(Pixel_FFFF))) *a6, vImage_Flags a7, double a8, double a9)
{
  return vImageVerticalShearD_ARGBFFFF(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShearD_ARGBFFFF(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

uint64_t closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t a1, uint64_t a2, char a3, void (*a4)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *, double, double), uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, double a10, _QWORD *a11, void (*a12)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *, double, double), uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16, uint64_t a17, uint64_t a18)
{
  int v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  _QWORD v31[6];

  v31[4] = *MEMORY[0x1E0C80C00];
  v20 = a3 & 1;
  v31[0] = a2;
  type metadata accessor for vImage.PixelBuffer(0, a15, *(_QWORD *)(*(_QWORD *)(a18 + 8) + 8), (uint64_t)a4);
  v31[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v31[1] = v21;
  v31[2] = v22;
  v31[3] = v23;
  return closure #1 in closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)v31, v20, a4, a5, a1, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, a16);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, char a2, uint64_t (*a3)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *), uint64_t a4, uint64_t a5, uint64_t a6, __int16 a7, char a8, _QWORD *a9, uint64_t (*a10)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *))
{
  char v11;
  char v12;

  if ((a7 & 0x100) == 0)
    a8 = a7;
  if ((a2 & 1) == 0)
  {
    v12 = a8;
    if ((*a9 & 0x8000000000000000) != 0)
    {
      __break(1u);
    }
    else if (!HIDWORD(*a9))
    {
      return a10(a5, result, 0, 0, a6, &v12);
    }
    __break(1u);
    goto LABEL_13;
  }
  v11 = a8;
  if ((*a9 & 0x8000000000000000) != 0)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  if (HIDWORD(*a9))
  {
LABEL_14:
    __break(1u);
    return result;
  }
  return a3(a5, result, 0, 0, a6, &v11);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, char a2, uint64_t (*a3)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, float *), double a4, double a5, float a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, _QWORD *a11, uint64_t (*a12)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, float *))
{
  float v13;
  float v14;

  if ((a10 & 0x100000000) == 0)
    a6 = *(float *)&a10;
  if ((a2 & 1) == 0)
  {
    v14 = a6;
    if ((*a11 & 0x8000000000000000) != 0)
    {
      __break(1u);
    }
    else if (!HIDWORD(*a11))
    {
      return a12(a8, result, 0, 0, a9, &v14);
    }
    __break(1u);
    goto LABEL_13;
  }
  v13 = a6;
  if ((*a11 & 0x8000000000000000) != 0)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  if (HIDWORD(*a11))
  {
LABEL_14:
    __break(1u);
    return result;
  }
  return a3(a8, result, 0, 0, a9, &v13);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t a1, int a2, void (*a3)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *, double, double), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, double a10, _QWORD *a11, void (*a12)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *, double, double), uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16)
{
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  uint64_t v24;
  char *v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  char *v29;
  uint64_t v30;
  uint64_t v31;
  char *v32;
  uint64_t v33;
  uint64_t v34;
  char *v35;
  uint64_t v36;
  uint64_t v37;
  char *v38;
  void (*v39)(char *, uint64_t, uint64_t);
  uint64_t v40;
  uint64_t result;
  uint64_t v42;
  _QWORD v43[2];
  void (*v44)(uint64_t, uint64_t, _QWORD, _QWORD, uint64_t, char *, double, double);
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  int v48;
  uint64_t v49;
  uint64_t v50;
  _QWORD *v51;

  v47 = a6;
  v49 = a7;
  v50 = a8;
  v45 = a1;
  v46 = a5;
  v43[1] = a4;
  v44 = a3;
  v48 = a2;
  v51 = a11;
  v18 = type metadata accessor for Optional();
  v19 = *(_QWORD *)(v18 - 8);
  v20 = MEMORY[0x1E0C80A78](v18);
  v22 = (char *)v43 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  v23 = MEMORY[0x1E0C80A78](v20);
  v25 = (char *)v43 - v24;
  v26 = *(_QWORD *)(a16 - 8);
  v27 = MEMORY[0x1E0C80A78](v23);
  v29 = (char *)v43 - ((v28 + 15) & 0xFFFFFFFFFFFFFFF0);
  v30 = MEMORY[0x1E0C80A78](v27);
  v32 = (char *)v43 - v31;
  v33 = MEMORY[0x1E0C80A78](v30);
  v35 = (char *)v43 - v34;
  MEMORY[0x1E0C80A78](v33);
  v38 = (char *)v43 - v37;
  v39 = *(void (**)(char *, uint64_t, uint64_t))(v19 + 16);
  if ((v48 & 1) == 0)
  {
    v40 = v36;
    v39(v22, v49, v36);
    (*(void (**)(char *, uint64_t, uint64_t))(v26 + 16))(v29, v50, a16);
    if ((*(unsigned int (**)(char *, uint64_t, uint64_t))(v26 + 48))(v22, 1, a16) == 1)
    {
      (*(void (**)(char *, char *, uint64_t))(v26 + 32))(v32, v29, a16);
      result = (*(uint64_t (**)(char *, uint64_t))(v19 + 8))(v22, v40);
    }
    else
    {
      (*(void (**)(char *, uint64_t))(v26 + 8))(v29, a16);
      result = (*(uint64_t (**)(char *, char *, uint64_t))(v26 + 32))(v32, v22, a16);
    }
    if ((*v51 & 0x8000000000000000) != 0)
    {
      __break(1u);
    }
    else if (!HIDWORD(*v51))
    {
      a12(v46, v45, 0, 0, v47, v32, a9, a10);
      v38 = v32;
      return (*(uint64_t (**)(char *, uint64_t))(v26 + 8))(v38, a16);
    }
    __break(1u);
    goto LABEL_17;
  }
  v42 = v36;
  v39(v25, v49, v36);
  (*(void (**)(char *, uint64_t, uint64_t))(v26 + 16))(v35, v50, a16);
  if ((*(unsigned int (**)(char *, uint64_t, uint64_t))(v26 + 48))(v25, 1, a16) == 1)
  {
    (*(void (**)(char *, char *, uint64_t))(v26 + 32))(v38, v35, a16);
    result = (*(uint64_t (**)(char *, uint64_t))(v19 + 8))(v25, v42);
  }
  else
  {
    (*(void (**)(char *, uint64_t))(v26 + 8))(v35, a16);
    result = (*(uint64_t (**)(char *, char *, uint64_t))(v26 + 32))(v38, v25, a16);
  }
  if ((*v51 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!HIDWORD(*v51))
  {
    v44(v46, v45, 0, 0, v47, v38, a9, a10);
    return (*(uint64_t (**)(char *, uint64_t))(v26 + 8))(v38, a16);
  }
LABEL_18:
  __break(1u);
  return result;
}

BOOL static vImage.ReflectionAxis.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void vImage.ReflectionAxis.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

uint64_t (*vImage.PixelBuffer<>.reflect(over:destination:)(uint64_t (*result)(uint64_t *, uint64_t *, _QWORD), uint64_t *a2))(uint64_t *, uint64_t *, _QWORD)
{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImage_Error (__cdecl *v9)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags);

  v3 = *v2;
  if (!*(_QWORD *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  v4 = *(_QWORD *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v6 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v7 = *(_QWORD *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v8 = *(_QWORD *)(v6 + 40);
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    return result;
  }
  if (*(_BYTE *)result)
    v9 = @nonobjc vImageHorizontalReflect_Planar8(_:_:_:);
  else
    v9 = @nonobjc vImageVerticalReflect_Planar8(_:_:_:);
  return specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t (*)(uint64_t *, uint64_t *, _QWORD))v9, 0, v6, v3);
}

{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImage_Error (__cdecl *v9)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags);

  v3 = *v2;
  if (!*(_QWORD *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  v4 = *(_QWORD *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v6 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v7 = *(_QWORD *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v8 = *(_QWORD *)(v6 + 40);
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    return result;
  }
  if (*(_BYTE *)result)
    v9 = (vImage_Error (__cdecl *)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags))@nonobjc vImageHorizontalReflect_Planar16F(_:_:_:);
  else
    v9 = @nonobjc vImageVerticalReflect_Planar16F(_:_:_:);
  return specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t (*)(uint64_t *, uint64_t *, _QWORD))v9, 0, v6, v3);
}

{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImage_Error (__cdecl *v9)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags);

  v3 = *v2;
  if (!*(_QWORD *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  v4 = *(_QWORD *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v6 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v7 = *(_QWORD *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v8 = *(_QWORD *)(v6 + 40);
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    return result;
  }
  if (*(_BYTE *)result)
    v9 = @nonobjc vImageHorizontalReflect_CbCr16F(_:_:_:);
  else
    v9 = @nonobjc vImageVerticalReflect_CbCr16F(_:_:_:);
  return specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t (*)(uint64_t *, uint64_t *, _QWORD))v9, 0, v6, v3);
}

{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImage_Error (__cdecl *v9)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags);

  v3 = *v2;
  if (!*(_QWORD *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  v4 = *(_QWORD *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v6 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v7 = *(_QWORD *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v8 = *(_QWORD *)(v6 + 40);
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    return result;
  }
  if (*(_BYTE *)result)
    v9 = @nonobjc vImageHorizontalReflect_ARGB16F(_:_:_:);
  else
    v9 = @nonobjc vImageVerticalReflect_ARGB16F(_:_:_:);
  return specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t (*)(uint64_t *, uint64_t *, _QWORD))v9, 0, v6, v3);
}

{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImage_Error (__cdecl *v9)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags);

  v3 = *v2;
  if (!*(_QWORD *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  v4 = *(_QWORD *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v6 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v7 = *(_QWORD *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v8 = *(_QWORD *)(v6 + 40);
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    return result;
  }
  if (*(_BYTE *)result)
    v9 = @nonobjc vImageHorizontalReflect_ARGB8888(_:_:_:);
  else
    v9 = @nonobjc vImageVerticalReflect_ARGB8888(_:_:_:);
  return specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t (*)(uint64_t *, uint64_t *, _QWORD))v9, 0, v6, v3);
}

{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImage_Error (__cdecl *v9)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags);

  v3 = *v2;
  if (!*(_QWORD *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  v4 = *(_QWORD *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v6 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v7 = *(_QWORD *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v8 = *(_QWORD *)(v6 + 40);
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    return result;
  }
  if (*(_BYTE *)result)
    v9 = @nonobjc vImageHorizontalReflect_ARGB16U(_:_:_:);
  else
    v9 = @nonobjc vImageVerticalReflect_ARGB16U(_:_:_:);
  return specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t (*)(uint64_t *, uint64_t *, _QWORD))v9, 0, v6, v3);
}

{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImage_Error (__cdecl *v9)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags);

  v3 = *v2;
  if (!*(_QWORD *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  v4 = *(_QWORD *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v6 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v7 = *(_QWORD *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v8 = *(_QWORD *)(v6 + 40);
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    return result;
  }
  if (*(_BYTE *)result)
    v9 = @nonobjc vImageHorizontalReflect_PlanarF(_:_:_:);
  else
    v9 = @nonobjc vImageVerticalReflect_PlanarF(_:_:_:);
  return specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t (*)(uint64_t *, uint64_t *, _QWORD))v9, 0, v6, v3);
}

{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImage_Error (__cdecl *v9)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags);

  v3 = *v2;
  if (!*(_QWORD *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  v4 = *(_QWORD *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = *(_QWORD *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v6 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v7 = *(_QWORD *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v8 = *(_QWORD *)(v6 + 40);
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    return result;
  }
  if (*(_BYTE *)result)
    v9 = @nonobjc vImageHorizontalReflect_ARGBFFFF(_:_:_:);
  else
    v9 = @nonobjc vImageVerticalReflect_ARGBFFFF(_:_:_:);
  return specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t (*)(uint64_t *, uint64_t *, _QWORD))v9, 0, v6, v3);
}

uint64_t (*specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)(uint64_t (*result)(uint64_t *, uint64_t *, _QWORD), uint64_t a2, uint64_t a3, uint64_t a4))(uint64_t *, uint64_t *, _QWORD)
{
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  __int128 v9;
  uint64_t v10;
  uint64_t v11;
  __int128 v12;
  uint64_t v13;
  uint64_t v14;

  v14 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a4 + 16))
  {
    __break(1u);
LABEL_10:
    __break(1u);
  }
  if (!*(_QWORD *)(a3 + 16))
    goto LABEL_10;
  v4 = *(_QWORD *)(a4 + 32);
  v5 = *(_QWORD *)(a3 + 32);
  if (v4)
  {
    if (!v5 || v4 != v5)
      goto LABEL_8;
    __break(1u);
  }
  if (v5)
  {
LABEL_8:
    v6 = *(_QWORD *)(a4 + 56);
    v11 = v4;
    v12 = *(_OWORD *)(a4 + 40);
    v13 = v6;
    v7 = *(_QWORD *)(a3 + 56);
    v8 = v5;
    v9 = *(_OWORD *)(a3 + 40);
    v10 = v7;
    return (uint64_t (*)(uint64_t *, uint64_t *, _QWORD))result(&v11, &v8, 0);
  }
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>._reflect(reflectFunc:destination:)(uint64_t (*a1)(uint64_t, _QWORD *, _QWORD), uint64_t a2, uint64_t *a3, uint64_t a4, uint64_t a5)
{
  uint64_t v9;
  uint64_t v10;
  uint64_t result;
  BOOL v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  _QWORD v16[5];

  v16[4] = *MEMORY[0x1E0C80C00];
  v9 = *a3;
  v10 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v16[0] = v9;
  result = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (v10)
  {
    if (result)
      v12 = v10 == result;
    else
      v12 = 0;
    if (!v12)
      goto LABEL_9;
    __break(1u);
  }
  if (result)
  {
LABEL_9:
    v16[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
    v16[1] = v13;
    v16[2] = v14;
    v16[3] = v15;
    return closure #1 in vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t)v16, v9, a1, a2, *(_QWORD *)(a4 + 16), a5);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in vImage.PixelBuffer<>._reflect(reflectFunc:destination:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, _QWORD *, _QWORD), uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  _QWORD v12[5];

  v12[4] = *MEMORY[0x1E0C80C00];
  v12[0] = a2;
  type metadata accessor for vImage.PixelBuffer(0, a5, *(_QWORD *)(*(_QWORD *)(a6 + 8) + 8), a4);
  v12[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v12[1] = v8;
  v12[2] = v9;
  v12[3] = v10;
  return a3(a1, v12, 0);
}

uint64_t vImage.PixelBuffer<>.scale(destination:)(uint64_t *a1)
{
  return vImage.PixelBuffer<>.scale(destination:)(a1, MEMORY[0x1E0C8D728]);
}

{
  return vImage.PixelBuffer<>.scale(destination:)(a1, MEMORY[0x1E0C8D710]);
}

{
  return vImage.PixelBuffer<>.scale(destination:)(a1, MEMORY[0x1E0C8D6E8]);
}

{
  return vImage.PixelBuffer<>.scale(destination:)(a1, MEMORY[0x1E0C8D720]);
}

{
  return vImage.PixelBuffer<>.scale(destination:)(a1, MEMORY[0x1E0C8D708]);
}

{
  return vImage.PixelBuffer<>.scale(destination:)(a1, MEMORY[0x1E0C8D6E0]);
}

{
  return vImage.PixelBuffer<>.scale(destination:)(a1, MEMORY[0x1E0C8D738]);
}

{
  return vImage.PixelBuffer<>.scale(destination:)(a1, MEMORY[0x1E0C8D6F8]);
}

uint64_t vImage.PixelBuffer<>.scale(useFloat16Accumulator:destination:)(char a1, uint64_t *a2)
{
  return vImage.PixelBuffer<>.scale(useFloat16Accumulator:destination:)(a1, a2, MEMORY[0x1E0C8D718]);
}

{
  return vImage.PixelBuffer<>.scale(useFloat16Accumulator:destination:)(a1, a2, MEMORY[0x1E0C8D700]);
}

{
  return vImage.PixelBuffer<>.scale(useFloat16Accumulator:destination:)(a1, a2, MEMORY[0x1E0C8D6D8]);
}

uint64_t vImage.PixelBuffer<>.scale(useFloat16Accumulator:destination:)(char a1, uint64_t *a2, uint64_t (*a3)(_OWORD *, _OWORD *, _QWORD, uint64_t))
{
  uint64_t *v3;
  uint64_t v4;
  __int128 v5;
  uint64_t v6;
  uint64_t v7;
  __int128 v8;
  _OWORD v10[2];
  _OWORD v11[2];
  uint64_t v12;

  v12 = *MEMORY[0x1E0C80C00];
  v4 = *v3;
  if (!*(_QWORD *)(*v3 + 16))
  {
    __break(1u);
LABEL_8:
    __break(1u);
  }
  v5 = *(_OWORD *)(v4 + 48);
  v11[0] = *(_OWORD *)(v4 + 32);
  v11[1] = v5;
  v6 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
    goto LABEL_8;
  if ((a1 & 1) != 0)
    v7 = 4096;
  else
    v7 = 0;
  v8 = *(_OWORD *)(v6 + 48);
  v10[0] = *(_OWORD *)(v6 + 32);
  v10[1] = v8;
  return a3(v11, v10, 0, v7);
}

uint64_t vImage.PixelBuffer<>.scale(destination:)(uint64_t *a1, uint64_t (*a2)(_OWORD *, _OWORD *, _QWORD, _QWORD))
{
  uint64_t *v2;
  uint64_t v3;
  __int128 v4;
  uint64_t v5;
  __int128 v6;
  _OWORD v8[2];
  _OWORD v9[2];
  uint64_t v10;

  v10 = *MEMORY[0x1E0C80C00];
  v3 = *v2;
  if (!*(_QWORD *)(*v2 + 16))
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  v4 = *(_OWORD *)(v3 + 48);
  v9[0] = *(_OWORD *)(v3 + 32);
  v9[1] = v4;
  v5 = *a1;
  if (!*(_QWORD *)(*a1 + 16))
    goto LABEL_5;
  v6 = *(_OWORD *)(v5 + 48);
  v8[0] = *(_OWORD *)(v5 + 32);
  v8[1] = v6;
  return a2(v9, v8, 0, 0);
}

uint64_t vImage.PixelBuffer<>.transform(_:backgroundColor:destination:)(_OWORD *a1, __int16 a2, uint64_t *a3)
{
  uint64_t *v3;
  uint64_t v5;
  uint64_t v6;
  __int128 v7;
  unint64_t v8;
  __int128 v9;
  _OWORD v11[3];
  _OWORD v12[2];
  uint64_t v13;

  v13 = *MEMORY[0x1E0C80C00];
  v5 = *a3;
  v6 = *v3;
  v7 = a1[1];
  v11[0] = *a1;
  v11[1] = v7;
  v11[2] = a1[2];
  if (!*(_QWORD *)(v6 + 16))
    __break(1u);
  if ((a2 & 0x100) != 0)
    v8 = 8;
  else
    v8 = 4;
  v9 = *(_OWORD *)(v6 + 48);
  v12[0] = *(_OWORD *)(v6 + 32);
  v12[1] = v9;
  return specialized closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)((uint64_t)v12, v5, (uint64_t)v11, a2 & 0x1FF, 0, v8);
}

uint64_t vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)(_OWORD *a1, uint64_t *a2, uint64_t a3, uint64_t a4, void (*a5)(uint64_t, uint64_t, _QWORD, uint64_t, char *, unint64_t), uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  uint64_t v15;
  __int128 v16;
  unint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  _OWORD v22[3];
  _QWORD v23[5];

  v23[4] = *MEMORY[0x1E0C80C00];
  v15 = *a2;
  v16 = a1[1];
  v22[0] = *a1;
  v22[1] = v16;
  v22[2] = a1[2];
  if ((*(unsigned int (**)(uint64_t, uint64_t, uint64_t))(*(_QWORD *)(a8 - 8) + 48))(a3, 1, a8) == 1)
    v17 = 8;
  else
    v17 = 4;
  v23[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v23[1] = v18;
  v23[2] = v19;
  v23[3] = v20;
  return closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)((uint64_t)v23, v15, a5, a6, (uint64_t)v22, a3, a4, v17, *(_QWORD *)(a7 + 16), a8, a9);
}

vImage_Error vImage.PixelBuffer<>.transform(_:backgroundColor:useFloat16Accumulator:destination:)(_OWORD *a1, int a2, char a3, uint64_t *a4)
{
  uint64_t *v4;

  return specialized vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)(a1, *a4, a2 & 0x1FFFF, 0, a3, *v4);
}

vImage_Error specialized vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)(_OWORD *a1, uint64_t a2, int a3, Pixel_16F a4, char a5, uint64_t a6)
{
  __int128 v6;
  __int128 v7;
  __int128 v8;
  Pixel_16F v9;
  int v10;
  int v11;
  vImage_AffineTransform_Double v13;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v16;

  v16 = *MEMORY[0x1E0C80C00];
  v6 = a1[1];
  *(_OWORD *)&v13.a = *a1;
  *(_OWORD *)&v13.c = v6;
  *(_OWORD *)&v13.tx = a1[2];
  if (!*(_QWORD *)(a6 + 16))
  {
    __break(1u);
LABEL_14:
    __break(1u);
  }
  v7 = *(_OWORD *)(a6 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(a6 + 32);
  *(_OWORD *)&src.width = v7;
  if (!*(_QWORD *)(a2 + 16))
    goto LABEL_14;
  v8 = *(_OWORD *)(a2 + 48);
  *(_OWORD *)&dest.data = *(_OWORD *)(a2 + 32);
  *(_OWORD *)&dest.width = v8;
  if ((a3 & 0x10000) != 0)
    v9 = a4;
  else
    v9 = a3;
  if ((a3 & 0x10000) != 0)
    v10 = 8;
  else
    v10 = 4;
  if ((a5 & 1) != 0)
    v11 = 4096;
  else
    v11 = 0;
  return vImageAffineWarpD_Planar16F(&src, &dest, 0, &v13, v9, v10 | v11);
}

uint64_t specialized vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)(_OWORD *a1, uint64_t a2, uint64_t a3, char a4, uint64_t a5, char a6, uint64_t a7, uint64_t (*a8)(_OWORD *, _OWORD *, _QWORD, _OWORD *, uint64_t, _QWORD))
{
  __int128 v8;
  __int128 v9;
  __int128 v10;
  int v11;
  int v12;
  _OWORD v14[3];
  _OWORD v15[2];
  _OWORD v16[2];
  uint64_t v17;

  v17 = *MEMORY[0x1E0C80C00];
  v8 = a1[1];
  v14[0] = *a1;
  v14[1] = v8;
  v14[2] = a1[2];
  if (!*(_QWORD *)(a7 + 16))
  {
    __break(1u);
LABEL_11:
    __break(1u);
  }
  v9 = *(_OWORD *)(a7 + 48);
  v16[0] = *(_OWORD *)(a7 + 32);
  v16[1] = v9;
  if (!*(_QWORD *)(a2 + 16))
    goto LABEL_11;
  v10 = *(_OWORD *)(a2 + 48);
  v15[0] = *(_OWORD *)(a2 + 32);
  v15[1] = v10;
  if ((a4 & 1) != 0)
  {
    v11 = 8;
  }
  else
  {
    a5 = a3;
    v11 = 4;
  }
  if ((a6 & 1) != 0)
    v12 = 4096;
  else
    v12 = 0;
  return a8(v16, v15, 0, v14, a5, v11 | v12);
}

uint64_t vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)(_OWORD *a1, uint64_t *a2, uint64_t a3, uint64_t a4, void (*a5)(uint64_t, uint64_t, _QWORD, uint64_t, char *), uint64_t a6, char a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v16;
  __int128 v17;
  int v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v25;
  _OWORD v26[3];
  _QWORD v27[5];

  v27[4] = *MEMORY[0x1E0C80C00];
  v16 = *a2;
  v17 = a1[1];
  v26[0] = *a1;
  v26[1] = v17;
  v26[2] = a1[2];
  v18 = (*(uint64_t (**)(uint64_t, uint64_t, uint64_t))(*(_QWORD *)(a9 - 8) + 48))(a3, 1, a9);
  v19 = 4;
  if (v18 == 1)
    v19 = 8;
  v20 = 4096;
  if ((a7 & 1) == 0)
    v20 = 0;
  v25 = v19 | v20;
  v27[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v27[1] = v21;
  v27[2] = v22;
  v27[3] = v23;
  return closure #1 in vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)((uint64_t)v27, v16, a5, a6, (uint64_t)v26, a3, a4, &v25, *(_QWORD *)(a8 + 16), a9, a10);
}

uint64_t vImage.PixelBuffer<>.transform(_:backgroundColor:useFloat16Accumulator:destination:)(_OWORD *a1, unint64_t a2, char a3, uint64_t *a4)
{
  uint64_t *v4;
  char v8;
  __int16 v9;
  unint64_t v10;
  uint64_t v11;
  uint64_t v12;

  v8 = BYTE4(a2) & 1;
  if ((a2 & 0x100000000) != 0)
  {
    v12 = 0;
  }
  else
  {
    v9 = a2;
    v10 = a2 >> 16;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    v11 = swift_allocObject();
    *(_OWORD *)(v11 + 16) = xmmword_1CAB5E440;
    *(_WORD *)(v11 + 32) = v9;
    v12 = v11 + 32;
    *(_WORD *)(v11 + 34) = v10;
  }
  specialized vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)(a1, *a4, v12, v8, (uint64_t)&unk_1E84EDDE8, a3 & 1, *v4, MEMORY[0x1E0C8CB58]);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.transform(_:backgroundColor:useFloat16Accumulator:destination:)(_OWORD *a1, unint64_t a2, char a3, char a4, uint64_t *a5)
{
  uint64_t *v5;
  char v9;
  __int16 v10;
  unint64_t v11;
  unint64_t v12;
  unint64_t v13;
  uint64_t v14;
  uint64_t v15;

  v9 = a3 & 1;
  if ((a3 & 1) != 0)
  {
    v15 = 0;
  }
  else
  {
    v10 = a2;
    v11 = a2 >> 16;
    v12 = HIDWORD(a2);
    v13 = HIWORD(a2);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    v14 = swift_allocObject();
    *(_OWORD *)(v14 + 16) = xmmword_1CAB5F170;
    *(_WORD *)(v14 + 32) = v10;
    v15 = v14 + 32;
    *(_WORD *)(v14 + 34) = v11;
    *(_WORD *)(v14 + 36) = v12;
    *(_WORD *)(v14 + 38) = v13;
  }
  specialized vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)(a1, *a5, v15, v9, (uint64_t)&unk_1E84EDE60, a4 & 1, *v5, MEMORY[0x1E0C8CB50]);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.transform(_:backgroundColor:destination:)(_OWORD *a1, unint64_t a2, uint64_t *a3)
{
  uint64_t *v3;
  unint64_t v7;
  unint64_t v8;
  unint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  __int128 v14;
  __int128 v15;
  unint64_t v16;
  _BYTE v18[16];
  _OWORD v19[3];
  _OWORD v20[2];
  uint64_t v21;

  v21 = *MEMORY[0x1E0C80C00];
  if ((a2 & 0x100000000) != 0)
  {
    v11 = 0;
  }
  else
  {
    v7 = a2 >> 8;
    v8 = a2 >> 16;
    v9 = a2 >> 24;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    v10 = swift_allocObject();
    *(_OWORD *)(v10 + 16) = xmmword_1CAB5F170;
    *(_BYTE *)(v10 + 32) = a2;
    v11 = v10 + 32;
    *(_BYTE *)(v10 + 33) = v7;
    *(_BYTE *)(v10 + 34) = v8;
    *(_BYTE *)(v10 + 35) = v9;
  }
  v12 = *a3;
  v13 = *v3;
  v14 = a1[1];
  v19[0] = *a1;
  v19[1] = v14;
  v19[2] = a1[2];
  if (!*(_QWORD *)(v13 + 16))
    __break(1u);
  v15 = *(_OWORD *)(v13 + 48);
  if ((a2 & 0x100000000) != 0)
    v16 = 8;
  else
    v16 = 4;
  v20[0] = *(_OWORD *)(v13 + 32);
  v20[1] = v15;
  specialized closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)((uint64_t)v20, v12, (uint64_t)v19, v11, (a2 & 0x100000000) >> 32, (uint64_t)&unk_1E84EDC28, v16, (uint64_t)v18, MEMORY[0x1E0C8CB30]);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

{
  uint64_t *v3;
  uint64_t v5;
  uint64_t v6;
  __int128 v7;
  unint64_t v8;
  __n128 v9;
  __int128 v10;
  _OWORD v12[3];
  _OWORD v13[2];
  uint64_t v14;

  v14 = *MEMORY[0x1E0C80C00];
  v5 = *a3;
  v6 = *v3;
  v7 = a1[1];
  v12[0] = *a1;
  v12[1] = v7;
  v12[2] = a1[2];
  if (!*(_QWORD *)(v6 + 16))
    __break(1u);
  if ((a2 & 0x100000000) != 0)
    v8 = 8;
  else
    v8 = 4;
  v10 = *(_OWORD *)(v6 + 48);
  v13[0] = *(_OWORD *)(v6 + 32);
  v9 = (__n128)v13[0];
  v13[1] = v10;
  v9.n128_u32[0] = 0;
  return specialized closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)((uint64_t)v13, v5, (uint64_t)v12, a2 | ((HIDWORD(a2) & 1) << 32), v8, v9);
}

uint64_t vImage.PixelBuffer<>.transform(_:backgroundColor:destination:)(_OWORD *a1, unint64_t a2, char a3, uint64_t *a4)
{
  uint64_t *v4;
  __int16 v8;
  unint64_t v9;
  unint64_t v10;
  unint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  __int128 v16;
  __int128 v17;
  unint64_t v18;
  _BYTE v20[16];
  _OWORD v21[3];
  _OWORD v22[2];
  uint64_t v23;

  v23 = *MEMORY[0x1E0C80C00];
  if ((a3 & 1) != 0)
  {
    v13 = 0;
  }
  else
  {
    v8 = a2;
    v9 = a2 >> 16;
    v10 = HIDWORD(a2);
    v11 = HIWORD(a2);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    v12 = swift_allocObject();
    *(_OWORD *)(v12 + 16) = xmmword_1CAB5F170;
    *(_WORD *)(v12 + 32) = v8;
    v13 = v12 + 32;
    *(_WORD *)(v12 + 34) = v9;
    *(_WORD *)(v12 + 36) = v10;
    *(_WORD *)(v12 + 38) = v11;
  }
  v14 = *a4;
  v15 = *v4;
  v16 = a1[1];
  v21[0] = *a1;
  v21[1] = v16;
  v21[2] = a1[2];
  if (!*(_QWORD *)(v15 + 16))
    __break(1u);
  v17 = *(_OWORD *)(v15 + 48);
  if ((a3 & 1) != 0)
    v18 = 8;
  else
    v18 = 4;
  v22[0] = *(_OWORD *)(v15 + 32);
  v22[1] = v17;
  specialized closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)((uint64_t)v22, v14, (uint64_t)v21, v13, a3 & 1, (uint64_t)&unk_1E84EDF50, v18, (uint64_t)v20, MEMORY[0x1E0C8CB28]);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.transform(_:backgroundColor:destination:)(_OWORD *a1, unint64_t a2, unint64_t a3, char a4, uint64_t *a5)
{
  uint64_t *v5;
  uint64_t v8;
  int v9;
  int v10;
  unint64_t v11;
  unint64_t v12;
  uint64_t inited;
  uint64_t v14;
  uint64_t v15;
  __int128 v16;
  unint64_t v17;
  __int128 v18;
  uint64_t v19;
  _BYTE v21[16];
  _OWORD v22[3];
  _OWORD v23[2];
  uint64_t v24;

  v8 = 0;
  v24 = *MEMORY[0x1E0C80C00];
  if ((a4 & 1) == 0)
  {
    v9 = a3;
    v10 = a2;
    v11 = HIDWORD(a2);
    v12 = HIDWORD(a3);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
    v8 = swift_allocObject();
    *(_OWORD *)(v8 + 16) = xmmword_1CAB5F170;
    *(_DWORD *)(v8 + 32) = v10;
    *(_DWORD *)(v8 + 36) = v11;
    *(_DWORD *)(v8 + 40) = v9;
    *(_DWORD *)(v8 + 44) = v12;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5E430;
  *(_DWORD *)(inited + 32) = 0;
  v14 = *a5;
  v15 = *v5;
  v16 = a1[1];
  v22[0] = *a1;
  v22[1] = v16;
  v22[2] = a1[2];
  if (!*(_QWORD *)(v15 + 16))
    __break(1u);
  if (v8)
    v17 = 4;
  else
    v17 = 8;
  v18 = *(_OWORD *)(v15 + 48);
  if (v8)
    v19 = v8 + 32;
  else
    v19 = 0;
  v23[0] = *(_OWORD *)(v15 + 32);
  v23[1] = v18;
  specialized closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)((uint64_t)v23, v14, (uint64_t)v22, v19, v8 == 0, inited + 32, v17, (uint64_t)v21, MEMORY[0x1E0C8CB38]);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

void vImage_AffineTransform_Double.init(a:b:c:d:tx:ty:)(double *a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>, double a5@<D3>, double a6@<D4>, double a7@<D5>)
{
  *a1 = a2;
  a1[1] = a3;
  a1[2] = a4;
  a1[3] = a5;
  a1[4] = a6;
  a1[5] = a7;
}

uint64_t specialized closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, __int16 a4, unsigned __int8 a5, unint64_t a6)
{
  __int128 v6;
  unsigned __int8 v7;
  _OWORD v9[2];
  uint64_t v10;

  v10 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a2 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  v6 = *(_OWORD *)(a2 + 48);
  v9[0] = *(_OWORD *)(a2 + 32);
  v9[1] = v6;
  if ((a6 & 0x8000000000000000) != 0)
  {
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
  }
  if (HIDWORD(a6))
    goto LABEL_10;
  if ((a4 & 0x100) != 0)
    v7 = a5;
  else
    v7 = a4;
  return MEMORY[0x1D1795110](a1, v9, 0, a3, v7);
}

uint64_t specialized closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, unint64_t a5, __n128 a6)
{
  __int128 v6;
  _OWORD v8[2];
  uint64_t v9;

  v9 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a2 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  v6 = *(_OWORD *)(a2 + 48);
  v8[0] = *(_OWORD *)(a2 + 32);
  v8[1] = v6;
  if ((a5 & 0x8000000000000000) != 0)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (HIDWORD(a5))
    goto LABEL_9;
  if ((a4 & 0x100000000) == 0)
    a6.n128_f32[0] = *(float *)&a4;
  return MEMORY[0x1D179511C](a1, v8, 0, a3, a6);
}

uint64_t specialized closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, char a5, uint64_t a6, unint64_t a7, uint64_t a8, uint64_t (*a9)(uint64_t, _OWORD *, _QWORD, uint64_t, uint64_t, unint64_t))
{
  __int128 v9;
  _OWORD v12[2];
  uint64_t v13;

  v13 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a2 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  v9 = *(_OWORD *)(a2 + 48);
  v12[0] = *(_OWORD *)(a2 + 32);
  v12[1] = v9;
  if ((a7 & 0x8000000000000000) != 0)
  {
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
  }
  if (HIDWORD(a7))
    goto LABEL_10;
  if ((a5 & 1) != 0)
    a4 = a6;
  return a9(a1, v12, 0, a3, a4, a7);
}

uint64_t closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)(uint64_t a1, uint64_t a2, void (*a3)(uint64_t, uint64_t, _QWORD, uint64_t, char *, unint64_t), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, unint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  _QWORD v22[5];

  v22[4] = *MEMORY[0x1E0C80C00];
  v22[0] = a2;
  type metadata accessor for vImage.PixelBuffer(0, a9, *(_QWORD *)(*(_QWORD *)(a11 + 8) + 8), a4);
  v22[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v22[1] = v14;
  v22[2] = v15;
  v22[3] = v16;
  return closure #1 in closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)((uint64_t)v22, a3, a4, a1, a5, a6, a7, a8, a9, a10);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)(uint64_t a1, void (*a2)(uint64_t, uint64_t, _QWORD, uint64_t, char *, unint64_t), uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, unint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  char *v22;
  uint64_t result;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  void (*v28)(uint64_t, uint64_t, _QWORD, uint64_t, char *, unint64_t);
  uint64_t v29;

  v28 = a2;
  v29 = a7;
  v25 = a4;
  v26 = a5;
  v27 = a3;
  v24 = a1;
  v12 = type metadata accessor for Optional();
  v13 = *(_QWORD *)(v12 - 8);
  v14 = MEMORY[0x1E0C80A78](v12);
  v16 = (char *)&v24 - v15;
  v17 = *(_QWORD *)(a10 - 8);
  v18 = MEMORY[0x1E0C80A78](v14);
  v20 = (char *)&v24 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1E0C80A78](v18);
  v22 = (char *)&v24 - v21;
  (*(void (**)(char *, uint64_t, uint64_t))(v13 + 16))(v16, a6, v12);
  (*(void (**)(char *, uint64_t, uint64_t))(v17 + 16))(v20, v29, a10);
  if ((*(unsigned int (**)(char *, uint64_t, uint64_t))(v17 + 48))(v16, 1, a10) == 1)
  {
    (*(void (**)(char *, char *, uint64_t))(v17 + 32))(v22, v20, a10);
    result = (*(uint64_t (**)(char *, uint64_t))(v13 + 8))(v16, v12);
  }
  else
  {
    (*(void (**)(char *, uint64_t))(v17 + 8))(v20, a10);
    result = (*(uint64_t (**)(char *, char *, uint64_t))(v17 + 32))(v22, v16, a10);
  }
  if ((a8 & 0x8000000000000000) != 0)
  {
    __break(1u);
  }
  else if (!HIDWORD(a8))
  {
    v28(v25, v24, 0, v26, v22, a8);
    return (*(uint64_t (**)(char *, uint64_t))(v17 + 8))(v22, a10);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)(uint64_t a1, uint64_t a2, void (*a3)(uint64_t, uint64_t, _QWORD, uint64_t, char *), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, _QWORD *a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  _QWORD v22[5];

  v22[4] = *MEMORY[0x1E0C80C00];
  v22[0] = a2;
  type metadata accessor for vImage.PixelBuffer(0, a9, *(_QWORD *)(*(_QWORD *)(a11 + 8) + 8), a4);
  v22[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v22[1] = v14;
  v22[2] = v15;
  v22[3] = v16;
  return closure #1 in closure #1 in vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)((uint64_t)v22, a3, a4, a1, a5, a6, a7, a8, a9, a10);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)(uint64_t a1, void (*a2)(uint64_t, uint64_t, _QWORD, uint64_t, char *), uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, _QWORD *a8, uint64_t a9, uint64_t a10)
{
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  char *v22;
  uint64_t result;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  void (*v28)(uint64_t, uint64_t, _QWORD, uint64_t, char *);
  _QWORD *v29;

  v28 = a2;
  v29 = a8;
  v25 = a4;
  v26 = a5;
  v27 = a3;
  v24 = a1;
  v12 = type metadata accessor for Optional();
  v13 = *(_QWORD *)(v12 - 8);
  v14 = MEMORY[0x1E0C80A78](v12);
  v16 = (char *)&v24 - v15;
  v17 = *(_QWORD *)(a10 - 8);
  v18 = MEMORY[0x1E0C80A78](v14);
  v20 = (char *)&v24 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1E0C80A78](v18);
  v22 = (char *)&v24 - v21;
  (*(void (**)(char *, uint64_t, uint64_t))(v13 + 16))(v16, a6, v12);
  (*(void (**)(char *, uint64_t, uint64_t))(v17 + 16))(v20, a7, a10);
  if ((*(unsigned int (**)(char *, uint64_t, uint64_t))(v17 + 48))(v16, 1, a10) == 1)
  {
    (*(void (**)(char *, char *, uint64_t))(v17 + 32))(v22, v20, a10);
    result = (*(uint64_t (**)(char *, uint64_t))(v13 + 8))(v16, v12);
  }
  else
  {
    (*(void (**)(char *, uint64_t))(v17 + 8))(v20, a10);
    result = (*(uint64_t (**)(char *, char *, uint64_t))(v17 + 32))(v22, v16, a10);
  }
  if ((*v29 & 0x8000000000000000) != 0)
  {
    __break(1u);
  }
  else if (!HIDWORD(*v29))
  {
    v28(v25, v24, 0, v26, v22);
    return (*(uint64_t (**)(char *, uint64_t))(v17 + 8))(v22, a10);
  }
  __break(1u);
  return result;
}

float vImage_AffineTransform.init(a:b:c:d:tx:ty:)@<S0>(float *a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>, double a5@<D3>, double a6@<D4>, double a7@<D5>)
{
  float v7;
  float v8;
  float v9;
  float v10;
  float v11;
  float result;

  v7 = a2;
  v8 = a3;
  v9 = a4;
  v10 = a5;
  v11 = a6;
  *a1 = v7;
  a1[1] = v8;
  a1[2] = v9;
  a1[3] = v10;
  result = a7;
  a1[4] = v11;
  a1[5] = result;
  return result;
}

unint64_t lazy protocol witness table accessor for type vImage.ShearDirection and conformance vImage.ShearDirection()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImage.ShearDirection and conformance vImage.ShearDirection;
  if (!lazy protocol witness table cache variable for type vImage.ShearDirection and conformance vImage.ShearDirection)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImage.ShearDirection, &type metadata for vImage.ShearDirection);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.ShearDirection and conformance vImage.ShearDirection);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type vImage.ReflectionAxis and conformance vImage.ReflectionAxis()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImage.ReflectionAxis and conformance vImage.ReflectionAxis;
  if (!lazy protocol witness table cache variable for type vImage.ReflectionAxis and conformance vImage.ReflectionAxis)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImage.ReflectionAxis, &type metadata for vImage.ReflectionAxis);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.ReflectionAxis and conformance vImage.ReflectionAxis);
  }
  return result;
}

ValueMetadata *type metadata accessor for vImage.Rotation()
{
  return &type metadata for vImage.Rotation;
}

ValueMetadata *type metadata accessor for vImage.ShearDirection()
{
  return &type metadata for vImage.ShearDirection;
}

uint64_t storeEnumTagSinglePayload for vImage.ShearDirection(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 1 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 1) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFF)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFE)
    return ((uint64_t (*)(void))((char *)&loc_1CAB29610 + 4 * byte_1CAB610A5[v4]))();
  *a1 = a2 + 1;
  return ((uint64_t (*)(void))((char *)sub_1CAB29644 + 4 * byte_1CAB610A0[v4]))();
}

uint64_t sub_1CAB29644(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB2964C(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB29654);
  return result;
}

uint64_t sub_1CAB29660(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB29668);
  *(_BYTE *)result = a2 + 1;
  return result;
}

uint64_t sub_1CAB2966C(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB29674(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vImage.ReflectionAxis()
{
  return &type metadata for vImage.ReflectionAxis;
}

double BNNS.FusedBinaryArithmeticParameters.layerParameters(inputA:inputB:output:)@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, _OWORD *a3@<X2>, uint64_t *a4@<X8>)
{
  uint64_t v4;
  int v7;
  int v8;
  int v9;
  uint64_t v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  int v16;
  uint64_t v17;
  uint64_t v18;
  double result;
  _BYTE v20[180];
  _BYTE v21[180];

  *(_OWORD *)&v21[116] = a2[7];
  *(_OWORD *)&v21[132] = a2[8];
  *(_OWORD *)&v21[148] = a2[9];
  *(_OWORD *)&v21[164] = a2[10];
  *(_OWORD *)&v21[52] = a2[3];
  *(_OWORD *)&v21[68] = a2[4];
  *(_OWORD *)&v21[84] = a2[5];
  *(_OWORD *)&v21[100] = a2[6];
  *(_OWORD *)&v21[4] = *a2;
  *(_OWORD *)&v21[20] = a2[1];
  *(_OWORD *)&v21[36] = a2[2];
  *(_OWORD *)&v20[116] = a3[7];
  *(_OWORD *)&v20[132] = a3[8];
  *(_OWORD *)&v20[148] = a3[9];
  *(_OWORD *)&v20[164] = a3[10];
  *(_OWORD *)&v20[52] = a3[3];
  *(_OWORD *)&v20[68] = a3[4];
  *(_OWORD *)&v20[84] = a3[5];
  *(_OWORD *)&v20[100] = a3[6];
  *(_OWORD *)&v20[4] = *a3;
  *(_OWORD *)&v20[20] = a3[1];
  v7 = *(unsigned __int8 *)(v4 + 8);
  v8 = *(unsigned __int8 *)(v4 + 9);
  v9 = *(unsigned __int8 *)(v4 + 10);
  *(_OWORD *)&v20[36] = a3[2];
  v10 = swift_slowAlloc();
  v11 = a1[9];
  *(_OWORD *)(v10 + 128) = a1[8];
  *(_OWORD *)(v10 + 144) = v11;
  *(_OWORD *)(v10 + 160) = a1[10];
  v12 = a1[5];
  *(_OWORD *)(v10 + 64) = a1[4];
  *(_OWORD *)(v10 + 80) = v12;
  v13 = a1[7];
  *(_OWORD *)(v10 + 96) = a1[6];
  *(_OWORD *)(v10 + 112) = v13;
  v14 = a1[1];
  *(_OWORD *)v10 = *a1;
  *(_OWORD *)(v10 + 16) = v14;
  v15 = a1[3];
  *(_OWORD *)(v10 + 32) = a1[2];
  *(_OWORD *)(v10 + 48) = v15;
  *(_OWORD *)(v10 + 324) = *(_OWORD *)&v21[144];
  *(_OWORD *)(v10 + 340) = *(_OWORD *)&v21[160];
  *(_OWORD *)(v10 + 244) = *(_OWORD *)&v21[64];
  *(_OWORD *)(v10 + 260) = *(_OWORD *)&v21[80];
  *(_OWORD *)(v10 + 276) = *(_OWORD *)&v21[96];
  *(_QWORD *)v4 = v10;
  *(_DWORD *)(v10 + 176) = v7;
  *(_DWORD *)(v10 + 356) = *(_DWORD *)&v21[176];
  *(_OWORD *)(v10 + 292) = *(_OWORD *)&v21[112];
  *(_OWORD *)(v10 + 308) = *(_OWORD *)&v21[128];
  *(_OWORD *)(v10 + 180) = *(_OWORD *)v21;
  *(_OWORD *)(v10 + 196) = *(_OWORD *)&v21[16];
  *(_OWORD *)(v10 + 212) = *(_OWORD *)&v21[32];
  *(_OWORD *)(v10 + 228) = *(_OWORD *)&v21[48];
  *(_DWORD *)(v10 + 360) = v8;
  *(_OWORD *)(v10 + 492) = *(_OWORD *)&v20[128];
  *(_OWORD *)(v10 + 508) = *(_OWORD *)&v20[144];
  *(_OWORD *)(v10 + 524) = *(_OWORD *)&v20[160];
  *(_DWORD *)(v10 + 540) = *(_DWORD *)&v20[176];
  *(_OWORD *)(v10 + 428) = *(_OWORD *)&v20[64];
  *(_OWORD *)(v10 + 444) = *(_OWORD *)&v20[80];
  *(_OWORD *)(v10 + 460) = *(_OWORD *)&v20[96];
  *(_OWORD *)(v10 + 476) = *(_OWORD *)&v20[112];
  *(_OWORD *)(v10 + 364) = *(_OWORD *)v20;
  *(_OWORD *)(v10 + 380) = *(_OWORD *)&v20[16];
  *(_OWORD *)(v10 + 396) = *(_OWORD *)&v20[32];
  *(_OWORD *)(v10 + 412) = *(_OWORD *)&v20[48];
  *(_DWORD *)(v10 + 544) = v9;
  v16 = dword_1CAB61294[*(char *)(v4 + 11)];
  type metadata accessor for BNNSLayerParametersArithmetic(0);
  a4[3] = v17;
  a4[4] = (uint64_t)&protocol witness table for BNNSLayerParametersArithmetic;
  v18 = swift_allocObject();
  *a4 = v18;
  *(_DWORD *)(v18 + 16) = v16;
  *(_QWORD *)(v18 + 24) = v10;
  *(_DWORD *)(v18 + 32) = 0;
  *(int32x2_t *)(v18 + 36) = vdup_n_s32(0x7FC00000u);
  *(_DWORD *)(v18 + 44) = 1;
  result = 0.0;
  *(_OWORD *)(v18 + 48) = 0u;
  *(_OWORD *)(v18 + 64) = 0u;
  return result;
}

void BNNS.FusedBinaryArithmeticParameters.inputADescriptorType.getter(_BYTE *a1@<X8>)
{
  uint64_t v1;

  *a1 = *(_BYTE *)(v1 + 8);
}

_BYTE *BNNS.FusedBinaryArithmeticParameters.inputADescriptorType.setter(_BYTE *result)
{
  uint64_t v1;

  *(_BYTE *)(v1 + 8) = *result;
  return result;
}

uint64_t (*BNNS.FusedBinaryArithmeticParameters.inputADescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedBinaryArithmeticParameters.inputBDescriptorType.getter(_BYTE *a1@<X8>)
{
  uint64_t v1;

  *a1 = *(_BYTE *)(v1 + 9);
}

_BYTE *BNNS.FusedBinaryArithmeticParameters.inputBDescriptorType.setter(_BYTE *result)
{
  uint64_t v1;

  *(_BYTE *)(v1 + 9) = *result;
  return result;
}

uint64_t (*BNNS.FusedBinaryArithmeticParameters.inputBDescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedBinaryArithmeticParameters.outputDescriptorType.getter(_BYTE *a1@<X8>)
{
  uint64_t v1;

  *a1 = *(_BYTE *)(v1 + 10);
}

_BYTE *BNNS.FusedBinaryArithmeticParameters.outputDescriptorType.setter(_BYTE *result)
{
  uint64_t v1;

  *(_BYTE *)(v1 + 10) = *result;
  return result;
}

uint64_t (*BNNS.FusedBinaryArithmeticParameters.outputDescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedBinaryArithmeticParameters.function.getter(_BYTE *a1@<X8>)
{
  uint64_t v1;

  *a1 = *(_BYTE *)(v1 + 11);
}

_BYTE *BNNS.FusedBinaryArithmeticParameters.function.setter(_BYTE *result)
{
  uint64_t v1;

  *(_BYTE *)(v1 + 11) = *result;
  return result;
}

uint64_t (*BNNS.FusedBinaryArithmeticParameters.function.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

char *BNNS.FusedBinaryArithmeticParameters.init(inputADescriptorType:inputBDescriptorType:outputDescriptorType:function:)@<X0>(char *result@<X0>, char *a2@<X1>, char *a3@<X2>, char *a4@<X3>, uint64_t a5@<X8>)
{
  char v5;
  char v6;
  char v7;
  char v8;

  v5 = *result;
  v6 = *a2;
  v7 = *a3;
  v8 = *a4;
  *(_QWORD *)a5 = 0;
  *(_BYTE *)(a5 + 8) = v5;
  *(_BYTE *)(a5 + 9) = v6;
  *(_BYTE *)(a5 + 10) = v7;
  *(_BYTE *)(a5 + 11) = v8;
  return result;
}

uint64_t BNNS.FusedParametersLayer.__allocating_init(inputA:inputB:output:fusedLayerParameters:filterParameters:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint32_t a5, size_t a6, int (__cdecl *a7)(void **, size_t, size_t), void (__cdecl *a8)(void *))
{
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  int v22;
  int v23;
  void *v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  __int128 v29;
  _DWORD v30[6];
  __int128 v31;
  uint64_t v32;
  int v33;
  uint64_t v34;
  int v35;
  __int128 v36;
  int v37;
  __int128 v38;
  uint64_t v39;
  char v40[40];
  _QWORD v41[5];
  _QWORD v42[5];
  _QWORD v43[3];
  uint64_t v44;
  uint64_t v45;
  _BYTE v46[24];
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;

  v49 = *MEMORY[0x1E0C80C00];
  if (*(_QWORD *)(a4 + 16) != 2)
  {
    __break(1u);
    goto LABEL_20;
  }
  outlined init with copy of BNNSOptimizer(a4 + 32, (uint64_t)&v33);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParameters);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableBinaryInputLayerParametersWrapper);
  if ((swift_dynamicCast() & 1) == 0)
  {
    v29 = 0u;
    memset(v30, 0, sizeof(v30));
    swift_bridgeObjectRelease();
    outlined destroy of FusableTernaryInputLayerParametersWrapper?((uint64_t)&v29, &demangling cache variable for type metadata for FusableBinaryInputLayerParametersWrapper?);
    return 0;
  }
  outlined init with take of FusableLayerParametersWrapper(&v29, (uint64_t)v46);
  if (*(_QWORD *)(a4 + 16) < 2uLL)
LABEL_20:
    __break(1u);
  outlined init with copy of BNNSOptimizer(a4 + 72, (uint64_t)&v33);
  swift_bridgeObjectRelease();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParametersWrapper);
  if ((swift_dynamicCast() & 1) == 0)
  {
    v29 = 0u;
    memset(v30, 0, sizeof(v30));
    outlined destroy of FusableTernaryInputLayerParametersWrapper?((uint64_t)&v29, &demangling cache variable for type metadata for FusableLayerParametersWrapper?);
LABEL_16:
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v46);
    return 0;
  }
  outlined init with take of FusableLayerParametersWrapper(&v29, (uint64_t)v43);
  v17 = v47;
  v16 = v48;
  __swift_mutable_project_boxed_opaque_existential_1((uint64_t)v46, v47);
  (*(void (**)(_QWORD *__return_ptr, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))(v16 + 8))(v42, a1, a2, a3, v17, v16);
  v18 = v44;
  v19 = v45;
  __swift_mutable_project_boxed_opaque_existential_1((uint64_t)v43, v44);
  (*(void (**)(_QWORD *__return_ptr, uint64_t, uint64_t, uint64_t, uint64_t))(v19 + 8))(v41, a3, a3, v18, v19);
  v20 = v44;
  v21 = v45;
  __swift_project_boxed_opaque_existential_1(v43, v44);
  v22 = (*(uint64_t (**)(uint64_t, uint64_t))(v21 + 16))(v20, v21);
  if ((v22 - 2) > 3)
  {
LABEL_15:
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v41);
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v42);
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v43);
    goto LABEL_16;
  }
  v23 = v22;
  outlined init with copy of BNNSOptimizer((uint64_t)v42, (uint64_t)v40);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
  type metadata accessor for BNNSLayerParametersArithmetic(0);
  swift_dynamicCast();
  LODWORD(v29) = v33;
  *((_QWORD *)&v29 + 1) = v34;
  v30[0] = v35;
  *(_OWORD *)&v30[1] = v36;
  v30[5] = v37;
  v31 = v38;
  v32 = v39;
  v24 = specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)((uint64_t)&v29, (uint64_t)v41, a5, a6, a7, a8, 8, v23);
  type metadata accessor for BNNS.FusedParametersLayer();
  v25 = swift_allocObject();
  v26 = v25;
  *(_QWORD *)(v25 + 24) = MEMORY[0x1E0DEE9D8];
  if (!v24)
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    goto LABEL_15;
  }
  *(_QWORD *)(v25 + 16) = v24;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<FusableLayerParametersWrapperDeallocatable?>);
  v27 = swift_allocObject();
  *(_OWORD *)(v27 + 16) = xmmword_1CAB5E440;
  outlined init with copy of BNNSOptimizer((uint64_t)v46, (uint64_t)&v33);
  swift_retain();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParametersWrapperDeallocatable);
  if ((swift_dynamicCast() & 1) == 0)
  {
    *(_QWORD *)(v27 + 64) = 0;
    *(_OWORD *)(v27 + 32) = 0u;
    *(_OWORD *)(v27 + 48) = 0u;
  }
  outlined init with copy of BNNSOptimizer((uint64_t)v43, (uint64_t)&v29);
  if ((swift_dynamicCast() & 1) == 0)
  {
    *(_QWORD *)(v27 + 104) = 0;
    *(_OWORD *)(v27 + 72) = 0u;
    *(_OWORD *)(v27 + 88) = 0u;
  }
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v41);
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v42);
  *(_QWORD *)(v26 + 24) = v27;
  swift_release();
  swift_bridgeObjectRelease();
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v43);
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v46);
  return v26;
}

uint64_t BNNS.FusedParametersLayer.apply(batchSize:inputA:inputB:output:for:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, char a5)
{
  uint64_t v5;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  unint64_t v16;
  size_t v17;
  int v18;
  uint64_t result;
  _BYTE *v20;
  _BYTE *v21;
  unint64_t v22;
  void *v23;
  const void **v24;
  void *out;
  uint64_t v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  unint64_t v38;
  unint64_t v39;
  _BYTE v40[136];
  _BYTE v41[136];
  _BYTE v42[136];
  _BYTE v43[136];
  _BYTE v44[136];
  _BYTE v45[8];
  _BYTE v46[8];
  _BYTE v47[8];
  uint64_t v48;
  uint64_t v49;
  void *v50;

  outlined init with take of UnsafeMutableRawPointer?(a2 + 136, (uint64_t)v47);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v47, (uint64_t)&v48);
  v10 = v48;
  if (v48
    && (outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v46),
        outlined init with take of UnsafeMutableRawPointer?((uint64_t)v46, (uint64_t)&v49),
        (v11 = v49) != 0)
    && (outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v45),
        outlined init with take of UnsafeMutableRawPointer?((uint64_t)v45, (uint64_t)&v50),
        v50))
  {
    out = v50;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
    v12 = swift_allocObject();
    *(_OWORD *)(v12 + 16) = xmmword_1CAB5E440;
    *(_QWORD *)(v12 + 32) = v10;
    v24 = (const void **)(v12 + 32);
    *(_QWORD *)(v12 + 40) = v11;
    v23 = *(void **)(v5 + 16);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
    v26 = swift_allocObject();
    *(_OWORD *)(v26 + 16) = xmmword_1CAB5E440;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v41);
    outlined init with take of BNNS.Shape((uint64_t)v41, (uint64_t)v42);
    outlined init with take of BNNS.Shape((uint64_t)v42, (uint64_t)v40);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v42, (uint64_t)v40);
    BNNS.Shape.stride.getter();
    *(_QWORD *)(v26 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v35, *((unint64_t *)&v35 + 1), v36, *((unint64_t *)&v36 + 1), v37, *((unint64_t *)&v37 + 1), v38, v39, v35, *((unint64_t *)&v35 + 1), v36, *((unint64_t *)&v36 + 1), v37, *((unint64_t *)&v37 + 1), v38, v39);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v40);
    outlined init with take of BNNS.Shape((uint64_t)v40, (uint64_t)v43);
    outlined init with take of BNNS.Shape((uint64_t)v43, (uint64_t)&v35);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v43, (uint64_t)&v35);
    BNNS.Shape.stride.getter();
    *(_QWORD *)(v26 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v27, v28, v29, v30, v31, v32, v33, v34, v27, v28, v29, v30, v31, v32, v33, v34);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v41);
    outlined init with take of BNNS.Shape((uint64_t)v41, (uint64_t)v44);
    outlined init with take of BNNS.Shape((uint64_t)v44, (uint64_t)v40);
    BNNS.Shape.size.getter();
    v13 = v35;
    v14 = v36;
    v15 = v37;
    v16 = v38;
    v22 = v39;
    outlined init with take of BNNS.Shape((uint64_t)v44, (uint64_t)v40);
    BNNS.Shape.stride.getter();
    v17 = specialized static BNNS.calculateBatchStride(size:stride:)(v13, *((unint64_t *)&v13 + 1), v14, *((unint64_t *)&v14 + 1), v15, *((unint64_t *)&v15 + 1), v16, v22, v35, *((unint64_t *)&v35 + 1), v36, *((unint64_t *)&v36 + 1), v37, *((unint64_t *)&v37 + 1), v38, v39);
    v18 = BNNSFusedFilterApplyMultiInputBatch(v23, a1, 2uLL, v24, (const size_t *)(v26 + 32), out, v17, (a5 & 1) == 0);
    swift_bridgeObjectRelease();
    result = swift_bridgeObjectRelease();
    if (!v18)
      return result;
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v20 = 0;
  }
  else
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v21 = 2;
  }
  return swift_willThrow();
}

uint64_t BNNS.FusedParametersLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingParameterGradients:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _OWORD *a5, __int128 *a6, _OWORD *a7, uint64_t a8)
{
  uint64_t v8;
  uint64_t v9;
  size_t v13;
  int64_t v14;
  uint64_t v15;
  __int128 *v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  _OWORD *v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  unint64_t v28;
  unint64_t v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  _BYTE *v48;
  char *v49;
  uint64_t v50;
  uint64_t i;
  uint64_t v52;
  char *v53;
  uint64_t v54;
  uint64_t j;
  uint64_t v56;
  uint64_t v58;
  _OWORD *v59;
  uint64_t v60;
  uint64_t v61;
  uint64_t v63;
  int v64;
  char *v65;
  BNNSNDArrayDescriptor v66;
  _OWORD v67[11];
  __int128 v68;
  __int128 v69;
  __int128 v70;
  __int128 v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  __int128 v76;
  __int128 v77;
  __int128 v78;
  uint64_t v79;

  v9 = v8;
  v13 = a1;
  v79 = *MEMORY[0x1E0C80C00];
  v14 = *(_QWORD *)(a8 + 16);
  v15 = MEMORY[0x1E0DEE9D8];
  if (v14)
  {
    v58 = a4;
    v59 = a7;
    v60 = a2;
    v61 = a3;
    v63 = v9;
    *(_QWORD *)&v67[0] = MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v14, 0);
    v16 = (__int128 *)(a8 + 32);
    v15 = *(_QWORD *)&v67[0];
    do
    {
      v17 = v16[9];
      v76 = v16[8];
      v77 = v17;
      v78 = v16[10];
      v18 = v16[5];
      v72 = v16[4];
      v73 = v18;
      v19 = v16[7];
      v74 = v16[6];
      v75 = v19;
      v20 = v16[1];
      v68 = *v16;
      v69 = v20;
      v21 = v16[3];
      v70 = v16[2];
      v71 = v21;
      v22 = (_OWORD *)swift_slowAlloc();
      v23 = v77;
      v22[8] = v76;
      v22[9] = v23;
      v22[10] = v78;
      v24 = v73;
      v22[4] = v72;
      v22[5] = v24;
      v25 = v75;
      v22[6] = v74;
      v22[7] = v25;
      v26 = v69;
      *v22 = v68;
      v22[1] = v26;
      v27 = v71;
      v22[2] = v70;
      v22[3] = v27;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(_QWORD *)(v15 + 16) + 1, 1);
        v15 = *(_QWORD *)&v67[0];
      }
      v29 = *(_QWORD *)(v15 + 16);
      v28 = *(_QWORD *)(v15 + 24);
      if (v29 >= v28 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v28 > 1), v29 + 1, 1);
        v15 = *(_QWORD *)&v67[0];
      }
      *(_QWORD *)(v15 + 16) = v29 + 1;
      *(_QWORD *)(v15 + 8 * v29 + 32) = v22;
      v16 += 11;
      --v14;
    }
    while (v14);
    v9 = v63;
    a3 = v61;
    v13 = a1;
    a7 = v59;
    a2 = v60;
    a4 = v58;
  }
  v30 = a6[9];
  v76 = a6[8];
  v77 = v30;
  v78 = a6[10];
  v31 = a6[5];
  v72 = a6[4];
  v73 = v31;
  v32 = a6[7];
  v74 = a6[6];
  v75 = v32;
  v33 = a6[1];
  v68 = *a6;
  v69 = v33;
  v34 = a6[3];
  v70 = a6[2];
  v71 = v34;
  v35 = a7[8];
  v36 = a7[9];
  v37 = a7[6];
  v67[7] = a7[7];
  v67[8] = v35;
  v38 = a7[10];
  v67[9] = v36;
  v67[10] = v38;
  v39 = a7[4];
  v40 = a7[5];
  v41 = a7[2];
  v67[3] = a7[3];
  v67[4] = v39;
  v65 = (char *)v15;
  v64 = 0;
  v67[5] = v40;
  v67[6] = v37;
  v42 = *a7;
  v67[1] = a7[1];
  v67[2] = v41;
  v43 = a5[9];
  *(_OWORD *)&v66.stride[7] = a5[8];
  *(_OWORD *)&v66.data_type = v43;
  *(_OWORD *)&v66.table_data_type = a5[10];
  v67[0] = v42;
  v44 = a5[5];
  *(_OWORD *)&v66.size[7] = a5[4];
  *(_OWORD *)&v66.stride[1] = v44;
  v45 = a5[7];
  *(_OWORD *)&v66.stride[3] = a5[6];
  *(_OWORD *)&v66.stride[5] = v45;
  v46 = a5[1];
  *(_OWORD *)&v66.flags = *a5;
  *(_OWORD *)&v66.size[1] = v46;
  v47 = a5[3];
  *(_OWORD *)&v66.size[3] = a5[2];
  *(_OWORD *)&v66.size[5] = v47;
  closure #1 in closure #1 in closure #2 in BNNS.FusedParametersLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingParameterGradients:)(&v66, a2, a3, (uint64_t)&v68, (uint64_t)v67, &v64, v9, v13, (uint64_t)a6, (uint64_t)a7, a4, (uint64_t)a5, &v65);
  if (v64)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v48 = 0;
    swift_willThrow();
    v49 = v65;
    v50 = *((_QWORD *)v65 + 2);
    if (v50)
    {
      swift_bridgeObjectRetain();
      for (i = 0; i != v50; ++i)
      {
        v52 = *(_QWORD *)&v49[8 * i + 32];
        if (v52)
          MEMORY[0x1D1794DA4](v52, -1, -1);
      }
LABEL_20:
      swift_bridgeObjectRelease();
    }
  }
  else
  {
    v53 = v65;
    v54 = *((_QWORD *)v65 + 2);
    if (v54)
    {
      swift_bridgeObjectRetain();
      for (j = 0; j != v54; ++j)
      {
        v56 = *(_QWORD *)&v53[8 * j + 32];
        if (v56)
          MEMORY[0x1D1794DA4](v56, -1, -1);
      }
      goto LABEL_20;
    }
  }
  return swift_bridgeObjectRelease();
}

uint64_t closure #1 in closure #1 in closure #2 in BNNS.FusedParametersLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingParameterGradients:)(BNNSNDArrayDescriptor *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, int *a6, uint64_t a7, size_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, char **a13)
{
  uint64_t v18;
  uint64_t v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  unint64_t v23;
  unint64_t v24;
  unint64_t v25;
  uint64_t v26;
  unint64_t v27;
  size_t out_delta_stride;
  char *v29;
  char isUniquelyReferenced_nonNull_native;
  int v31;
  uint64_t result;
  void *out;
  size_t out_stride;
  const size_t *v35;
  uint64_t v36;
  void *v37;
  void **in;
  uint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  unint64_t v46;
  unint64_t v47;
  unint64_t v48;
  unint64_t v49;
  unint64_t v50;
  _BYTE v51[136];
  unint64_t v52;
  unint64_t v53;
  unint64_t v54;
  unint64_t v55;
  unint64_t v56;
  unint64_t v57;
  unint64_t v58;
  unint64_t v59;
  _BYTE v60[136];
  unint64_t v61;
  unint64_t v62;
  unint64_t v63;
  unint64_t v64;
  unint64_t v65;
  unint64_t v66;
  unint64_t v67;
  unint64_t v68;
  _BYTE v69[136];
  _BYTE v70[136];
  __int128 v71;
  __int128 v72;
  __int128 v73;
  unint64_t v74;
  unint64_t v75;
  _BYTE v76[136];
  _BYTE v77[136];
  _BYTE v78[8];
  _BYTE v79[8];
  _BYTE v80[8];
  uint64_t v81;
  uint64_t v82;
  void *v83;
  uint64_t v84;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer?>);
  v18 = swift_allocObject();
  *(_OWORD *)(v18 + 16) = xmmword_1CAB5E440;
  v19 = v18;
  outlined init with take of UnsafeMutableRawPointer?(a2 + 136, (uint64_t)v80);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v80, (uint64_t)&v81);
  *(_QWORD *)(v19 + 32) = v81;
  in = (void **)(v19 + 32);
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v79);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v79, (uint64_t)&v82);
  *(_QWORD *)(v19 + 40) = v82;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>>);
  v36 = swift_allocObject();
  *(_OWORD *)(v36 + 16) = xmmword_1CAB5E440;
  *(_QWORD *)(v36 + 32) = a4;
  *(_QWORD *)(v36 + 40) = a5;
  v37 = *(void **)(a7 + 16);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  v84 = swift_allocObject();
  *(_OWORD *)(v84 + 16) = xmmword_1CAB5E440;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v77);
  outlined init with take of BNNS.Shape((uint64_t)v77, (uint64_t)v70);
  outlined init with take of BNNS.Shape((uint64_t)v70, (uint64_t)v76);
  BNNS.Shape.size.getter();
  v20 = v71;
  v21 = v72;
  v22 = v73;
  v23 = v74;
  v24 = v75;
  outlined init with take of BNNS.Shape((uint64_t)v70, (uint64_t)v76);
  BNNS.Shape.stride.getter();
  v25 = specialized static BNNS.calculateBatchStride(size:stride:)(v20, *((unint64_t *)&v20 + 1), v21, *((unint64_t *)&v21 + 1), v22, *((unint64_t *)&v22 + 1), v23, v24, v71, *((unint64_t *)&v71 + 1), v72, *((unint64_t *)&v72 + 1), v73, *((unint64_t *)&v73 + 1), v74, v75);
  v26 = v84;
  *(_QWORD *)(v84 + 32) = v25;
  v35 = (const size_t *)(v26 + 32);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v76);
  outlined init with take of BNNS.Shape((uint64_t)v76, (uint64_t)&v71);
  outlined init with take of BNNS.Shape((uint64_t)&v71, (uint64_t)v69);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)&v71, (uint64_t)v69);
  BNNS.Shape.stride.getter();
  v27 = specialized static BNNS.calculateBatchStride(size:stride:)(v61, v62, v63, v64, v65, v66, v67, v68, v61, v62, v63, v64, v65, v66, v67, v68);
  *(_QWORD *)(v84 + 40) = v27;
  v42 = swift_allocObject();
  *(_OWORD *)(v42 + 16) = xmmword_1CAB5E440;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v69);
  outlined init with take of BNNS.Shape((uint64_t)v69, (uint64_t)v76);
  outlined init with take of BNNS.Shape((uint64_t)v76, (uint64_t)v77);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v76, (uint64_t)v77);
  BNNS.Shape.stride.getter();
  *(_QWORD *)(v42 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v61, v62, v63, v64, v65, v66, v67, v68, v61, v62, v63, v64, v65, v66, v67, v68);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v61);
  outlined init with take of BNNS.Shape((uint64_t)&v61, (uint64_t)v77);
  outlined init with take of BNNS.Shape((uint64_t)v77, (uint64_t)v60);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v77, (uint64_t)v60);
  BNNS.Shape.stride.getter();
  *(_QWORD *)(v42 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v52, v53, v54, v55, v56, v57, v58, v59, v52, v53, v54, v55, v56, v57, v58, v59);
  outlined init with take of UnsafeMutableRawPointer?(a11 + 136, (uint64_t)v78);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v78, (uint64_t)&v83);
  out = v83;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v60);
  outlined init with take of BNNS.Shape((uint64_t)v60, (uint64_t)&v61);
  outlined init with take of BNNS.Shape((uint64_t)&v61, (uint64_t)v69);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)&v61, (uint64_t)v69);
  BNNS.Shape.stride.getter();
  out_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v52, v53, v54, v55, v56, v57, v58, v59, v52, v53, v54, v55, v56, v57, v58, v59);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v52);
  outlined init with take of BNNS.Shape((uint64_t)&v52, (uint64_t)v69);
  outlined init with take of BNNS.Shape((uint64_t)v69, (uint64_t)v51);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v69, (uint64_t)v51);
  BNNS.Shape.stride.getter();
  out_delta_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v43, v44, v45, v46, v47, v48, v49, v50, v43, v44, v45, v46, v47, v48, v49, v50);
  v29 = *a13;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
  *a13 = v29;
  if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    v29 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v29 + 2), 0, v29);
  *a13 = v29;
  swift_bridgeObjectRetain();
  v31 = BNNSFusedFilterApplyBackwardMultiInputBatch(v37, a8, 2uLL, (const void **)in, v35, (BNNSNDArrayDescriptor **)(v36 + 32), (const size_t *)(v42 + 32), out, out_stride, a1, out_delta_stride, (BNNSNDArrayDescriptor **)v29 + 4);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease_n();
  result = swift_bridgeObjectRelease_n();
  *a6 = v31;
  return result;
}

uint64_t dispatch thunk of FusableBinaryInputLayerParametersWrapper.layerParameters(inputA:inputB:output:)(uint64_t *a1, uint64_t *a2, uint64_t *a3, uint64_t a4, uint64_t a5)
{
  uint64_t v5;
  int v6;
  uint64_t v7;
  int v8;
  uint64_t v9;
  int v10;
  uint64_t v11;
  int v12;
  uint64_t v13;
  int v14;
  uint64_t v15;
  int v16;
  uint64_t (*v17)(uint64_t *, uint64_t *, uint64_t *);
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  uint64_t v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  uint64_t v33;
  int v34;
  uint64_t v35;
  int v36;
  uint64_t v37;
  uint64_t v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  uint64_t v47;
  int v48;
  uint64_t v49;
  int v50;
  uint64_t v51;
  uint64_t v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  uint64_t v61;
  int v62;
  uint64_t v63;
  int v64;
  uint64_t v65;

  v5 = a1[17];
  v6 = *((_DWORD *)a1 + 36);
  v7 = a1[19];
  v8 = *((_DWORD *)a1 + 40);
  v9 = a2[17];
  v10 = *((_DWORD *)a2 + 36);
  v11 = a2[19];
  v12 = *((_DWORD *)a2 + 40);
  v13 = a3[17];
  v14 = *((_DWORD *)a3 + 36);
  v15 = a3[19];
  v16 = *((_DWORD *)a3 + 40);
  v17 = *(uint64_t (**)(uint64_t *, uint64_t *, uint64_t *))(a5 + 8);
  v52 = *a1;
  v53 = *(_OWORD *)(a1 + 1);
  v54 = *(_OWORD *)(a1 + 3);
  v55 = *(_OWORD *)(a1 + 5);
  v56 = *(_OWORD *)(a1 + 7);
  v57 = *(_OWORD *)(a1 + 9);
  v58 = *(_OWORD *)(a1 + 11);
  v59 = *(_OWORD *)(a1 + 13);
  v60 = *(_OWORD *)(a1 + 15);
  v61 = v5;
  v62 = v6;
  v63 = v7;
  v64 = v8;
  v65 = *(uint64_t *)((char *)a1 + 164);
  v38 = *a2;
  v39 = *(_OWORD *)(a2 + 1);
  v40 = *(_OWORD *)(a2 + 3);
  v41 = *(_OWORD *)(a2 + 5);
  v42 = *(_OWORD *)(a2 + 7);
  v43 = *(_OWORD *)(a2 + 9);
  v44 = *(_OWORD *)(a2 + 11);
  v45 = *(_OWORD *)(a2 + 13);
  v46 = *(_OWORD *)(a2 + 15);
  v47 = v9;
  v48 = v10;
  v49 = v11;
  v50 = v12;
  v51 = *(uint64_t *)((char *)a2 + 164);
  v24 = *a3;
  v25 = *(_OWORD *)(a3 + 1);
  v26 = *(_OWORD *)(a3 + 3);
  v18 = *(_OWORD *)(a3 + 7);
  v19 = *(_OWORD *)(a3 + 9);
  v20 = *(_OWORD *)(a3 + 11);
  v21 = *(_OWORD *)(a3 + 13);
  v22 = *(_OWORD *)(a3 + 15);
  v27 = *(_OWORD *)(a3 + 5);
  v28 = v18;
  v29 = v19;
  v30 = v20;
  v31 = v21;
  v32 = v22;
  v33 = v13;
  v34 = v14;
  v35 = v15;
  v36 = v16;
  v37 = *(uint64_t *)((char *)a3 + 164);
  return v17(&v52, &v38, &v24);
}

ValueMetadata *type metadata accessor for BNNS.FusedBinaryArithmeticParameters()
{
  return &type metadata for BNNS.FusedBinaryArithmeticParameters;
}

uint64_t sub_1CAB2ACC4()
{
  return swift_deallocObject();
}

uint64_t *Quadrature.init(integrator:absoluteTolerance:relativeTolerance:)@<X0>(uint64_t *result@<X0>, uint64_t a2@<X8>, double a3@<D0>, double a4@<D1>)
{
  uint64_t v4;
  int v5;
  BOOL v6;
  uint64_t v7;
  int v8;

  v4 = *result;
  v5 = *((unsigned __int8 *)result + 16);
  if (*((_BYTE *)result + 16))
  {
    v6 = v5 == 1;
    if (v5 == 1)
      v7 = *result;
    else
      v7 = 0;
    v4 = 0;
    if (v6)
      v8 = 2;
    else
      v8 = 0;
  }
  else
  {
    v7 = result[1];
    v8 = 1;
  }
  *(_DWORD *)a2 = v8;
  *(double *)(a2 + 8) = a3;
  *(double *)(a2 + 16) = a4;
  *(_QWORD *)(a2 + 24) = v4;
  *(_QWORD *)(a2 + 32) = v7;
  return result;
}

double Quadrature.absoluteTolerance.getter()
{
  uint64_t v0;

  return *(double *)(v0 + 8);
}

void Quadrature.absoluteTolerance.setter(double a1)
{
  uint64_t v1;

  *(double *)(v1 + 8) = a1;
}

double (*Quadrature.absoluteTolerance.modify(_QWORD *a1))(uint64_t a1)
{
  uint64_t v1;

  a1[1] = v1;
  *a1 = *(_QWORD *)(v1 + 8);
  return Quadrature.absoluteTolerance.modify;
}

double Quadrature.absoluteTolerance.modify(uint64_t a1)
{
  double result;

  result = *(double *)a1;
  *(_QWORD *)(*(_QWORD *)(a1 + 8) + 8) = *(_QWORD *)a1;
  return result;
}

double Quadrature.relativeTolerance.getter()
{
  uint64_t v0;

  return *(double *)(v0 + 16);
}

void Quadrature.relativeTolerance.setter(double a1)
{
  uint64_t v1;

  *(double *)(v1 + 16) = a1;
}

double (*Quadrature.relativeTolerance.modify(_QWORD *a1))(uint64_t a1)
{
  uint64_t v1;

  a1[1] = v1;
  *a1 = *(_QWORD *)(v1 + 16);
  return Quadrature.relativeTolerance.modify;
}

double Quadrature.relativeTolerance.modify(uint64_t a1)
{
  double result;

  result = *(double *)a1;
  *(_QWORD *)(*(_QWORD *)(a1 + 8) + 16) = *(_QWORD *)a1;
  return result;
}

uint64_t Quadrature.integrate(over:integrand:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>, double a4@<D0>, double a5@<D1>)
{
  uint64_t v5;
  quadrature_integrator v11;
  double v12;
  double v13;
  size_t v14;
  size_t v15;
  uint64_t v16;
  uint64_t v17;
  double v18;
  uint64_t result;
  double v20;
  __int128 v21;
  _QWORD v22[2];
  double abs_error;
  quadrature_status status;
  quadrature_integrate_options options;
  quadrature_integrate_function __f;
  uint64_t v27;

  v27 = *MEMORY[0x1E0C80C00];
  v11 = *(_DWORD *)v5;
  v12 = *(double *)(v5 + 8);
  v13 = *(double *)(v5 + 16);
  v14 = *(_QWORD *)(v5 + 24);
  v15 = *(_QWORD *)(v5 + 32);
  status = QUADRATURE_SUCCESS;
  abs_error = 0.0;
  v16 = swift_allocObject();
  *(_QWORD *)(v16 + 16) = a1;
  *(_QWORD *)(v16 + 24) = a2;
  v17 = swift_allocObject();
  *(_QWORD *)(v17 + 16) = partial apply for thunk for @callee_guaranteed (@unowned UnsafeBufferPointer<Double>, @unowned UnsafeMutableBufferPointer<Double>) -> ();
  *(_QWORD *)(v17 + 24) = v16;
  v22[0] = partial apply for thunk for @escaping @callee_guaranteed (@unowned UnsafeBufferPointer<Double>, @unowned UnsafeMutableBufferPointer<Double>) -> ();
  v22[1] = v17;
  __f.fun = (quadrature_function_array)@objc closure #1 in closure #1 in closure #1 in Quadrature.integrate(over:integrand:);
  __f.fun_arg = v22;
  options.integrator = v11;
  options.abs_tolerance = v12;
  options.rel_tolerance = v13;
  options.qag_points_per_interval = v14;
  options.max_intervals = v15;
  swift_retain();
  v18 = quadrature_integrate(&__f, a4, a5, &options, &status, &abs_error, 0, 0);
  swift_release();
  LOBYTE(a2) = swift_isEscapingClosureAtFileLocation();
  result = swift_release();
  if ((a2 & 1) != 0)
    __break(1u);
  if (status > QUADRATURE_ALLOC_ERROR)
  {
    switch(status)
    {
      case QUADRATURE_INVALID_ARG_ERROR:
        v21 = xmmword_1CAB61300;
        goto LABEL_14;
      case QUADRATURE_ERROR:
        *(_QWORD *)a3 = 0;
        *(_QWORD *)(a3 + 8) = 0;
        goto LABEL_15;
      case QUADRATURE_SUCCESS:
        v20 = abs_error;
        *(double *)a3 = v18;
        *(double *)(a3 + 8) = v20;
        *(_BYTE *)(a3 + 16) = 0;
        return result;
    }
  }
  else
  {
    if (status == QUADRATURE_INTEGRATE_BAD_BEHAVIOUR_ERROR)
    {
      v21 = xmmword_1CAB612D0;
      goto LABEL_14;
    }
    if (status == QUADRATURE_INTEGRATE_MAX_EVAL_ERROR)
    {
      v21 = xmmword_1CAB612E0;
      goto LABEL_14;
    }
  }
  v21 = xmmword_1CAB612F0;
LABEL_14:
  *(_OWORD *)a3 = v21;
LABEL_15:
  *(_BYTE *)(a3 + 16) = 1;
  return result;
}

{
  uint64_t v5;
  quadrature_integrator v11;
  double v12;
  double v13;
  size_t v14;
  size_t v15;
  uint64_t v16;
  uint64_t v17;
  double v18;
  uint64_t result;
  double v20;
  __int128 v21;
  _QWORD v22[2];
  double abs_error;
  quadrature_status status;
  quadrature_integrate_options options;
  quadrature_integrate_function __f;
  uint64_t v27;

  v27 = *MEMORY[0x1E0C80C00];
  v11 = *(_DWORD *)v5;
  v12 = *(double *)(v5 + 8);
  v13 = *(double *)(v5 + 16);
  v14 = *(_QWORD *)(v5 + 24);
  v15 = *(_QWORD *)(v5 + 32);
  status = QUADRATURE_SUCCESS;
  abs_error = 0.0;
  v16 = swift_allocObject();
  *(_QWORD *)(v16 + 16) = a1;
  *(_QWORD *)(v16 + 24) = a2;
  v17 = swift_allocObject();
  *(_QWORD *)(v17 + 16) = partial apply for thunk for @callee_guaranteed (@unowned UnsafeBufferPointer<Double>, @unowned UnsafeMutableBufferPointer<Double>) -> ();
  *(_QWORD *)(v17 + 24) = v16;
  v22[0] = partial apply for thunk for @escaping @callee_guaranteed (@unowned Double) -> (@unowned Double);
  v22[1] = v17;
  __f.fun = (quadrature_function_array)@objc closure #1 in closure #1 in closure #1 in Quadrature.integrate(over:integrand:);
  __f.fun_arg = v22;
  options.integrator = v11;
  options.abs_tolerance = v12;
  options.rel_tolerance = v13;
  options.qag_points_per_interval = v14;
  options.max_intervals = v15;
  swift_retain();
  v18 = quadrature_integrate(&__f, a4, a5, &options, &status, &abs_error, 0, 0);
  swift_release();
  LOBYTE(a2) = swift_isEscapingClosureAtFileLocation();
  result = swift_release();
  if ((a2 & 1) != 0)
    __break(1u);
  if (status > QUADRATURE_ALLOC_ERROR)
  {
    switch(status)
    {
      case QUADRATURE_INVALID_ARG_ERROR:
        v21 = xmmword_1CAB61300;
        goto LABEL_14;
      case QUADRATURE_ERROR:
        *(_QWORD *)a3 = 0;
        *(_QWORD *)(a3 + 8) = 0;
        goto LABEL_15;
      case QUADRATURE_SUCCESS:
        v20 = abs_error;
        *(double *)a3 = v18;
        *(double *)(a3 + 8) = v20;
        *(_BYTE *)(a3 + 16) = 0;
        return result;
    }
  }
  else
  {
    if (status == QUADRATURE_INTEGRATE_BAD_BEHAVIOUR_ERROR)
    {
      v21 = xmmword_1CAB612D0;
      goto LABEL_14;
    }
    if (status == QUADRATURE_INTEGRATE_MAX_EVAL_ERROR)
    {
      v21 = xmmword_1CAB612E0;
      goto LABEL_14;
    }
  }
  v21 = xmmword_1CAB612F0;
LABEL_14:
  *(_OWORD *)a3 = v21;
LABEL_15:
  *(_BYTE *)(a3 + 16) = 1;
  return result;
}

void (**@objc closure #1 in closure #1 in closure #1 in Quadrature.integrate(over:integrand:)(void (**result)(_QWORD *, _QWORD *), uint64_t a2, uint64_t a3, uint64_t a4))(_QWORD *, _QWORD *)
{
  void (*v4)(_QWORD *, _QWORD *);
  _QWORD v5[2];
  _QWORD v6[2];

  if (result)
  {
    v4 = *result;
    v6[0] = a3;
    v6[1] = a2;
    v5[0] = a4;
    v5[1] = a2;
    swift_retain();
    v4(v6, v5);
    return (void (**)(_QWORD *, _QWORD *))swift_release();
  }
  return result;
}

Accelerate::Quadrature::Error __swiftcall Quadrature.Error.init(quadratureStatus:)(quadrature_status quadratureStatus)
{
  _BYTE *v1;

  if (quadratureStatus <= -100)
  {
    if (quadratureStatus == QUADRATURE_INTEGRATE_BAD_BEHAVIOUR_ERROR)
    {
      *v1 = 4;
      return (char)quadratureStatus;
    }
    if (quadratureStatus == QUADRATURE_INTEGRATE_MAX_EVAL_ERROR)
    {
      *v1 = 3;
      return (char)quadratureStatus;
    }
LABEL_9:
    *v1 = 2;
    return (char)quadratureStatus;
  }
  if (quadratureStatus == QUADRATURE_INTERNAL_ERROR)
    goto LABEL_9;
  if (quadratureStatus == QUADRATURE_ERROR)
  {
    *v1 = 0;
    return (char)quadratureStatus;
  }
  if (quadratureStatus != QUADRATURE_INVALID_ARG_ERROR)
    goto LABEL_9;
  *v1 = 1;
  return (char)quadratureStatus;
}

uint64_t sub_1CAB2B038()
{
  return swift_deallocObject();
}

uint64_t partial apply for thunk for @callee_guaranteed (@unowned UnsafeBufferPointer<Double>, @unowned UnsafeMutableBufferPointer<Double>) -> ()()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(v0 + 16))();
}

uint64_t sub_1CAB2B068()
{
  swift_release();
  return swift_deallocObject();
}

uint64_t partial apply for thunk for @escaping @callee_guaranteed (@unowned UnsafeBufferPointer<Double>, @unowned UnsafeMutableBufferPointer<Double>) -> ()(_QWORD *a1, _QWORD *a2)
{
  uint64_t v2;

  return (*(uint64_t (**)(_QWORD, _QWORD, _QWORD, _QWORD))(v2 + 16))(*a1, a1[1], *a2, a2[1]);
}

void (**closure #1 in closure #1 in closure #1 in Quadrature.integrate(over:integrand:)(void (**result)(uint64_t *__return_ptr, uint64_t *), uint64_t a2, uint64_t *a3, _QWORD *a4))(uint64_t *__return_ptr, uint64_t *)
{
  uint64_t v4;
  void (*v7)(uint64_t *__return_ptr, uint64_t *);
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;

  if (result)
  {
    v4 = a2;
    if (a2 < 0)
    {
      __break(1u);
    }
    else if (a2)
    {
      v7 = *result;
      swift_retain();
      do
      {
        v8 = *a3++;
        v10 = v8;
        v7(&v9, &v10);
        *a4++ = v9;
        --v4;
      }
      while (v4);
      return (void (**)(uint64_t *__return_ptr, uint64_t *))swift_release();
    }
  }
  return result;
}

void static Quadrature.Integrator.nonAdaptive.getter(uint64_t a1@<X8>)
{
  *(_QWORD *)a1 = 0;
  *(_QWORD *)(a1 + 8) = 0;
  *(_BYTE *)(a1 + 16) = 2;
}

_QWORD *static Quadrature.Integrator.adaptive(pointsPerInterval:maxIntervals:)@<X0>(_QWORD *result@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  *(_QWORD *)a3 = *result;
  *(_QWORD *)(a3 + 8) = a2;
  *(_BYTE *)(a3 + 16) = 0;
  return result;
}

uint64_t static Quadrature.Integrator.adaptiveWithSingularities(maxIntervals:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X8>)
{
  *(_QWORD *)a2 = result;
  *(_QWORD *)(a2 + 8) = 0;
  *(_BYTE *)(a2 + 16) = 1;
  return result;
}

uint64_t Quadrature.QAGPointsPerInterval.points.getter()
{
  uint64_t v0;

  return *(_QWORD *)v0;
}

void static Quadrature.QAGPointsPerInterval.fifteen.getter(_QWORD *a1@<X8>)
{
  *a1 = 15;
}

void static Quadrature.QAGPointsPerInterval.twentyOne.getter(_QWORD *a1@<X8>)
{
  *a1 = 21;
}

void static Quadrature.QAGPointsPerInterval.thirtyOne.getter(_QWORD *a1@<X8>)
{
  *a1 = 31;
}

void static Quadrature.QAGPointsPerInterval.fortyOne.getter(_QWORD *a1@<X8>)
{
  *a1 = 41;
}

void static Quadrature.QAGPointsPerInterval.fiftyOne.getter(_QWORD *a1@<X8>)
{
  *a1 = 51;
}

void static Quadrature.QAGPointsPerInterval.sixtyOne.getter(_QWORD *a1@<X8>)
{
  *a1 = 61;
}

uint64_t Quadrature.Error.errorDescription.getter()
{
  unsigned __int8 *v0;

  return ((uint64_t (*)(uint64_t, unint64_t))((char *)sub_1CAB2B410 + 4 * byte_1CAB61310[*v0]))(0x20636972656E6547, 0xEE002E726F727265);
}

unint64_t sub_1CAB2B410()
{
  return 0xD000000000000011;
}

unint64_t sub_1CAB2B42C()
{
  return 0xD00000000000003FLL;
}

BOOL static Quadrature.Error.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void Quadrature.Error.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

Swift::Int Quadrature.Error.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t sub_1CAB2B500()
{
  return swift_deallocObject();
}

void partial apply for thunk for @escaping @callee_guaranteed (@unowned Double) -> (@unowned Double)(double *a1@<X0>, double *a2@<X8>)
{
  uint64_t v2;

  *a2 = (*(double (**)(double))(v2 + 16))(*a1);
}

unint64_t lazy protocol witness table accessor for type Quadrature.Error and conformance Quadrature.Error()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type Quadrature.Error and conformance Quadrature.Error;
  if (!lazy protocol witness table cache variable for type Quadrature.Error and conformance Quadrature.Error)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for Quadrature.Error, &type metadata for Quadrature.Error);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Quadrature.Error and conformance Quadrature.Error);
  }
  return result;
}

double sub_1CAB2B584@<D0>(uint64_t a1@<X0>, double *a2@<X8>)
{
  double result;

  result = *(double *)(a1 + 8);
  *a2 = result;
  return result;
}

double sub_1CAB2B590(double *a1, uint64_t a2)
{
  double result;

  result = *a1;
  *(double *)(a2 + 8) = *a1;
  return result;
}

double sub_1CAB2B59C@<D0>(uint64_t a1@<X0>, double *a2@<X8>)
{
  double result;

  result = *(double *)(a1 + 16);
  *a2 = result;
  return result;
}

double sub_1CAB2B5A8(double *a1, uint64_t a2)
{
  double result;

  result = *a1;
  *(double *)(a2 + 16) = *a1;
  return result;
}

ValueMetadata *type metadata accessor for Quadrature()
{
  return &type metadata for Quadrature;
}

__n128 __swift_memcpy17_8(__n128 *a1, __n128 *a2)
{
  __n128 result;

  result = *a2;
  a1[1].n128_u8[0] = a2[1].n128_u8[0];
  *a1 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for Quadrature.Integrator(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  int v4;

  if (!a2)
    return 0;
  if (a2 >= 0xFE && *(_BYTE *)(a1 + 17))
    return (*(_DWORD *)a1 + 254);
  v3 = *(unsigned __int8 *)(a1 + 16);
  if (v3 <= 2)
    v4 = -1;
  else
    v4 = v3 ^ 0xFF;
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for Quadrature.Integrator(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFD)
  {
    *(_BYTE *)(result + 16) = 0;
    *(_QWORD *)result = a2 - 254;
    *(_QWORD *)(result + 8) = 0;
    if (a3 >= 0xFE)
      *(_BYTE *)(result + 17) = 1;
  }
  else
  {
    if (a3 >= 0xFE)
      *(_BYTE *)(result + 17) = 0;
    if (a2)
      *(_BYTE *)(result + 16) = -(char)a2;
  }
  return result;
}

uint64_t getEnumTag for Quadrature.Integrator(uint64_t a1)
{
  if (*(unsigned __int8 *)(a1 + 16) <= 1u)
    return *(unsigned __int8 *)(a1 + 16);
  else
    return (*(_DWORD *)a1 + 2);
}

uint64_t destructiveInjectEnumTag for Quadrature.Integrator(uint64_t result, unsigned int a2)
{
  if (a2 >= 2)
  {
    *(_QWORD *)result = a2 - 2;
    *(_QWORD *)(result + 8) = 0;
    LOBYTE(a2) = 2;
  }
  *(_BYTE *)(result + 16) = a2;
  return result;
}

ValueMetadata *type metadata accessor for Quadrature.Integrator()
{
  return &type metadata for Quadrature.Integrator;
}

ValueMetadata *type metadata accessor for Quadrature.QAGPointsPerInterval()
{
  return &type metadata for Quadrature.QAGPointsPerInterval;
}

uint64_t storeEnumTagSinglePayload for Quadrature.Error(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 4 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 4) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFC)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFB)
    return ((uint64_t (*)(void))((char *)&loc_1CAB2B700 + 4 * byte_1CAB6131A[v4]))();
  *a1 = a2 + 4;
  return ((uint64_t (*)(void))((char *)sub_1CAB2B734 + 4 * byte_1CAB61315[v4]))();
}

uint64_t sub_1CAB2B734(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB2B73C(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB2B744);
  return result;
}

uint64_t sub_1CAB2B750(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB2B758);
  *(_BYTE *)result = a2 + 4;
  return result;
}

uint64_t sub_1CAB2B75C(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB2B764(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for Quadrature.Error()
{
  return &type metadata for Quadrature.Error;
}

uint64_t static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:)(a1, a2, v2[4], v2[5], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:)(a1, a2, v2[4], v2[5], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:));
}

uint64_t static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  return static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:));
}

{
  return static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:));
}

uint64_t static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t, _QWORD *))
{
  uint64_t v12;
  _QWORD v14[8];

  v12 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  v14[2] = a3;
  v14[3] = a4;
  v14[4] = a1;
  v14[5] = a2;
  return a6(v12, a5, v14);
}

uint64_t closure #1 in static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t *a7, unint64_t *a8, void (*a9)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16;
  uint64_t v17;
  uint64_t result;

  v16 = __swift_instantiateConcreteTypeFromMangledName(a7);
  v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a8, a7);
  a9(a3, a4, a1, a5, v16, a6, v17);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v12;
  uint64_t result;
  uint64_t v14;

  v12 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  result = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a7 + 8) + 16))(a5);
  if (result >= v12)
    v14 = v12;
  else
    v14 = result;
  if (v14 < 0)
  {
    __break(1u);
  }
  else if (*(_QWORD *)(a1 + 16))
  {
    MEMORY[0x1E0C80A78](result);
    return (*(uint64_t (**)(uint64_t))(a7 + 16))(a8);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(uint64_t result, uint64_t a2, uint64_t a3, _QWORD *a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, uint64_t, uint64_t, _QWORD, uint64_t, uint64_t, uint64_t))
{
  if (result)
  {
    if (*a4)
      return a7(a3 + 32, 1, result, 1, *a4, 1, a5, a6);
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:));
}

{
  return partial apply for closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:));
}

uint64_t partial apply for closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  _QWORD v6[4];
  __int128 v7;

  v3 = *(_QWORD *)(v2 + 16);
  v4 = *(_QWORD *)(v2 + 32);
  v6[2] = *(_QWORD *)(v2 + 56);
  v6[3] = a1;
  v7 = *(_OWORD *)(v2 + 64);
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v4 + 24))(a2, v6, MEMORY[0x1E0DEE9C0] + 8, v3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C780]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C778]);
}

double BNNS.FusedConvolutionParameters.layerParameters(input:output:)@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, uint64_t *a3@<X8>)
{
  uint64_t v3;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  int v18;
  uint64_t v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  uint64_t v31;
  uint64_t v32;
  double result;
  int v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;
  uint64_t v56;
  uint64_t v57;
  uint64_t v58;
  uint64_t v59;
  uint64_t v60;
  _OWORD __src[33];
  _QWORD v62[20];
  int v63;
  uint64_t v64;
  int v65;
  __int128 v66;
  __int128 v67;
  __int128 v68;
  __int128 v69;
  __int128 v70;
  __int128 v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  __int128 v76;
  _BYTE v77[192];

  v7 = *(_OWORD *)(v3 + 120);
  v8 = *(_OWORD *)(v3 + 152);
  v74 = *(_OWORD *)(v3 + 136);
  v75 = v8;
  v76 = *(_OWORD *)(v3 + 168);
  v9 = *(_OWORD *)(v3 + 72);
  v10 = *(_OWORD *)(v3 + 88);
  v69 = *(_OWORD *)(v3 + 56);
  v70 = v9;
  v71 = v10;
  v11 = *(_OWORD *)(v3 + 104);
  v73 = v7;
  v72 = v11;
  v12 = *(_OWORD *)(v3 + 24);
  v66 = *(_OWORD *)(v3 + 8);
  v67 = v12;
  v68 = *(_OWORD *)(v3 + 40);
  outlined init with take of BNNSNDArrayDescriptor?(v3 + 184, (uint64_t)v77);
  v13 = *(_QWORD *)(v3 + 408);
  v14 = *(_QWORD *)(v3 + 416);
  if (*(_BYTE *)(v3 + 440) == 1)
  {
    v59 = *(_QWORD *)(v3 + 424);
    v60 = *(_QWORD *)(v3 + 432);
    v57 = *(_QWORD *)(v3 + 408);
    v58 = *(_QWORD *)(v3 + 416);
    v14 = 0;
    v13 = 0;
  }
  else
  {
    v59 = 0;
    v60 = 0;
    v57 = 0;
    v58 = 0;
  }
  v54 = *(_QWORD *)(v3 + 376);
  v55 = *(_QWORD *)(v3 + 368);
  v52 = *(_QWORD *)(v3 + 392);
  v53 = *(_QWORD *)(v3 + 384);
  v56 = *(_QWORD *)(v3 + 400);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v77, (uint64_t)v62);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v62) == 1)
  {
    v50 = 0;
    v51 = 0;
    v48 = 0;
    v49 = 0;
    v46 = 0;
    v47 = 0;
    v44 = 0;
    v45 = 0;
    v42 = 0;
    v43 = 0;
    v40 = 0;
    v41 = 0;
    v38 = 0;
    v39 = 0;
    v37 = 0;
    v15 = 0;
    v16 = 0;
    v17 = 0;
    v18 = 0;
    v19 = 0;
    v35 = 0;
    v36 = 0;
  }
  else
  {
    v36 = v62[0];
    v50 = v62[1];
    v51 = v62[2];
    v48 = v62[4];
    v49 = v62[3];
    v46 = v62[6];
    v47 = v62[5];
    v44 = v62[8];
    v45 = v62[7];
    v42 = v62[10];
    v43 = v62[9];
    v40 = v62[12];
    v41 = v62[11];
    v38 = v62[14];
    v39 = v62[13];
    v37 = v62[15];
    v15 = v62[16];
    v16 = v62[17];
    v35 = v62[18];
    v17 = v62[19];
    v18 = v63;
    v19 = v64;
    v34 = v65;
  }
  v20 = a1[9];
  __src[8] = a1[8];
  __src[9] = v20;
  v21 = a1[5];
  __src[4] = a1[4];
  __src[5] = v21;
  v22 = a1[7];
  __src[6] = a1[6];
  __src[7] = v22;
  v23 = a1[1];
  __src[0] = *a1;
  __src[1] = v23;
  v24 = a1[3];
  __src[2] = a1[2];
  __src[3] = v24;
  __src[18] = v73;
  __src[19] = v74;
  __src[20] = v75;
  __src[21] = v76;
  __src[14] = v69;
  __src[15] = v70;
  v25 = a1[10];
  __src[16] = v71;
  __src[17] = v72;
  __src[10] = v25;
  __src[11] = v66;
  __src[12] = v67;
  __src[13] = v68;
  v26 = a2[9];
  __src[30] = a2[8];
  __src[31] = v26;
  __src[32] = a2[10];
  v27 = a2[5];
  __src[26] = a2[4];
  __src[27] = v27;
  v28 = a2[7];
  __src[28] = a2[6];
  __src[29] = v28;
  v29 = a2[1];
  __src[22] = *a2;
  __src[23] = v29;
  v30 = a2[3];
  __src[24] = a2[2];
  __src[25] = v30;
  type metadata accessor for BNNSLayerParametersConvolution(0);
  a3[3] = v31;
  a3[4] = (uint64_t)&protocol witness table for BNNSLayerParametersConvolution;
  v32 = swift_allocObject();
  *a3 = v32;
  memcpy((void *)(v32 + 16), __src, 0x210uLL);
  *(_QWORD *)(v32 + 552) = v50;
  *(_QWORD *)(v32 + 544) = v36;
  *(_QWORD *)(v32 + 560) = v51;
  *(_QWORD *)(v32 + 568) = v49;
  *(_QWORD *)(v32 + 576) = v48;
  *(_QWORD *)(v32 + 584) = v47;
  *(_QWORD *)(v32 + 592) = v46;
  *(_QWORD *)(v32 + 600) = v45;
  *(_QWORD *)(v32 + 608) = v44;
  *(_QWORD *)(v32 + 616) = v43;
  *(_QWORD *)(v32 + 624) = v42;
  *(_QWORD *)(v32 + 632) = v41;
  *(_QWORD *)(v32 + 640) = v40;
  *(_QWORD *)(v32 + 648) = v39;
  *(_QWORD *)(v32 + 656) = v38;
  *(_QWORD *)(v32 + 664) = v37;
  *(_QWORD *)(v32 + 672) = v15;
  *(_QWORD *)(v32 + 680) = v16;
  *(_QWORD *)(v32 + 688) = v35;
  *(_QWORD *)(v32 + 696) = v17;
  *(_DWORD *)(v32 + 704) = v18;
  *(_QWORD *)(v32 + 708) = v19;
  *(_DWORD *)(v32 + 716) = v34;
  *(_QWORD *)(v32 + 720) = 0x7FC0000000000000;
  *(_QWORD *)(v32 + 728) = 0x17FC00000;
  result = 0.0;
  *(_OWORD *)(v32 + 736) = 0u;
  *(_OWORD *)(v32 + 752) = 0u;
  *(_QWORD *)(v32 + 768) = v55;
  *(_QWORD *)(v32 + 776) = v54;
  *(_QWORD *)(v32 + 784) = v53;
  *(_QWORD *)(v32 + 792) = v52;
  *(_QWORD *)(v32 + 800) = v13;
  *(_QWORD *)(v32 + 808) = v14;
  *(_QWORD *)(v32 + 816) = v56;
  *(_QWORD *)(v32 + 824) = v57;
  *(_QWORD *)(v32 + 832) = v58;
  *(_QWORD *)(v32 + 840) = v59;
  *(_QWORD *)(v32 + 848) = v60;
  return result;
}

void *BNNS.FusedConvolutionParameters.init(type:weights:bias:stride:dilationStride:groupSize:padding:)@<X0>(char *a1@<X0>, _OWORD *a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, void *a9@<X8>, __int128 *a10)
{
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  char v23;
  uint64_t v24;
  uint64_t v25;
  char v26;
  __int128 v28;
  _BYTE v29[184];
  _BYTE v30[184];
  _QWORD __src[57];

  outlined init with take of BNNSNDArrayDescriptor?(a3, (uint64_t)v29);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v29, (uint64_t)v30);
  v18 = a2[6];
  *(_OWORD *)&__src[15] = a2[7];
  v19 = a2[9];
  *(_OWORD *)&__src[17] = a2[8];
  *(_OWORD *)&__src[19] = v19;
  *(_OWORD *)&__src[21] = a2[10];
  v20 = a2[2];
  *(_OWORD *)&__src[7] = a2[3];
  v21 = a2[5];
  *(_OWORD *)&__src[9] = a2[4];
  *(_OWORD *)&__src[11] = v21;
  *(_OWORD *)&__src[13] = v18;
  v22 = a2[1];
  *(_OWORD *)&__src[1] = *a2;
  *(_OWORD *)&__src[3] = v22;
  *(_OWORD *)&__src[5] = v20;
  v23 = *a1;
  v28 = *a10;
  v24 = *((_QWORD *)a10 + 2);
  v25 = *((_QWORD *)a10 + 3);
  LOBYTE(__src[0]) = v23;
  v26 = *((_BYTE *)a10 + 32);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v30, (uint64_t)&__src[23]);
  __src[46] = a4;
  __src[47] = a5;
  __src[48] = a6;
  __src[49] = a7;
  *(_OWORD *)&__src[51] = v28;
  __src[50] = a8;
  __src[53] = v24;
  __src[54] = v25;
  LOBYTE(__src[55]) = v26;
  return memcpy(a9, __src, 0x1B9uLL);
}

void BNNS.FusedConvolutionParameters.type.getter(_BYTE *a1@<X8>)
{
  _BYTE *v1;

  *a1 = *v1;
}

_BYTE *BNNS.FusedConvolutionParameters.type.setter(_BYTE *result)
{
  _BYTE *v1;

  *v1 = *result;
  return result;
}

uint64_t (*BNNS.FusedConvolutionParameters.type.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

__n128 BNNS.FusedConvolutionParameters.weights.getter@<Q0>(uint64_t a1@<X8>)
{
  uint64_t v1;
  __int128 v2;
  __int128 v3;
  __int128 v4;
  __int128 v5;
  __int128 v6;
  __n128 result;

  v2 = *(_OWORD *)(v1 + 120);
  v3 = *(_OWORD *)(v1 + 152);
  *(_OWORD *)(a1 + 128) = *(_OWORD *)(v1 + 136);
  *(_OWORD *)(a1 + 144) = v3;
  *(_OWORD *)(a1 + 160) = *(_OWORD *)(v1 + 168);
  v4 = *(_OWORD *)(v1 + 56);
  v5 = *(_OWORD *)(v1 + 88);
  *(_OWORD *)(a1 + 64) = *(_OWORD *)(v1 + 72);
  *(_OWORD *)(a1 + 80) = v5;
  *(_OWORD *)(a1 + 96) = *(_OWORD *)(v1 + 104);
  *(_OWORD *)(a1 + 112) = v2;
  v6 = *(_OWORD *)(v1 + 24);
  *(_OWORD *)a1 = *(_OWORD *)(v1 + 8);
  *(_OWORD *)(a1 + 16) = v6;
  result = *(__n128 *)(v1 + 40);
  *(__n128 *)(a1 + 32) = result;
  *(_OWORD *)(a1 + 48) = v4;
  return result;
}

__n128 BNNS.FusedConvolutionParameters.weights.setter(uint64_t a1)
{
  uint64_t v1;
  __int128 v2;
  __int128 v3;
  __int128 v4;
  __int128 v5;
  __n128 result;
  __int128 v7;

  v2 = *(_OWORD *)(a1 + 96);
  *(_OWORD *)(v1 + 120) = *(_OWORD *)(a1 + 112);
  v3 = *(_OWORD *)(a1 + 144);
  *(_OWORD *)(v1 + 136) = *(_OWORD *)(a1 + 128);
  *(_OWORD *)(v1 + 152) = v3;
  *(_OWORD *)(v1 + 168) = *(_OWORD *)(a1 + 160);
  v4 = *(_OWORD *)(a1 + 32);
  *(_OWORD *)(v1 + 56) = *(_OWORD *)(a1 + 48);
  v5 = *(_OWORD *)(a1 + 80);
  *(_OWORD *)(v1 + 72) = *(_OWORD *)(a1 + 64);
  *(_OWORD *)(v1 + 88) = v5;
  *(_OWORD *)(v1 + 104) = v2;
  result = *(__n128 *)a1;
  v7 = *(_OWORD *)(a1 + 16);
  *(_OWORD *)(v1 + 8) = *(_OWORD *)a1;
  *(_OWORD *)(v1 + 24) = v7;
  *(_OWORD *)(v1 + 40) = v4;
  return result;
}

uint64_t (*BNNS.FusedConvolutionParameters.weights.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedConvolutionParameters.bias.getter@<X0>(uint64_t a1@<X8>)
{
  uint64_t v1;
  _BYTE v4[184];

  outlined init with take of BNNSNDArrayDescriptor?(v1 + 184, (uint64_t)v4);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v4, a1);
}

uint64_t BNNS.FusedConvolutionParameters.bias.setter(uint64_t a1)
{
  uint64_t v1;

  return outlined init with take of BNNSNDArrayDescriptor?(a1, v1 + 184);
}

uint64_t (*BNNS.FusedConvolutionParameters.bias.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedConvolutionParameters.stride.getter()
{
  uint64_t v0;

  return *(_QWORD *)(v0 + 368);
}

uint64_t BNNS.FusedConvolutionParameters.stride.setter(uint64_t result, uint64_t a2)
{
  uint64_t v2;

  *(_QWORD *)(v2 + 368) = result;
  *(_QWORD *)(v2 + 376) = a2;
  return result;
}

uint64_t (*BNNS.FusedConvolutionParameters.stride.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedConvolutionParameters.dilationStride.getter()
{
  uint64_t v0;

  return *(_QWORD *)(v0 + 384);
}

uint64_t BNNS.FusedConvolutionParameters.dilationStride.setter(uint64_t result, uint64_t a2)
{
  uint64_t v2;

  *(_QWORD *)(v2 + 384) = result;
  *(_QWORD *)(v2 + 392) = a2;
  return result;
}

uint64_t (*BNNS.FusedConvolutionParameters.dilationStride.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedConvolutionParameters.groupSize.getter()
{
  uint64_t v0;

  return *(_QWORD *)(v0 + 400);
}

uint64_t BNNS.FusedConvolutionParameters.groupSize.setter(uint64_t result)
{
  uint64_t v1;

  *(_QWORD *)(v1 + 400) = result;
  return result;
}

uint64_t (*BNNS.FusedConvolutionParameters.groupSize.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

__n128 BNNS.FusedConvolutionParameters.padding.getter@<Q0>(uint64_t a1@<X8>)
{
  uint64_t v1;
  char v2;
  __n128 result;
  __int128 v4;

  v2 = *(_BYTE *)(v1 + 440);
  result = *(__n128 *)(v1 + 408);
  v4 = *(_OWORD *)(v1 + 424);
  *(__n128 *)a1 = result;
  *(_OWORD *)(a1 + 16) = v4;
  *(_BYTE *)(a1 + 32) = v2;
  return result;
}

__n128 BNNS.FusedConvolutionParameters.padding.setter(uint64_t a1)
{
  uint64_t v1;
  char v2;
  __n128 result;
  __int128 v4;

  v2 = *(_BYTE *)(a1 + 32);
  result = *(__n128 *)a1;
  v4 = *(_OWORD *)(a1 + 16);
  *(_OWORD *)(v1 + 408) = *(_OWORD *)a1;
  *(_OWORD *)(v1 + 424) = v4;
  *(_BYTE *)(v1 + 440) = v2;
  return result;
}

uint64_t (*BNNS.FusedConvolutionParameters.padding.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t protocol witness for FusableLayerParametersWrapper.filterType.getter in conformance BNNS.FusedConvolutionParameters()
{
  _BYTE *v0;

  if (*v0)
    return 6;
  else
    return 0;
}

void *__swift_memcpy441_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x1B9uLL);
}

uint64_t getEnumTagSinglePayload for BNNS.FusedConvolutionParameters(unsigned __int8 *a1, unsigned int a2)
{
  unsigned int v3;
  BOOL v4;
  int v5;

  if (!a2)
    return 0;
  if (a2 >= 0xFF && a1[441])
    return (*(_DWORD *)a1 + 255);
  v3 = *a1;
  v4 = v3 >= 2;
  v5 = v3 - 2;
  if (!v4)
    v5 = -1;
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.FusedConvolutionParameters(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_BYTE *)(result + 440) = 0;
    *(_OWORD *)(result + 424) = 0u;
    *(_OWORD *)(result + 408) = 0u;
    *(_OWORD *)(result + 392) = 0u;
    *(_OWORD *)(result + 376) = 0u;
    *(_OWORD *)(result + 360) = 0u;
    *(_OWORD *)(result + 344) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(_QWORD *)result = a2 - 255;
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 441) = 1;
  }
  else
  {
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 441) = 0;
    if (a2)
      *(_BYTE *)result = a2 + 1;
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.FusedConvolutionParameters()
{
  return &type metadata for BNNS.FusedConvolutionParameters;
}

uint64_t sub_1CAB2C38C()
{
  return swift_deallocObject();
}

uint64_t vDSP.Radix.fftRadix.getter()
{
  unsigned __int8 *v0;

  return *v0;
}

BOOL static vDSP.Radix.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void vDSP.Radix.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

Swift::Int vDSP.Radix.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t *vDSP.FFT.__allocating_init(log2n:radix:ofType:)(uint64_t a1, char *a2)
{
  swift_allocObject();
  return vDSP.FFT.init(log2n:radix:ofType:)(a1, a2);
}

uint64_t *vDSP.FFT.init(log2n:radix:ofType:)(uint64_t a1, char *a2)
{
  uint64_t *v2;
  uint64_t *v3;
  uint64_t v5;
  char v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t AssociatedTypeWitness;
  uint64_t AssociatedConformanceWitness;
  uint64_t v11;
  uint64_t v12;
  char v14;

  v3 = v2;
  v5 = *v3;
  v6 = *a2;
  v3[2] = a1;
  *((_BYTE *)v3 + 24) = v6;
  v8 = *(_QWORD *)(v5 + 80);
  v7 = *(_QWORD *)(v5 + 88);
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v14 = v6;
  AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  v11 = (*(uint64_t (**)(uint64_t, char *, uint64_t, uint64_t))(AssociatedConformanceWitness + 16))(a1, &v14, AssociatedTypeWitness, AssociatedConformanceWitness);
  if (v11)
  {
    v3[4] = v11;
  }
  else
  {
    type metadata accessor for vDSP.FFT(0, v8, v7, v12);
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v3;
}

uint64_t type metadata accessor for vDSP.FFT(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return __swift_instantiateGenericMetadata(a1, a2, a3, a4, (uint64_t)&nominal type descriptor for vDSP.FFT);
}

uint64_t vDSP.FFT.transform<A>(input:output:direction:)(uint64_t a1, uint64_t a2, char *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v8;
  char v9;
  uint64_t v10;
  uint64_t v11;
  _QWORD v13[7];
  char v14;

  v9 = *a3;
  v10 = *(_QWORD *)(v8 + 32);
  v11 = *(_QWORD *)(v8 + 16);
  v13[2] = a4;
  v13[3] = a5;
  v13[4] = v10;
  v13[5] = v11;
  v13[6] = a2;
  v14 = v9;
  return _ss17withUnsafePointer2to_q0_x_q0_SPyxGq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(a1, (uint64_t)partial apply for closure #1 in static vDSP_FFTFunctions.fftTransform<A>(fftSetup:log2n:source:destination:direction:), (uint64_t)v13, a4, MEMORY[0x1E0DEDCE8], MEMORY[0x1E0DEE9C0] + 8, MEMORY[0x1E0DEDD18], a8);
}

uint64_t static vDSP_FFTFunctions.fftTransform<A>(fftSetup:log2n:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, char *a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  char v8;
  _QWORD v10[7];
  char v11;

  v8 = *a5;
  v10[2] = a6;
  v10[3] = a7;
  v10[4] = a1;
  v10[5] = a2;
  v10[6] = a4;
  v11 = v8;
  return _ss17withUnsafePointer2to_q0_x_q0_SPyxGq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(a3, (uint64_t)partial apply for closure #1 in static vDSP_FFTFunctions.fftTransform<A>(fftSetup:log2n:source:destination:direction:), (uint64_t)v10, a6, MEMORY[0x1E0DEDCE8], MEMORY[0x1E0DEE9C0] + 8, MEMORY[0x1E0DEDD18], a8);
}

uint64_t vDSP.FFT.forward(input:output:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return vDSP.FFT.forward(input:output:)(a1, a2, a3, 0);
}

uint64_t vDSP.FFT.inverse(input:output:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return vDSP.FFT.forward(input:output:)(a1, a2, a3, 1);
}

uint64_t vDSP.FFT.forward(input:output:)(uint64_t a1, uint64_t a2, uint64_t a3, char a4)
{
  uint64_t v4;
  uint64_t (*v6)(_QWORD *, uint64_t, char *, uint64_t, _UNKNOWN **);
  uint64_t v7;
  char v9;
  _QWORD v10[2];

  v10[0] = a1;
  v10[1] = a2;
  v9 = a4;
  v6 = *(uint64_t (**)(_QWORD *, uint64_t, char *, uint64_t, _UNKNOWN **))(*(_QWORD *)v4 + 128);
  type metadata accessor for DSPSplitComplex(0);
  return v6(v10, a3, &v9, v7, &protocol witness table for DSPSplitComplex);
}

uint64_t vDSP.FFT.deinit()
{
  uint64_t v0;
  uint64_t AssociatedTypeWitness;
  uint64_t v2;
  uint64_t AssociatedConformanceWitness;

  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v2 = *(_QWORD *)(v0 + 32);
  AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  (*(void (**)(uint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness + 40))(v2, AssociatedTypeWitness, AssociatedConformanceWitness);
  return v0;
}

uint64_t vDSP.FFT.__deallocating_deinit()
{
  vDSP.FFT.deinit();
  return swift_deallocClassInstance();
}

void vDSP.FFT2D.__allocating_init(width:height:ofType:)(uint64_t a1, uint64_t a2)
{
  swift_allocObject();
  vDSP.FFT2D.init(width:height:ofType:)(a1, a2);
}

void vDSP.FFT2D.init(width:height:ofType:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  float v3;
  char v4;

  *(_QWORD *)(v2 + 40) = a1;
  *(_QWORD *)(v2 + 48) = a2;
  if ((unsigned __int128)(a1 * (__int128)a2) >> 64 != (a1 * a2) >> 63)
  {
    __break(1u);
    goto LABEL_7;
  }
  v3 = log2f((float)(a1 * a2));
  if ((~LODWORD(v3) & 0x7F800000) == 0)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (v3 <= -1.0)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (v3 < 1.8447e19)
  {
    v4 = 0;
    vDSP.FFT.init(log2n:radix:ofType:)((unint64_t)v3, &v4);
    return;
  }
LABEL_9:
  __break(1u);
}

uint64_t vDSP.FFT2D.transform<A>(input:output:direction:)(uint64_t a1, uint64_t a2, char *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v8;
  char v9;
  uint64_t v10;
  _QWORD v12[5];
  __int128 v13;
  uint64_t v14;
  char v15;

  v9 = *a3;
  v10 = *(_QWORD *)(v8 + 32);
  v12[2] = a4;
  v12[3] = a5;
  v12[4] = v10;
  v13 = *(_OWORD *)(v8 + 40);
  v14 = a2;
  v15 = v9;
  return _ss17withUnsafePointer2to_q0_x_q0_SPyxGq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(a1, (uint64_t)partial apply for closure #1 in static vDSP_FFTFunctions.fftTransform2D<A>(fftSetup:width:height:source:destination:direction:), (uint64_t)v12, a4, MEMORY[0x1E0DEDCE8], MEMORY[0x1E0DEE9C0] + 8, MEMORY[0x1E0DEDD18], a8);
}

uint64_t static vDSP_FFTFunctions.fftTransform2D<A>(fftSetup:width:height:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, char *a6, uint64_t a7, uint64_t a8)
{
  char v8;
  _QWORD v10[8];
  char v11;

  v8 = *a6;
  v10[2] = a7;
  v10[3] = a8;
  v10[4] = a1;
  v10[5] = a2;
  v10[6] = a3;
  v10[7] = a5;
  v11 = v8;
  return _ss17withUnsafePointer2to_q0_x_q0_SPyxGq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(a4, (uint64_t)partial apply for closure #1 in static vDSP_FFTFunctions.fftTransform2D<A>(fftSetup:width:height:source:destination:direction:), (uint64_t)v10, a7, MEMORY[0x1E0DEDCE8], MEMORY[0x1E0DEE9C0] + 8, MEMORY[0x1E0DEDD18], a8);
}

void vDSP.FFT2D.__allocating_init(log2n:radix:ofType:)()
{
  _swift_stdlib_reportUnimplementedInitializer();
  __break(1u);
}

void vDSP.FFT2D.init(log2n:radix:ofType:)()
{
  specialized vDSP.FFT2D.init(log2n:radix:ofType:)();
}

uint64_t vDSP.FFT2D.__deallocating_deinit()
{
  vDSP.FFT.deinit();
  return swift_deallocClassInstance();
}

FFTSetup static vDSP_SplitComplexFloat.makeFFTSetup(log2n:radix:)(vDSP_Length a1, unsigned __int8 *a2)
{
  return vDSP_create_fftsetup(a1, *a2);
}

uint64_t static vDSP_SplitComplexFloat.transform(fftSetup:log2n:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _BYTE *a5)
{
  return static vDSP_SplitComplexFloat.transform(fftSetup:log2n:source:destination:direction:)(a1, a2, a3, a4, a5, MEMORY[0x1E0C8C140]);
}

void static vDSP_SplitComplexFloat.transform2D(fftSetup:width:height:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, unsigned __int8 *a6)
{
  specialized static vDSP_SplitComplexFloat.transform2D(fftSetup:width:height:source:destination:direction:)(a1, a2, a3, a4, a5, a6, MEMORY[0x1E0C8C118]);
}

FFTSetup protocol witness for static vDSP_FourierTransformFunctions.makeFFTSetup(log2n:radix:) in conformance vDSP_SplitComplexFloat(vDSP_Length a1, unsigned __int8 *a2)
{
  return vDSP_create_fftsetup(a1, *a2);
}

void protocol witness for static vDSP_FourierTransformFunctions.transform(fftSetup:log2n:source:destination:direction:) in conformance vDSP_SplitComplexFloat(OpaqueFFTSetup *a1, vDSP_Length __Log2N, DSPSplitComplex *__A, const DSPSplitComplex *a4, _BYTE *a5)
{
  FFTDirection v5;

  if (*a5)
    v5 = -1;
  else
    v5 = 1;
  vDSP_fft_zrop(a1, __A, 1, a4, 1, __Log2N, v5);
}

void protocol witness for static vDSP_FourierTransformFunctions.transform2D(fftSetup:width:height:source:destination:direction:) in conformance vDSP_SplitComplexFloat(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, unsigned __int8 *a6)
{
  specialized static vDSP_SplitComplexFloat.transform2D(fftSetup:width:height:source:destination:direction:)(a1, a2, a3, a4, a5, a6, MEMORY[0x1E0C8C118]);
}

FFTSetupD static vDSP_SplitComplexDouble.makeFFTSetup(log2n:radix:)(vDSP_Length a1, unsigned __int8 *a2)
{
  return vDSP_create_fftsetupD(a1, *a2);
}

uint64_t static vDSP_SplitComplexDouble.transform(fftSetup:log2n:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _BYTE *a5)
{
  return static vDSP_SplitComplexFloat.transform(fftSetup:log2n:source:destination:direction:)(a1, a2, a3, a4, a5, MEMORY[0x1E0C8C148]);
}

uint64_t static vDSP_SplitComplexFloat.transform(fftSetup:log2n:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _BYTE *a5, uint64_t (*a6)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v6;

  if (*a5)
    v6 = 0xFFFFFFFFLL;
  else
    v6 = 1;
  return a6(a1, a3, 1, a4, 1, a2, v6);
}

void static vDSP_SplitComplexDouble.transform2D(fftSetup:width:height:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, unsigned __int8 *a6)
{
  specialized static vDSP_SplitComplexFloat.transform2D(fftSetup:width:height:source:destination:direction:)(a1, a2, a3, a4, a5, a6, MEMORY[0x1E0C8C120]);
}

FFTSetupD protocol witness for static vDSP_FourierTransformFunctions.makeFFTSetup(log2n:radix:) in conformance vDSP_SplitComplexDouble(vDSP_Length a1, unsigned __int8 *a2)
{
  return vDSP_create_fftsetupD(a1, *a2);
}

void protocol witness for static vDSP_FourierTransformFunctions.transform(fftSetup:log2n:source:destination:direction:) in conformance vDSP_SplitComplexDouble(OpaqueFFTSetupD *a1, vDSP_Length __Log2N, DSPDoubleSplitComplex *__A, const DSPDoubleSplitComplex *a4, _BYTE *a5)
{
  FFTDirection v5;

  if (*a5)
    v5 = -1;
  else
    v5 = 1;
  vDSP_fft_zropD(a1, __A, 1, a4, 1, __Log2N, v5);
}

void protocol witness for static vDSP_FourierTransformFunctions.transform2D(fftSetup:width:height:source:destination:direction:) in conformance vDSP_SplitComplexDouble(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, unsigned __int8 *a6)
{
  specialized static vDSP_SplitComplexFloat.transform2D(fftSetup:width:height:source:destination:direction:)(a1, a2, a3, a4, a5, a6, MEMORY[0x1E0C8C120]);
}

uint64_t closure #1 in static vDSP_FFTFunctions.fftTransform<A>(fftSetup:log2n:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, char a5)
{
  char v9;
  uint64_t AssociatedTypeWitness;
  uint64_t AssociatedConformanceWitness;
  char v13;

  v9 = a5 & 1;
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v13 = v9;
  AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  return (*(uint64_t (**)(uint64_t, uint64_t, uint64_t, uint64_t, char *, uint64_t, uint64_t))(AssociatedConformanceWitness
                                                                                                 + 24))(a2, a3, a1, a4, &v13, AssociatedTypeWitness, AssociatedConformanceWitness);
}

uint64_t closure #1 in static vDSP_FFTFunctions.fftTransform2D<A>(fftSetup:width:height:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, char a6)
{
  char v10;
  uint64_t AssociatedTypeWitness;
  uint64_t AssociatedConformanceWitness;
  char v15;

  v10 = a6 & 1;
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v15 = v10;
  AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  return (*(uint64_t (**)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, char *, uint64_t, uint64_t))(AssociatedConformanceWitness + 32))(a2, a3, a4, a1, a5, &v15, AssociatedTypeWitness, AssociatedConformanceWitness);
}

float *DSPSplitComplex.init(fromInputArray:realParts:imaginaryParts:)(uint64_t a1, char **a2, char **a3)
{
  char *v6;
  char *v7;
  unint64_t v8;
  const DSPComplex *v9;
  DSPSplitComplex __Z;
  uint64_t v12;

  v12 = *MEMORY[0x1E0C80C00];
  v6 = *a2;
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    v6 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v6 + 2), 0, v6);
  *a2 = v6;
  v7 = *a3;
  swift_bridgeObjectRetain();
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    v7 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v7 + 2), 0, v7);
  *a3 = v7;
  swift_bridgeObjectRelease();
  __Z.realp = (float *)(v6 + 32);
  __Z.imagp = (float *)(v7 + 32);
  v8 = *(_QWORD *)(a1 + 16);
  if (v8 >> 61)
    __break(1u);
  v9 = (const DSPComplex *)specialized _copyCollectionToContiguousArray<A>(_:)((const void *)(a1 + 32), v8 >> 1);
  vDSP_ctoz(v9 + 4, 2, &__Z, 1, *(_QWORD *)(a1 + 16) >> 1);
  swift_bridgeObjectRelease();
  swift_release();
  return __Z.realp;
}

double *DSPDoubleSplitComplex.init(fromInputArray:realParts:imaginaryParts:)(uint64_t a1, char **a2, char **a3)
{
  char *v6;
  char *v7;
  unint64_t v8;
  const DSPDoubleComplex *v9;
  DSPDoubleSplitComplex __Z;
  uint64_t v12;

  v12 = *MEMORY[0x1E0C80C00];
  v6 = *a2;
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    v6 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v6 + 2), 0, v6);
  *a2 = v6;
  v7 = *a3;
  swift_bridgeObjectRetain();
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    v7 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v7 + 2), 0, v7);
  *a3 = v7;
  swift_bridgeObjectRelease();
  __Z.realp = (double *)(v6 + 32);
  __Z.imagp = (double *)(v7 + 32);
  v8 = *(_QWORD *)(a1 + 16);
  if (v8 >> 60)
    __break(1u);
  v9 = (const DSPDoubleComplex *)specialized _copyCollectionToContiguousArray<A>(_:)((const void *)(a1 + 32), v8 >> 1);
  vDSP_ctozD(v9 + 2, 2, &__Z, 1, *(_QWORD *)(a1 + 16) >> 1);
  swift_bridgeObjectRelease();
  swift_release();
  return __Z.realp;
}

uint64_t Array<A>.init(fromSplitComplex:scale:count:)(float *a1, float *a2, int64_t a3, float a4)
{
  uint64_t v4;
  vDSP_Length v9;
  uint64_t v10;
  uint64_t v11;
  DSPSplitComplex __Z;
  uint64_t v14;

  v14 = *MEMORY[0x1E0C80C00];
  if (a3 >= 0)
    v4 = a3;
  else
    v4 = a3 + 1;
  if (a3 < -1)
    __break(1u);
  v9 = v4 >> 1;
  if (a3 < 2)
  {
    v10 = MEMORY[0x1E0DEE9D8];
  }
  else
  {
    type metadata accessor for DSPComplex(0);
    v10 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v10 + 16) = v9;
  }
  __Z.realp = a1;
  __Z.imagp = a2;
  vDSP_ztoc(&__Z, 1, (DSPComplex *)(v10 + 32), 2, v9);
  *(_QWORD *)(v10 + 16) = v9;
  v11 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a3, v10, a3, a4);
  swift_bridgeObjectRelease();
  return v11;
}

uint64_t Array<A>.init(fromSplitComplex:scale:count:)(double *a1, double *a2, int64_t a3, double a4)
{
  uint64_t v4;
  vDSP_Length v9;
  uint64_t v10;
  uint64_t v11;
  DSPDoubleSplitComplex __Z;
  uint64_t v14;

  v14 = *MEMORY[0x1E0C80C00];
  if (a3 >= 0)
    v4 = a3;
  else
    v4 = a3 + 1;
  if (a3 < -1)
    __break(1u);
  v9 = v4 >> 1;
  if (a3 < 2)
  {
    v10 = MEMORY[0x1E0DEE9D8];
  }
  else
  {
    type metadata accessor for DSPDoubleComplex(0);
    v10 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v10 + 16) = v9;
  }
  __Z.realp = a1;
  __Z.imagp = a2;
  vDSP_ztocD(&__Z, 1, (DSPDoubleComplex *)(v10 + 32), 2, v9);
  *(_QWORD *)(v10 + 16) = v9;
  v11 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a3, v10, a3, a4);
  swift_bridgeObjectRelease();
  return v11;
}

_QWORD *specialized _copyCollectionToContiguousArray<A>(_:)(const void *a1, uint64_t a2)
{
  size_t v4;
  _QWORD *v5;
  int64_t v6;
  uint64_t v7;

  if (!a2)
    return (_QWORD *)MEMORY[0x1E0DEE9D8];
  if (a2 <= 0)
  {
    v4 = 8 * a2;
    v5 = (_QWORD *)MEMORY[0x1E0DEE9D8];
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
    v4 = 8 * a2;
    v5 = (_QWORD *)swift_allocObject();
    v6 = _swift_stdlib_malloc_size(v5);
    v7 = v6 - 32;
    if (v6 < 32)
      v7 = v6 - 25;
    v5[2] = a2;
    v5[3] = 2 * (v7 >> 3);
  }
  memcpy(v5 + 4, a1, v4);
  return v5;
}

{
  return specialized _copyCollectionToContiguousArray<A>(_:)(a1, a2, &demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
}

{
  size_t v4;
  _QWORD *v5;
  int64_t v6;
  uint64_t v7;

  if (!a2)
    return (_QWORD *)MEMORY[0x1E0DEE9D8];
  if (a2 <= 0)
  {
    v4 = 8 * a2;
    v5 = (_QWORD *)MEMORY[0x1E0DEE9D8];
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<DSPComplex>);
    v4 = 8 * a2;
    v5 = (_QWORD *)swift_allocObject();
    v6 = _swift_stdlib_malloc_size(v5);
    v7 = v6 - 32;
    if (v6 < 32)
      v7 = v6 - 25;
    v5[2] = a2;
    v5[3] = 2 * (v7 >> 3);
  }
  memcpy(v5 + 4, a1, v4);
  return v5;
}

{
  size_t v4;
  _QWORD *v5;
  int64_t v6;
  uint64_t v7;

  if (!a2)
    return (_QWORD *)MEMORY[0x1E0DEE9D8];
  if (a2 <= 0)
  {
    v4 = 16 * a2;
    v5 = (_QWORD *)MEMORY[0x1E0DEE9D8];
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<DSPDoubleComplex>);
    v4 = 16 * a2;
    v5 = (_QWORD *)swift_allocObject();
    v6 = _swift_stdlib_malloc_size(v5);
    v7 = v6 - 32;
    if (v6 < 32)
      v7 = v6 - 17;
    v5[2] = a2;
    v5[3] = 2 * (v7 >> 4);
  }
  memcpy(v5 + 4, a1, v4);
  return v5;
}

{
  return specialized _copyCollectionToContiguousArray<A>(_:)(a1, a2, &demangling cache variable for type metadata for _ContiguousArrayStorage<UInt32>);
}

_QWORD *specialized _copyCollectionToContiguousArray<A>(_:)(const void *a1, uint64_t a2, uint64_t *a3)
{
  size_t v5;
  _QWORD *v6;
  int64_t v7;
  uint64_t v8;

  if (!a2)
    return (_QWORD *)MEMORY[0x1E0DEE9D8];
  if (a2 <= 0)
  {
    v5 = 4 * a2;
    v6 = (_QWORD *)MEMORY[0x1E0DEE9D8];
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(a3);
    v5 = 4 * a2;
    v6 = (_QWORD *)swift_allocObject();
    v7 = _swift_stdlib_malloc_size(v6);
    v8 = v7 - 32;
    if (v7 < 32)
      v8 = v7 - 29;
    v6[2] = a2;
    v6[3] = 2 * (v8 >> 2);
  }
  memcpy(v6 + 4, a1, v5);
  return v6;
}

void specialized vDSP.FFT2D.init(log2n:radix:ofType:)()
{
  _swift_stdlib_reportUnimplementedInitializer();
  __break(1u);
}

void specialized static vDSP_SplitComplexFloat.transform2D(fftSetup:width:height:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, unsigned __int8 *a6, void (*a7)(uint64_t, uint64_t, uint64_t, _QWORD, uint64_t, uint64_t, _QWORD, unint64_t, unint64_t, int))
{
  int v12;
  float v13;
  float v14;
  int v15;

  v12 = *a6;
  v13 = log2f((float)a2);
  if ((~LODWORD(v13) & 0x7F800000) == 0)
  {
    __break(1u);
    goto LABEL_12;
  }
  if (v13 <= -1.0)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  if (v13 >= 1.8447e19)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  v14 = log2f((float)a3);
  if ((~LODWORD(v14) & 0x7F800000) == 0)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  if (v14 <= -1.0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  if (v14 >= 1.8447e19)
  {
LABEL_16:
    __break(1u);
    return;
  }
  if (v12)
    v15 = -1;
  else
    v15 = 1;
  a7(a1, a4, 1, 0, a5, 1, 0, (unint64_t)v13, (unint64_t)v14, v15);
}

uint64_t specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(vDSP_Length a1, uint64_t a2, vDSP_Length a3, float a4)
{
  uint64_t v8;
  float __B;
  uint64_t v11;

  v11 = *MEMORY[0x1E0C80C00];
  if ((a1 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (a1)
  {
    v8 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v8 + 16) = a1;
  }
  else
  {
    v8 = MEMORY[0x1E0DEE9D8];
  }
  if (*(_QWORD *)(a2 + 16) >> 60)
    goto LABEL_10;
  __B = a4;
  if ((a3 & 0x8000000000000000) != 0)
  {
LABEL_11:
    __break(1u);
LABEL_12:
    __break(1u);
  }
  vDSP_vsmul((const float *)(a2 + 32), 1, &__B, (float *)(v8 + 32), 1, a3);
  if (a1 < a3)
    goto LABEL_12;
  *(_QWORD *)(v8 + 16) = a3;
  return v8;
}

uint64_t specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(vDSP_Length a1, uint64_t a2, vDSP_Length a3, double a4)
{
  uint64_t v8;
  double v10[2];

  v10[1] = *(double *)MEMORY[0x1E0C80C00];
  if ((a1 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (a1)
  {
    v8 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v8 + 16) = a1;
  }
  else
  {
    v8 = MEMORY[0x1E0DEE9D8];
  }
  if (*(_QWORD *)(a2 + 16) >> 59)
    goto LABEL_10;
  v10[0] = a4;
  if ((a3 & 0x8000000000000000) != 0)
  {
LABEL_11:
    __break(1u);
LABEL_12:
    __break(1u);
  }
  vDSP_vsmulD((const double *)(a2 + 32), 1, v10, (double *)(v8 + 32), 1, a3);
  if (a1 < a3)
    goto LABEL_12;
  *(_QWORD *)(v8 + 16) = a3;
  return v8;
}

unint64_t lazy protocol witness table accessor for type vDSP.Radix and conformance vDSP.Radix()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vDSP.Radix and conformance vDSP.Radix;
  if (!lazy protocol witness table cache variable for type vDSP.Radix and conformance vDSP.Radix)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vDSP.Radix, &type metadata for vDSP.Radix);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.Radix and conformance vDSP.Radix);
  }
  return result;
}

_UNKNOWN **associated type witness table accessor for vDSP_FourierTransformable.FFTFunctions : vDSP_FourierTransformFunctions in DSPSplitComplex()
{
  return &protocol witness table for vDSP_SplitComplexFloat;
}

_UNKNOWN **associated type witness table accessor for vDSP_FourierTransformable.FFTFunctions : vDSP_FourierTransformFunctions in DSPDoubleSplitComplex()
{
  return &protocol witness table for vDSP_SplitComplexDouble;
}

uint64_t storeEnumTagSinglePayload for vDSP.Radix(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 2 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 2) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFE)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFD)
    return ((uint64_t (*)(void))((char *)&loc_1CAB2D6F8 + 4 * byte_1CAB61495[v4]))();
  *a1 = a2 + 2;
  return ((uint64_t (*)(void))((char *)sub_1CAB2D72C + 4 * asc_1CAB61490[v4]))();
}

uint64_t sub_1CAB2D72C(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB2D734(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB2D73CLL);
  return result;
}

uint64_t sub_1CAB2D748(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB2D750);
  *(_BYTE *)result = a2 + 2;
  return result;
}

uint64_t sub_1CAB2D754(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB2D75C(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vDSP.Radix()
{
  return &type metadata for vDSP.Radix;
}

uint64_t type metadata completion function for vDSP.FFT()
{
  return swift_initClassMetadata2();
}

uint64_t method lookup function for vDSP.FFT()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of vDSP.FFT.__allocating_init(log2n:radix:ofType:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(v0 + 120))();
}

uint64_t dispatch thunk of vDSP.FFT.transform<A>(input:output:direction:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 128))();
}

uint64_t dispatch thunk of vDSP.FFT.forward(input:output:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 136))();
}

uint64_t dispatch thunk of vDSP.FFT.inverse(input:output:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 144))();
}

uint64_t type metadata completion function for vDSP.FFT2D()
{
  return swift_initClassMetadata2();
}

uint64_t type metadata accessor for vDSP.FFT2D(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return __swift_instantiateGenericMetadata(a1, a2, a3, a4, (uint64_t)&nominal type descriptor for vDSP.FFT2D);
}

uint64_t method lookup function for vDSP.FFT2D()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of vDSP.FFT2D.__allocating_init(width:height:ofType:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(v0 + 184))();
}

uint64_t dispatch thunk of static vDSP_FourierTransformFunctions.makeFFTSetup(log2n:radix:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return (*(uint64_t (**)(void))(a4 + 16))();
}

uint64_t dispatch thunk of static vDSP_FourierTransformFunctions.transform(fftSetup:log2n:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  return (*(uint64_t (**)(void))(a7 + 24))();
}

uint64_t dispatch thunk of static vDSP_FourierTransformFunctions.transform2D(fftSetup:width:height:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  return (*(uint64_t (**)(void))(a8 + 32))();
}

uint64_t dispatch thunk of static vDSP_FourierTransformFunctions.destroySetup(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 40))();
}

ValueMetadata *type metadata accessor for vDSP_SplitComplexFloat()
{
  return &type metadata for vDSP_SplitComplexFloat;
}

ValueMetadata *type metadata accessor for vDSP_SplitComplexDouble()
{
  return &type metadata for vDSP_SplitComplexDouble;
}

uint64_t partial apply for closure #1 in static vDSP_FFTFunctions.fftTransform2D<A>(fftSetup:width:height:source:destination:direction:)(uint64_t a1)
{
  uint64_t v1;

  return closure #1 in static vDSP_FFTFunctions.fftTransform2D<A>(fftSetup:width:height:source:destination:direction:)(a1, *(_QWORD *)(v1 + 32), *(_QWORD *)(v1 + 40), *(_QWORD *)(v1 + 48), *(_QWORD *)(v1 + 56), *(_BYTE *)(v1 + 64));
}

uint64_t partial apply for closure #1 in static vDSP_FFTFunctions.fftTransform<A>(fftSetup:log2n:source:destination:direction:)(uint64_t a1)
{
  uint64_t v1;

  return closure #1 in static vDSP_FFTFunctions.fftTransform<A>(fftSetup:log2n:source:destination:direction:)(a1, *(_QWORD *)(v1 + 32), *(_QWORD *)(v1 + 40), *(_QWORD *)(v1 + 48), *(_BYTE *)(v1 + 56));
}

uint64_t static BNNS.RandomGeneratorMethod.== infix(_:_:)()
{
  return 1;
}

void BNNS.RandomGeneratorMethod.hash(into:)()
{
  Hasher._combine(_:)(0);
}

Swift::Int BNNS.RandomGeneratorMethod.hashValue.getter()
{
  Hasher.init(_seed:)();
  Hasher._combine(_:)(0);
  return Hasher._finalize()();
}

uint64_t BNNS.RandomGeneratorState.deinit()
{
  uint64_t v0;

  MEMORY[0x1D1794DA4](*(_QWORD *)(v0 + 24), -1, -1);
  return v0;
}

uint64_t BNNS.RandomGeneratorState.__deallocating_deinit()
{
  uint64_t v0;

  MEMORY[0x1D1794DA4](*(_QWORD *)(v0 + 24), -1, -1);
  return swift_deallocClassInstance();
}

uint64_t BNNS.RandomGenerator.__allocating_init(method:seed:filterParameters:)(uint64_t a1, uint64_t a2, char a3, int a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v13;
  uint64_t v14;
  int *v15;
  uint64_t v16;
  int v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  v13 = swift_allocObject();
  if ((a3 & 1) == 0)
  {
    if (a6 == 1)
    {
      v16 = a2;
      v15 = 0;
    }
    else
    {
      v18 = a4;
      v19 = a5;
      v20 = a6;
      v21 = a7;
      v15 = &v18;
      v16 = a2;
    }
    v14 = MEMORY[0x1D17944EC](0, v16, v15);
    if (!v14)
      goto LABEL_10;
    goto LABEL_12;
  }
  if (a6 != 1)
  {
    v18 = a4;
    v19 = a5;
    v20 = a6;
    v21 = a7;
    v14 = MEMORY[0x1D17944E0](0, &v18);
    if (!v14)
      goto LABEL_10;
LABEL_12:
    *(_QWORD *)(v13 + 16) = v14;
    *(_QWORD *)(v13 + 24) = a2;
    *(_BYTE *)(v13 + 32) = a3 & 1;
    return v13;
  }
  v14 = MEMORY[0x1D17944E0](0, 0);
  if (v14)
    goto LABEL_12;
LABEL_10:
  swift_deallocPartialClassInstance();
  return 0;
}

uint64_t BNNS.RandomGenerator.init(method:seed:filterParameters:)(uint64_t a1, uint64_t a2, char a3, int a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v7;
  uint64_t v10;
  int *v11;
  int v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;

  v17 = *MEMORY[0x1E0C80C00];
  if ((a3 & 1) == 0)
  {
    if (a6 == 1)
    {
      v11 = 0;
    }
    else
    {
      v13 = a4;
      v14 = a5;
      v15 = a6;
      v16 = a7;
      v11 = &v13;
    }
    v10 = MEMORY[0x1D17944EC](0, a2, v11);
    if (!v10)
      goto LABEL_10;
    goto LABEL_12;
  }
  if (a6 != 1)
  {
    v13 = a4;
    v14 = a5;
    v15 = a6;
    v16 = a7;
    v10 = MEMORY[0x1D17944E0](0, &v13);
    if (!v10)
      goto LABEL_10;
LABEL_12:
    *(_QWORD *)(v7 + 16) = v10;
    *(_QWORD *)(v7 + 24) = a2;
    *(_BYTE *)(v7 + 32) = a3 & 1;
    return v7;
  }
  v10 = MEMORY[0x1D17944E0](0, 0);
  if (v10)
    goto LABEL_12;
LABEL_10:
  type metadata accessor for BNNS.RandomGenerator();
  swift_deallocPartialClassInstance();
  return 0;
}

