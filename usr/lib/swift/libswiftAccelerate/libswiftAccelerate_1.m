uint64_t type metadata accessor for BNNS.RandomGenerator()
{
  return objc_opt_self();
}

uint64_t BNNS.RandomGenerator.deinit()
{
  uint64_t v0;

  MEMORY[0x1D179451C](*(_QWORD *)(v0 + 16));
  return v0;
}

uint64_t BNNS.RandomGenerator.__deallocating_deinit()
{
  uint64_t v0;

  MEMORY[0x1D179451C](*(_QWORD *)(v0 + 16));
  return swift_deallocClassInstance();
}

uint64_t BNNS.RandomGenerator.state.getter()
{
  uint64_t v0;
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;

  type metadata accessor for BNNS.RandomGeneratorState();
  v1 = swift_allocObject();
  v2 = *(_QWORD *)(v0 + 16);
  swift_retain();
  v3 = MEMORY[0x1D1794894](v2);
  *(_QWORD *)(v1 + 16) = v3;
  v4 = swift_slowAlloc();
  swift_release();
  *(_QWORD *)(v1 + 24) = v4;
  MEMORY[0x1D179487C](*(_QWORD *)(v0 + 16), v3, v4);
  return v1;
}

uint64_t type metadata accessor for BNNS.RandomGeneratorState()
{
  return objc_opt_self();
}

uint64_t BNNS.RandomGenerator.state.setter(uint64_t a1)
{
  uint64_t v1;

  MEMORY[0x1D1794888](*(_QWORD *)(v1 + 16), *(_QWORD *)(a1 + 16), *(_QWORD *)(a1 + 24));
  return swift_release();
}

uint64_t (*BNNS.RandomGenerator.state.modify(uint64_t *a1))(_QWORD *a1)
{
  uint64_t v1;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;

  a1[1] = v1;
  type metadata accessor for BNNS.RandomGeneratorState();
  v3 = swift_allocObject();
  v4 = *(_QWORD *)(v1 + 16);
  swift_retain();
  v5 = MEMORY[0x1D1794894](v4);
  *(_QWORD *)(v3 + 16) = v5;
  v6 = swift_slowAlloc();
  swift_release();
  *(_QWORD *)(v3 + 24) = v6;
  MEMORY[0x1D179487C](*(_QWORD *)(v1 + 16), v5, v6);
  *a1 = v3;
  return BNNS.RandomGenerator.state.modify;
}

uint64_t BNNS.RandomGenerator.state.modify(_QWORD *a1)
{
  MEMORY[0x1D1794888](*(_QWORD *)(a1[1] + 16), *(_QWORD *)(*a1 + 16), *(_QWORD *)(*a1 + 24));
  return swift_release();
}

uint64_t static BNNSNDArrayDescriptor.allocate<A>(randomUniformUsing:range:shape:batchSize:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X3>, uint64_t a4@<X4>, uint64_t a5@<X5>, uint64_t a6@<X8>)
{
  uint64_t v11;
  uint64_t v12;
  char *v13;
  uint64_t v14;
  void *v15;
  void (*v16)(char *, uint64_t, uint64_t);
  float v17;
  uint64_t v18;
  uint64_t v20;
  uint64_t v21;
  BNNSNDArrayDescriptor v22;
  BNNSNDArrayDescriptor v23;
  _BYTE v24[136];
  _DWORD v25[46];
  uint64_t v26;

  v21 = a6;
  v26 = *MEMORY[0x1E0C80C00];
  v11 = *(_QWORD *)(a4 - 8);
  MEMORY[0x1E0C80A78](a1);
  v13 = (char *)&v20 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  outlined init with take of BNNS.Shape(v14, (uint64_t)v24);
  (*(void (**)(uint64_t, uint64_t))(a5 + 8))(a4, a5);
  helper #1 <A>(_:) in static BNNSNDArrayDescriptor.allocateUninitialized(scalarType:shape:batchSize:)(a3, (uint64_t)v24, &v23);
  v15 = *(void **)(a1 + 16);
  v16 = *(void (**)(char *, uint64_t, uint64_t))(v11 + 16);
  v16(v13, a2, a4);
  lazy protocol witness table accessor for type Float and conformance Float();
  BinaryFloatingPoint.init<A>(_:)();
  v17 = *(float *)v25;
  v18 = type metadata accessor for ClosedRange();
  v16(v13, a2 + *(int *)(v18 + 36), a4);
  BinaryFloatingPoint.init<A>(_:)();
  if (BNNSRandomFillUniformFloat(v15, &v23, v17, *(float *)&v22.flags))
  {
    if (v23.data)
      MEMORY[0x1D1794DA4](v23.data, -1, -1);
    _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&v22);
  }
  else
  {
    v22 = v23;
    _sSo21BNNSNDArrayDescriptoraSgWOi_((uint64_t)&v22);
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v22, (uint64_t)v25);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v25, v21);
}

uint64_t static BNNSNDArrayDescriptor.allocate<A>(randomUniformUsing:range:shape:batchSize:)@<X0>(char *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X8>)
{
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  char *v40;
  uint64_t v41;
  uint64_t v42;
  char *v43;
  uint64_t v44;
  char *v45;
  void (*v46)(char *, uint64_t, uint64_t);
  uint64_t v47;
  void (*v48)(char *, uint64_t);
  uint64_t v49;
  char v50;
  char *v51;
  char *v52;
  uint64_t v53;
  char v54;
  uint64_t v55;
  char *v56;
  int64_t v57;
  char v58;
  uint64_t v59;
  int64_t v60;
  uint64_t v61;
  char *v62;
  uint64_t v63;
  char *v64;
  char v65;
  char *v66;
  char *v67;
  uint64_t v68;
  uint64_t v69;
  uint64_t v70;
  uint64_t v71;
  uint64_t v72;
  uint64_t v73;
  char v74;
  uint64_t v75;
  char *v76;
  uint64_t v77;
  char *v78;
  char *v79;
  char *v80;
  char v81;
  char *v82;
  char *v83;
  char *v84;
  int64_t v85;
  uint64_t v86;
  char v88;
  uint64_t v89;
  char *v90;
  char v91;
  uint64_t v92;
  char *v93;
  char v94;
  char *v95;
  BOOL i;
  char *v97;
  char *v98;
  uint64_t v99;
  uint64_t v100;
  char *v101;
  uint64_t v102;
  uint64_t v103;
  uint64_t v104;
  uint64_t AssociatedConformanceWitness;
  char *v106;
  char *v107;
  uint64_t v108;
  uint64_t v109;
  char *v110;
  char *v111;
  uint64_t AssociatedTypeWitness;
  char *v113;
  uint64_t v114;
  char *v115;
  uint64_t v116;
  char *v117;
  char *v118;
  uint64_t v119;
  void *v120;
  char *v121;
  char *v122;
  uint64_t v123;
  char *v124;
  char *v125;
  char *v126;
  char *v127;
  char *v128;
  BNNSNDArrayDescriptor v129;
  _QWORD v130[23];
  BNNSNDArrayDescriptor v131;
  _BYTE v132[136];
  uint64_t v133;

  v123 = a4;
  v124 = a1;
  v119 = a8;
  v133 = *MEMORY[0x1E0C80C00];
  v12 = *(_QWORD *)(a7 + 8);
  v114 = *(_QWORD *)(*(_QWORD *)(v12 + 24) + 16);
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v13 = MEMORY[0x1E0C80A78](AssociatedTypeWitness);
  v113 = (char *)&v109 - v14;
  v15 = *(_QWORD *)(a5 - 8);
  v16 = MEMORY[0x1E0C80A78](v13);
  v110 = (char *)&v109 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v18 = MEMORY[0x1E0C80A78](v16);
  v117 = (char *)&v109 - v19;
  v20 = MEMORY[0x1E0C80A78](v18);
  v126 = (char *)&v109 - v21;
  v22 = MEMORY[0x1E0C80A78](v20);
  v127 = (char *)&v109 - v23;
  v24 = MEMORY[0x1E0C80A78](v22);
  v111 = (char *)&v109 - v25;
  v26 = MEMORY[0x1E0C80A78](v24);
  v115 = (char *)&v109 - v27;
  v28 = MEMORY[0x1E0C80A78](v26);
  v121 = (char *)&v109 - v29;
  v30 = MEMORY[0x1E0C80A78](v28);
  v128 = (char *)&v109 - v31;
  v32 = MEMORY[0x1E0C80A78](v30);
  v125 = (char *)&v109 - v33;
  v34 = MEMORY[0x1E0C80A78](v32);
  v122 = (char *)&v109 - v35;
  v36 = MEMORY[0x1E0C80A78](v34);
  v118 = (char *)&v109 - v37;
  v38 = MEMORY[0x1E0C80A78](v36);
  v40 = (char *)&v109 - v39;
  v41 = MEMORY[0x1E0C80A78](v38);
  v43 = (char *)&v109 - v42;
  MEMORY[0x1E0C80A78](v41);
  v45 = (char *)&v109 - v44;
  outlined init with take of BNNS.Shape(a3, (uint64_t)v132);
  (*(void (**)(uint64_t, uint64_t))(a6 + 8))(a5, a6);
  helper #1 <A>(_:) in static BNNSNDArrayDescriptor.allocateUninitialized(scalarType:shape:batchSize:)(v123, (uint64_t)v132, &v131);
  v120 = (void *)*((_QWORD *)v124 + 2);
  v46 = *(void (**)(char *, uint64_t, uint64_t))(v15 + 16);
  v123 = a2;
  v46(v45, a2, a5);
  LOBYTE(a3) = dispatch thunk of static BinaryInteger.isSigned.getter();
  v46(v43, (uint64_t)v45, a5);
  v116 = v15;
  if ((a3 & 1) == 0)
  {
    v48 = *(void (**)(char *, uint64_t))(v15 + 8);
    v48(v43, a5);
    goto LABEL_7;
  }
  v47 = dispatch thunk of BinaryInteger.bitWidth.getter();
  v48 = *(void (**)(char *, uint64_t))(v15 + 8);
  v48(v43, a5);
  if (v47 <= 64)
  {
LABEL_7:
    v51 = v126;
LABEL_8:
    v49 = (uint64_t)v127;
    v52 = v122;
    goto LABEL_9;
  }
  v49 = (uint64_t)v40;
  v46(v40, (uint64_t)v45, a5);
  v130[0] = 0x8000000000000000;
  v50 = dispatch thunk of static BinaryInteger.isSigned.getter();
  v51 = v126;
  if ((v50 & 1) != 0)
  {
    if (dispatch thunk of BinaryInteger.bitWidth.getter() < 64)
      goto LABEL_54;
LABEL_47:
    lazy protocol witness table accessor for type Int64 and conformance Int64();
    v90 = v118;
    dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
    v77 = dispatch thunk of static Comparable.< infix(_:_:)();
    v48(v90, a5);
    v48((char *)v49, a5);
    v49 = (uint64_t)v127;
    v52 = v122;
    if ((v77 & 1) == 0)
      goto LABEL_9;
    goto LABEL_73;
  }
  v88 = dispatch thunk of static BinaryInteger.isSigned.getter();
  v89 = dispatch thunk of BinaryInteger.bitWidth.getter();
  if ((v88 & 1) != 0)
  {
    if (v89 <= 64)
      goto LABEL_63;
    goto LABEL_47;
  }
  if (v89 >= 64)
  {
    v48((char *)v49, a5);
    goto LABEL_8;
  }
LABEL_54:
  v77 = dispatch thunk of BinaryInteger._lowWord.getter();
  v48((char *)v49, a5);
  for (i = v77 < v130[0]; ; i = v103 < v102)
  {
    v49 = (uint64_t)v127;
    v52 = v122;
    if (i)
      goto LABEL_73;
LABEL_9:
    v53 = dispatch thunk of BinaryInteger.bitWidth.getter();
    v46(v52, (uint64_t)v45, a5);
    if (v53 < 65)
    {
      v59 = dispatch thunk of BinaryInteger.bitWidth.getter();
      v48(v52, a5);
      if (v59 != 64 || (dispatch thunk of static BinaryInteger.isSigned.getter() & 1) != 0)
        goto LABEL_20;
    }
    else
    {
      v48(v52, a5);
    }
    v46(v125, (uint64_t)v45, a5);
    v130[0] = 0x7FFFFFFFFFFFFFFFLL;
    v54 = dispatch thunk of static BinaryInteger.isSigned.getter();
    v55 = dispatch thunk of BinaryInteger.bitWidth.getter();
    if ((v54 & 1) != 0)
    {
      if (v55 > 64)
      {
        lazy protocol witness table accessor for type Int64 and conformance Int64();
        v56 = v118;
        dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
        v57 = (int64_t)v125;
        v58 = dispatch thunk of static Comparable.< infix(_:_:)();
        v48(v56, a5);
        v48((char *)v57, a5);
        v51 = v126;
        v49 = (uint64_t)v127;
        if ((v58 & 1) != 0)
          goto LABEL_36;
        goto LABEL_20;
      }
LABEL_19:
      v60 = (int64_t)v125;
      v61 = dispatch thunk of BinaryInteger._lowWord.getter();
      v48((char *)v60, a5);
      v51 = v126;
      v49 = (uint64_t)v127;
      if (v130[0] < v61)
        goto LABEL_36;
      goto LABEL_20;
    }
    if (v55 <= 63)
      goto LABEL_19;
    *(_QWORD *)&v129.flags = 0x7FFFFFFFFFFFFFFFLL;
    v79 = v118;
    (*(void (**)(char *, char *, uint64_t))(v116 + 32))(v118, v125, a5);
    lazy protocol witness table accessor for type Int64 and conformance Int64();
    v80 = v115;
    dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
    v81 = dispatch thunk of static Comparable.< infix(_:_:)();
    v82 = v80;
    v51 = v126;
    v48(v82, a5);
    v48(v79, a5);
    v49 = (uint64_t)v127;
    if ((v81 & 1) != 0)
    {
LABEL_36:
      __break(1u);
LABEL_37:
      *(_QWORD *)&v129.flags = 0x7FFFFFFFFFFFFFFFLL;
      v83 = v118;
      (*(void (**)(char *, char *, uint64_t))(v116 + 32))(v118, v51, a5);
      lazy protocol witness table accessor for type Int64 and conformance Int64();
      v84 = v115;
      dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
      v77 = dispatch thunk of static Comparable.< infix(_:_:)();
      ((void (*)(char *, uint64_t))v45)(v84, a5);
      v78 = v83;
LABEL_38:
      ((void (*)(char *, uint64_t))v45)(v78, a5);
      if ((v77 & 1) == 0)
        goto LABEL_39;
      __break(1u);
LABEL_72:
      v48((char *)v77, a5);
LABEL_73:
      __break(1u);
LABEL_74:
      v48((char *)v77, a5);
      __break(1u);
    }
LABEL_20:
    v62 = v51;
    v125 = (char *)dispatch thunk of BinaryInteger._lowWord.getter();
    v48(v45, a5);
    v122 = *(char **)(*(_QWORD *)(v12 + 32) + 8);
    v63 = type metadata accessor for ClosedRange();
    v64 = v128;
    v46(v128, v123 + *(int *)(v63 + 36), a5);
    v65 = dispatch thunk of static BinaryInteger.isSigned.getter();
    v66 = v121;
    v67 = v64;
    v45 = (char *)v46;
    v46(v121, (uint64_t)v67, a5);
    v124 = (char *)v48;
    if ((v65 & 1) == 0)
    {
      v48(v66, a5);
      v51 = v62;
      goto LABEL_26;
    }
    v68 = dispatch thunk of BinaryInteger.bitWidth.getter();
    v48(v66, a5);
    i = v68 <= 64;
    v51 = v62;
    if (i)
      goto LABEL_26;
    v46((char *)v49, (uint64_t)v128, a5);
    v130[0] = 0x8000000000000000;
    if ((dispatch thunk of static BinaryInteger.isSigned.getter() & 1) != 0)
    {
      v69 = dispatch thunk of BinaryInteger.bitWidth.getter();
      v48 = (void (*)(char *, uint64_t))v124;
      if (v69 < 64)
      {
        v70 = dispatch thunk of BinaryInteger._lowWord.getter();
        v71 = v49;
        goto LABEL_61;
      }
      lazy protocol witness table accessor for type Int64 and conformance Int64();
      v97 = v118;
      dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
      v94 = dispatch thunk of static Comparable.< infix(_:_:)();
      v48(v97, a5);
      v95 = (char *)v49;
LABEL_56:
      v48(v95, a5);
      if ((v94 & 1) == 0)
        goto LABEL_26;
      goto LABEL_62;
    }
    v91 = dispatch thunk of static BinaryInteger.isSigned.getter();
    v92 = dispatch thunk of BinaryInteger.bitWidth.getter();
    v48 = (void (*)(char *, uint64_t))v124;
    if ((v91 & 1) == 0)
      break;
    if (v92 > 64)
    {
      lazy protocol witness table accessor for type Int64 and conformance Int64();
      v93 = v118;
      dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
      v45 = v127;
      v94 = dispatch thunk of static Comparable.< infix(_:_:)();
      v48(v93, a5);
      v95 = v45;
      goto LABEL_56;
    }
    v104 = AssociatedTypeWitness;
    AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
    MEMORY[0x1D17943A8](&unk_1CAB61788, 256, v104, AssociatedConformanceWitness);
    v106 = v118;
    dispatch thunk of ExpressibleByFloatLiteral.init(floatLiteral:)();
    v107 = v127;
    v49 = dispatch thunk of static Comparable.< infix(_:_:)();
    v48(v106, a5);
    v77 = (uint64_t)v110;
    (*(void (**)(char *, char *, uint64_t))(v116 + 32))(v110, v107, a5);
    if ((v49 & 1) != 0)
      goto LABEL_74;
    v45 = (char *)v130[0];
    v108 = dispatch thunk of BinaryInteger._lowWord.getter();
    v48((char *)v77, a5);
    if (v108 >= (uint64_t)v45)
      goto LABEL_26;
LABEL_62:
    __break(1u);
LABEL_63:
    v99 = AssociatedTypeWitness;
    v100 = swift_getAssociatedConformanceWitness();
    MEMORY[0x1D17943A8](&unk_1CAB61788, 256, v99, v100);
    v101 = v118;
    dispatch thunk of ExpressibleByFloatLiteral.init(floatLiteral:)();
    LODWORD(v124) = dispatch thunk of static Comparable.< infix(_:_:)();
    v48(v101, a5);
    v77 = (uint64_t)v111;
    (*(void (**)(char *, uint64_t, uint64_t))(v116 + 32))(v111, v49, a5);
    if ((v124 & 1) != 0)
      goto LABEL_72;
    v102 = v130[0];
    v103 = dispatch thunk of BinaryInteger._lowWord.getter();
    v48((char *)v77, a5);
  }
  if (v92 < 64)
  {
    v98 = v127;
    v70 = dispatch thunk of BinaryInteger._lowWord.getter();
    v71 = (uint64_t)v98;
LABEL_61:
    v48((char *)v71, a5);
    if (v70 >= v130[0])
      goto LABEL_26;
    goto LABEL_62;
  }
  ((void (*)(char *, uint64_t))v124)(v127, a5);
LABEL_26:
  v48 = (void (*)(char *, uint64_t))v128;
  v72 = dispatch thunk of BinaryInteger.bitWidth.getter();
  v46(v51, (uint64_t)v48, a5);
  if (v72 >= 65)
  {
    v45 = v124;
    ((void (*)(char *, uint64_t))v124)(v51, a5);
    goto LABEL_30;
  }
  v73 = dispatch thunk of BinaryInteger.bitWidth.getter();
  v45 = v124;
  ((void (*)(char *, uint64_t))v124)(v51, a5);
  if (v73 != 64 || (dispatch thunk of static BinaryInteger.isSigned.getter() & 1) != 0)
    goto LABEL_39;
LABEL_30:
  v51 = v117;
  v46(v117, (uint64_t)v48, a5);
  v130[0] = 0x7FFFFFFFFFFFFFFFLL;
  v74 = dispatch thunk of static BinaryInteger.isSigned.getter();
  v75 = dispatch thunk of BinaryInteger.bitWidth.getter();
  if ((v74 & 1) == 0)
  {
    if (v75 <= 63)
      goto LABEL_34;
    goto LABEL_37;
  }
  if (v75 > 64)
  {
    lazy protocol witness table accessor for type Int64 and conformance Int64();
    v76 = v118;
    dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
    v77 = dispatch thunk of static Comparable.< infix(_:_:)();
    ((void (*)(char *, uint64_t))v45)(v76, a5);
    v78 = v51;
    goto LABEL_38;
  }
LABEL_34:
  dispatch thunk of BinaryInteger._lowWord.getter();
  ((void (*)(char *, uint64_t))v45)(v51, a5);
LABEL_39:
  v85 = dispatch thunk of BinaryInteger._lowWord.getter();
  ((void (*)(_QWORD, uint64_t))v45)(v48, a5);
  if (BNNSRandomFillUniformInt(v120, &v131, (int64_t)v125, v85))
  {
    v86 = v119;
    if (v131.data)
      MEMORY[0x1D1794DA4](v131.data, -1, -1);
    _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&v129);
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v129, (uint64_t)v130);
  }
  else
  {
    v129 = v131;
    _sSo21BNNSNDArrayDescriptoraSgWOi_((uint64_t)&v129);
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v129, (uint64_t)v130);
    v86 = v119;
  }
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v130, v86);
}

unint64_t lazy protocol witness table accessor for type Int64 and conformance Int64()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type Int64 and conformance Int64;
  if (!lazy protocol witness table cache variable for type Int64 and conformance Int64)
  {
    result = MEMORY[0x1D1794D08](MEMORY[0x1E0DEDCA0], MEMORY[0x1E0DEDC60]);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Int64 and conformance Int64);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type BNNS.RandomGeneratorMethod and conformance BNNS.RandomGeneratorMethod()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNS.RandomGeneratorMethod and conformance BNNS.RandomGeneratorMethod;
  if (!lazy protocol witness table cache variable for type BNNS.RandomGeneratorMethod and conformance BNNS.RandomGeneratorMethod)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNS.RandomGeneratorMethod, &type metadata for BNNS.RandomGeneratorMethod);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.RandomGeneratorMethod and conformance BNNS.RandomGeneratorMethod);
  }
  return result;
}

uint64_t sub_1CAB2EE48@<X0>(uint64_t *a1@<X8>)
{
  uint64_t result;

  result = BNNS.RandomGenerator.state.getter();
  *a1 = result;
  return result;
}

void sub_1CAB2EE70()
{
  JUMPOUT(0x1D1794888);
}

uint64_t storeEnumTagSinglePayload for BNNS.RandomGeneratorMethod(uint64_t a1, int a2, int a3)
{
  int v3;
  uint64_t v4;

  if ((a3 + 1) >= 0x10000)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 1) < 0x100)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3)
    v4 = v4;
  else
    v4 = 0;
  if (a2)
    return ((uint64_t (*)(void))((char *)sub_1CAB2EEC4 + 4 * asc_1CAB61680[v4]))();
  else
    return ((uint64_t (*)(void))((char *)sub_1CAB2EEE4 + 4 * byte_1CAB61685[v4]))();
}

_BYTE *sub_1CAB2EEC4(_BYTE *result, char a2)
{
  *result = a2;
  return result;
}

_BYTE *sub_1CAB2EEE4(_BYTE *result)
{
  *result = 0;
  return result;
}

_DWORD *sub_1CAB2EEEC(_DWORD *result, int a2)
{
  *result = a2;
  return result;
}

_WORD *sub_1CAB2EEF4(_WORD *result, __int16 a2)
{
  *result = a2;
  return result;
}

_WORD *sub_1CAB2EEFC(_WORD *result)
{
  *result = 0;
  return result;
}

_DWORD *sub_1CAB2EF04(_DWORD *result)
{
  *result = 0;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.RandomGeneratorMethod()
{
  return &type metadata for BNNS.RandomGeneratorMethod;
}

uint64_t method lookup function for BNNS.RandomGeneratorState()
{
  return swift_lookUpClassMethod();
}

uint64_t method lookup function for BNNS.RandomGenerator()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of BNNS.RandomGenerator.__allocating_init(method:seed:filterParameters:)(uint64_t a1, uint64_t a2, char a3)
{
  uint64_t v3;

  return (*(uint64_t (**)(uint64_t, uint64_t, _QWORD))(v3 + 120))(a1, a2, a3 & 1);
}

uint64_t dispatch thunk of BNNS.RandomGenerator.state.getter()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 128))();
}

uint64_t dispatch thunk of BNNS.RandomGenerator.state.setter()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 136))();
}

uint64_t dispatch thunk of BNNS.RandomGenerator.state.modify()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 144))();
}

void vImage.PixelBuffer<>.clip(to:destination:)(uint64_t *a1, uint64_t a2, uint64_t a3, float a4, float a5)
{
  uint64_t *v5;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  vDSP_Length v14;
  uint64_t v15;
  const float *v16;
  float *v17;
  float __C;
  float __B;
  uint64_t v20;
  uint64_t v21;

  v21 = *MEMORY[0x1E0C80C00];
  v10 = *a1;
  v11 = *v5;
  swift_bridgeObjectRetain();
  v12 = vImage.PixelBuffer<>.count.getter(a2, a3);
  v20 = v10;
  v13 = vImage.PixelBuffer<>.count.getter(a2, a3);
  swift_bridgeObjectRelease();
  if (v12 != v13)
  {
    __break(1u);
    goto LABEL_7;
  }
  v14 = vImage.PixelBuffer<>.count.getter(a2, a3);
  __C = a5;
  __B = a4;
  v20 = v11;
  v15 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v15)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  v16 = (const float *)v15;
  v20 = v10;
  v17 = (float *)vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v17)
    goto LABEL_9;
  if ((v14 & 0x8000000000000000) != 0)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  vDSP_vclip(v16, 1, &__B, &__C, v17, 1, v14);
}

void vImage.PixelBuffer<>.colorThreshold(_:destination:)(uint64_t *a1, uint64_t a2, uint64_t a3, float a4)
{
  uint64_t *v4;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  vDSP_Length v12;
  uint64_t v13;
  const float *v14;
  float *v15;
  float *v16;
  float v17;
  float __C;
  _QWORD __B[2];

  __B[1] = *MEMORY[0x1E0C80C00];
  v8 = *a1;
  v9 = *v4;
  swift_bridgeObjectRetain();
  v10 = vImage.PixelBuffer<>.count.getter(a2, a3);
  __B[0] = v8;
  v11 = vImage.PixelBuffer<>.count.getter(a2, a3);
  swift_bridgeObjectRelease();
  if (v10 != v11)
  {
    __break(1u);
    goto LABEL_7;
  }
  v12 = vImage.PixelBuffer<>.count.getter(a2, a3);
  __B[0] = v9;
  v13 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v13)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  v14 = (const float *)v13;
  __B[0] = v8;
  v15 = (float *)vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v15)
    goto LABEL_9;
  *(float *)__B = a4;
  v17 = 0.0;
  __C = 1.0;
  if ((v12 & 0x8000000000000000) != 0)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  v16 = v15;
  vDSP_vthrsc(v14, 1, (const float *)__B, &__C, v15, 1, v12);
  vDSP_vclip(v16, 1, &v17, &__C, v16, 1, v12);
}

void vImage.PixelBuffer<>.linearInterpolate(bufferB:interpolationConstant:destination:)(uint64_t *a1, uint64_t *a2, uint64_t a3, uint64_t a4, float a5)
{
  uint64_t *v5;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  vDSP_Length v16;
  uint64_t v17;
  const float *v18;
  uint64_t v19;
  const float *v20;
  float *v21;
  float v22;
  uint64_t v23;

  v9 = *a1;
  v10 = *a2;
  v11 = *v5;
  swift_bridgeObjectRetain();
  v12 = vImage.PixelBuffer<>.count.getter(a3, a4);
  v23 = v10;
  v13 = vImage.PixelBuffer<>.count.getter(a3, a4);
  swift_bridgeObjectRelease();
  if (v12 != v13)
  {
    __break(1u);
    goto LABEL_9;
  }
  swift_bridgeObjectRetain();
  v14 = vImage.PixelBuffer<>.count.getter(a3, a4);
  v23 = v9;
  v15 = vImage.PixelBuffer<>.count.getter(a3, a4);
  swift_bridgeObjectRelease();
  if (v14 != v15)
  {
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  v16 = vImage.PixelBuffer<>.count.getter(a3, a4);
  v23 = v11;
  v17 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v17)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  v18 = (const float *)v17;
  v23 = v9;
  v19 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v19)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v20 = (const float *)v19;
  v23 = v10;
  v21 = (float *)vImage.PixelBuffer<>.vImageBuffer.getter();
  if (v21)
  {
    v22 = a5;
    if ((v16 & 0x8000000000000000) == 0)
    {
      vDSP_vintb(v18, 1, v20, 1, &v22, v21, 1, v16);
      return;
    }
    goto LABEL_10;
  }
LABEL_13:
  __break(1u);
}

uint64_t vImage.Error.init(vImageError:)@<X0>(uint64_t a1@<X0>, char *a2@<X8>)
{
  uint64_t result;
  char v4;
  char v5;

  result = vImage.Error.init(rawValue:)(a1, &v5);
  v4 = v5;
  if (v5 == 20)
    v4 = 11;
  *a2 = v4;
  return result;
}

uint64_t vImage.Error.init(rawValue:)@<X0>(uint64_t result@<X0>, char *a2@<X8>)
{
  char v2;

  v2 = 2;
  switch(result)
  {
    case -21784:
      *a2 = 19;
      break;
    case -21783:
      *a2 = 18;
      break;
    case -21782:
      *a2 = 17;
      break;
    case -21781:
      *a2 = 16;
      break;
    case -21780:
      *a2 = 15;
      break;
    case -21779:
      *a2 = 14;
      break;
    case -21778:
      *a2 = 13;
      break;
    case -21777:
      *a2 = 12;
      break;
    case -21776:
      *a2 = 11;
      break;
    case -21775:
      *a2 = 10;
      break;
    case -21774:
      *a2 = 9;
      break;
    case -21773:
      *a2 = 8;
      break;
    case -21772:
      *a2 = 7;
      break;
    case -21771:
      *a2 = 6;
      break;
    case -21770:
      *a2 = 5;
      break;
    case -21769:
      *a2 = 4;
      break;
    case -21768:
      *a2 = 3;
      break;
    case -21767:
      goto LABEL_22;
    case -21766:
      v2 = 1;
LABEL_22:
      *a2 = v2;
      break;
    default:
      if (result)
        *a2 = 20;
      else
        *a2 = 0;
      break;
  }
  return result;
}

uint64_t vImage.Error.rawValue.getter()
{
  char *v0;

  return qword_1CAB618F0[*v0];
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance vImage.Error(char *a1, char *a2)
{
  return qword_1CAB618F0[*a1] == qword_1CAB618F0[*a2];
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance vImage.Error()
{
  char *v0;
  uint64_t v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(qword_1CAB618F0[v1]);
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance vImage.Error()
{
  char *v0;

  Hasher._combine(_:)(qword_1CAB618F0[*v0]);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance vImage.Error()
{
  char *v0;
  uint64_t v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(qword_1CAB618F0[v1]);
  return Hasher._finalize()();
}

uint64_t protocol witness for RawRepresentable.init(rawValue:) in conformance vImage.Error@<X0>(uint64_t *a1@<X0>, char *a2@<X8>)
{
  return vImage.Error.init(rawValue:)(*a1, a2);
}

void protocol witness for RawRepresentable.rawValue.getter in conformance vImage.Error(_QWORD *a1@<X8>)
{
  char *v1;

  *a1 = qword_1CAB618F0[*v1];
}

uint64_t protocol witness for Error._code.getter in conformance vImage.Error()
{
  lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
  lazy protocol witness table accessor for type Int and conformance Int();
  return Error<>._code.getter();
}

uint64_t getEnumTagSinglePayload for vImage.Error(unsigned __int8 *a1, unsigned int a2)
{
  int v2;
  int v3;
  int v4;
  unsigned int v6;
  BOOL v7;
  int v8;

  if (!a2)
    return 0;
  if (a2 < 0xED)
    goto LABEL_17;
  if (a2 + 19 >= 0xFFFF00)
    v2 = 4;
  else
    v2 = 2;
  if ((a2 + 19) >> 8 < 0xFF)
    v3 = 1;
  else
    v3 = v2;
  if (v3 == 4)
  {
    v4 = *(_DWORD *)(a1 + 1);
    if (v4)
      return (*a1 | (v4 << 8)) - 19;
  }
  else
  {
    if (v3 == 2)
    {
      v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1))
        goto LABEL_17;
      return (*a1 | (v4 << 8)) - 19;
    }
    v4 = a1[1];
    if (a1[1])
      return (*a1 | (v4 << 8)) - 19;
  }
LABEL_17:
  v6 = *a1;
  v7 = v6 >= 0x14;
  v8 = v6 - 20;
  if (!v7)
    v8 = -1;
  return (v8 + 1);
}

uint64_t storeEnumTagSinglePayload for vImage.Error(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 19 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 19) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xED)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xEC)
    return ((uint64_t (*)(void))((char *)&loc_1CAB2F7A8 + 4 * byte_1CAB617A8[v4]))();
  *a1 = a2 + 19;
  return ((uint64_t (*)(void))((char *)sub_1CAB2F7DC + 4 * byte_1CAB617A3[v4]))();
}

uint64_t sub_1CAB2F7DC(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB2F7E4(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB2F7ECLL);
  return result;
}

uint64_t sub_1CAB2F7F8(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB2F800);
  *(_BYTE *)result = a2 + 19;
  return result;
}

uint64_t sub_1CAB2F804(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB2F80C(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vImage.Error()
{
  return &type metadata for vImage.Error;
}

unint64_t lazy protocol witness table accessor for type Int and conformance Int()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type Int and conformance Int;
  if (!lazy protocol witness table cache variable for type Int and conformance Int)
  {
    result = MEMORY[0x1D1794D08](MEMORY[0x1E0DEB470], MEMORY[0x1E0DEB418]);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Int and conformance Int);
  }
  return result;
}

{
  unint64_t result;

  result = lazy protocol witness table cache variable for type Int and conformance Int;
  if (!lazy protocol witness table cache variable for type Int and conformance Int)
  {
    result = MEMORY[0x1D1794D08](MEMORY[0x1E0DEB440], MEMORY[0x1E0DEB418]);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Int and conformance Int);
  }
  return result;
}

{
  unint64_t result;

  result = lazy protocol witness table cache variable for type Int and conformance Int;
  if (!lazy protocol witness table cache variable for type Int and conformance Int)
  {
    result = MEMORY[0x1D1794D08](MEMORY[0x1E0DEB460], MEMORY[0x1E0DEB418]);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Int and conformance Int);
  }
  return result;
}

__n128 static BNNS.quantize(batchSize:input:output:axis:scale:bias:filterParameters:)(size_t a1, _OWORD *a2, _OWORD *a3, uint64_t a4, char a5, uint64_t a6, uint64_t a7, uint32_t a8, size_t a9, int (__cdecl *a10)(void **, size_t, size_t), void (__cdecl *a11)(void *))
{
  __n128 result;

  static BNNS.quantize(batchSize:input:output:axis:scale:bias:filterParameters:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, 0);
  return result;
}

__n128 static BNNS.dequantize(batchSize:input:output:axis:scale:bias:filterParameters:)(size_t a1, _OWORD *a2, _OWORD *a3, uint64_t a4, char a5, uint64_t a6, uint64_t a7, uint32_t a8, size_t a9, int (__cdecl *a10)(void **, size_t, size_t), void (__cdecl *a11)(void *))
{
  __n128 result;

  static BNNS.quantize(batchSize:input:output:axis:scale:bias:filterParameters:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, 1);
  return result;
}

void static BNNS.quantize(batchSize:input:output:axis:scale:bias:filterParameters:)(size_t a1, _OWORD *a2, _OWORD *a3, uint64_t a4, char a5, uint64_t a6, uint64_t a7, uint32_t a8, size_t a9, int (__cdecl *a10)(void **, size_t, size_t), void (__cdecl *a11)(void *), char a12)
{
  void *v12;

  specialized static BNNS.quantizeDequantize(_:batchSize:input:output:axis:scale:bias:filterParameters:)(a12, a1, a2, a3, a4, a5 & 1, a6, a7, a8, a9, a10, a11);
  if (v12)

}

uint64_t specialized static BNNS.quantizeDequantize(_:batchSize:input:output:axis:scale:bias:filterParameters:)(char a1, size_t a2, _OWORD *a3, _OWORD *a4, uint64_t a5, char a6, uint64_t a7, uint64_t a8, uint32_t a9, size_t a10, int (__cdecl *a11)(void **, size_t, size_t), void (__cdecl *a12)(void *))
{
  uint64_t result;
  BNNSQuantizerFunction v20;
  uint64_t v21;
  size_t v22;
  uint64_t v23;
  size_t v24;
  size_t v25;
  uint64_t v26;
  int v27;
  BNNSDataType v28;
  void *v29;
  void *v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  uint64_t v39;
  uint64_t v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  size_t v51;
  BNNSFilterParameters *p_filter_params;
  _BYTE *v53;
  int v54;
  BNNSDataType v55;
  void *v56;
  void *v57;
  size_t v58;
  size_t v59;
  size_t v60;
  size_t v61;
  size_t v62;
  size_t v63;
  size_t v64;
  uint64_t v65;
  size_t v66;
  size_t v67;
  size_t v68;
  size_t v69;
  size_t v70;
  size_t v71;
  size_t v72;
  size_t v73;
  uint64_t v74;
  size_t v75;
  unint64_t v77;
  unint64_t v78;
  unint64_t v79;
  unint64_t v80;
  unint64_t v81;
  unint64_t v82;
  unint64_t v83;
  unint64_t v84;
  BNNSFilterParameters filter_params;
  _BYTE v86[136];
  unint64_t v87;
  unint64_t v88;
  unint64_t v89;
  unint64_t v90;
  unint64_t v91;
  unint64_t v92;
  unint64_t v93;
  unint64_t v94;
  _BYTE v95[136];
  _BYTE v96[136];
  _BYTE v97[136];
  _BYTE v98[180];
  BNNSLayerParametersQuantization layer_params;
  _BYTE v100[184];
  _BYTE v101[184];
  _BYTE v102[113];
  _BYTE v103[9];
  _OWORD v104[23];
  _OWORD v105[23];
  _BYTE v106[184];
  _BYTE v107[184];
  uint64_t v108;

  v108 = *MEMORY[0x1E0C80C00];
  outlined init with take of BNNSNDArrayDescriptor?(a8, (uint64_t)v101);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v101, (uint64_t)v106);
  outlined init with take of BNNSNDArrayDescriptor?(a7, (uint64_t)v100);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v100, (uint64_t)v107);
  outlined init with take of BNNSNDArrayDescriptor?(a7, (uint64_t)v105);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v105) != 1)
  {
    *(_OWORD *)((char *)&v105[19] + 8) = v105[8];
    *(_OWORD *)((char *)&v105[20] + 8) = v105[9];
    *(_OWORD *)((char *)&v105[21] + 8) = v105[10];
    *(_OWORD *)((char *)&v105[15] + 8) = v105[4];
    *(_OWORD *)((char *)&v105[16] + 8) = v105[5];
    *(_OWORD *)((char *)&v105[17] + 8) = v105[6];
    *(_OWORD *)((char *)&v105[18] + 8) = v105[7];
    *(_OWORD *)((char *)&v105[11] + 8) = v105[0];
    *(_OWORD *)((char *)&v105[12] + 8) = v105[1];
    *(_OWORD *)((char *)&v105[13] + 8) = v105[2];
    *(_OWORD *)((char *)&v105[14] + 8) = v105[3];
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v98);
    outlined init with take of BNNS.Shape((uint64_t)v98, (uint64_t)&layer_params);
    result = _s10Accelerate4BNNSO5ShapeOWOg((uint64_t)&layer_params);
    if ((_DWORD)result)
    {
      __break(1u);
LABEL_23:
      __break(1u);
      return result;
    }
  }
  outlined init with take of BNNSNDArrayDescriptor?(a8, (uint64_t)v104);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v104) != 1)
  {
    *(_OWORD *)((char *)&v104[19] + 8) = v104[8];
    *(_OWORD *)((char *)&v104[20] + 8) = v104[9];
    *(_OWORD *)((char *)&v104[21] + 8) = v104[10];
    *(_OWORD *)((char *)&v104[15] + 8) = v104[4];
    *(_OWORD *)((char *)&v104[16] + 8) = v104[5];
    *(_OWORD *)((char *)&v104[17] + 8) = v104[6];
    *(_OWORD *)((char *)&v104[18] + 8) = v104[7];
    *(_OWORD *)((char *)&v104[11] + 8) = v104[0];
    *(_OWORD *)((char *)&v104[12] + 8) = v104[1];
    *(_OWORD *)((char *)&v104[13] + 8) = v104[2];
    *(_OWORD *)((char *)&v104[14] + 8) = v104[3];
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v98);
    outlined init with take of BNNS.Shape((uint64_t)v98, (uint64_t)&layer_params);
    result = _s10Accelerate4BNNSO5ShapeOWOg((uint64_t)&layer_params);
    if ((_DWORD)result)
      goto LABEL_23;
  }
  v20 = a1 & 1;
  v21 = 1 << a5;
  if ((unint64_t)a5 >= 0x40)
    v21 = 0;
  if (a6 & 1 | (a5 < 0) | ((unint64_t)(a5 - 65) < 0xFFFFFFFFFFFFFF7FLL))
    v22 = 0;
  else
    v22 = v21;
  v71 = v22;
  outlined init with take of BNNSNDArrayDescriptor?(a7, (uint64_t)v103);
  v23 = 0;
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v103) == 1)
  {
    v24 = 0;
    v72 = 0;
    v73 = 0;
    v69 = 0;
    v70 = 0;
    v67 = 0;
    v68 = 0;
    v66 = 0;
    v63 = 0;
    v64 = 0;
    v61 = 0;
    v62 = 0;
    v25 = 0;
    v59 = 0;
    v60 = 0;
    v57 = 0;
    v58 = 0;
    v56 = 0;
    v55 = 0;
    v74 = 0;
    v26 = 0;
    v65 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v107, (uint64_t)v98);
    v74 = *(_QWORD *)v98;
    v24 = *(_QWORD *)&v98[8];
    v72 = *(_QWORD *)&v98[24];
    v73 = *(_QWORD *)&v98[16];
    v69 = *(_QWORD *)&v98[40];
    v70 = *(_QWORD *)&v98[32];
    v67 = *(_QWORD *)&v98[56];
    v68 = *(_QWORD *)&v98[48];
    v66 = *(_QWORD *)&v98[64];
    v63 = *(_QWORD *)&v98[80];
    v64 = *(_QWORD *)&v98[72];
    v25 = *(_QWORD *)&v98[96];
    v61 = *(_QWORD *)&v98[104];
    v62 = *(_QWORD *)&v98[88];
    v59 = *(_QWORD *)&v98[120];
    v60 = *(_QWORD *)&v98[112];
    v57 = *(void **)&v98[136];
    v58 = *(_QWORD *)&v98[128];
    v65 = *(_QWORD *)&v98[144];
    v56 = *(void **)&v98[152];
    v26 = *(_QWORD *)&v98[164];
    v54 = *(_DWORD *)&v98[172];
    v55 = *(_DWORD *)&v98[160];
  }
  outlined init with take of BNNSNDArrayDescriptor?(a8, (uint64_t)v102);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v102) == 1)
  {
    v28 = 0;
    v29 = 0;
    v30 = 0;
    v31 = 0uLL;
    v32 = 0uLL;
    v33 = 0uLL;
    v34 = 0uLL;
    v35 = 0uLL;
    v36 = 0uLL;
    v37 = 0uLL;
    v38 = 0uLL;
    v39 = 0;
    v40 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v106, (uint64_t)v98);
    v39 = *(_QWORD *)v98;
    v31 = *(_OWORD *)&v98[8];
    v32 = *(_OWORD *)&v98[24];
    v33 = *(_OWORD *)&v98[40];
    v34 = *(_OWORD *)&v98[56];
    v35 = *(_OWORD *)&v98[72];
    v36 = *(_OWORD *)&v98[88];
    v37 = *(_OWORD *)&v98[104];
    v38 = *(_OWORD *)&v98[120];
    v30 = *(void **)&v98[136];
    v40 = *(_QWORD *)&v98[144];
    v29 = *(void **)&v98[152];
    v23 = *(_QWORD *)&v98[164];
    v28 = *(_DWORD *)&v98[160];
    v27 = *(_DWORD *)&v98[172];
  }
  v41 = a3[6];
  *(_OWORD *)&v98[116] = a3[7];
  v42 = a3[9];
  *(_OWORD *)&v98[132] = a3[8];
  *(_OWORD *)&v98[148] = v42;
  *(_OWORD *)&v98[164] = a3[10];
  v43 = a3[2];
  *(_OWORD *)&v98[52] = a3[3];
  v44 = a3[5];
  *(_OWORD *)&v98[68] = a3[4];
  *(_OWORD *)&v98[84] = v44;
  *(_OWORD *)&v98[100] = v41;
  v45 = a3[1];
  *(_OWORD *)&v98[4] = *a3;
  *(_OWORD *)&v98[20] = v45;
  *(_OWORD *)&v98[36] = v43;
  layer_params.axis_mask = v71;
  layer_params.function = v20;
  *((_DWORD *)&layer_params.i_desc.data_bias + 1) = *(_DWORD *)&v98[176];
  *(_OWORD *)((char *)&layer_params.i_desc.stride[6] + 4) = *(_OWORD *)&v98[128];
  *(_OWORD *)((char *)&layer_params.i_desc.data + 4) = *(_OWORD *)&v98[144];
  *(_OWORD *)((char *)&layer_params.i_desc.table_data + 4) = *(_OWORD *)&v98[160];
  *(_OWORD *)((char *)&layer_params.i_desc.size[6] + 4) = *(_OWORD *)&v98[64];
  *(_OWORD *)((char *)layer_params.i_desc.stride + 4) = *(_OWORD *)&v98[80];
  *(_OWORD *)((char *)&layer_params.i_desc.stride[2] + 4) = *(_OWORD *)&v98[96];
  *(_OWORD *)((char *)&layer_params.i_desc.stride[4] + 4) = *(_OWORD *)&v98[112];
  *(_OWORD *)(&layer_params.function + 1) = *(_OWORD *)v98;
  *(_OWORD *)((char *)layer_params.i_desc.size + 4) = *(_OWORD *)&v98[16];
  *(_OWORD *)((char *)&layer_params.i_desc.size[2] + 4) = *(_OWORD *)&v98[32];
  *(_OWORD *)((char *)&layer_params.i_desc.size[4] + 4) = *(_OWORD *)&v98[48];
  layer_params.scale.size[0] = v24;
  layer_params.scale.size[1] = v73;
  layer_params.scale.size[2] = v72;
  layer_params.scale.size[3] = v70;
  layer_params.scale.size[4] = v69;
  layer_params.scale.size[5] = v68;
  layer_params.scale.size[6] = v67;
  layer_params.scale.size[7] = v66;
  layer_params.scale.stride[0] = v64;
  layer_params.scale.stride[1] = v63;
  layer_params.scale.stride[2] = v62;
  layer_params.scale.stride[3] = v25;
  layer_params.scale.stride[4] = v61;
  layer_params.scale.stride[5] = v60;
  layer_params.scale.stride[6] = v59;
  layer_params.scale.stride[7] = v58;
  layer_params.scale.data = v57;
  layer_params.scale.table_data = v56;
  layer_params.scale.table_data_type = v55;
  *((_DWORD *)&layer_params.scale.data_bias + 1) = v54;
  layer_params.bias.data = v30;
  layer_params.bias.table_data = v29;
  layer_params.bias.table_data_type = v28;
  *(_QWORD *)&layer_params.scale.data_scale = v26;
  *(_OWORD *)layer_params.bias.size = v31;
  *(_OWORD *)&layer_params.bias.size[2] = v32;
  *(_OWORD *)&layer_params.bias.size[4] = v33;
  *(_OWORD *)&layer_params.bias.size[6] = v34;
  *(_OWORD *)layer_params.bias.stride = v35;
  *(_OWORD *)&layer_params.bias.stride[2] = v36;
  *(_OWORD *)&layer_params.bias.stride[4] = v37;
  *(_OWORD *)&layer_params.bias.stride[6] = v38;
  *(_QWORD *)&layer_params.bias.data_scale = v23;
  *((_DWORD *)&layer_params.bias.data_bias + 1) = v27;
  v46 = a4[9];
  *(_OWORD *)&layer_params.o_desc.stride[7] = a4[8];
  *(_OWORD *)&layer_params.o_desc.data_type = v46;
  *(_OWORD *)&layer_params.o_desc.table_data_type = a4[10];
  v47 = a4[5];
  *(_OWORD *)&layer_params.o_desc.size[7] = a4[4];
  *(_OWORD *)&layer_params.o_desc.stride[1] = v47;
  v48 = a4[6];
  *(_OWORD *)&layer_params.o_desc.stride[5] = a4[7];
  *(_OWORD *)&layer_params.o_desc.stride[3] = v48;
  v49 = a4[1];
  *(_OWORD *)&layer_params.o_desc.flags = *a4;
  *(_OWORD *)&layer_params.o_desc.size[1] = v49;
  v50 = a4[2];
  *(_OWORD *)&layer_params.o_desc.size[5] = a4[3];
  *(_OWORD *)&layer_params.o_desc.size[3] = v50;
  *(_QWORD *)&layer_params.scale.flags = v74;
  *(_QWORD *)&layer_params.scale.data_type = v65;
  *(_QWORD *)&layer_params.bias.flags = v39;
  *(_QWORD *)&layer_params.bias.data_type = v40;
  if (a11 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v95);
    outlined init with take of BNNS.Shape((uint64_t)v95, (uint64_t)v96);
    outlined init with take of BNNS.Shape((uint64_t)v96, (uint64_t)v97);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v96, (uint64_t)v97);
    BNNS.Shape.stride.getter();
    v75 = specialized static BNNS.calculateBatchStride(size:stride:)(v87, v88, v89, v90, v91, v92, v93, v94, v87, v88, v89, v90, v91, v92, v93, v94);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v87);
    outlined init with take of BNNS.Shape((uint64_t)&v87, (uint64_t)v97);
    outlined init with take of BNNS.Shape((uint64_t)v97, (uint64_t)v86);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v97, (uint64_t)v86);
    BNNS.Shape.stride.getter();
    v51 = specialized static BNNS.calculateBatchStride(size:stride:)(v77, v78, v79, v80, v81, v82, v83, v84, v77, v78, v79, v80, v81, v82, v83, v84);
    p_filter_params = 0;
  }
  else
  {
    filter_params.flags = a9;
    filter_params.n_threads = a10;
    filter_params.alloc_memory = a11;
    filter_params.free_memory = a12;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v95);
    outlined init with take of BNNS.Shape((uint64_t)v95, (uint64_t)v96);
    outlined init with take of BNNS.Shape((uint64_t)v96, (uint64_t)v97);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v96, (uint64_t)v97);
    BNNS.Shape.stride.getter();
    v75 = specialized static BNNS.calculateBatchStride(size:stride:)(v87, v88, v89, v90, v91, v92, v93, v94, v87, v88, v89, v90, v91, v92, v93, v94);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v87);
    outlined init with take of BNNS.Shape((uint64_t)&v87, (uint64_t)v97);
    outlined init with take of BNNS.Shape((uint64_t)v97, (uint64_t)v86);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v97, (uint64_t)v86);
    BNNS.Shape.stride.getter();
    v51 = specialized static BNNS.calculateBatchStride(size:stride:)(v77, v78, v79, v80, v81, v82, v83, v84, v77, v78, v79, v80, v81, v82, v83, v84);
    p_filter_params = &filter_params;
  }
  result = BNNSDirectApplyQuantizer(&layer_params, p_filter_params, a2, v75, v51);
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v53 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.computeNorm(input:output:axes:)(_OWORD *a1, _OWORD *a2, uint64_t a3)
{
  __int128 v3;
  __int128 v4;
  __int128 v5;
  __int128 v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  uint32_t v13;
  uint64_t result;
  _BYTE *v15;
  BNNSNDArrayDescriptor v16;
  BNNSNDArrayDescriptor src;
  uint64_t v18;

  v18 = *MEMORY[0x1E0C80C00];
  v3 = a1[9];
  *(_OWORD *)&src.stride[7] = a1[8];
  *(_OWORD *)&src.data_type = v3;
  *(_OWORD *)&src.table_data_type = a1[10];
  v4 = a1[5];
  *(_OWORD *)&src.size[7] = a1[4];
  *(_OWORD *)&src.stride[1] = v4;
  v5 = a1[7];
  *(_OWORD *)&src.stride[3] = a1[6];
  *(_OWORD *)&src.stride[5] = v5;
  v6 = a1[1];
  *(_OWORD *)&src.flags = *a1;
  *(_OWORD *)&src.size[1] = v6;
  v7 = a1[3];
  *(_OWORD *)&src.size[3] = a1[2];
  *(_OWORD *)&src.size[5] = v7;
  v8 = a2[9];
  *(_OWORD *)&v16.stride[7] = a2[8];
  *(_OWORD *)&v16.data_type = v8;
  *(_OWORD *)&v16.table_data_type = a2[10];
  v9 = a2[5];
  *(_OWORD *)&v16.size[7] = a2[4];
  *(_OWORD *)&v16.stride[1] = v9;
  v10 = a2[7];
  *(_OWORD *)&v16.stride[3] = a2[6];
  *(_OWORD *)&v16.stride[5] = v10;
  v11 = a2[1];
  *(_OWORD *)&v16.flags = *a2;
  *(_OWORD *)&v16.size[1] = v11;
  v12 = a2[3];
  *(_OWORD *)&v16.size[3] = a2[2];
  *(_OWORD *)&v16.size[5] = v12;
  v13 = specialized static BNNS.computeAxisFlags(_:)(a3);
  result = BNNSComputeNorm(&v16, &src, BNNSL2Norm, v13);
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v15 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.computeNormBackward(input:output:axes:outputGradient:generatingInputGradient:)(uint64_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, _OWORD *a5)
{
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  const void *v17;
  const void *v18;
  uint32_t v19;
  uint64_t result;
  _BYTE *v21;
  BNNSNDArrayDescriptor v22;
  BNNSNDArrayDescriptor in_delta;
  _BYTE v24[8];
  _BYTE v25[8];
  const void *v26;
  _QWORD v27[2];

  v27[1] = *MEMORY[0x1E0C80C00];
  v7 = a5[9];
  *(_OWORD *)&in_delta.stride[7] = a5[8];
  *(_OWORD *)&in_delta.data_type = v7;
  *(_OWORD *)&in_delta.table_data_type = a5[10];
  v8 = a5[5];
  *(_OWORD *)&in_delta.size[7] = a5[4];
  *(_OWORD *)&in_delta.stride[1] = v8;
  v9 = a5[7];
  *(_OWORD *)&in_delta.stride[3] = a5[6];
  *(_OWORD *)&in_delta.stride[5] = v9;
  v10 = a5[1];
  *(_OWORD *)&in_delta.flags = *a5;
  *(_OWORD *)&in_delta.size[1] = v10;
  v11 = a5[3];
  *(_OWORD *)&in_delta.size[3] = a5[2];
  *(_OWORD *)&in_delta.size[5] = v11;
  v12 = a4[9];
  *(_OWORD *)&v22.stride[7] = a4[8];
  *(_OWORD *)&v22.data_type = v12;
  *(_OWORD *)&v22.table_data_type = a4[10];
  v13 = a4[5];
  *(_OWORD *)&v22.size[7] = a4[4];
  *(_OWORD *)&v22.stride[1] = v13;
  v14 = a4[7];
  *(_OWORD *)&v22.stride[3] = a4[6];
  *(_OWORD *)&v22.stride[5] = v14;
  v15 = a4[1];
  *(_OWORD *)&v22.flags = *a4;
  *(_OWORD *)&v22.size[1] = v15;
  v16 = a4[3];
  *(_OWORD *)&v22.size[3] = a4[2];
  *(_OWORD *)&v22.size[5] = v16;
  outlined init with take of UnsafeMutableRawPointer?(a1 + 136, (uint64_t)v25);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v25, (uint64_t)&v26);
  v17 = v26;
  if (!v26
    || (outlined init with take of UnsafeMutableRawPointer?(a2 + 136, (uint64_t)v24),
        outlined init with take of UnsafeMutableRawPointer?((uint64_t)v24, (uint64_t)v27),
        (v18 = (const void *)v27[0]) == 0)
    || (v19 = specialized static BNNS.computeAxisFlags(_:)(a3),
        result = BNNSComputeNormBackward(v17, &in_delta, v18, &v22, BNNSL2Norm, v19),
        (_DWORD)result))
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v21 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t BLAS.ThreadingModel.init(rawValue:)@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = result;
  return result;
}

void static BLAS.ThreadingModel.multiThreaded.getter(_DWORD *a1@<X8>)
{
  *a1 = 0;
}

void static BLAS.ThreadingModel.singleThreaded.getter(_DWORD *a1@<X8>)
{
  *a1 = 1;
}

uint64_t BLAS.ThreadingModel.rawValue.getter()
{
  unsigned int *v0;

  return *v0;
}

uint64_t BLAS.ThreadingModel.rawValue.setter(uint64_t result)
{
  _DWORD *v1;

  *v1 = result;
  return result;
}

uint64_t (*BLAS.ThreadingModel.rawValue.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t static BLAS.threadingModel.getter@<X0>(_DWORD *a1@<X8>)
{
  uint64_t result;

  result = BLASGetThreading();
  *a1 = result;
  return result;
}

uint64_t static BLAS.threadingModel.setter()
{
  return BLASSetThreading();
}

uint64_t (*static BLAS.threadingModel.modify(_DWORD *a1))()
{
  *a1 = BLASGetThreading();
  return static BLAS.threadingModel.modify;
}

uint64_t static BLAS.threadingModel.modify()
{
  return BLASSetThreading();
}

ValueMetadata *type metadata accessor for BLAS()
{
  return &type metadata for BLAS;
}

ValueMetadata *type metadata accessor for BLAS.ThreadingModel()
{
  return &type metadata for BLAS.ThreadingModel;
}

uint64_t vImageConverterRef.sourceBuffers(colorSpace:)(void *a1)
{
  return vImageConverterRef.sourceBuffers(colorSpace:)(a1, MEMORY[0x1E0C8D148], MEMORY[0x1E0C8D140]);
}

uint64_t vImageConverterRef.sourceBufferCount.getter()
{
  return vImageConverterRef.sourceBufferCount.getter(MEMORY[0x1E0C8D140]);
}

uint64_t vImageConverterRef.destinationBuffers(colorSpace:)(void *a1)
{
  return vImageConverterRef.sourceBuffers(colorSpace:)(a1, MEMORY[0x1E0C8D130], MEMORY[0x1E0C8D138]);
}

uint64_t vImageConverterRef.sourceBuffers(colorSpace:)(void *a1, uint64_t (*a2)(uint64_t), uint64_t (*a3)(uint64_t))
{
  uint64_t v3;
  uint64_t v5;
  const void *v7;
  uint64_t result;
  _QWORD *v9;
  CGColorSpace *v10;
  uint64_t v11;

  v5 = v3;
  v7 = (const void *)a2(v3);
  result = a3(v5);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    v9 = specialized _copyCollectionToContiguousArray<A>(_:)(v7, result);
    v10 = a1;
    v11 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSays6UInt32VG_10Accelerate6vImageO10BufferTypeOSgs5NeverOTg507_sSo18vf15ConverterRefa10e41E13sourceBuffers10colorSpaceSayAC01vA0O10gh26OSgGSo07CGColorhC0a_tFAJs6D6VXEfU_So07CGColorP3RefaTf1cn_nTf4ng_nTm((uint64_t)v9, v10);
    swift_release();

    return v11;
  }
  return result;
}

uint64_t vImageConverterRef.destinationBufferCount.getter()
{
  return vImageConverterRef.sourceBufferCount.getter(MEMORY[0x1E0C8D138]);
}

uint64_t vImageConverterRef.sourceBufferCount.getter(uint64_t (*a1)(uint64_t))
{
  uint64_t v1;
  uint64_t result;

  result = a1(v1);
  if (result < 0)
    __break(1u);
  return result;
}

uint64_t vImageConverterRef.mustOperateOutOfPlace(source:destination:flags:)(void *a1, vImagePixelCount a2, vImagePixelCount a3, size_t a4, void *a5, vImagePixelCount a6, vImagePixelCount a7, size_t a8, vImage_Flags *a9)
{
  vImageConverter *v9;
  vImage_Flags v10;
  vImage_Error MustOperateOutOfPlace;
  uint64_t v12;
  char v13;
  char *v14;
  char *v15;
  char data;
  vImage_Buffer dests;
  vImage_Buffer srcs;
  uint64_t v20;

  v20 = *MEMORY[0x1E0C80C00];
  v10 = *a9;
  srcs.data = a1;
  srcs.height = a2;
  srcs.width = a3;
  srcs.rowBytes = a4;
  dests.data = a5;
  dests.height = a6;
  dests.width = a7;
  dests.rowBytes = a8;
  MustOperateOutOfPlace = vImageConverter_MustOperateOutOfPlace(v9, &srcs, &dests, v10);
  if (MustOperateOutOfPlace == -21780)
  {
    v13 = 1;
  }
  else
  {
    v12 = MustOperateOutOfPlace;
    if (MustOperateOutOfPlace)
    {
      lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
      swift_allocError();
      v15 = v14;
      vImage.Error.init(rawValue:)(v12, (char *)&srcs);
      data = (char)srcs.data;
      if (LOBYTE(srcs.data) == 20)
        data = 11;
      *v15 = data;
      swift_willThrow();
    }
    else
    {
      v13 = 0;
    }
  }
  return v13 & 1;
}

vImageConverterRef static vImageConverterRef.make(sourceFormat:destinationFormat:flags:)(uint64_t a1, uint64_t a2, vImage_Flags *a3)
{
  vImage_Flags v3;
  __int128 v4;
  __int128 v5;
  vImageConverterRef v6;
  vImageConverterRef v7;
  vImage_Error v8;
  char *v9;
  char v10;
  vImage_Error error;
  vImage_CGImageFormat srcFormat;
  vImage_CGImageFormat destFormat;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v3 = *a3;
  error = 0;
  v4 = *(_OWORD *)(a2 + 16);
  *(_OWORD *)&destFormat.bitsPerComponent = *(_OWORD *)a2;
  *(_OWORD *)&destFormat.bitmapInfo = v4;
  *(_QWORD *)&destFormat.renderingIntent = *(_QWORD *)(a2 + 32);
  v5 = *(_OWORD *)(a1 + 16);
  *(_OWORD *)&srcFormat.bitsPerComponent = *(_OWORD *)a1;
  *(_OWORD *)&srcFormat.bitmapInfo = v5;
  *(_QWORD *)&srcFormat.renderingIntent = *(_QWORD *)(a1 + 32);
  v6 = vImageConverter_CreateWithCGImageFormat(&srcFormat, &destFormat, 0, v3, &error);
  v7 = v6;
  v8 = error;
  if (error)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    if ((unint64_t)(v8 + 21784) >= 0x13)
      v10 = 11;
    else
      v10 = -5 - v8;
  }
  else
  {
    if (v6)
      return v7;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v10 = 11;
  }
  *v9 = v10;
  swift_willThrow();
  return v7;
}

vImageConverterRef static vImageConverterRef.make(sourceFormat:destinationFormat:flags:)(uint64_t a1, vImageCVImageFormat *a2, vImage_Flags *a3)
{
  vImage_Flags v3;
  __int128 v4;
  vImageConverterRef v5;
  vImageConverterRef v6;
  vImage_Error v7;
  char *v8;
  char v9;
  vImage_Error error;
  vImage_CGImageFormat srcFormat;
  uint64_t v13;

  v13 = *MEMORY[0x1E0C80C00];
  v3 = *a3;
  error = 0;
  v4 = *(_OWORD *)(a1 + 16);
  *(_OWORD *)&srcFormat.bitsPerComponent = *(_OWORD *)a1;
  *(_OWORD *)&srcFormat.bitmapInfo = v4;
  *(_QWORD *)&srcFormat.renderingIntent = *(_QWORD *)(a1 + 32);
  v5 = vImageConverter_CreateForCGToCVImageFormat(&srcFormat, a2, 0, v3, &error);
  v6 = v5;
  v7 = error;
  if (error)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    if ((unint64_t)(v7 + 21784) >= 0x13)
      v9 = 11;
    else
      v9 = -5 - v7;
  }
  else
  {
    if (v5)
      return v6;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v9 = 11;
  }
  *v8 = v9;
  swift_willThrow();
  return v6;
}

vImageConverterRef static vImageConverterRef.make(sourceFormat:destinationFormat:flags:)(vImageCVImageFormat *a1, uint64_t a2, vImage_Flags *a3)
{
  vImage_Flags v3;
  __int128 v4;
  vImageConverterRef v5;
  vImageConverterRef v6;
  vImage_Error v7;
  char *v8;
  char v9;
  vImage_Error error;
  vImage_CGImageFormat destFormat;
  uint64_t v13;

  v13 = *MEMORY[0x1E0C80C00];
  v3 = *a3;
  error = -21776;
  v4 = *(_OWORD *)(a2 + 16);
  *(_OWORD *)&destFormat.bitsPerComponent = *(_OWORD *)a2;
  *(_OWORD *)&destFormat.bitmapInfo = v4;
  *(_QWORD *)&destFormat.renderingIntent = *(_QWORD *)(a2 + 32);
  v5 = vImageConverter_CreateForCVToCGImageFormat(a1, &destFormat, 0, v3, &error);
  v6 = v5;
  v7 = error;
  if (error)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    if ((unint64_t)(v7 + 21784) >= 0x13)
      v9 = 11;
    else
      v9 = -5 - v7;
  }
  else
  {
    if (v5)
      return v6;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v9 = 11;
  }
  *v8 = v9;
  swift_willThrow();
  return v6;
}

vImage_Error vImageConverterRef.convert(source:destination:flags:)(void *a1, vImagePixelCount a2, vImagePixelCount a3, size_t a4, vImage_Buffer *dests, vImage_Flags *a6)
{
  vImageConverter *v6;
  vImage_Flags v8;
  vImage_Error result;
  uint64_t v10;
  char *v11;
  char *v12;
  char data;
  vImage_Buffer srcs;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v8 = *a6;
  srcs.data = a1;
  srcs.height = a2;
  srcs.width = a3;
  srcs.rowBytes = a4;
  result = vImageConvert_AnyToAny(v6, &srcs, dests, 0, v8);
  if (result)
  {
    v10 = result;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v12 = v11;
    vImage.Error.init(rawValue:)(v10, (char *)&srcs);
    data = (char)srcs.data;
    if (LOBYTE(srcs.data) == 20)
      data = 11;
    *v12 = data;
    return swift_willThrow();
  }
  return result;
}

uint64_t _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSays6UInt32VG_10Accelerate6vImageO10BufferTypeOSgs5NeverOTg507_sSo18vf15ConverterRefa10e41E13sourceBuffers10colorSpaceSayAC01vA0O10gh26OSgGSo07CGColorhC0a_tFAJs6D6VXEfU_So07CGColorP3RefaTf1cn_nTf4ng_nTm(uint64_t a1, CGColorSpace *a2)
{
  int64_t v2;
  uint64_t v3;
  unsigned int *v6;
  unsigned int v7;
  CGColorSpaceModel Model;
  char v9;
  unint64_t v10;
  unint64_t v11;
  char v13;
  uint64_t v14;

  v2 = *(_QWORD *)(a1 + 16);
  v3 = MEMORY[0x1E0DEE9D8];
  if (v2)
  {
    v14 = MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v2, 0);
    v3 = v14;
    v6 = (unsigned int *)(a1 + 32);
    do
    {
      v7 = *v6++;
      Model = CGColorSpaceGetModel(a2);
      vImage.BufferType.init(bufferTypeCode:model:)(v7, Model, &v13);
      v9 = v13;
      v14 = v3;
      v11 = *(_QWORD *)(v3 + 16);
      v10 = *(_QWORD *)(v3 + 24);
      if (v11 >= v10 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v10 > 1), v11 + 1, 1);
        v3 = v14;
      }
      *(_QWORD *)(v3 + 16) = v11 + 1;
      *(_BYTE *)(v3 + v11 + 32) = v9;
      --v2;
    }
    while (v2);
  }
  return v3;
}

uint64_t vDSP.Biquad.init(coefficients:channelCount:sectionCount:ofType:)@<X0>(uint64_t result@<X0>, unint64_t a2@<X1>, unint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t *a7@<X8>)
{
  uint64_t v10;

  if (is_mul_ok(a2, 5uLL))
  {
    if (is_mul_ok(5 * a2, a3))
    {
      v10 = result;
      if (*(_QWORD *)(result + 16) == 5 * a2 * a3)
      {
        type metadata accessor for vDSP.BiquadRef(0, a5, a6, a4);
        swift_allocObject();
        result = vDSP.BiquadRef.init(coefficients:channelCount:sectionCount:ofType:)(v10, a2, a3);
        if (result)
        {
LABEL_8:
          *a7 = result;
          return result;
        }
      }
      else
      {
        swift_bridgeObjectRelease();
      }
      result = 0;
      goto LABEL_8;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t type metadata accessor for vDSP.BiquadRef(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return __swift_instantiateGenericMetadata(a1, a2, a3, a4, (uint64_t)&nominal type descriptor for vDSP.BiquadRef);
}

uint64_t vDSP.BiquadRef.__allocating_init(coefficients:channelCount:sectionCount:ofType:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  swift_allocObject();
  return vDSP.BiquadRef.init(coefficients:channelCount:sectionCount:ofType:)(a1, a2, a3);
}

uint64_t vDSP.Biquad.apply<A>(input:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  (*(void (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  return Array.init(unsafeUninitializedCapacity:initializingWith:)();
}

uint64_t closure #1 in vDSP.Biquad.apply<A>(input:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t result;

  v13 = type metadata accessor for vDSP.Biquad(0, a5, a7, a4);
  v14 = type metadata accessor for UnsafeMutableBufferPointer();
  v15 = MEMORY[0x1D1794D08](&protocol conformance descriptor for UnsafeMutableBufferPointer<A>, v14);
  vDSP.Biquad.apply<A, B>(input:output:)(a4, a1, v13, a6, v14, a8, v15);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
  *a2 = result;
  return result;
}

uint64_t partial apply for closure #1 in vDSP.Biquad.apply<A>(input:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in vDSP.Biquad.apply<A>(input:)(a1, a2, v2[6], v2[7], v2[2], v2[3], v2[4], v2[5]);
}

uint64_t vDSP.Biquad.apply<A, B>(input:output:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t *v7;
  uint64_t *v8;
  char v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t result;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;

  v8 = v7;
  type metadata accessor for vDSP.BiquadRef(0, *(_QWORD *)(a3 + 16), *(_QWORD *)(a3 + 24), a4);
  v15 = isKnownUniquelyReferenced<A>(_:)();
  v16 = *v7;
  if ((v15 & 1) != 0)
    return vDSP.BiquadRef.apply<A, B>(input:output:)(a1, a2, a4, a5, a6, a7);
  v22 = a5;
  v23 = a6;
  v17 = *(_QWORD *)(v16 + 24);
  v18 = *(_QWORD *)(v16 + 32);
  swift_allocObject();
  v19 = swift_bridgeObjectRetain();
  result = vDSP.BiquadRef.init(coefficients:channelCount:sectionCount:ofType:)(v19, v17, v18);
  if (result)
  {
    v21 = result;
    swift_release();
    *v8 = v21;
    a6 = v23;
    a5 = v22;
    return vDSP.BiquadRef.apply<A, B>(input:output:)(a1, a2, a4, a5, a6, a7);
  }
  __break(1u);
  return result;
}

uint64_t vDSP.BiquadRef.init(coefficients:channelCount:sectionCount:ofType:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  _QWORD *v3;
  _QWORD *v4;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  uint64_t v11;
  uint64_t AssociatedTypeWitness;
  uint64_t result;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t AssociatedConformanceWitness;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;

  v4 = v3;
  v8 = *(_QWORD *)(*v3 + 80);
  v22 = *(_QWORD *)(v8 - 8);
  MEMORY[0x1E0C80A78](a1);
  v10 = (char *)&v21 - ((v9 + 15) & 0xFFFFFFFFFFFFFFF0);
  v24 = *(_QWORD *)(v11 + 88);
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  MEMORY[0x1E0C80A78](AssociatedTypeWitness);
  v3[2] = a1;
  v3[3] = a2;
  v23 = a2;
  v3[4] = a3;
  swift_getAssociatedConformanceWitness();
  swift_bridgeObjectRetain();
  dispatch thunk of _ExpressibleByBuiltinFloatLiteral.init(_builtinFloatLiteral:)();
  result = dispatch thunk of ExpressibleByFloatLiteral.init(floatLiteral:)();
  if (a3 < 0)
  {
    __break(1u);
    goto LABEL_9;
  }
  v14 = 2 * a3;
  if (2 * a3 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (__OFADD__(v14, 2))
  {
LABEL_10:
    __break(1u);
    return result;
  }
  v15 = specialized Array.init(repeating:count:)((uint64_t)v10, v14 + 2, v8);
  (*(void (**)(char *, uint64_t))(v22 + 8))(v10, v8);
  v4[5] = v15;
  v16 = v24;
  v17 = swift_getAssociatedTypeWitness();
  AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  v19 = (*(uint64_t (**)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness + 16))(v23, a1, a3, v17, AssociatedConformanceWitness);
  swift_bridgeObjectRelease();
  if (v19)
  {
    v4[6] = v19;
  }
  else
  {
    swift_bridgeObjectRelease();
    swift_bridgeObjectRelease();
    type metadata accessor for vDSP.BiquadRef(0, v8, v16, v20);
    swift_deallocPartialClassInstance();
    return 0;
  }
  return (uint64_t)v4;
}

uint64_t vDSP.BiquadRef.apply<A, B>(input:output:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  _QWORD *v6;
  _QWORD *v7;
  uint64_t v14;
  uint64_t v15;
  uint64_t result;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  _BYTE v22[16];
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;

  v7 = v6;
  v14 = *v6;
  v15 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  result = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a6 + 8) + 16))(a4);
  if (result >= v15)
    v17 = v15;
  else
    v17 = result;
  if (v17 < 0)
  {
    __break(1u);
  }
  else
  {
    v18 = v7[3];
    v19 = v7[6];
    if (v18 == 1)
    {
      v21 = v7[4];
      swift_beginAccess();
      static vDSP.BiquadFunctions.applyBiquadSingle<A, B, C>(source:destination:delays:setup:sectionCount:count:)(a1, a2, v7 + 5, v19, v21, v17, a3, a4, *(_QWORD *)(v14 + 80), a5, a6);
      return swift_endAccess();
    }
    else
    {
      v20 = *(_QWORD *)(v14 + 88);
      v23 = a3;
      v24 = a4;
      v25 = a5;
      v26 = a6;
      v27 = v20;
      v28 = v20;
      v29 = a2;
      v30 = v17;
      v31 = v18;
      v32 = v19;
      return (*(uint64_t (**)(uint64_t (*)(), _BYTE *, uint64_t, uint64_t, uint64_t))(a5 + 24))(partial apply for closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:), v22, MEMORY[0x1E0DEE9C0] + 8, a3, a5);
    }
  }
  return result;
}

uint64_t static vDSP.BiquadFunctions.applyBiquadSingle<A, B, C>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1, uint64_t a2, uint64_t *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t AssociatedTypeWitness;
  uint64_t AssociatedConformanceWitness;
  uint64_t v21;
  uint64_t v22;

  type metadata accessor for Array();
  Array.reserveCapacity(_:)(0);
  v14 = *a3;
  swift_bridgeObjectRetain();
  if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && (v14 < 0 || (v14 & 0x4000000000000000) != 0))
  {
    if (MEMORY[0x1D1794120](v14, a9))
    {
      v21 = type metadata accessor for _ArrayBuffer();
      MEMORY[0x1D1794D08](MEMORY[0x1E0DEC400], v21);
      v22 = Array.init<A>(_:)();
      destructiveProjectEnumData for BNNS.ActivationFunction(v22);
      swift_unknownObjectRetain();
      v16 = _ContiguousArrayBuffer.firstElementAddress.getter();
      swift_release();
      goto LABEL_12;
    }
    swift_bridgeObjectRelease();
    v16 = 0;
  }
  else
  {
    swift_bridgeObjectRelease();
    if ((_swift_isClassOrObjCExistentialType() & 1) != 0)
    {
      v15 = *(unsigned __int8 *)(*(_QWORD *)(a9 - 8) + 80);
      v16 = (v14 & 0xFFFFFFFFFFFFFF8) + ((v15 + 32) & ~v15);
    }
    else
    {
      v17 = *(unsigned __int8 *)(*(_QWORD *)(a9 - 8) + 80);
      v16 = v14 + ((v17 + 32) & ~v17);
    }
  }
  if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && (v14 < 0 || (v14 & 0x4000000000000000) != 0))
  {
    specialized _ArrayBuffer._nonNative.getter(v14);
    swift_unknownObjectRetain();
    if (v16)
      goto LABEL_12;
    goto LABEL_11;
  }
  _swift_isClassOrObjCExistentialType();
  swift_bridgeObjectRetain();
  if (!v16)
LABEL_11:
    v16 = ~*(_DWORD *)(*(_QWORD *)(a9 - 8) + 80) | 0xFFFFFFFFFFFFFF00;
LABEL_12:
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  (*(void (**)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness + 24))(a1, a2, v16, a4, a5, a6, a7, a8, a10, a11, AssociatedTypeWitness, AssociatedConformanceWitness);
  return swift_unknownObjectRelease();
}

uint64_t static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, __int128 a9, uint64_t a10)
{
  _QWORD v11[5];
  __int128 v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;

  v11[2] = a6;
  v11[3] = a7;
  v11[4] = a8;
  v12 = a9;
  v13 = a10;
  v14 = a2;
  v15 = a5;
  v16 = a4;
  v17 = a3;
  return (*(uint64_t (**)(uint64_t (*)(), _QWORD *, uint64_t, uint64_t, uint64_t))(a8 + 24))(partial apply for closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:), v11, MEMORY[0x1E0DEE9C0] + 8, a6, a8);
}

uint64_t vDSP.BiquadRef.deinit(uint64_t a1)
{
  uint64_t v1;

  static vDSP.BiquadFunctions.destroySetup<A>(ofType:channelCount:biquadSetup:)(a1, *(_QWORD *)(v1 + 24), *(_QWORD *)(v1 + 48));
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  return v1;
}

uint64_t static vDSP.BiquadFunctions.destroySetup<A>(ofType:channelCount:biquadSetup:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t AssociatedTypeWitness;
  uint64_t AssociatedConformanceWitness;

  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  return (*(uint64_t (**)(uint64_t, uint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness + 40))(a2, a3, AssociatedTypeWitness, AssociatedConformanceWitness);
}

uint64_t vDSP.BiquadRef.__deallocating_deinit(uint64_t a1)
{
  vDSP.BiquadRef.deinit(a1);
  return swift_deallocClassInstance();
}

uint64_t closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)()
{
  uint64_t result;
  uint64_t v1;

  swift_getAssociatedTypeWitness();
  result = UnsafeBufferPointer.baseAddress.getter();
  if (result)
  {
    MEMORY[0x1E0C80A78](result);
    return (*(uint64_t (**)(_QWORD))(v1 + 16))(partial apply for closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:));
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)(uint64_t a1, unint64_t a2, unint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  uint64_t result;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  char *v25;
  unint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  unint64_t v32;
  char *v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t AssociatedTypeWitness;
  uint64_t v37;
  uint64_t v38;
  uint64_t AssociatedConformanceWitness;
  _QWORD v40[10];
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  char *v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t *v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  unint64_t v52;
  uint64_t v53;
  unint64_t v54;
  uint64_t v55;

  v50 = a4;
  swift_getAssociatedTypeWitness();
  result = UnsafeBufferPointer.baseAddress.getter();
  if (result)
  {
    if (a3)
    {
      v47 = a11;
      v48 = &v41;
      v45 = a10;
      v53 = 0;
      v54 = a3;
      v52 = a2 / a3;
      v18 = MEMORY[0x1E0C80A78](result);
      v49 = a5;
      v19 = a6;
      v40[2] = a6;
      v40[3] = a7;
      v40[4] = a8;
      v40[5] = a9;
      v46 = a9;
      v40[6] = v20;
      v40[7] = v21;
      v40[8] = v50;
      v40[9] = v22;
      v23 = a8;
      v50 = v18;
      v24 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Range<UInt>);
      v42 = v24;
      swift_getAssociatedTypeWitness();
      v43 = a7;
      v25 = (char *)type metadata accessor for UnsafePointer();
      v44 = v25;
      v26 = lazy protocol witness table accessor for type Range<UInt> and conformance <> Range<A>();
      v27 = v51;
      v55 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:), (uint64_t)v40, v24, v25, MEMORY[0x1E0DEDCE8], v26, MEMORY[0x1E0DEDD18], v28);
      v53 = 0;
      v54 = a3;
      MEMORY[0x1E0C80A78](v55);
      v29 = v43;
      v40[-8] = v19;
      v40[-7] = v29;
      v30 = v46;
      v40[-6] = v23;
      v40[-5] = v30;
      v31 = v47;
      v40[-4] = v45;
      v40[-3] = v31;
      v32 = v52;
      v40[-2] = v50;
      v40[-1] = v32;
      v33 = (char *)type metadata accessor for UnsafeMutablePointer();
      v35 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #2 in closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:), (uint64_t)&v40[-10], v42, v33, MEMORY[0x1E0DEDCE8], v26, MEMORY[0x1E0DEDD18], v34);
      v51 = v27;
      v53 = v35;
      AssociatedTypeWitness = swift_getAssociatedTypeWitness();
      type metadata accessor for Array();
      Array.reserveCapacity(_:)(0);
      v37 = v55;
      type metadata accessor for Array();
      swift_bridgeObjectRetain();
      Array.reserveCapacity(_:)(0);
      v38 = v53;
      AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
      (*(void (**)(uint64_t, uint64_t, uint64_t, unint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness
                                                                                            + 32))(v49, v37 + 32, v38 + 32, v52, AssociatedTypeWitness, AssociatedConformanceWitness);
      swift_bridgeObjectRelease();
      return swift_bridgeObjectRelease_n();
    }
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t closure #2 in closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)@<X0>(unint64_t *a1@<X0>, unint64_t a2@<X2>, _QWORD *a3@<X8>)
{
  unint64_t v5;
  uint64_t AssociatedTypeWitness;
  uint64_t result;
  uint64_t v8;

  v5 = *a1;
  swift_getAssociatedTypeWitness();
  type metadata accessor for UnsafeMutablePointer();
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  type metadata accessor for UnsafeMutablePointer();
  result = swift_dynamicCast();
  if (is_mul_ok(v5, a2))
  {
    if (((v5 * a2) & 0x8000000000000000) == 0)
    {
      *a3 = v8 + *(_QWORD *)(*(_QWORD *)(AssociatedTypeWitness - 8) + 72) * v5 * a2;
      return result;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t static vDSP.VectorizableFloat.makeBiquadSetup(channelCount:coefficients:sectionCount:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.VectorizableFloat.makeBiquadSetup(channelCount:coefficients:sectionCount:)(a1, a2, a3, MEMORY[0x1E0C8BFD0], MEMORY[0x1E0C8C008]);
}

uint64_t static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return specialized static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(a1, a2, a3, a4, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:));
}

void static vDSP.VectorizableFloat.applyMulti(setup:pInputs:pOutputs:count:)(vDSP_biquadm_SetupStruct *a1, const float **a2, float **__Y, vDSP_Length __N)
{
  vDSP_biquadm(a1, a2, 1, __Y, 1, __N);
}

uint64_t static vDSP.VectorizableFloat.destroySetup(channelCount:biquadSetup:)(uint64_t a1, uint64_t a2)
{
  return static vDSP.VectorizableFloat.destroySetup(channelCount:biquadSetup:)(a1, a2, MEMORY[0x1E0C8BFE0], MEMORY[0x1E0C8C018]);
}

uint64_t protocol witness for static vDSP_BiquadFunctions.makeBiquadSetup(channelCount:coefficients:sectionCount:) in conformance vDSP.VectorizableFloat(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return protocol witness for static vDSP_BiquadFunctions.makeBiquadSetup(channelCount:coefficients:sectionCount:) in conformance vDSP.VectorizableFloat(a1, a2, a3, a4, a5, MEMORY[0x1E0C8BFD0], MEMORY[0x1E0C8C008]);
}

uint64_t protocol witness for static vDSP_BiquadFunctions.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:) in conformance vDSP.VectorizableFloat(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return specialized static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(a1, a2, a3, a4, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:));
}

void protocol witness for static vDSP_BiquadFunctions.applyMulti(setup:pInputs:pOutputs:count:) in conformance vDSP.VectorizableFloat(vDSP_biquadm_SetupStruct *a1, const float **a2, float **__Y, vDSP_Length __N)
{
  vDSP_biquadm(a1, a2, 1, __Y, 1, __N);
}

uint64_t protocol witness for static vDSP_BiquadFunctions.destroySetup(channelCount:biquadSetup:) in conformance vDSP.VectorizableFloat(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return protocol witness for static vDSP_BiquadFunctions.destroySetup(channelCount:biquadSetup:) in conformance vDSP.VectorizableFloat(a1, a2, a3, a4, MEMORY[0x1E0C8BFE0], MEMORY[0x1E0C8C018]);
}

uint64_t static vDSP.VectorizableDouble.makeBiquadSetup(channelCount:coefficients:sectionCount:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.VectorizableFloat.makeBiquadSetup(channelCount:coefficients:sectionCount:)(a1, a2, a3, MEMORY[0x1E0C8BFD8], MEMORY[0x1E0C8C010]);
}

uint64_t static vDSP.VectorizableFloat.makeBiquadSetup(channelCount:coefficients:sectionCount:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(uint64_t, uint64_t), uint64_t (*a5)(uint64_t, uint64_t, uint64_t))
{
  uint64_t v6;

  v6 = a2 + 32;
  if (a1 == 1)
    return a4(v6, a3);
  else
    return a5(v6, a3, a1);
}

uint64_t static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return specialized static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(a1, a2, a3, a4, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:));
}

void static vDSP.VectorizableDouble.applyMulti(setup:pInputs:pOutputs:count:)(vDSP_biquadm_SetupStructD *a1, const double **a2, double **__Y, vDSP_Length __N)
{
  vDSP_biquadmD(a1, a2, 1, __Y, 1, __N);
}

uint64_t static vDSP.VectorizableDouble.destroySetup(channelCount:biquadSetup:)(uint64_t a1, uint64_t a2)
{
  return static vDSP.VectorizableFloat.destroySetup(channelCount:biquadSetup:)(a1, a2, MEMORY[0x1E0C8BFE8], MEMORY[0x1E0C8C020]);
}

uint64_t static vDSP.VectorizableFloat.destroySetup(channelCount:biquadSetup:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t), uint64_t (*a4)(uint64_t))
{
  if (a1 != 1)
    a3 = a4;
  return a3(a2);
}

uint64_t protocol witness for static vDSP_BiquadFunctions.makeBiquadSetup(channelCount:coefficients:sectionCount:) in conformance vDSP.VectorizableDouble(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return protocol witness for static vDSP_BiquadFunctions.makeBiquadSetup(channelCount:coefficients:sectionCount:) in conformance vDSP.VectorizableFloat(a1, a2, a3, a4, a5, MEMORY[0x1E0C8BFD8], MEMORY[0x1E0C8C010]);
}

uint64_t protocol witness for static vDSP_BiquadFunctions.makeBiquadSetup(channelCount:coefficients:sectionCount:) in conformance vDSP.VectorizableFloat(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t), uint64_t (*a7)(uint64_t, uint64_t, uint64_t))
{
  uint64_t v8;

  v8 = a2 + 32;
  if (a1 == 1)
    return a6(v8, a3);
  else
    return a7(v8, a3, a1);
}

uint64_t protocol witness for static vDSP_BiquadFunctions.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:) in conformance vDSP.VectorizableDouble(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return specialized static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(a1, a2, a3, a4, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:));
}

void protocol witness for static vDSP_BiquadFunctions.applyMulti(setup:pInputs:pOutputs:count:) in conformance vDSP.VectorizableDouble(vDSP_biquadm_SetupStructD *a1, const double **a2, double **__Y, vDSP_Length __N)
{
  vDSP_biquadmD(a1, a2, 1, __Y, 1, __N);
}

uint64_t protocol witness for static vDSP_BiquadFunctions.destroySetup(channelCount:biquadSetup:) in conformance vDSP.VectorizableDouble(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return protocol witness for static vDSP_BiquadFunctions.destroySetup(channelCount:biquadSetup:) in conformance vDSP.VectorizableFloat(a1, a2, a3, a4, MEMORY[0x1E0C8BFE8], MEMORY[0x1E0C8C020]);
}

uint64_t protocol witness for static vDSP_BiquadFunctions.destroySetup(channelCount:biquadSetup:) in conformance vDSP.VectorizableFloat(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(uint64_t), uint64_t (*a6)(uint64_t))
{
  if (a1 == 1)
    return a5(a2);
  else
    return a6(a2);
}

uint64_t specialized static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  _QWORD v11[12];

  v11[2] = a6;
  v11[3] = a7;
  v11[4] = a8;
  v11[5] = a9;
  v11[6] = a1;
  v11[7] = a4;
  v11[8] = a3;
  v11[9] = a5;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t, uint64_t))(a9 + 16))(a10, v11, MEMORY[0x1E0DEE9C0] + 8, a7, a9);
}

_UNKNOWN **associated type witness table accessor for vDSP_FloatingPointBiquadFilterable.BiquadFunctions : vDSP_BiquadFunctions in Float()
{
  return &protocol witness table for vDSP.VectorizableFloat;
}

_UNKNOWN **associated type witness table accessor for vDSP_FloatingPointBiquadFilterable.BiquadFunctions : vDSP_BiquadFunctions in Double()
{
  return &protocol witness table for vDSP.VectorizableDouble;
}

uint64_t type metadata accessor for vDSP.Biquad(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return __swift_instantiateGenericMetadata(a1, a2, a3, a4, (uint64_t)&nominal type descriptor for vDSP.Biquad);
}

uint64_t type metadata completion function for vDSP.BiquadRef()
{
  return swift_initClassMetadata2();
}

uint64_t dispatch thunk of static vDSP_BiquadFunctions.makeBiquadSetup(channelCount:coefficients:sectionCount:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return (*(uint64_t (**)(void))(a5 + 16))();
}

uint64_t dispatch thunk of static vDSP_BiquadFunctions.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  return (*(uint64_t (**)(void))(a12 + 24))();
}

uint64_t dispatch thunk of static vDSP_BiquadFunctions.applyMulti(setup:pInputs:pOutputs:count:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return (*(uint64_t (**)(void))(a6 + 32))();
}

uint64_t dispatch thunk of static vDSP_BiquadFunctions.destroySetup(channelCount:biquadSetup:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return (*(uint64_t (**)(void))(a4 + 40))();
}

uint64_t partial apply for closure #1 in static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(a1, a2, MEMORY[0x1E0C8BFC8]);
}

uint64_t partial apply for closure #1 in static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(a1, a2, MEMORY[0x1E0C8BFC0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t result, uint64_t a2, uint64_t (*a3)(_QWORD, _QWORD, uint64_t, uint64_t, uint64_t, uint64_t, _QWORD))
{
  uint64_t v3;
  uint64_t v4;

  if (result)
  {
    v4 = **(_QWORD **)(v3 + 32);
    if (v4)
      return a3(*(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24), result, 1, v4, 1, *(_QWORD *)(v3 + 40));
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)()
{
  return closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)();
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)(uint64_t a1)
{
  uint64_t v1;

  return closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)(a1, *(_QWORD *)(v1 + 64), *(_QWORD *)(v1 + 72), *(_QWORD *)(v1 + 80), *(_QWORD *)(v1 + 88), *(_QWORD *)(v1 + 16), *(_QWORD *)(v1 + 24), *(_QWORD *)(v1 + 32), *(_QWORD *)(v1 + 40), *(_QWORD *)(v1 + 48), *(_QWORD *)(v1 + 56));
}

unint64_t *partial apply for closure #1 in closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)@<X0>(unint64_t *result@<X0>, _QWORD *a2@<X8>)
{
  uint64_t v2;
  unint64_t v4;
  unint64_t v5;
  unint64_t v6;
  uint64_t v7;

  v4 = *(_QWORD *)(v2 + 72);
  v5 = *result;
  if (is_mul_ok(*result, v4))
  {
    v6 = v5 * v4;
    if (((v5 * v4) & 0x8000000000000000) == 0)
    {
      v7 = *(_QWORD *)(v2 + 64);
      result = (unint64_t *)swift_getAssociatedTypeWitness();
      *a2 = v7 + *(_QWORD *)(*(result - 1) + 72) * v6;
      return result;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

unint64_t lazy protocol witness table accessor for type Range<UInt> and conformance <> Range<A>()
{
  unint64_t result;
  uint64_t v1;
  unint64_t v2;
  _QWORD v3[2];

  result = lazy protocol witness table cache variable for type Range<UInt> and conformance <> Range<A>;
  if (!lazy protocol witness table cache variable for type Range<UInt> and conformance <> Range<A>)
  {
    v1 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for Range<UInt>);
    v2 = lazy protocol witness table accessor for type Int and conformance Int();
    v3[0] = MEMORY[0x1E0DEBBD0];
    v3[1] = v2;
    result = MEMORY[0x1D1794D08](MEMORY[0x1E0DEB8C0], v1, v3);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Range<UInt> and conformance <> Range<A>);
  }
  return result;
}

uint64_t partial apply for closure #2 in closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)@<X0>(unint64_t *a1@<X0>, _QWORD *a2@<X8>)
{
  uint64_t v2;

  return closure #2 in closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)(a1, *(_QWORD *)(v2 + 72), a2);
}

uint64_t BNNS.DropoutLayer.__allocating_init(input:output:rate:seed:control:filterParameters:)(_OWORD *a1, __int128 *a2, int a3, char a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9)
{
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  int *v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  int v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  _BYTE __dst[352];
  float v39;
  int v40;
  char v41;
  _OWORD __src[22];
  uint64_t v43;

  v43 = *MEMORY[0x1E0C80C00];
  v16 = a2[8];
  v17 = a2[9];
  v18 = a2[6];
  __src[18] = a2[7];
  __src[19] = v16;
  v19 = a2[10];
  __src[20] = v17;
  __src[21] = v19;
  v20 = a2[4];
  v21 = a2[5];
  v22 = a2[2];
  __src[14] = a2[3];
  __src[15] = v20;
  __src[16] = v21;
  __src[17] = v18;
  v23 = *a2;
  __src[12] = a2[1];
  __src[13] = v22;
  v24 = a1[9];
  __src[8] = a1[8];
  __src[9] = v24;
  __src[10] = a1[10];
  __src[11] = v23;
  v25 = a1[5];
  __src[4] = a1[4];
  __src[5] = v25;
  v26 = a1[7];
  __src[6] = a1[6];
  __src[7] = v26;
  v27 = a1[1];
  __src[0] = *a1;
  __src[1] = v27;
  v28 = a1[3];
  __src[2] = a1[2];
  __src[3] = v28;
  memcpy(__dst, __src, sizeof(__dst));
  v39 = a9;
  v40 = a3;
  v41 = a4;
  if (a7 == 1)
  {
    v29 = 0;
  }
  else
  {
    v34 = a5;
    v35 = a6;
    v36 = a7;
    v37 = a8;
    v29 = &v34;
  }
  v30 = MEMORY[0x1D17945B8](__dst, v29);
  type metadata accessor for BNNS.DropoutLayer();
  v31 = swift_allocObject();
  v32 = v31;
  if (v30)
  {
    *(_QWORD *)(v31 + 16) = v30;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v32;
}

uint64_t type metadata accessor for BNNS.DropoutLayer()
{
  return objc_opt_self();
}

uint64_t BNNS.DropoutLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.DropoutLayer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

uint64_t BNNS.FullyConnectedLayer.__allocating_init(input:output:weights:bias:activation:filterParameters:)(_OWORD *a1, _OWORD *a2, __int128 *a3, uint64_t a4, uint64_t *a5, uint32_t a6, size_t a7, int (__cdecl *a8)(void **, size_t, size_t), void (__cdecl *a9)(void *))
{
  uint64_t v14;
  char v15;
  size_t v16;
  void *v17;
  void *v18;
  BNNSDataType v19;
  uint64_t v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  BNNSFilterParameters *p_filter_params;
  void *v42;
  uint64_t v43;
  uint64_t v44;
  int v46;
  uint64_t v49;
  size_t v50;
  size_t v51;
  size_t v52;
  size_t v53;
  size_t v54;
  size_t v55;
  size_t v56;
  size_t v57;
  size_t v58;
  uint64_t v59;
  size_t v60;
  size_t v61;
  size_t v62;
  size_t v63;
  size_t v64;
  size_t v65;
  BNNSFilterParameters filter_params;
  _OWORD __src[33];
  BNNSLayerParametersFullyConnected __dst;
  BNNSActivation v70;
  _BYTE v71[184];
  _BYTE v72[184];
  _BYTE v73[184];
  uint64_t v74;

  v74 = *MEMORY[0x1E0C80C00];
  outlined init with take of BNNSNDArrayDescriptor?(a4, (uint64_t)v71);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v71, (uint64_t)v73);
  v14 = *a5;
  v15 = *((_BYTE *)a5 + 8);
  outlined init with take of BNNSNDArrayDescriptor?(a4, (uint64_t)v72);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v72) == 1)
  {
    v64 = 0;
    v65 = 0;
    v62 = 0;
    v63 = 0;
    v60 = 0;
    v61 = 0;
    v57 = 0;
    v58 = 0;
    v55 = 0;
    v56 = 0;
    v53 = 0;
    v54 = 0;
    v51 = 0;
    v52 = 0;
    v50 = 0;
    v16 = 0;
    v17 = 0;
    v18 = 0;
    v19 = 0;
    v59 = 0;
    v20 = 0;
    v49 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v73, (uint64_t)__src);
    v59 = *(_QWORD *)&__src[0];
    v64 = *(_QWORD *)&__src[1];
    v65 = *((_QWORD *)&__src[0] + 1);
    v62 = *(_QWORD *)&__src[2];
    v63 = *((_QWORD *)&__src[1] + 1);
    v60 = *(_QWORD *)&__src[3];
    v61 = *((_QWORD *)&__src[2] + 1);
    v57 = *(_QWORD *)&__src[4];
    v58 = *((_QWORD *)&__src[3] + 1);
    v55 = *(_QWORD *)&__src[5];
    v56 = *((_QWORD *)&__src[4] + 1);
    v53 = *(_QWORD *)&__src[6];
    v54 = *((_QWORD *)&__src[5] + 1);
    v51 = *(_QWORD *)&__src[7];
    v52 = *((_QWORD *)&__src[6] + 1);
    v16 = *(_QWORD *)&__src[8];
    v50 = *((_QWORD *)&__src[7] + 1);
    v17 = (void *)*((_QWORD *)&__src[8] + 1);
    v18 = (void *)*((_QWORD *)&__src[9] + 1);
    v49 = *(_QWORD *)&__src[9];
    v19 = __src[10];
    v20 = *(_QWORD *)((char *)&__src[10] + 4);
    v46 = HIDWORD(__src[10]);
  }
  *(_QWORD *)&__src[0] = v14;
  BYTE8(__src[0]) = v15;
  BNNS.ActivationFunction.bnnsActivation.getter();
  v21 = a1[9];
  __src[8] = a1[8];
  __src[9] = v21;
  v22 = a1[5];
  __src[4] = a1[4];
  __src[5] = v22;
  v23 = a1[7];
  __src[6] = a1[6];
  __src[7] = v23;
  v24 = a1[1];
  __src[0] = *a1;
  __src[1] = v24;
  v25 = a1[3];
  __src[2] = a1[2];
  __src[3] = v25;
  v26 = a3[8];
  v27 = a3[9];
  v28 = a3[6];
  __src[18] = a3[7];
  __src[19] = v26;
  v29 = a3[10];
  __src[20] = v27;
  __src[21] = v29;
  v30 = a3[4];
  v31 = a3[5];
  v32 = a3[2];
  __src[14] = a3[3];
  __src[15] = v30;
  v33 = a1[10];
  __src[16] = v31;
  __src[17] = v28;
  v34 = *a3;
  v35 = a3[1];
  __src[10] = v33;
  __src[11] = v34;
  __src[12] = v35;
  __src[13] = v32;
  v36 = a2[9];
  __src[30] = a2[8];
  __src[31] = v36;
  __src[32] = a2[10];
  v37 = a2[5];
  __src[26] = a2[4];
  __src[27] = v37;
  v38 = a2[7];
  __src[28] = a2[6];
  __src[29] = v38;
  v39 = a2[1];
  __src[22] = *a2;
  __src[23] = v39;
  v40 = a2[3];
  __src[24] = a2[2];
  __src[25] = v40;
  memcpy(&__dst, __src, 0x210uLL);
  *(_QWORD *)&__dst.bias.flags = v59;
  __dst.bias.size[0] = v65;
  __dst.bias.size[1] = v64;
  __dst.bias.size[2] = v63;
  __dst.bias.size[3] = v62;
  __dst.bias.size[4] = v61;
  __dst.bias.size[5] = v60;
  __dst.bias.size[6] = v58;
  __dst.bias.size[7] = v57;
  __dst.bias.stride[0] = v56;
  __dst.bias.stride[1] = v55;
  __dst.bias.stride[2] = v54;
  __dst.bias.stride[3] = v53;
  __dst.bias.stride[4] = v52;
  __dst.bias.stride[5] = v51;
  __dst.bias.stride[6] = v50;
  __dst.bias.stride[7] = v16;
  __dst.bias.data = v17;
  *(_QWORD *)&__dst.bias.data_type = v49;
  __dst.bias.table_data = v18;
  __dst.bias.table_data_type = v19;
  *(_QWORD *)&__dst.bias.data_scale = v20;
  *((_DWORD *)&__dst.bias.data_bias + 1) = v46;
  __dst.activation = v70;
  if (a8 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    p_filter_params = 0;
  }
  else
  {
    filter_params.flags = a6;
    filter_params.n_threads = a7;
    filter_params.alloc_memory = a8;
    filter_params.free_memory = a9;
    p_filter_params = &filter_params;
  }
  v42 = BNNSFilterCreateLayerFullyConnected(&__dst, p_filter_params);
  type metadata accessor for BNNS.FullyConnectedLayer();
  v43 = swift_allocObject();
  v44 = v43;
  if (v42)
  {
    *(_QWORD *)(v43 + 16) = v42;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v44;
}

uint64_t type metadata accessor for BNNS.FullyConnectedLayer()
{
  return objc_opt_self();
}

uint64_t BNNS.FullyConnectedLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.FullyConnectedLayer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

uint64_t _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF(void (*a1)(char *, char *), uint64_t a2, uint64_t a3, char *a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v9;
  uint64_t AssociatedTypeWitness;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  Swift::Int v19;
  uint64_t result;
  char *v21;
  void (*v22)(char *, _QWORD);
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  char *v30;
  char *v31;
  uint64_t v32;
  void (*v33)(char *, char *);
  uint64_t v34;
  char *v35;
  char *v36;
  uint64_t v37;
  char v38[32];
  uint64_t v39;

  v27 = a5;
  v28 = a8;
  v33 = a1;
  v34 = a2;
  v26 = *(_QWORD *)(a5 - 8);
  MEMORY[0x1E0C80A78](a1);
  v35 = (char *)&v24 - ((v9 + 15) & 0xFFFFFFFFFFFFFFF0);
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v11 = *(_QWORD *)(AssociatedTypeWitness - 8);
  v12 = MEMORY[0x1E0C80A78](AssociatedTypeWitness);
  v31 = (char *)&v24 - v13;
  v36 = a4;
  MEMORY[0x1E0C80A78](v12);
  v30 = (char *)&v24 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0);
  v15 = swift_getAssociatedTypeWitness();
  v29 = *(_QWORD *)(v15 - 8);
  MEMORY[0x1E0C80A78](v15);
  v17 = (char *)&v24 - v16;
  v18 = dispatch thunk of Collection.count.getter();
  if (!v18)
    return static Array._allocateUninitialized(_:)();
  v19 = v18;
  v25 = v15;
  v39 = MEMORY[0x1D17942F4](v36);
  v32 = type metadata accessor for ContiguousArray();
  ContiguousArray.reserveCapacity(_:)(v19);
  v36 = v17;
  result = dispatch thunk of Collection.startIndex.getter();
  if (v19 < 0)
  {
    __break(1u);
  }
  else
  {
    v21 = v31;
    while (1)
    {
      v22 = (void (*)(char *, _QWORD))dispatch thunk of Collection.subscript.read();
      (*(void (**)(char *))(v11 + 16))(v21);
      v22(v38, 0);
      v23 = v37;
      v33(v21, v35);
      if (v23)
        break;
      v37 = 0;
      (*(void (**)(char *, uint64_t))(v11 + 8))(v21, AssociatedTypeWitness);
      ContiguousArray.append(_:)();
      dispatch thunk of Collection.formIndex(after:)();
      if (!--v19)
      {
        (*(void (**)(char *, uint64_t))(v29 + 8))(v36, v25);
        return v39;
      }
    }
    (*(void (**)(char *, uint64_t))(v11 + 8))(v21, AssociatedTypeWitness);
    (*(void (**)(char *, uint64_t))(v29 + 8))(v36, v25);
    swift_release();
    return (*(uint64_t (**)(uint64_t, char *, uint64_t))(v26 + 32))(v28, v35, v27);
  }
  return result;
}

vImage_Error vImageConverterRef.convert<A, B>(from:to:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  vImageConverter *v6;
  vImagePixelCount v12;
  vImagePixelCount v13;
  size_t v14;
  vImage_Error result;
  vImage_Error v16;
  char *v17;
  char v18;
  vImage_Error v19;
  vImage_Buffer v20;
  uint64_t v21;

  v21 = *MEMORY[0x1E0C80C00];
  v19 = 0;
  type metadata accessor for vImage.PixelBuffer(0, a3, *(_QWORD *)(a5 + 8), a4);
  v20.data = (void *)vImage.PixelBuffer<>.vImageBuffer.getter();
  v20.height = v12;
  v20.width = v13;
  v20.rowBytes = v14;
  result = closure #1 in vImageConverterRef.convert<A, B>(from:to:)(&v20, a2, &v19, v6, a3, a4, a5, a6);
  v16 = v19;
  if (v19)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    if ((unint64_t)(v16 + 21784) >= 0x13)
      v18 = 11;
    else
      v18 = -5 - v16;
    *v17 = v18;
    return swift_willThrow();
  }
  return result;
}

vImage_Error closure #1 in vImageConverterRef.convert<A, B>(from:to:)(const vImage_Buffer *a1, uint64_t a2, vImage_Error *a3, vImageConverter *a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  vImagePixelCount v11;
  vImagePixelCount v12;
  size_t v13;
  vImage_Error result;
  vImage_Buffer dests;
  uint64_t v16;

  v16 = *MEMORY[0x1E0C80C00];
  type metadata accessor for vImage.PixelBuffer(0, a6, *(_QWORD *)(a8 + 8), (uint64_t)a4);
  dests.data = (void *)vImage.PixelBuffer<>.vImageBuffer.getter();
  dests.height = v11;
  dests.width = v12;
  dests.rowBytes = v13;
  result = vImageConvert_AnyToAny(a4, a1, &dests, 0, 0x100u);
  *a3 = result;
  return result;
}

uint64_t vImageConverterRef.convert<A, B>(from:to:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v11;
  char *v12;
  char *v13;
  uint64_t v14;
  uint64_t v15;
  const vImage_Buffer *v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  const vImage_Buffer *v21;
  vImage_Error v22;
  uint64_t result;
  char *v24;
  char *v25;
  char v26;
  vImageConverter *converter;
  char v28[16];
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  char v33[16];
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;

  v37 = a6;
  v38 = a1;
  v34 = a3;
  v35 = a4;
  v36 = a5;
  type metadata accessor for vImage.PixelBuffer(255, a3, *(_QWORD *)(a5 + 8), a4);
  v11 = type metadata accessor for Array();
  type metadata accessor for vImage_Buffer(0);
  v13 = v12;
  v14 = MEMORY[0x1D1794D08](MEMORY[0x1E0DEAF50], v11);
  v16 = (const vImage_Buffer *)_sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in vImageConverterRef.convert<A, B>(from:to:), (uint64_t)v33, v11, v13, MEMORY[0x1E0DEDCE8], v14, MEMORY[0x1E0DEDD18], v15);
  v38 = a2;
  v29 = a3;
  v30 = a4;
  v31 = a5;
  v32 = a6;
  type metadata accessor for vImage.PixelBuffer(255, a4, *(_QWORD *)(a6 + 8), v17);
  v18 = type metadata accessor for Array();
  v19 = MEMORY[0x1D1794D08](MEMORY[0x1E0DEAF50], v18);
  v21 = (const vImage_Buffer *)_sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #2 in vImageConverterRef.convert<A, B>(from:to:), (uint64_t)v28, v18, v13, MEMORY[0x1E0DEDCE8], v19, MEMORY[0x1E0DEDD18], v20);
  v22 = vImageConvert_AnyToAny(converter, v16 + 1, v21 + 1, 0, 0x100u);
  swift_bridgeObjectRelease();
  result = swift_bridgeObjectRelease();
  if (v22)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v25 = v24;
    vImage.Error.init(rawValue:)(v22, (char *)&v38);
    v26 = v38;
    if (v38 == 20)
      v26 = 11;
    *v25 = v26;
    return swift_willThrow();
  }
  return result;
}

uint64_t partial apply for closure #1 in vImageConverterRef.convert<A, B>(from:to:)@<X0>(uint64_t a1@<X3>, uint64_t *a2@<X8>)
{
  uint64_t v2;
  uint64_t result;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;

  type metadata accessor for vImage.PixelBuffer(0, *(_QWORD *)(v2 + 16), *(_QWORD *)(*(_QWORD *)(v2 + 32) + 8), a1);
  result = vImage.PixelBuffer<>.vImageBuffer.getter();
  *a2 = result;
  a2[1] = v5;
  a2[2] = v6;
  a2[3] = v7;
  return result;
}

uint64_t partial apply for closure #2 in vImageConverterRef.convert<A, B>(from:to:)@<X0>(uint64_t a1@<X3>, uint64_t *a2@<X8>)
{
  uint64_t v2;
  uint64_t result;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;

  type metadata accessor for vImage.PixelBuffer(0, *(_QWORD *)(v2 + 24), *(_QWORD *)(*(_QWORD *)(v2 + 40) + 8), a1);
  result = vImage.PixelBuffer<>.vImageBuffer.getter();
  *a2 = result;
  a2[1] = v5;
  a2[2] = v6;
  a2[3] = v7;
  return result;
}

vImageConverterRef static vImageConverterRef.make(sourceFormat:destinationFormat:colorConversionInfo:)(uint64_t a1, uint64_t a2, CGColorConversionInfoRef colorConversionInfoRef)
{
  __int128 v3;
  __int128 v4;
  vImageConverterRef v5;
  vImageConverterRef v6;
  vImage_Error v7;
  char *v8;
  char v9;
  vImage_Error error;
  vImage_CGImageFormat sFormat;
  vImage_CGImageFormat dFormat;
  uint64_t v14;

  v14 = *MEMORY[0x1E0C80C00];
  error = 0;
  v3 = *(_OWORD *)(a2 + 16);
  *(_OWORD *)&dFormat.bitsPerComponent = *(_OWORD *)a2;
  *(_OWORD *)&dFormat.bitmapInfo = v3;
  *(_QWORD *)&dFormat.renderingIntent = *(_QWORD *)(a2 + 32);
  v4 = *(_OWORD *)(a1 + 16);
  *(_OWORD *)&sFormat.bitsPerComponent = *(_OWORD *)a1;
  *(_OWORD *)&sFormat.bitmapInfo = v4;
  *(_QWORD *)&sFormat.renderingIntent = *(_QWORD *)(a1 + 32);
  v5 = vImageConverter_CreateWithCGColorConversionInfo(colorConversionInfoRef, &sFormat, &dFormat, 0, 0x100u, &error);
  v6 = v5;
  v7 = error;
  if (error)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    if ((unint64_t)(v7 + 21784) >= 0x13)
      v9 = 11;
    else
      v9 = -5 - v7;
  }
  else
  {
    if (v5)
      return v6;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v9 = 11;
  }
  *v8 = v9;
  swift_willThrow();
  return v6;
}

int64_t vImageConverterRef.makeCVToCGPixelBuffers(referencing:)(uint64_t a1)
{
  return vImageConverterRef.makeCVToCGPixelBuffers(referencing:)(a1, MEMORY[0x1E0C8D140], (uint64_t (*)(char *, uint64_t, uint64_t, uint64_t))MEMORY[0x1E0C8CC10]);
}

int64_t vImageConverterRef.makeCGToCVPixelBuffers(referencing:)(uint64_t a1)
{
  return vImageConverterRef.makeCVToCGPixelBuffers(referencing:)(a1, MEMORY[0x1E0C8D138], MEMORY[0x1E0C8CC18]);
}

int64_t vImageConverterRef.makeCVToCGPixelBuffers(referencing:)(uint64_t a1, uint64_t (*a2)(uint64_t), uint64_t (*a3)(char *, uint64_t, uint64_t, uint64_t))
{
  uint64_t v3;
  int64_t result;
  int64_t v7;
  char *v8;
  unint64_t v9;
  uint64_t v10;
  __int128 v11;
  unint64_t v12;
  unint64_t v13;
  char *v14;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  char *v18;
  char v19;
  int64_t v20;
  uint64_t *v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  unint64_t v25;
  unint64_t v26;
  __int128 v27;
  uint64_t v28;

  result = a2(v3);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    v7 = result;
    v8 = (char *)MEMORY[0x1E0DEE9D8];
    if (result)
    {
      v28 = MEMORY[0x1E0DEE9D8];
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, result, 0);
      v8 = (char *)v28;
      v9 = *(_QWORD *)(v28 + 16);
      v10 = 32 * v9 + 32;
      v11 = 0uLL;
      do
      {
        v28 = (uint64_t)v8;
        v12 = *((_QWORD *)v8 + 3);
        v13 = v9 + 1;
        if (v9 >= v12 >> 1)
        {
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v12 > 1), v9 + 1, 1);
          v11 = 0uLL;
          v8 = (char *)v28;
        }
        *((_QWORD *)v8 + 2) = v13;
        v14 = &v8[v10];
        *(_OWORD *)v14 = v11;
        *((_OWORD *)v14 + 1) = v11;
        v10 += 32;
        v9 = v13;
        --v7;
      }
      while (v7);
    }
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      v8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v8 + 2), 0, v8);
    v15 = a3(v8 + 32, v3, a1, 768);
    if (v15)
    {
      v16 = v15;
      swift_bridgeObjectRelease();
      lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
      swift_allocError();
      v18 = v17;
      vImage.Error.init(rawValue:)(v16, (char *)&v28);
      v19 = v28;
      if (v28 == 20)
        v19 = 11;
      *v18 = v19;
      swift_willThrow();
    }
    else
    {
      v20 = *((_QWORD *)v8 + 2);
      if (v20)
      {
        v28 = MEMORY[0x1E0DEE9D8];
        swift_bridgeObjectRetain();
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v20, 0);
        v16 = v28;
        v21 = (uint64_t *)(v8 + 56);
        do
        {
          v27 = *(_OWORD *)(v21 - 3);
          v22 = *(v21 - 1);
          v23 = *v21;
          __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
          v24 = swift_allocObject();
          *(_OWORD *)(v24 + 16) = xmmword_1CAB5E430;
          *(_OWORD *)(v24 + 32) = v27;
          *(_QWORD *)(v24 + 48) = v22;
          *(_QWORD *)(v24 + 56) = v23;
          *(_QWORD *)(v24 + 64) = 0;
          v28 = v16;
          v26 = *(_QWORD *)(v16 + 16);
          v25 = *(_QWORD *)(v16 + 24);
          if (v26 >= v25 >> 1)
          {
            specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v25 > 1), v26 + 1, 1);
            v16 = v28;
          }
          v21 += 4;
          *(_QWORD *)(v16 + 16) = v26 + 1;
          *(_QWORD *)(v16 + 8 * v26 + 32) = v24;
          --v20;
        }
        while (v20);
        swift_bridgeObjectRelease_n();
      }
      else
      {
        swift_bridgeObjectRelease();
        return MEMORY[0x1E0DEE9D8];
      }
    }
    return v16;
  }
  return result;
}

double static BNNSActivation.identity.getter@<D0>(uint64_t a1@<X8>)
{
  double result;

  *(_DWORD *)a1 = 0;
  *(int32x2_t *)(a1 + 4) = vdup_n_s32(0x7FC00000u);
  *(_DWORD *)(a1 + 12) = 1;
  result = 0.0;
  *(_OWORD *)(a1 + 16) = 0u;
  *(_OWORD *)(a1 + 32) = 0u;
  return result;
}

uint64_t static BNNSActivationFunction.rectifiedLinear.getter()
{
  return 1;
}

void __swiftcall BNNSActivation.init(function:alpha:beta:)(BNNSActivation *__return_ptr retstr, BNNSActivationFunction function, Swift::Float alpha, Swift::Float beta)
{
  if (function - 9 < 2)
  {
    __break(1u);
  }
  else
  {
    retstr->function = function;
    retstr->alpha = alpha;
    retstr->beta = beta;
    retstr->iscale = 1;
    *(_OWORD *)&retstr->ioffset = 0u;
    *(_OWORD *)&retstr->ioffset_per_channel = 0u;
  }
}

uint64_t static BNNSActivationFunction.leakyRectifiedLinear.getter()
{
  return 2;
}

uint64_t static BNNSActivationFunction.sigmoid.getter()
{
  return 3;
}

uint64_t static BNNSActivationFunction.tanh.getter()
{
  return 4;
}

uint64_t static BNNSActivationFunction.scaledTanh.getter()
{
  return 5;
}

uint64_t static BNNSActivationFunction.abs.getter()
{
  return 6;
}

uint64_t static BNNSActivationFunction.linear.getter()
{
  return 7;
}

uint64_t static BNNSActivationFunction.clamp.getter()
{
  return 8;
}

uint64_t static BNNSActivationFunction.softmax.getter()
{
  return 11;
}

uint64_t static BNNSPoolingFunction.max.getter()
{
  return 0;
}

uint64_t static BNNSDataType.float16.getter()
{
  return 65552;
}

uint64_t static BNNSDataType.float.getter()
{
  return 65568;
}

uint64_t static BNNSDataType.int8.getter()
{
  return 131080;
}

uint64_t static BNNSDataType.int16.getter()
{
  return 131088;
}

uint64_t static BNNSDataType.int32.getter()
{
  return 131104;
}

uint64_t static BNNSDataType.indexed8.getter()
{
  return 524296;
}

uint64_t static BNNSDataType.uint8.getter()
{
  return 262152;
}

uint64_t static BNNSDataType.uint16.getter()
{
  return 262160;
}

uint64_t static BNNSDataType.uint32.getter()
{
  return 262176;
}

uint64_t BNNSDataTypeFloat16.getter()
{
  swift_beginAccess();
  return BNNSDataTypeFloat16;
}

uint64_t BNNSDataTypeFloat16.setter(BNNSDataType a1)
{
  uint64_t result;

  result = swift_beginAccess();
  BNNSDataTypeFloat16 = a1;
  return result;
}

uint64_t (*BNNSDataTypeFloat16.modify())()
{
  swift_beginAccess();
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSDataTypeFloat32.getter()
{
  swift_beginAccess();
  return BNNSDataTypeFloat32;
}

uint64_t BNNSDataTypeFloat32.setter(BNNSDataType a1)
{
  uint64_t result;

  result = swift_beginAccess();
  BNNSDataTypeFloat32 = a1;
  return result;
}

uint64_t (*BNNSDataTypeFloat32.modify())()
{
  swift_beginAccess();
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSDataTypeInt8.getter()
{
  swift_beginAccess();
  return BNNSDataTypeInt8;
}

uint64_t BNNSDataTypeInt8.setter(BNNSDataType a1)
{
  uint64_t result;

  result = swift_beginAccess();
  BNNSDataTypeInt8 = a1;
  return result;
}

uint64_t (*BNNSDataTypeInt8.modify())()
{
  swift_beginAccess();
  return BNNSDataTypeInt8.modify;
}

uint64_t BNNSDataTypeInt16.getter()
{
  swift_beginAccess();
  return BNNSDataTypeInt16;
}

uint64_t BNNSDataTypeInt16.setter(BNNSDataType a1)
{
  uint64_t result;

  result = swift_beginAccess();
  BNNSDataTypeInt16 = a1;
  return result;
}

uint64_t (*BNNSDataTypeInt16.modify())()
{
  swift_beginAccess();
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSDataTypeInt32.getter()
{
  swift_beginAccess();
  return BNNSDataTypeInt32;
}

uint64_t BNNSDataTypeInt32.setter(BNNSDataType a1)
{
  uint64_t result;

  result = swift_beginAccess();
  BNNSDataTypeInt32 = a1;
  return result;
}

uint64_t (*BNNSDataTypeInt32.modify())()
{
  swift_beginAccess();
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSDataTypeIndexed8.getter()
{
  swift_beginAccess();
  return BNNSDataTypeIndexed8;
}

uint64_t BNNSDataTypeIndexed8.setter(BNNSDataType a1)
{
  uint64_t result;

  result = swift_beginAccess();
  BNNSDataTypeIndexed8 = a1;
  return result;
}

uint64_t (*BNNSDataTypeIndexed8.modify())()
{
  swift_beginAccess();
  return BNNSDataTypeFloat32.modify;
}

uint64_t static BNNSPoolingFunction.average.getter()
{
  return 1;
}

uint64_t BNNSPoolingFunctionMax.getter()
{
  swift_beginAccess();
  return BNNSPoolingFunctionMax;
}

uint64_t BNNSPoolingFunctionMax.setter(BNNSPoolingFunction a1)
{
  uint64_t result;

  result = swift_beginAccess();
  BNNSPoolingFunctionMax = a1;
  return result;
}

uint64_t (*BNNSPoolingFunctionMax.modify())()
{
  swift_beginAccess();
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSPoolingFunctionAverage.getter()
{
  swift_beginAccess();
  return BNNSPoolingFunctionAverage;
}

uint64_t BNNSPoolingFunctionAverage.setter(BNNSPoolingFunction a1)
{
  uint64_t result;

  result = swift_beginAccess();
  BNNSPoolingFunctionAverage = a1;
  return result;
}

uint64_t (*BNNSPoolingFunctionAverage.modify())()
{
  swift_beginAccess();
  return BNNSDataTypeFloat32.modify;
}

uint64_t static BNNSActivationFunction.identity.getter()
{
  return 0;
}

uint64_t static BNNSActivationFunction.integerLinearSaturate.getter()
{
  return 9;
}

uint64_t static BNNSActivationFunction.integerLinearSaturatePerChannel.getter()
{
  return 10;
}

uint64_t BNNSActivationFunctionIdentity.getter()
{
  swift_beginAccess();
  return BNNSActivationFunctionIdentity;
}

uint64_t BNNSActivationFunctionIdentity.setter(BNNSActivationFunction a1)
{
  uint64_t result;

  result = swift_beginAccess();
  BNNSActivationFunctionIdentity = a1;
  return result;
}

uint64_t (*BNNSActivationFunctionIdentity.modify())()
{
  swift_beginAccess();
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSActivationFunctionRectifiedLinear.getter()
{
  swift_beginAccess();
  return BNNSActivationFunctionRectifiedLinear;
}

uint64_t BNNSActivationFunctionRectifiedLinear.setter(BNNSActivationFunction a1)
{
  uint64_t result;

  result = swift_beginAccess();
  BNNSActivationFunctionRectifiedLinear = a1;
  return result;
}

uint64_t (*BNNSActivationFunctionRectifiedLinear.modify())()
{
  swift_beginAccess();
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSActivationFunctionLeakyRectifiedLinear.getter()
{
  swift_beginAccess();
  return BNNSActivationFunctionLeakyRectifiedLinear;
}

uint64_t BNNSActivationFunctionLeakyRectifiedLinear.setter(BNNSActivationFunction a1)
{
  uint64_t result;

  result = swift_beginAccess();
  BNNSActivationFunctionLeakyRectifiedLinear = a1;
  return result;
}

uint64_t (*BNNSActivationFunctionLeakyRectifiedLinear.modify())()
{
  swift_beginAccess();
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSActivationFunctionSigmoid.getter()
{
  swift_beginAccess();
  return BNNSActivationFunctionSigmoid;
}

uint64_t BNNSActivationFunctionSigmoid.setter(BNNSActivationFunction a1)
{
  uint64_t result;

  result = swift_beginAccess();
  BNNSActivationFunctionSigmoid = a1;
  return result;
}

uint64_t (*BNNSActivationFunctionSigmoid.modify())()
{
  swift_beginAccess();
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSActivationFunctionTanh.getter()
{
  swift_beginAccess();
  return BNNSActivationFunctionTanh;
}

uint64_t BNNSActivationFunctionTanh.setter(BNNSActivationFunction a1)
{
  uint64_t result;

  result = swift_beginAccess();
  BNNSActivationFunctionTanh = a1;
  return result;
}

uint64_t (*BNNSActivationFunctionTanh.modify())()
{
  swift_beginAccess();
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSActivationFunctionScaledTanh.getter()
{
  swift_beginAccess();
  return BNNSActivationFunctionScaledTanh;
}

uint64_t BNNSActivationFunctionScaledTanh.setter(BNNSActivationFunction a1)
{
  uint64_t result;

  result = swift_beginAccess();
  BNNSActivationFunctionScaledTanh = a1;
  return result;
}

uint64_t (*BNNSActivationFunctionScaledTanh.modify())()
{
  swift_beginAccess();
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSActivationFunctionAbs.getter()
{
  swift_beginAccess();
  return BNNSActivationFunctionAbs;
}

uint64_t BNNSActivationFunctionAbs.setter(BNNSActivationFunction a1)
{
  uint64_t result;

  result = swift_beginAccess();
  BNNSActivationFunctionAbs = a1;
  return result;
}

uint64_t (*BNNSActivationFunctionAbs.modify())()
{
  swift_beginAccess();
  return BNNSDataTypeFloat32.modify;
}

uint64_t static BNNSFlags.useClientPointer.getter()
{
  return 1;
}

uint64_t BNNSFlagsUseClientPtr.getter()
{
  swift_beginAccess();
  return BNNSFlagsUseClientPtr;
}

uint64_t BNNSFlagsUseClientPtr.setter(BNNSFlags a1)
{
  uint64_t result;

  result = swift_beginAccess();
  BNNSFlagsUseClientPtr = a1;
  return result;
}

uint64_t (*BNNSFlagsUseClientPtr.modify())()
{
  swift_beginAccess();
  return BNNSDataTypeFloat32.modify;
}

void __swiftcall BNNSImageStackDescriptor.init(width:height:channels:row_stride:image_stride:data_type:)(BNNSImageStackDescriptor *__return_ptr retstr, Swift::Int width, Swift::Int height, Swift::Int channels, Swift::Int row_stride, Swift::Int image_stride, BNNSDataType data_type)
{
  if (data_type == BNNSDataTypeIndexed8)
  {
    __break(1u);
  }
  else
  {
    retstr->width = width;
    retstr->height = height;
    retstr->channels = channels;
    retstr->row_stride = row_stride;
    retstr->image_stride = image_stride;
    retstr->data_type = data_type;
    *(_QWORD *)&retstr->data_scale = 1065353216;
  }
}

BNNSVectorDescriptor __swiftcall BNNSVectorDescriptor.init(size:data_type:)(Swift::Int size, BNNSDataType data_type)
{
  float v2;
  float v3;
  BNNSVectorDescriptor result;

  if (data_type == BNNSDataTypeIndexed8)
  {
    __break(1u);
  }
  else
  {
    v2 = 1.0;
    v3 = 0.0;
  }
  result.size = size;
  result.data_bias = v3;
  result.data_scale = v2;
  result.data_type = data_type;
  return result;
}

uint64_t BNNSLayerData.init(data:data_type:data_scale:data_bias:)@<X0>(uint64_t result@<X0>, int a2@<W1>, uint64_t a3@<X8>, float a4@<S0>, float a5@<S1>)
{
  if (a2 == 524296)
  {
    __break(1u);
  }
  else
  {
    *(_QWORD *)a3 = result;
    *(_DWORD *)(a3 + 8) = a2;
    *(float *)(a3 + 12) = a4;
    *(float *)(a3 + 16) = a5;
    *(_QWORD *)(a3 + 24) = 0;
  }
  return result;
}

void static BNNSLayerData.zero.getter(uint64_t a1@<X8>)
{
  *(_QWORD *)(a1 + 24) = 0;
  *(_QWORD *)a1 = 0;
  *(_QWORD *)(a1 + 8) = 0;
  *(_DWORD *)(a1 + 16) = 0;
}

double static BNNSLayerData.indexed8(data:data_table:)@<D0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  double result;

  *(_QWORD *)a3 = a1;
  *(_DWORD *)(a3 + 8) = 524296;
  *(_QWORD *)&result = 1065353216;
  *(_QWORD *)(a3 + 12) = 1065353216;
  *(_QWORD *)(a3 + 24) = a2;
  return result;
}

int32x2_t static BNNSActivation.integerLinearSaturate(scale:offset:shift:)@<D0>(int a1@<W0>, int a2@<W1>, int a3@<W2>, uint64_t a4@<X8>)
{
  int32x2_t result;

  *(_DWORD *)a4 = 9;
  result = vdup_n_s32(0x7FC00000u);
  *(int32x2_t *)(a4 + 4) = result;
  *(_DWORD *)(a4 + 12) = a1;
  *(_DWORD *)(a4 + 16) = a2;
  *(_DWORD *)(a4 + 20) = a3;
  *(_QWORD *)(a4 + 32) = 0;
  *(_QWORD *)(a4 + 40) = 0;
  *(_QWORD *)(a4 + 24) = 0;
  return result;
}

double static BNNSActivation.integerLinearSaturatePerChannel(scale:offset:shift:)@<D0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X8>)
{
  double result;

  *(_DWORD *)a4 = 10;
  *(int32x2_t *)(a4 + 4) = vdup_n_s32(0x7FC00000u);
  *(_QWORD *)&result = 1;
  *(_QWORD *)(a4 + 12) = 1;
  *(_DWORD *)(a4 + 20) = 0;
  *(_QWORD *)(a4 + 24) = a1;
  *(_QWORD *)(a4 + 32) = a2;
  *(_QWORD *)(a4 + 40) = a3;
  return result;
}

void __swiftcall BNNSConvolutionLayerParameters.init(x_stride:y_stride:x_padding:y_padding:k_width:k_height:in_channels:out_channels:weights:)(BNNSConvolutionLayerParameters *__return_ptr retstr, Swift::Int x_stride, Swift::Int y_stride, Swift::Int x_padding, Swift::Int y_padding, Swift::Int k_width, Swift::Int k_height, Swift::Int in_channels, Swift::Int out_channels, BNNSLayerData *weights)
{
  const void *data;
  BNNSDataType data_type;
  const float *data_table;

  data = weights->data;
  data_type = weights->data_type;
  data_table = weights->data_table;
  retstr->x_stride = x_stride;
  retstr->y_stride = y_stride;
  retstr->x_padding = x_padding;
  retstr->y_padding = y_padding;
  retstr->k_width = k_width;
  retstr->k_height = k_height;
  retstr->in_channels = in_channels;
  retstr->out_channels = out_channels;
  retstr->weights.data = data;
  retstr->weights.data_type = data_type;
  *(_QWORD *)&retstr->weights.data_scale = *(_QWORD *)&weights->data_scale;
  retstr->bias.data_table = 0;
  retstr->activation.function = BNNSActivationFunctionIdentity;
  retstr->bias.data = 0;
  *(_QWORD *)&retstr->bias.data_type = 0;
  retstr->weights.data_table = data_table;
  retstr->bias.data_bias = 0.0;
  *(int32x2_t *)&retstr->activation.alpha = vdup_n_s32(0x7FC00000u);
  retstr->activation.iscale = 1;
  *(_OWORD *)&retstr->activation.ioffset = 0u;
  *(_OWORD *)&retstr->activation.ioffset_per_channel = 0u;
}

void __swiftcall BNNSPoolingLayerParameters.init(x_stride:y_stride:x_padding:y_padding:k_width:k_height:in_channels:out_channels:pooling_function:)(BNNSPoolingLayerParameters *__return_ptr retstr, Swift::Int x_stride, Swift::Int y_stride, Swift::Int x_padding, Swift::Int y_padding, Swift::Int k_width, Swift::Int k_height, Swift::Int in_channels, Swift::Int out_channels, BNNSPoolingFunction pooling_function)
{
  retstr->x_stride = x_stride;
  retstr->y_stride = y_stride;
  retstr->x_padding = x_padding;
  retstr->y_padding = y_padding;
  retstr->k_width = k_width;
  retstr->k_height = k_height;
  retstr->in_channels = in_channels;
  retstr->out_channels = out_channels;
  retstr->pooling_function = pooling_function;
  retstr->bias.data_table = 0;
  retstr->activation.function = BNNSActivationFunctionIdentity;
  retstr->bias.data = 0;
  *(_QWORD *)&retstr->bias.data_type = 0;
  retstr->bias.data_bias = 0.0;
  *(int32x2_t *)&retstr->activation.alpha = vdup_n_s32(0x7FC00000u);
  retstr->activation.iscale = 1;
  *(_OWORD *)&retstr->activation.ioffset = 0u;
  *(_OWORD *)&retstr->activation.ioffset_per_channel = 0u;
}

void __swiftcall BNNSFullyConnectedLayerParameters.init(in_size:out_size:weights:)(BNNSFullyConnectedLayerParameters *__return_ptr retstr, Swift::Int in_size, Swift::Int out_size, BNNSLayerData *weights)
{
  const void *data;
  BNNSDataType data_type;
  const float *data_table;

  data = weights->data;
  data_type = weights->data_type;
  data_table = weights->data_table;
  retstr->in_size = in_size;
  retstr->out_size = out_size;
  retstr->weights.data = data;
  retstr->weights.data_type = data_type;
  *(_QWORD *)&retstr->weights.data_scale = *(_QWORD *)&weights->data_scale;
  retstr->bias.data_table = 0;
  retstr->activation.function = BNNSActivationFunctionIdentity;
  retstr->bias.data = 0;
  *(_QWORD *)&retstr->bias.data_type = 0;
  retstr->weights.data_table = data_table;
  retstr->bias.data_bias = 0.0;
  *(int32x2_t *)&retstr->activation.alpha = vdup_n_s32(0x7FC00000u);
  retstr->activation.iscale = 1;
  *(_OWORD *)&retstr->activation.ioffset = 0u;
  *(_OWORD *)&retstr->activation.ioffset_per_channel = 0u;
}

uint64_t AccelerateBuffer<>.withUnsafeBufferPointer<A>(_:)@<X0>(uint64_t a1@<X3>, uint64_t a2@<X8>)
{
  uint64_t v2;
  uint64_t v5;
  uint64_t v6;
  char *v7;
  uint64_t result;
  uint64_t v9;
  uint64_t v10;

  v5 = type metadata accessor for Optional();
  MEMORY[0x1E0C80A78](v5);
  v7 = (char *)&v10 - v6;
  result = dispatch thunk of Sequence.withContiguousStorageIfAvailable<A>(_:)();
  if (!v2)
  {
    v9 = *(_QWORD *)(a1 - 8);
    result = (*(uint64_t (**)(char *, uint64_t, uint64_t))(v9 + 48))(v7, 1, a1);
    if ((_DWORD)result == 1)
      __break(1u);
    else
      return (*(uint64_t (**)(uint64_t, char *, uint64_t))(v9 + 32))(a2, v7, a1);
  }
  return result;
}

uint64_t AccelerateMutableBuffer<>.withUnsafeMutableBufferPointer<A>(_:)@<X0>(uint64_t a1@<X3>, uint64_t a2@<X8>)
{
  uint64_t v2;
  uint64_t v5;
  uint64_t v6;
  char *v7;
  uint64_t result;
  uint64_t v9;
  uint64_t v10;

  v5 = type metadata accessor for Optional();
  MEMORY[0x1E0C80A78](v5);
  v7 = (char *)&v10 - v6;
  result = dispatch thunk of MutableCollection.withContiguousMutableStorageIfAvailable<A>(_:)();
  if (!v2)
  {
    v9 = *(_QWORD *)(a1 - 8);
    result = (*(uint64_t (**)(char *, uint64_t, uint64_t))(v9 + 48))(v7, 1, a1);
    if ((_DWORD)result == 1)
      __break(1u);
    else
      return (*(uint64_t (**)(uint64_t, char *, uint64_t))(v9 + 32))(a2, v7, a1);
  }
  return result;
}

_QWORD *protocol witness for AccelerateMutableBuffer.withUnsafeMutableBufferPointer<A>(_:) in conformance [A](void (*a1)(_QWORD *), uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t *v4;
  uint64_t v6;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  _QWORD v16[2];
  uint64_t v17;

  v17 = a3;
  v6 = (uint64_t)v4;
  Array._makeMutableAndUnique()();
  v8 = *v4;
  v9 = *(_QWORD *)(a4 + 16);
  if ((_swift_isClassOrObjCExistentialType() & 1) != 0)
    v10 = v8 & 0xFFFFFFFFFFFFFF8;
  else
    v10 = v8;
  v11 = *(_QWORD *)(v10 + 16);
  if ((_swift_isClassOrObjCExistentialType() & 1) != 0)
    v12 = v8 & 0xFFFFFFFFFFFFFF8;
  else
    v12 = v8;
  v13 = *(unsigned __int8 *)(*(_QWORD *)(v9 - 8) + 80);
  v14 = v12 + ((v13 + 32) & ~v13);
  v16[0] = v14;
  v16[1] = v11;
  a1(v16);
  return $defer #1 <A><A1>() in Array.withUnsafeMutableBufferPointer<A>(_:)(v16, v14, v11, v6, v9, v17, (uint64_t (*)(_QWORD, uint64_t))MEMORY[0x1E0DEAEC8]);
}

void protocol witness for AccelerateBuffer.count.getter in conformance [A]()
{
  JUMPOUT(0x1D1794114);
}

uint64_t protocol witness for AccelerateBuffer.withUnsafeBufferPointer<A>(_:) in conformance [A]()
{
  return Array.withUnsafeBufferPointer<A>(_:)();
}

_QWORD *protocol witness for AccelerateMutableBuffer.withUnsafeMutableBufferPointer<A>(_:) in conformance ContiguousArray<A>(void (*a1)(_QWORD *), uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  _QWORD v12[2];
  uint64_t v13;

  v13 = a3;
  ContiguousArray._makeMutableAndUnique()();
  v7 = *(_QWORD *)(*(_QWORD *)v4 + 16);
  v8 = *(_QWORD *)(a4 + 16);
  v9 = *(unsigned __int8 *)(*(_QWORD *)(v8 - 8) + 80);
  v12[0] = *(_QWORD *)v4 + ((v9 + 32) & ~v9);
  v10 = v12[0];
  v12[1] = v7;
  a1(v12);
  return $defer #1 <A><A1>() in Array.withUnsafeMutableBufferPointer<A>(_:)(v12, v10, v7, v4, v8, v13, (uint64_t (*)(_QWORD, uint64_t))MEMORY[0x1E0DEC708]);
}

uint64_t protocol witness for AccelerateBuffer.count.getter in conformance ContiguousArray<A>()
{
  return ContiguousArray.count.getter();
}

uint64_t protocol witness for AccelerateBuffer.withUnsafeBufferPointer<A>(_:) in conformance ContiguousArray<A>()
{
  return ContiguousArray.withUnsafeBufferPointer<A>(_:)();
}

_QWORD *protocol witness for AccelerateMutableBuffer.withUnsafeMutableBufferPointer<A>(_:) in conformance ArraySlice<A>(void (*a1)(_QWORD *), uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4;
  uint64_t v6;
  Swift::Int v7;
  uint64_t v8;
  _QWORD v10[4];

  v10[3] = a3;
  v6 = *(_QWORD *)(a4 + 16);
  v7 = ArraySlice._getCount()();
  ArraySlice._makeMutableAndUnique()();
  v10[0] = *(_QWORD *)(v4 + 8) + *(_QWORD *)(*(_QWORD *)(v6 - 8) + 72) * *(_QWORD *)(v4 + 16);
  v8 = v10[0];
  v10[1] = v7;
  a1(v10);
  return $defer #1 <A><A1>() in ArraySlice.withUnsafeMutableBufferPointer<A>(_:)(v10, v8, v7);
}

uint64_t protocol witness for AccelerateBuffer.count.getter in conformance ArraySlice<A>()
{
  return ArraySlice.count.getter();
}

uint64_t protocol witness for AccelerateBuffer.withUnsafeBufferPointer<A>(_:) in conformance ArraySlice<A>()
{
  return ArraySlice.withUnsafeBufferPointer<A>(_:)();
}

uint64_t protocol witness for AccelerateBuffer.withUnsafeBufferPointer<A>(_:) in conformance UnsafeBufferPointer<A>@<X0>(uint64_t a1@<X2>, uint64_t a2@<X3>, uint64_t a3@<X8>)
{
  MEMORY[0x1D1794D08](MEMORY[0x1E0DEA5E8], a2);
  return AccelerateBuffer<>.withUnsafeBufferPointer<A>(_:)(a1, a3);
}

uint64_t protocol witness for AccelerateMutableBuffer.withUnsafeMutableBufferPointer<A>(_:) in conformance UnsafeMutableBufferPointer<A>@<X0>(uint64_t a1@<X2>, uint64_t a2@<X3>, uint64_t a3@<X8>)
{
  MEMORY[0x1D1794D08](MEMORY[0x1E0DEB9E0], a2);
  return AccelerateMutableBuffer<>.withUnsafeMutableBufferPointer<A>(_:)(a1, a3);
}

uint64_t protocol witness for AccelerateBuffer.withUnsafeBufferPointer<A>(_:) in conformance UnsafeMutableBufferPointer<A>@<X0>(uint64_t a1@<X2>, uint64_t a2@<X3>, uint64_t a3@<X8>)
{
  MEMORY[0x1D1794D08](MEMORY[0x1E0DEB9F0], a2);
  return AccelerateBuffer<>.withUnsafeBufferPointer<A>(_:)(a1, a3);
}

uint64_t protocol witness for AccelerateBuffer.count.getter in conformance <> Slice<A>(uint64_t a1)
{
  MEMORY[0x1D1794D08](MEMORY[0x1E0DEDE48], a1);
  return Collection.count.getter();
}

uint64_t protocol witness for AccelerateBuffer.withUnsafeBufferPointer<A>(_:) in conformance <> Slice<A>@<X0>(uint64_t a1@<X2>, uint64_t a2@<X3>, uint64_t a3@<X8>)
{
  MEMORY[0x1D1794D08](MEMORY[0x1E0DEDE48], a2);
  return AccelerateBuffer<>.withUnsafeBufferPointer<A>(_:)(a1, a3);
}

uint64_t protocol witness for AccelerateMutableBuffer.withUnsafeMutableBufferPointer<A>(_:) in conformance <> Slice<A>@<X0>(uint64_t a1@<X2>, uint64_t a2@<X3>, uint64_t a3@<X4>, uint64_t a4@<X8>)
{
  uint64_t v7;

  v7 = *(_QWORD *)(a3 - 16);
  MEMORY[0x1D1794D08](MEMORY[0x1E0DEDE30], a2, &v7);
  return AccelerateMutableBuffer<>.withUnsafeMutableBufferPointer<A>(_:)(a1, a4);
}

uint64_t AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:));
}

{
  return AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:));
}

{
  return AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:));
}

{
  return AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:));
}

uint64_t closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(uint64_t result, void (*a2)(uint64_t *))
{
  uint64_t v2;
  uint64_t v4;
  uint64_t v5;

  v2 = *(_QWORD *)result;
  if (*(_QWORD *)result)
  {
    v4 = *(_QWORD *)(result + 8);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    result = swift_allocObject();
    *(_OWORD *)(result + 16) = xmmword_1CAB5E430;
    if ((v4 & 0x8000000000000000) == 0)
    {
      *(_QWORD *)(result + 32) = v2;
      *(_QWORD *)(result + 40) = 1;
      *(_QWORD *)(result + 48) = v4;
      *(_QWORD *)(result + 56) = v4;
      *(_QWORD *)(result + 64) = 0;
      v5 = result;
      a2(&v5);
      return swift_bridgeObjectRelease();
    }
    __break(1u);
  }
  __break(1u);
  return result;
}

{
  uint64_t v2;
  uint64_t v3;
  uint64_t v5;

  v2 = *(_QWORD *)result;
  if (*(_QWORD *)result)
  {
    v3 = *(_QWORD *)(result + 8);
    if ((unint64_t)(v3 - 0x2000000000000000) >> 62 == 3)
    {
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
      result = swift_allocObject();
      *(_OWORD *)(result + 16) = xmmword_1CAB5E430;
      if ((v3 & 0x8000000000000000) == 0)
      {
        *(_QWORD *)(result + 32) = v2;
        *(_QWORD *)(result + 40) = 1;
        *(_QWORD *)(result + 48) = v3;
        *(_QWORD *)(result + 56) = 4 * v3;
        *(_QWORD *)(result + 64) = 0;
        v5 = result;
        a2(&v5);
        return swift_bridgeObjectRelease();
      }
    }
    else
    {
      __break(1u);
    }
    __break(1u);
  }
  __break(1u);
  return result;
}

{
  uint64_t v2;
  uint64_t v3;
  uint64_t v5;

  v2 = *(_QWORD *)result;
  if (*(_QWORD *)result)
  {
    v3 = *(_QWORD *)(result + 8);
    if (v3 + 0x4000000000000000 < 0)
    {
      __break(1u);
    }
    else
    {
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
      result = swift_allocObject();
      *(_OWORD *)(result + 16) = xmmword_1CAB5E430;
      if ((v3 & 0x8000000000000000) == 0)
      {
        *(_QWORD *)(result + 32) = v2;
        *(_QWORD *)(result + 40) = 1;
        *(_QWORD *)(result + 48) = v3;
        *(_QWORD *)(result + 56) = 2 * v3;
        *(_QWORD *)(result + 64) = 0;
        v5 = result;
        a2(&v5);
        return swift_bridgeObjectRelease();
      }
    }
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(uint64_t a1)
{
  return partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(a1, (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:));
}

{
  return partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(a1);
}

{
  uint64_t v1;

  return closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(a1, *(void (**)(uint64_t *))(v1 + 40));
}

{
  return partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(a1, (uint64_t (*)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:));
}

uint64_t partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(uint64_t a1, uint64_t (*a2)(uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD))
{
  _QWORD *v2;

  return a2(a1, v2[5], v2[6], v2[2], v2[3], v2[4]);
}

uint64_t AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  _QWORD v7[8];

  v7[2] = a3;
  v7[3] = a4;
  v7[4] = a5;
  v7[5] = a1;
  v7[6] = a2;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(a5 + 16))(a6, v7, a4, a3);
}

Accelerate::AccelerateMatrixOrder_optional __swiftcall AccelerateMatrixOrder.init(rawValue:)(Swift::Int rawValue)
{
  Accelerate::AccelerateMatrixOrder_optional v1;

  if (rawValue == 102)
    v1.value = Accelerate_AccelerateMatrixOrder_columnMajor;
  else
    v1.value = Accelerate_AccelerateMatrixOrder_unknownDefault;
  if (rawValue == 101)
    return 0;
  else
    return v1;
}

uint64_t AccelerateMatrixOrder.rawValue.getter(char a1)
{
  return (a1 & 1u) + 101;
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance AccelerateMatrixOrder()
{
  unsigned __int8 *v0;
  int v1;
  Swift::UInt v2;

  v1 = *v0;
  Hasher.init(_seed:)();
  if (v1)
    v2 = 102;
  else
    v2 = 101;
  Hasher._combine(_:)(v2);
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance AccelerateMatrixOrder()
{
  _BYTE *v0;
  Swift::UInt v1;

  if (*v0)
    v1 = 102;
  else
    v1 = 101;
  Hasher._combine(_:)(v1);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance AccelerateMatrixOrder()
{
  unsigned __int8 *v0;
  int v1;
  Swift::UInt v2;

  v1 = *v0;
  Hasher.init(_seed:)();
  if (v1)
    v2 = 102;
  else
    v2 = 101;
  Hasher._combine(_:)(v2);
  return Hasher._finalize()();
}

_QWORD *protocol witness for RawRepresentable.init(rawValue:) in conformance AccelerateMatrixOrder@<X0>(_QWORD *result@<X0>, char *a2@<X8>)
{
  char v2;
  char v3;

  if (*result == 102)
    v2 = 1;
  else
    v2 = 2;
  if (*result == 101)
    v3 = 0;
  else
    v3 = v2;
  *a2 = v3;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance AccelerateMatrixOrder(uint64_t *a1@<X8>)
{
  _BYTE *v1;
  uint64_t v2;

  v2 = 101;
  if (*v1)
    v2 = 102;
  *a1 = v2;
}

uint64_t vImage.PixelBuffer<>.rowCount.getter()
{
  return vImage.PixelBuffer.height.getter();
}

uint64_t vImage.PixelBuffer<>.columnCount.getter()
{
  return vImage.PixelBuffer.width.getter();
}

uint64_t vImage.PixelBuffer<>.accelerateMatrixOrder.getter()
{
  return 0;
}

uint64_t vImage.PixelBuffer<>.leadingDimension.getter(uint64_t a1, uint64_t a2)
{
  return vImage.PixelBuffer<>.rowStride.getter(a1, a2);
}

uint64_t instantiation function for generic protocol witness table for [A](uint64_t a1)
{
  uint64_t result;

  result = MEMORY[0x1D1794D08](&protocol conformance descriptor for [A]);
  *(_QWORD *)(a1 + 8) = result;
  return result;
}

uint64_t instantiation function for generic protocol witness table for ContiguousArray<A>(uint64_t a1)
{
  uint64_t result;

  result = MEMORY[0x1D1794D08](&protocol conformance descriptor for ContiguousArray<A>);
  *(_QWORD *)(a1 + 8) = result;
  return result;
}

uint64_t instantiation function for generic protocol witness table for ArraySlice<A>(uint64_t a1)
{
  uint64_t result;

  result = MEMORY[0x1D1794D08](&protocol conformance descriptor for ArraySlice<A>);
  *(_QWORD *)(a1 + 8) = result;
  return result;
}

uint64_t instantiation function for generic protocol witness table for UnsafeMutableBufferPointer<A>(uint64_t a1)
{
  uint64_t result;

  result = MEMORY[0x1D1794D08](&protocol conformance descriptor for UnsafeMutableBufferPointer<A>);
  *(_QWORD *)(a1 + 8) = result;
  return result;
}

uint64_t instantiation function for generic protocol witness table for <> Slice<A>(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t result;
  uint64_t v5;

  v5 = *(_QWORD *)(*(_QWORD *)a3 + 8);
  result = MEMORY[0x1D1794D08](&protocol conformance descriptor for <> Slice<A>, a2, &v5);
  *(_QWORD *)(a1 + 8) = result;
  return result;
}

unint64_t lazy protocol witness table accessor for type AccelerateMatrixOrder and conformance AccelerateMatrixOrder()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type AccelerateMatrixOrder and conformance AccelerateMatrixOrder;
  if (!lazy protocol witness table cache variable for type AccelerateMatrixOrder and conformance AccelerateMatrixOrder)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for AccelerateMatrixOrder, &type metadata for AccelerateMatrixOrder);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type AccelerateMatrixOrder and conformance AccelerateMatrixOrder);
  }
  return result;
}

uint64_t protocol witness for AccelerateMatrixBuffer.leadingDimension.getter in conformance <> vImage.PixelBuffer<A>(uint64_t a1, uint64_t a2)
{
  return vImage.PixelBuffer<>.leadingDimension.getter(a1, *(_QWORD *)(a2 - 8));
}

uint64_t instantiation function for generic protocol witness table for <> vImage.PixelBuffer<A>(uint64_t a1, uint64_t a2, uint64_t *a3)
{
  uint64_t result;
  uint64_t v5;

  v5 = *a3;
  result = MEMORY[0x1D1794D08](&protocol conformance descriptor for <> vImage.PixelBuffer<A>, a2, &v5);
  *(_QWORD *)(a1 + 8) = result;
  return result;
}

{
  uint64_t result;
  uint64_t v5;

  v5 = *a3;
  result = MEMORY[0x1D1794D08](&protocol conformance descriptor for <> vImage.PixelBuffer<A>, a2, &v5);
  *(_QWORD *)(a1 + 8) = result;
  return result;
}

uint64_t dispatch thunk of AccelerateBuffer.withUnsafeBufferPointer<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return (*(uint64_t (**)(void))(a5 + 24))();
}

uint64_t dispatch thunk of AccelerateMutableBuffer.withUnsafeMutableBufferPointer<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return (*(uint64_t (**)(void))(a5 + 16))();
}

uint64_t storeEnumTagSinglePayload for AccelerateMatrixOrder(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 1 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 1) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFF)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFE)
    return ((uint64_t (*)(void))((char *)&loc_1CAB35324 + 4 * byte_1CAB61BC5[v4]))();
  *a1 = a2 + 1;
  return ((uint64_t (*)(void))((char *)sub_1CAB35358 + 4 * byte_1CAB61BC0[v4]))();
}

uint64_t sub_1CAB35358(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB35360(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB35368);
  return result;
}

uint64_t sub_1CAB35374(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB3537CLL);
  *(_BYTE *)result = a2 + 1;
  return result;
}

uint64_t sub_1CAB35380(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB35388(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for AccelerateMatrixOrder()
{
  return &type metadata for AccelerateMatrixOrder;
}

uint64_t dispatch thunk of AccelerateMatrixBuffer.rowCount.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

uint64_t dispatch thunk of AccelerateMatrixBuffer.columnCount.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 24))();
}

uint64_t dispatch thunk of AccelerateMatrixBuffer.accelerateMatrixOrder.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 32))() & 1;
}

uint64_t dispatch thunk of AccelerateMatrixBuffer.leadingDimension.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 40))();
}

uint64_t dispatch thunk of AccelerateMatrixBuffer.withUnsafeBufferPointer<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return (*(uint64_t (**)(void))(a5 + 48))();
}

_QWORD *$defer #1 <A><A1>() in Array.withUnsafeMutableBufferPointer<A>(_:)(_QWORD *result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(_QWORD, uint64_t))
{
  if (*result)
  {
    if (*result == a2)
    {
      if (result[1] == a3)
        return (_QWORD *)a7(0, a5);
    }
    else
    {
      __break(1u);
    }
    __break(1u);
  }
  __break(1u);
  return result;
}

_QWORD *$defer #1 <A><A1>() in ArraySlice.withUnsafeMutableBufferPointer<A>(_:)(_QWORD *result, uint64_t a2, uint64_t a3)
{
  if (*result)
  {
    if (*result == a2)
    {
      if (result[1] == a3)
        return (_QWORD *)type metadata accessor for ArraySlice();
    }
    else
    {
      __break(1u);
    }
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t dispatch thunk of AccelerateMutableMatrixBuffer.withUnsafeMutableBufferPointer<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return dispatch thunk of AccelerateMutableBuffer.withUnsafeMutableBufferPointer<A>(_:)(a1, a2, a3, a4, a5);
}

uint64_t BNNSNDArrayDescriptor.shape.getter@<X0>(uint64_t a1@<X8>)
{
  uint64_t v1;
  int v3;
  __int128 v4;
  __int128 v5;
  __int128 v6;
  __int128 v7;
  uint64_t v8;
  uint64_t v9;
  __int128 v10;
  uint64_t v11;
  uint64_t v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  uint64_t v17;
  __int128 v18;
  __int128 v19;
  uint64_t v20;
  uint64_t v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  uint64_t v25;
  uint64_t v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  uint64_t v30;
  uint64_t v31;
  __int128 v32;
  __int128 v33;
  uint64_t v34;
  uint64_t v35;
  __int128 v36;
  uint64_t v37;
  uint64_t v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  uint64_t result;
  Swift::String v52;
  __int128 v53;
  _BYTE v54[96];
  __int128 v55;

  v3 = *(_DWORD *)(v1 + 4);
  if (v3 < 0x40000)
  {
    if (v3 >= 196608)
    {
      if (v3 <= 196609)
      {
        if (v3 == 196608)
        {
          v20 = *(_QWORD *)(v1 + 24);
          v21 = *(_QWORD *)(v1 + 88);
          v53 = *(_OWORD *)(v1 + 8);
          *(_QWORD *)v54 = v20;
          *(_OWORD *)&v54[8] = *(_OWORD *)(v1 + 72);
          *(_QWORD *)&v54[24] = v21;
          _s10Accelerate4BNNSO5ShapeOWOi4_((uint64_t)&v53);
        }
        else
        {
          v42 = *(_QWORD *)(v1 + 24);
          v43 = *(_QWORD *)(v1 + 88);
          v53 = *(_OWORD *)(v1 + 8);
          *(_QWORD *)v54 = v42;
          *(_OWORD *)&v54[8] = *(_OWORD *)(v1 + 72);
          *(_QWORD *)&v54[24] = v43;
          _s10Accelerate4BNNSO5ShapeOWOi7_((uint64_t)&v53);
        }
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      }
      switch(v3)
      {
        case 196610:
          v25 = *(_QWORD *)(v1 + 24);
          v26 = *(_QWORD *)(v1 + 88);
          v53 = *(_OWORD *)(v1 + 8);
          *(_QWORD *)v54 = v25;
          *(_OWORD *)&v54[8] = *(_OWORD *)(v1 + 72);
          *(_QWORD *)&v54[24] = v26;
          _s10Accelerate4BNNSO5ShapeOWOi8_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
        case 229376:
          v11 = *(_QWORD *)(v1 + 24);
          v12 = *(_QWORD *)(v1 + 88);
          v53 = *(_OWORD *)(v1 + 8);
          *(_QWORD *)v54 = v11;
          *(_OWORD *)&v54[8] = *(_OWORD *)(v1 + 72);
          *(_QWORD *)&v54[24] = v12;
          _s10Accelerate4BNNSO5ShapeOWOi6_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
        case 229377:
          v37 = *(_QWORD *)(v1 + 24);
          v38 = *(_QWORD *)(v1 + 88);
          v53 = *(_OWORD *)(v1 + 8);
          *(_QWORD *)v54 = v37;
          *(_OWORD *)&v54[8] = *(_OWORD *)(v1 + 72);
          *(_QWORD *)&v54[24] = v38;
          _s10Accelerate4BNNSO5ShapeOWOi5_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      }
    }
    else if (v3 <= 0x20000)
    {
      if (v3 == 0x10000)
      {
        v17 = *(_QWORD *)(v1 + 72);
        *(_QWORD *)&v53 = *(_QWORD *)(v1 + 8);
        *((_QWORD *)&v53 + 1) = v17;
        _s10Accelerate4BNNSO5ShapeOWOi_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      }
      if (v3 == 0x20000)
      {
        v39 = *(_OWORD *)(v1 + 72);
        v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)v54 = v39;
        _s10Accelerate4BNNSO5ShapeOWOi1_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      }
    }
    else
    {
      switch(v3)
      {
        case 131073:
          v22 = *(_OWORD *)(v1 + 72);
          v53 = *(_OWORD *)(v1 + 8);
          *(_OWORD *)v54 = v22;
          _s10Accelerate4BNNSO5ShapeOWOi0_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
        case 163840:
          v7 = *(_OWORD *)(v1 + 72);
          v53 = *(_OWORD *)(v1 + 8);
          *(_OWORD *)v54 = v7;
          _s10Accelerate4BNNSO5ShapeOWOi3_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
        case 163841:
          v33 = *(_OWORD *)(v1 + 72);
          v53 = *(_OWORD *)(v1 + 8);
          *(_OWORD *)v54 = v33;
          _s10Accelerate4BNNSO5ShapeOWOi2_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      }
    }
  }
  else if (v3 < 425984)
  {
    if (v3 <= 294912)
    {
      if (v3 == 0x40000)
      {
        v18 = *(_OWORD *)(v1 + 24);
        v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)v54 = v18;
        v19 = *(_OWORD *)(v1 + 88);
        *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 72);
        *(_OWORD *)&v54[32] = v19;
        _s10Accelerate4BNNSO5ShapeOWOi9_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      }
      if (v3 == 294912)
      {
        v40 = *(_OWORD *)(v1 + 24);
        v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)v54 = v40;
        v41 = *(_OWORD *)(v1 + 88);
        *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 72);
        *(_OWORD *)&v54[32] = v41;
        _s10Accelerate4BNNSO5ShapeOWOi11_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      }
    }
    else
    {
      switch(v3)
      {
        case 294913:
          v23 = *(_OWORD *)(v1 + 24);
          v53 = *(_OWORD *)(v1 + 8);
          *(_OWORD *)v54 = v23;
          v24 = *(_OWORD *)(v1 + 88);
          *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 72);
          *(_OWORD *)&v54[32] = v24;
          _s10Accelerate4BNNSO5ShapeOWOi10_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
        case 360448:
          v8 = *(_QWORD *)(v1 + 40);
          v9 = *(_QWORD *)(v1 + 104);
          v10 = *(_OWORD *)(v1 + 24);
          v53 = *(_OWORD *)(v1 + 8);
          *(_OWORD *)v54 = v10;
          *(_QWORD *)&v54[16] = v8;
          *(_OWORD *)&v54[24] = *(_OWORD *)(v1 + 72);
          *(_OWORD *)&v54[40] = *(_OWORD *)(v1 + 88);
          *(_QWORD *)&v54[56] = v9;
          _s10Accelerate4BNNSO5ShapeOWOi13_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
        case 360449:
          v34 = *(_QWORD *)(v1 + 40);
          v35 = *(_QWORD *)(v1 + 104);
          v36 = *(_OWORD *)(v1 + 24);
          v53 = *(_OWORD *)(v1 + 8);
          *(_OWORD *)v54 = v36;
          *(_QWORD *)&v54[16] = v34;
          *(_OWORD *)&v54[24] = *(_OWORD *)(v1 + 72);
          *(_OWORD *)&v54[40] = *(_OWORD *)(v1 + 88);
          *(_QWORD *)&v54[56] = v35;
          _s10Accelerate4BNNSO5ShapeOWOi12_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      }
    }
  }
  else if (v3 > 491520)
  {
    switch(v3)
    {
      case 491521:
        v30 = *(_QWORD *)(v1 + 56);
        v31 = *(_QWORD *)(v1 + 120);
        v32 = *(_OWORD *)(v1 + 24);
        v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)v54 = v32;
        *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 40);
        *(_QWORD *)&v54[32] = v30;
        *(_OWORD *)&v54[40] = *(_OWORD *)(v1 + 72);
        *(_OWORD *)&v54[56] = *(_OWORD *)(v1 + 88);
        *(_OWORD *)&v54[72] = *(_OWORD *)(v1 + 104);
        *(_QWORD *)&v54[88] = v31;
        _s10Accelerate4BNNSO5ShapeOWOi16_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      case 557056:
        v13 = *(_OWORD *)(v1 + 24);
        v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)v54 = v13;
        v14 = *(_OWORD *)(v1 + 56);
        *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 40);
        *(_OWORD *)&v54[32] = v14;
        v15 = *(_OWORD *)(v1 + 88);
        *(_OWORD *)&v54[48] = *(_OWORD *)(v1 + 72);
        *(_OWORD *)&v54[64] = v15;
        v16 = *(_OWORD *)(v1 + 120);
        *(_OWORD *)&v54[80] = *(_OWORD *)(v1 + 104);
        v55 = v16;
        _s10Accelerate4BNNSO5ShapeOWOi19_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      case 557057:
        v47 = *(_OWORD *)(v1 + 24);
        v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)v54 = v47;
        v48 = *(_OWORD *)(v1 + 56);
        *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 40);
        *(_OWORD *)&v54[32] = v48;
        v49 = *(_OWORD *)(v1 + 88);
        *(_OWORD *)&v54[48] = *(_OWORD *)(v1 + 72);
        *(_OWORD *)&v54[64] = v49;
        v50 = *(_OWORD *)(v1 + 120);
        *(_OWORD *)&v54[80] = *(_OWORD *)(v1 + 104);
        v55 = v50;
        _s10Accelerate4BNNSO5ShapeOWOi18_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
    }
  }
  else
  {
    switch(v3)
    {
      case 425984:
        v27 = *(_OWORD *)(v1 + 24);
        v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)v54 = v27;
        v28 = *(_OWORD *)(v1 + 72);
        *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 40);
        *(_OWORD *)&v54[32] = v28;
        v29 = *(_OWORD *)(v1 + 104);
        *(_OWORD *)&v54[48] = *(_OWORD *)(v1 + 88);
        *(_OWORD *)&v54[64] = v29;
        _s10Accelerate4BNNSO5ShapeOWOi15_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      case 425985:
        v4 = *(_OWORD *)(v1 + 24);
        v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)v54 = v4;
        v5 = *(_OWORD *)(v1 + 72);
        *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 40);
        *(_OWORD *)&v54[32] = v5;
        v6 = *(_OWORD *)(v1 + 104);
        *(_OWORD *)&v54[48] = *(_OWORD *)(v1 + 88);
        *(_OWORD *)&v54[64] = v6;
        _s10Accelerate4BNNSO5ShapeOWOi14_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      case 491520:
        v44 = *(_QWORD *)(v1 + 56);
        v45 = *(_QWORD *)(v1 + 120);
        v46 = *(_OWORD *)(v1 + 24);
        v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)v54 = v46;
        *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 40);
        *(_QWORD *)&v54[32] = v44;
        *(_OWORD *)&v54[40] = *(_OWORD *)(v1 + 72);
        *(_OWORD *)&v54[56] = *(_OWORD *)(v1 + 88);
        *(_OWORD *)&v54[72] = *(_OWORD *)(v1 + 104);
        *(_QWORD *)&v54[88] = v45;
        _s10Accelerate4BNNSO5ShapeOWOi17_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
    }
  }
  _StringGuts.grow(_:)(18);
  v52._object = (void *)0x80000001CAB65F90;
  v52._countAndFlagsBits = 0xD000000000000010;
  String.append(_:)(v52);
  type metadata accessor for BNNSDataLayout(0);
  _print_unlocked<A, B>(_:_:)();
  result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

__n128 static BNNSNDArrayDescriptor.allocateUninitialized(scalarType:shape:batchSize:)@<Q0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X8>)
{
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __n128 result;
  _OWORD v14[11];
  _BYTE v15[136];

  outlined init with take of BNNS.Shape(a3, (uint64_t)v15);
  (*(void (**)(uint64_t, uint64_t))(a2 + 8))(a1, a2);
  helper #1 <A>(_:) in static BNNSNDArrayDescriptor.allocateUninitialized(scalarType:shape:batchSize:)(a4, (uint64_t)v15, v14);
  v9 = v14[9];
  *(_OWORD *)(a5 + 128) = v14[8];
  *(_OWORD *)(a5 + 144) = v9;
  *(_OWORD *)(a5 + 160) = v14[10];
  v10 = v14[5];
  *(_OWORD *)(a5 + 64) = v14[4];
  *(_OWORD *)(a5 + 80) = v10;
  v11 = v14[7];
  *(_OWORD *)(a5 + 96) = v14[6];
  *(_OWORD *)(a5 + 112) = v11;
  v12 = v14[1];
  *(_OWORD *)a5 = v14[0];
  *(_OWORD *)(a5 + 16) = v12;
  result = (__n128)v14[3];
  *(_OWORD *)(a5 + 32) = v14[2];
  *(__n128 *)(a5 + 48) = result;
  return result;
}

Swift::Void __swiftcall BNNSNDArrayDescriptor.deallocate()()
{
  uint64_t v0;
  uint64_t v1;
  uint64_t v2;

  outlined init with take of BNNSNDArrayDescriptor?(v0 + 136, (uint64_t)&v1, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v1, (uint64_t)&v2, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  if (v2)
    MEMORY[0x1D1794DA4](v2, -1, -1);
}

uint64_t static Float.bnnsDataType.getter()
{
  return 65568;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance Float()
{
  return 65568;
}

uint64_t static Float16.bnnsDataType.getter()
{
  return 65552;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance Float16()
{
  return 65552;
}

uint64_t static Int8.bnnsDataType.getter()
{
  return 131080;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance Int8()
{
  return 131080;
}

uint64_t static Int16.bnnsDataType.getter()
{
  return 131088;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance Int16()
{
  return 131088;
}

uint64_t static Int32.bnnsDataType.getter()
{
  return 131104;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance Int32()
{
  return 131104;
}

uint64_t static UInt8.bnnsDataType.getter()
{
  return 262152;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance UInt8()
{
  return 262152;
}

uint64_t static UInt16.bnnsDataType.getter()
{
  return 262160;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance UInt16()
{
  return 262160;
}

uint64_t static UInt32.bnnsDataType.getter()
{
  return 262176;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance UInt32()
{
  return 262176;
}

uint64_t static Bool.bnnsDataType.getter()
{
  return 1048584;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance Bool()
{
  return 1048584;
}

uint64_t static Int64.bnnsDataType.getter()
{
  return 131136;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance Int64()
{
  return 131136;
}

uint64_t static UInt64.bnnsDataType.getter()
{
  return 262208;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance UInt64()
{
  return 262208;
}

__n128 BNNSNDArrayDescriptor.init(dataType:shape:)@<Q0>(uint64_t a1@<X1>, uint64_t a2@<X8>)
{
  __int128 v3;
  __int128 v4;
  __int128 v5;
  __int128 v6;
  __n128 result;
  _BYTE v8[152];
  __int128 v9;
  __int128 v10;
  __n128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  _BYTE v19[136];

  outlined init with take of BNNS.Shape(a1, (uint64_t)v19);
  outlined init with take of BNNS.Shape((uint64_t)v19, (uint64_t)v8);
  specialized static BNNS.makeArrayDescriptor(shape:data:dataType:)((uint64_t)v8);
  v3 = v17;
  *(_OWORD *)(a2 + 128) = v16;
  *(_OWORD *)(a2 + 144) = v3;
  *(_OWORD *)(a2 + 160) = v18;
  v4 = v13;
  *(_OWORD *)(a2 + 64) = v12;
  *(_OWORD *)(a2 + 80) = v4;
  v5 = v15;
  *(_OWORD *)(a2 + 96) = v14;
  *(_OWORD *)(a2 + 112) = v5;
  v6 = v9;
  *(_OWORD *)a2 = *(_OWORD *)&v8[136];
  *(_OWORD *)(a2 + 16) = v6;
  result = v11;
  *(_OWORD *)(a2 + 32) = v10;
  *(__n128 *)(a2 + 48) = result;
  return result;
}

uint64_t BNNSNDArrayDescriptor.init(data:scalarType:shape:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X8>)
{
  _BYTE v12[184];
  _BYTE v13[136];

  outlined init with take of BNNS.Shape(a5, (uint64_t)v13);
  (*(void (**)(uint64_t, uint64_t))(a4 + 8))(a3, a4);
  helper #1 <A>(_:) in BNNSNDArrayDescriptor.init(data:scalarType:shape:)(a1, a2, (uint64_t)v13, a3, (uint64_t)v12);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v12, a6, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
}

void helper #1 <A>(_:) in BNNSNDArrayDescriptor.init(data:scalarType:shape:)(uint64_t a1@<X1>, uint64_t a2@<X2>, uint64_t a3@<X3>, uint64_t a4@<X5>, uint64_t a5@<X8>)
{
  uint64_t v5;
  uint64_t v6;
  uint64_t v10;
  unint64_t v11;
  unint64_t v12;
  unint64_t v13;
  unint64_t v14;
  unint64_t v15;
  unint64_t v16;
  unint64_t v17;
  unint64_t v18;
  _BYTE v19[192];

  if (a1)
    v5 = a2 - a1;
  else
    v5 = 0;
  v6 = *(_QWORD *)(*(_QWORD *)(a4 - 8) + 72);
  if (!v6)
  {
    __break(1u);
LABEL_14:
    __break(1u);
    return;
  }
  if (v5 == 0x8000000000000000 && v6 == -1)
    goto LABEL_14;
  v10 = v5 / v6;
  outlined init with take of BNNS.Shape(a3, (uint64_t)v19);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape(a3, (uint64_t)v19);
  BNNS.Shape.stride.getter();
  if (v10 == specialized static BNNS.calculateBatchStride(size:stride:)(v11, v12, v13, v14, v15, v16, v17, v18, v11, v12, v13, v14, v15, v16, v17, v18))
  {
    outlined init with take of BNNS.Shape(a3, (uint64_t)v19);
    specialized static BNNS.makeArrayDescriptor(shape:data:dataType:)((uint64_t)v19);
    _sSo21BNNSNDArrayDescriptoraSgWOi_((uint64_t)&v11);
  }
  else
  {
    _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&v11);
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v11, (uint64_t)v19, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v19, a5, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
}

uint64_t BNNSNDArrayDescriptor.init<A>(data:shape:)@<X0>(uint64_t a1@<X1>, uint64_t a2@<X2>, uint64_t a3@<X3>, uint64_t a4@<X4>, uint64_t a5@<X8>)
{
  unint64_t v10;
  unint64_t v11;
  unint64_t v12;
  unint64_t v13;
  unint64_t v14;
  unint64_t v15;
  unint64_t v16;
  unint64_t v17;
  _BYTE v18[184];
  _BYTE v19[144];

  outlined init with take of BNNS.Shape(a2, (uint64_t)v19);
  if (UnsafeBufferPointer.baseAddress.getter()
    && (outlined init with take of BNNS.Shape((uint64_t)v19, (uint64_t)v18),
        BNNS.Shape.size.getter(),
        outlined init with take of BNNS.Shape((uint64_t)v19, (uint64_t)v18),
        BNNS.Shape.stride.getter(),
        specialized static BNNS.calculateBatchStride(size:stride:)(v10, v11, v12, v13, v14, v15, v16, v17, v10, v11, v12, v13, v14, v15, v16, v17) == a1))
  {
    outlined init with take of BNNS.Shape((uint64_t)v19, (uint64_t)v18);
    (*(void (**)(uint64_t, uint64_t))(a4 + 8))(a3, a4);
    specialized static BNNS.makeArrayDescriptor(shape:data:dataType:)((uint64_t)v18);
    _sSo21BNNSNDArrayDescriptoraSgWOi_((uint64_t)&v10);
  }
  else
  {
    _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&v10);
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v10, (uint64_t)v18, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v18, a5, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
}

void helper #1 <A>(_:) in static BNNSNDArrayDescriptor.allocateUninitialized(scalarType:shape:batchSize:)(uint64_t a1@<X1>, uint64_t a2@<X2>, _OWORD *a3@<X8>)
{
  int64_t v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  unint64_t v12;
  unint64_t v13;
  unint64_t v14;
  unint64_t v15;
  unint64_t v16;
  unint64_t v17;
  unint64_t v18;
  unint64_t v19;
  _OWORD v20[11];

  outlined init with take of BNNS.Shape(a2, (uint64_t)v20);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape(a2, (uint64_t)v20);
  BNNS.Shape.stride.getter();
  v6 = specialized static BNNS.calculateBatchStride(size:stride:)(v12, v13, v14, v15, v16, v17, v18, v19, v12, v13, v14, v15, v16, v17, v18, v19);
  if ((unsigned __int128)(a1 * (__int128)v6) >> 64 == (a1 * v6) >> 63)
  {
    static UnsafeMutablePointer.allocate(capacity:)();
    outlined init with take of BNNS.Shape(a2, (uint64_t)&v12);
    specialized static BNNS.makeArrayDescriptor(shape:data:dataType:)((uint64_t)&v12);
    v7 = v20[9];
    a3[8] = v20[8];
    a3[9] = v7;
    a3[10] = v20[10];
    v8 = v20[5];
    a3[4] = v20[4];
    a3[5] = v8;
    v9 = v20[7];
    a3[6] = v20[6];
    a3[7] = v9;
    v10 = v20[1];
    *a3 = v20[0];
    a3[1] = v10;
    v11 = v20[3];
    a3[2] = v20[2];
    a3[3] = v11;
  }
  else
  {
    __break(1u);
  }
}

int64_t static BNNSNDArrayDescriptor.allocate<A>(initializingFrom:shape:batchSize:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, _OWORD *a7@<X8>)
{
  uint64_t v11;
  uint64_t AssociatedTypeWitness;
  uint64_t v13;
  uint64_t v14;
  uint64_t (*v15)(uint64_t, uint64_t);
  unint64_t v16;
  unint64_t v17;
  unint64_t v18;
  unint64_t v19;
  unint64_t v20;
  unint64_t v21;
  unint64_t v22;
  int64_t v23;
  int64_t result;
  uint64_t v25;
  unint64_t v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  char *v37;
  uint64_t v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  uint64_t v44;
  char *v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  int v51;
  unint64_t v52;
  uint64_t v53;
  uint64_t v54;
  unint64_t v55;
  unint64_t v56;
  unint64_t v57;
  unint64_t v58;
  unint64_t v59;
  unint64_t v60;
  unint64_t v61;
  unint64_t v62;
  unint64_t v63;
  unint64_t v64;
  unint64_t v65;
  unint64_t v66;
  unint64_t v67;
  unint64_t v68;
  unint64_t v69;
  unint64_t v70;
  _OWORD v71[11];
  _BYTE v72[144];

  v54 = a3;
  v48 = a1;
  v11 = *(_QWORD *)(a5 + 8);
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v46 = *(_QWORD *)(AssociatedTypeWitness - 8);
  v47 = AssociatedTypeWitness;
  MEMORY[0x1E0C80A78](AssociatedTypeWitness);
  v45 = (char *)&v44 - v13;
  outlined init with take of BNNS.Shape(a2, (uint64_t)v72);
  v49 = v11;
  v50 = a4;
  v14 = swift_getAssociatedTypeWitness();
  v15 = *(uint64_t (**)(uint64_t, uint64_t))(a6 + 8);
  v53 = v14;
  v51 = v15(v14, a6);
  outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)v71);
  BNNS.Shape.size.getter();
  v17 = v63;
  v16 = v64;
  v18 = v65;
  v19 = v66;
  v20 = v67;
  v21 = v68;
  v22 = v69;
  v52 = v70;
  outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)v71);
  BNNS.Shape.stride.getter();
  v23 = specialized static BNNS.calculateBatchStride(size:stride:)(v17, v16, v18, v19, v20, v21, v22, v52, v63, v64, v65, v66, v67, v68, v69, v70);
  result = v54 * v23;
  if ((unsigned __int128)(v54 * (__int128)v23) >> 64 != (v54 * v23) >> 63)
  {
    __break(1u);
    goto LABEL_6;
  }
  v25 = static UnsafeMutablePointer.allocate(capacity:)();
  outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)&v63);
  v44 = v25;
  specialized static BNNS.makeArrayDescriptor(shape:data:dataType:)((uint64_t)&v63);
  outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)&v63);
  BNNS.Shape.size.getter();
  v26 = v55;
  v27 = v56;
  v28 = v57;
  v29 = v58;
  v30 = v59;
  v31 = v60;
  v32 = v61;
  v52 = v62;
  outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)&v63);
  BNNS.Shape.stride.getter();
  result = specialized static BNNS.calculateBatchStride(size:stride:)(v26, v27, v28, v29, v30, v31, v32, v52, v55, v56, v57, v58, v59, v60, v61, v62);
  if ((unsigned __int128)(v54 * (__int128)result) >> 64 != (v54 * result) >> 63)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  v33 = v53;
  v34 = UnsafeMutableBufferPointer.init(start:count:)();
  v36 = v35;
  v37 = v45;
  v38 = UnsafeMutableBufferPointer.initialize<A>(from:)();
  (*(void (**)(char *, uint64_t))(v46 + 8))(v37, v47);
  result = MEMORY[0x1D17941F8](v34, v36, v33);
  if (v38 == result)
  {
    v39 = v71[9];
    a7[8] = v71[8];
    a7[9] = v39;
    a7[10] = v71[10];
    v40 = v71[5];
    a7[4] = v71[4];
    a7[5] = v40;
    v41 = v71[7];
    a7[6] = v71[6];
    a7[7] = v41;
    v42 = v71[1];
    *a7 = v71[0];
    a7[1] = v42;
    v43 = v71[3];
    a7[2] = v71[2];
    a7[3] = v43;
    return result;
  }
LABEL_7:
  __break(1u);
  return result;
}

int64_t static BNNSNDArrayDescriptor.allocate<A>(randomIn:shape:batchSize:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, _OWORD *a7@<X8>)
{
  uint64_t v8;
  int64_t result;
  __int128 v10;
  uint64_t v11;
  unint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  _OWORD v21[2];
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  __int128 v26;
  unint64_t v27;
  uint64_t v28;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  uint64_t v37;
  _OWORD v38[11];
  _BYTE v39[144];

  v25 = a6;
  *(_QWORD *)&v26 = a4;
  *((_QWORD *)&v26 + 1) = a5;
  v28 = a3;
  v24 = a1;
  outlined init with take of BNNS.Shape(a2, (uint64_t)v39);
  outlined init with take of BNNS.Shape((uint64_t)v39, (uint64_t)v38);
  BNNS.Shape.size.getter();
  v27 = v36;
  outlined init with take of BNNS.Shape((uint64_t)v39, (uint64_t)v38);
  BNNS.Shape.stride.getter();
  v8 = v28;
  result = specialized static BNNS.calculateBatchStride(size:stride:)(v29, v30, v31, v32, v33, v34, v35, v36, v29, v30, v31, v32, v33, v34, v35, v36);
  if ((unsigned __int128)(v8 * (__int128)result) >> 64 == (v8 * result) >> 63)
  {
    if (((v28 * result) & 0x8000000000000000) == 0)
    {
      *(_QWORD *)&v38[0] = 0;
      *((_QWORD *)&v38[0] + 1) = v28 * result;
      MEMORY[0x1E0C80A78](result);
      v10 = v26;
      v21[1] = v26;
      v22 = v25;
      v23 = v24;
      v11 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Range<Int>);
      v12 = lazy protocol witness table accessor for type Range<Int> and conformance <> Range<A>();
      v37 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in static BNNSNDArrayDescriptor.allocate<A>(randomIn:shape:batchSize:), (uint64_t)v21, v11, (char *)v10, MEMORY[0x1E0DEDCE8], v12, MEMORY[0x1E0DEDD18], v13);
      outlined init with take of BNNS.Shape((uint64_t)v39, (uint64_t)&v29);
      v14 = type metadata accessor for Array();
      v15 = MEMORY[0x1D1794D08](MEMORY[0x1E0DEAF50], v14);
      static BNNSNDArrayDescriptor.allocate<A>(initializingFrom:shape:batchSize:)((uint64_t)&v37, (uint64_t)&v29, v8, v14, v15, *((uint64_t *)&v10 + 1), v38);
      result = swift_bridgeObjectRelease();
      v16 = v38[9];
      a7[8] = v38[8];
      a7[9] = v16;
      a7[10] = v38[10];
      v17 = v38[5];
      a7[4] = v38[4];
      a7[5] = v17;
      v18 = v38[7];
      a7[6] = v38[6];
      a7[7] = v18;
      v19 = v38[1];
      *a7 = v38[0];
      a7[1] = v19;
      v20 = v38[3];
      a7[2] = v38[2];
      a7[3] = v20;
      return result;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

int64_t static BNNSNDArrayDescriptor.allocate<A>(randomIn:shape:batchSize:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, _OWORD *a8@<X8>)
{
  uint64_t v9;
  int64_t result;
  __int128 v11;
  uint64_t v12;
  unint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  _OWORD v22[3];
  uint64_t v23;
  uint64_t v24;
  __int128 v25;
  __int128 v26;
  unint64_t v27;
  uint64_t v28;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  uint64_t v37;
  _OWORD v38[11];
  _BYTE v39[144];

  *(_QWORD *)&v25 = a6;
  *((_QWORD *)&v25 + 1) = a7;
  *(_QWORD *)&v26 = a4;
  *((_QWORD *)&v26 + 1) = a5;
  v28 = a3;
  v24 = a1;
  outlined init with take of BNNS.Shape(a2, (uint64_t)v39);
  outlined init with take of BNNS.Shape((uint64_t)v39, (uint64_t)v38);
  BNNS.Shape.size.getter();
  v27 = v36;
  outlined init with take of BNNS.Shape((uint64_t)v39, (uint64_t)v38);
  BNNS.Shape.stride.getter();
  v9 = v28;
  result = specialized static BNNS.calculateBatchStride(size:stride:)(v29, v30, v31, v32, v33, v34, v35, v36, v29, v30, v31, v32, v33, v34, v35, v36);
  if ((unsigned __int128)(v9 * (__int128)result) >> 64 == (v9 * result) >> 63)
  {
    if (((v28 * result) & 0x8000000000000000) == 0)
    {
      *(_QWORD *)&v38[0] = 0;
      *((_QWORD *)&v38[0] + 1) = v28 * result;
      MEMORY[0x1E0C80A78](result);
      v11 = v26;
      v22[1] = v26;
      v22[2] = v25;
      v23 = v24;
      v12 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Range<Int>);
      v13 = lazy protocol witness table accessor for type Range<Int> and conformance <> Range<A>();
      v37 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in static BNNSNDArrayDescriptor.allocate<A>(randomIn:shape:batchSize:), (uint64_t)v22, v12, (char *)v11, MEMORY[0x1E0DEDCE8], v13, MEMORY[0x1E0DEDD18], v14);
      outlined init with take of BNNS.Shape((uint64_t)v39, (uint64_t)&v29);
      v15 = type metadata accessor for Array();
      v16 = MEMORY[0x1D1794D08](MEMORY[0x1E0DEAF50], v15);
      static BNNSNDArrayDescriptor.allocate<A>(initializingFrom:shape:batchSize:)((uint64_t)&v37, (uint64_t)&v29, v9, v15, v16, *((uint64_t *)&v11 + 1), v38);
      result = swift_bridgeObjectRelease();
      v17 = v38[9];
      a8[8] = v38[8];
      a8[9] = v17;
      a8[10] = v38[10];
      v18 = v38[5];
      a8[4] = v38[4];
      a8[5] = v18;
      v19 = v38[7];
      a8[6] = v38[6];
      a8[7] = v19;
      v20 = v38[1];
      *a8 = v38[0];
      a8[1] = v20;
      v21 = v38[3];
      a8[2] = v38[2];
      a8[3] = v21;
      return result;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

int64_t static BNNSNDArrayDescriptor.allocate<A, B>(randomIn:using:shape:batchSize:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, char *a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, _OWORD *a9@<X8>)
{
  uint64_t v10;
  int64_t result;
  uint64_t v12;
  char *v13;
  uint64_t v14;
  uint64_t v15;
  unint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  _QWORD v25[9];
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  char *v30;
  uint64_t v31;
  unint64_t v32;
  uint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  uint64_t v42;
  _OWORD v43[11];
  _BYTE v44[144];

  v29 = a8;
  v30 = a5;
  v31 = a7;
  v27 = a6;
  v28 = a2;
  v33 = a4;
  v26 = a1;
  outlined init with take of BNNS.Shape(a3, (uint64_t)v44);
  outlined init with take of BNNS.Shape((uint64_t)v44, (uint64_t)v43);
  BNNS.Shape.size.getter();
  v32 = v41;
  outlined init with take of BNNS.Shape((uint64_t)v44, (uint64_t)v43);
  BNNS.Shape.stride.getter();
  v10 = v33;
  result = specialized static BNNS.calculateBatchStride(size:stride:)(v34, v35, v36, v37, v38, v39, v40, v41, v34, v35, v36, v37, v38, v39, v40, v41);
  if ((unsigned __int128)(v10 * (__int128)result) >> 64 == (v10 * result) >> 63)
  {
    if (((v33 * result) & 0x8000000000000000) == 0)
    {
      *(_QWORD *)&v43[0] = 0;
      *((_QWORD *)&v43[0] + 1) = v33 * result;
      MEMORY[0x1E0C80A78](result);
      v13 = v30;
      v12 = v31;
      v25[2] = v30;
      v25[3] = v27;
      v25[4] = v31;
      v25[5] = v29;
      v25[6] = v14;
      v25[7] = v26;
      v25[8] = v28;
      v15 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Range<Int>);
      v16 = lazy protocol witness table accessor for type Range<Int> and conformance <> Range<A>();
      v42 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in static BNNSNDArrayDescriptor.allocate<A, B>(randomIn:using:shape:batchSize:), (uint64_t)v25, v15, v13, MEMORY[0x1E0DEDCE8], v16, MEMORY[0x1E0DEDD18], v17);
      outlined init with take of BNNS.Shape((uint64_t)v44, (uint64_t)&v34);
      v18 = type metadata accessor for Array();
      v19 = MEMORY[0x1D1794D08](MEMORY[0x1E0DEAF50], v18);
      static BNNSNDArrayDescriptor.allocate<A>(initializingFrom:shape:batchSize:)((uint64_t)&v42, (uint64_t)&v34, v10, v18, v19, v12, v43);
      result = swift_bridgeObjectRelease();
      v20 = v43[9];
      a9[8] = v43[8];
      a9[9] = v20;
      a9[10] = v43[10];
      v21 = v43[5];
      a9[4] = v43[4];
      a9[5] = v21;
      v22 = v43[7];
      a9[6] = v43[6];
      a9[7] = v22;
      v23 = v43[1];
      *a9 = v43[0];
      a9[1] = v23;
      v24 = v43[3];
      a9[2] = v43[2];
      a9[3] = v24;
      return result;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

{
  uint64_t v10;
  int64_t result;
  uint64_t v12;
  char *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  unint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  _QWORD v26[11];
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  char *v31;
  uint64_t v32;
  unint64_t v33;
  uint64_t v34;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  uint64_t v43;
  _OWORD v44[11];
  _BYTE v45[144];

  v30 = a8;
  v31 = a5;
  v32 = a7;
  v27 = a6;
  v28 = a1;
  v34 = a4;
  v29 = a2;
  outlined init with take of BNNS.Shape(a3, (uint64_t)v45);
  outlined init with take of BNNS.Shape((uint64_t)v45, (uint64_t)v44);
  BNNS.Shape.size.getter();
  v33 = v42;
  outlined init with take of BNNS.Shape((uint64_t)v45, (uint64_t)v44);
  BNNS.Shape.stride.getter();
  v10 = v34;
  result = specialized static BNNS.calculateBatchStride(size:stride:)(v35, v36, v37, v38, v39, v40, v41, v42, v35, v36, v37, v38, v39, v40, v41, v42);
  if ((unsigned __int128)(v10 * (__int128)result) >> 64 == (v10 * result) >> 63)
  {
    if (((v34 * result) & 0x8000000000000000) == 0)
    {
      *(_QWORD *)&v44[0] = 0;
      *((_QWORD *)&v44[0] + 1) = v34 * result;
      MEMORY[0x1E0C80A78](result);
      v13 = v31;
      v12 = v32;
      v26[2] = v31;
      v26[3] = v27;
      v26[4] = v32;
      v26[5] = v30;
      v26[6] = v15;
      v26[7] = v14;
      v26[8] = v28;
      v26[9] = v29;
      v16 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Range<Int>);
      v17 = lazy protocol witness table accessor for type Range<Int> and conformance <> Range<A>();
      v43 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in static BNNSNDArrayDescriptor.allocate<A, B>(randomIn:using:shape:batchSize:), (uint64_t)v26, v16, v13, MEMORY[0x1E0DEDCE8], v17, MEMORY[0x1E0DEDD18], v18);
      outlined init with take of BNNS.Shape((uint64_t)v45, (uint64_t)&v35);
      v19 = type metadata accessor for Array();
      v20 = MEMORY[0x1D1794D08](MEMORY[0x1E0DEAF50], v19);
      static BNNSNDArrayDescriptor.allocate<A>(initializingFrom:shape:batchSize:)((uint64_t)&v43, (uint64_t)&v35, v10, v19, v20, v12, v44);
      result = swift_bridgeObjectRelease();
      v21 = v44[9];
      a9[8] = v44[8];
      a9[9] = v21;
      a9[10] = v44[10];
      v22 = v44[5];
      a9[4] = v44[4];
      a9[5] = v22;
      v23 = v44[7];
      a9[6] = v44[6];
      a9[7] = v23;
      v24 = v44[1];
      *a9 = v44[0];
      a9[1] = v24;
      v25 = v44[3];
      a9[2] = v44[2];
      a9[3] = v25;
      return result;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

int64_t static BNNSNDArrayDescriptor.allocate<A>(repeating:shape:batchSize:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, _OWORD *a6@<X8>)
{
  int64_t result;
  uint64_t v8;
  uint64_t v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  unint64_t v19;
  unint64_t v20;
  unint64_t v21;
  unint64_t v22;
  unint64_t v23;
  unint64_t v24;
  unint64_t v25;
  unint64_t v26;
  uint64_t v27;
  _OWORD v28[11];
  _BYTE v29[144];

  outlined init with take of BNNS.Shape(a2, (uint64_t)v29);
  outlined init with take of BNNS.Shape((uint64_t)v29, (uint64_t)v28);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v29, (uint64_t)v28);
  BNNS.Shape.stride.getter();
  result = specialized static BNNS.calculateBatchStride(size:stride:)(v19, v20, v21, v22, v23, v24, v25, v26, v19, v20, v21, v22, v23, v24, v25, v26);
  if ((unsigned __int128)(a3 * (__int128)result) >> 64 == (a3 * result) >> 63)
  {
    v27 = specialized Array.init(repeating:count:)(a1, a3 * result, a4);
    outlined init with take of BNNS.Shape((uint64_t)v29, (uint64_t)&v19);
    v8 = type metadata accessor for Array();
    v9 = MEMORY[0x1D1794D08](MEMORY[0x1E0DEAF50], v8);
    static BNNSNDArrayDescriptor.allocate<A>(initializingFrom:shape:batchSize:)((uint64_t)&v27, (uint64_t)&v19, a3, v8, v9, a5, v28);
    result = swift_bridgeObjectRelease();
    v10 = v28[9];
    a6[8] = v28[8];
    a6[9] = v10;
    a6[10] = v28[10];
    v11 = v28[5];
    a6[4] = v28[4];
    a6[5] = v11;
    v12 = v28[7];
    a6[6] = v28[6];
    a6[7] = v12;
    v13 = v28[1];
    *a6 = v28[0];
    a6[1] = v13;
    v14 = v28[3];
    a6[2] = v28[2];
    a6[3] = v14;
  }
  else
  {
    __break(1u);
  }
  return result;
}

int64_t BNNSNDArrayDescriptor.makeArray<A>(of:batchSize:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  int64_t result;
  uint64_t v5;
  uint64_t v6;
  unint64_t v7;
  unint64_t v8;
  unint64_t v9;
  unint64_t v10;
  unint64_t v11;
  unint64_t v12;
  unint64_t v13;
  unint64_t v14;
  _BYTE v15[136];
  _QWORD v16[17];
  _BYTE v17[136];
  _BYTE v18[8];
  uint64_t v19;

  outlined init with take of BNNSNDArrayDescriptor?(v2 + 136, (uint64_t)v18, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v18, (uint64_t)&v19, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  if (!v19)
    return 0;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v16);
  outlined init with take of BNNS.Shape((uint64_t)v16, (uint64_t)v17);
  outlined init with take of BNNS.Shape((uint64_t)v17, (uint64_t)v15);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v17, (uint64_t)v15);
  BNNS.Shape.stride.getter();
  result = specialized static BNNS.calculateBatchStride(size:stride:)(v7, v8, v9, v10, v11, v12, v13, v14, v7, v8, v9, v10, v11, v12, v13, v14);
  if ((unsigned __int128)(a2 * (__int128)result) >> 64 == (a2 * result) >> 63)
  {
    v16[0] = UnsafeBufferPointer.init(start:count:)();
    v16[1] = v5;
    v6 = type metadata accessor for UnsafeBufferPointer();
    MEMORY[0x1D1794D08](MEMORY[0x1E0DEA5D8], v6);
    return Array.init<A>(_:)();
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t BNNSNDArrayDescriptor.init(data:scalarType:shape:batchSize:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X8>)
{
  uint64_t result;
  _BYTE v14[184];
  _BYTE v15[136];

  result = outlined init with take of BNNS.Shape(a5, (uint64_t)v15);
  if (a6 < 1)
  {
    __break(1u);
  }
  else
  {
    (*(void (**)(uint64_t, uint64_t))(a4 + 8))(a3, a4);
    helper #1 <A>(_:) in BNNSNDArrayDescriptor.init(data:scalarType:shape:batchSize:)(a1, a2, (uint64_t)v15, a6, a3, (uint64_t)v14);
    return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v14, a7, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  }
  return result;
}

void helper #1 <A>(_:) in BNNSNDArrayDescriptor.init(data:scalarType:shape:batchSize:)(uint64_t a1@<X1>, uint64_t a2@<X2>, uint64_t a3@<X3>, uint64_t a4@<X4>, uint64_t a5@<X6>, uint64_t a6@<X8>)
{
  uint64_t v6;
  uint64_t v7;
  int64_t v11;
  uint64_t v12;
  unint64_t v14;
  unint64_t v15;
  unint64_t v16;
  unint64_t v17;
  unint64_t v18;
  unint64_t v19;
  unint64_t v20;
  unint64_t v21;
  _BYTE v22[192];

  if (a1)
    v6 = a2 - a1;
  else
    v6 = 0;
  v7 = *(_QWORD *)(*(_QWORD *)(a5 - 8) + 72);
  if (!v7)
  {
    __break(1u);
    goto LABEL_15;
  }
  if (v6 == 0x8000000000000000 && v7 == -1)
    goto LABEL_16;
  v12 = v6;
  outlined init with take of BNNS.Shape(a3, (uint64_t)v22);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape(a3, (uint64_t)v22);
  BNNS.Shape.stride.getter();
  v11 = specialized static BNNS.calculateBatchStride(size:stride:)(v14, v15, v16, v17, v18, v19, v20, v21, v14, v15, v16, v17, v18, v19, v20, v21);
  if ((unsigned __int128)(v11 * (__int128)a4) >> 64 != (v11 * a4) >> 63)
  {
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
    return;
  }
  if (v12 / v7 == v11 * a4)
  {
    outlined init with take of BNNS.Shape(a3, (uint64_t)v22);
    specialized static BNNS.makeArrayDescriptor(shape:data:dataType:)((uint64_t)v22);
    _sSo21BNNSNDArrayDescriptoraSgWOi_((uint64_t)&v14);
  }
  else
  {
    _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&v14);
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v14, (uint64_t)v22, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v22, a6, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
}

uint64_t BNNSNDArrayDescriptor.init<A>(data:shape:batchSize:)@<X0>(uint64_t a1@<X1>, uint64_t a2@<X2>, uint64_t a3@<X3>, uint64_t a4@<X4>, uint64_t a5@<X5>, uint64_t a6@<X8>)
{
  uint64_t result;
  unint64_t v12;
  unint64_t v13;
  unint64_t v14;
  unint64_t v15;
  unint64_t v16;
  unint64_t v17;
  unint64_t v18;
  unint64_t v19;
  _BYTE v20[184];
  _BYTE v21[144];

  result = outlined init with take of BNNS.Shape(a2, (uint64_t)v21);
  if (a3 < 1)
  {
    __break(1u);
  }
  else
  {
    if (!UnsafeBufferPointer.baseAddress.getter())
      goto LABEL_6;
    outlined init with take of BNNS.Shape((uint64_t)v21, (uint64_t)v20);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v21, (uint64_t)v20);
    BNNS.Shape.stride.getter();
    result = specialized static BNNS.calculateBatchStride(size:stride:)(v12, v13, v14, v15, v16, v17, v18, v19, v12, v13, v14, v15, v16, v17, v18, v19);
    if ((unsigned __int128)(result * (__int128)a3) >> 64 == (result * a3) >> 63)
    {
      if (result * a3 == a1)
      {
        outlined init with take of BNNS.Shape((uint64_t)v21, (uint64_t)v20);
        (*(void (**)(uint64_t, uint64_t))(a5 + 8))(a4, a5);
        specialized static BNNS.makeArrayDescriptor(shape:data:dataType:)((uint64_t)v20);
        _sSo21BNNSNDArrayDescriptoraSgWOi_((uint64_t)&v12);
LABEL_7:
        outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v12, (uint64_t)v20, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
        return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v20, a6, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      }
LABEL_6:
      _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&v12);
      goto LABEL_7;
    }
  }
  __break(1u);
  return result;
}

uint64_t specialized Array.init(repeating:count:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t result;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  void (*v10)(uint64_t, uint64_t, uint64_t);
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;

  result = static Array._allocateUninitialized(_:)();
  v15 = result;
  if (a2 < 0)
  {
    __break(1u);
  }
  else
  {
    if (a2)
    {
      v8 = v7;
      v9 = *(_QWORD *)(a3 - 8);
      v10 = *(void (**)(uint64_t, uint64_t, uint64_t))(v9 + 16);
      v10(v7, a1, a3);
      v11 = a2 - 1;
      if (v11)
      {
        v12 = *(_QWORD *)(v9 + 72);
        v13 = v8 + v12;
        do
        {
          v10(v13, a1, a3);
          v13 += v12;
          --v11;
        }
        while (v11);
      }
    }
    v14 = type metadata accessor for Array();
    destructiveProjectEnumData for BNNS.ActivationFunction(v14);
    return v15;
  }
  return result;
}

uint64_t partial apply for closure #1 in static BNNSNDArrayDescriptor.allocate<A>(randomIn:shape:batchSize:)()
{
  return static FixedWidthInteger.random(in:)();
}

{
  return static BinaryFloatingPoint<>.random(in:)();
}

unint64_t lazy protocol witness table accessor for type Range<Int> and conformance <> Range<A>()
{
  unint64_t result;
  uint64_t v1;
  unint64_t v2;
  _QWORD v3[2];

  result = lazy protocol witness table cache variable for type Range<Int> and conformance <> Range<A>;
  if (!lazy protocol witness table cache variable for type Range<Int> and conformance <> Range<A>)
  {
    v1 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for Range<Int>);
    v2 = lazy protocol witness table accessor for type Int and conformance Int();
    v3[0] = MEMORY[0x1E0DEB458];
    v3[1] = v2;
    result = MEMORY[0x1D1794D08](MEMORY[0x1E0DEB8C0], v1, v3);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Range<Int> and conformance <> Range<A>);
  }
  return result;
}

uint64_t partial apply for closure #1 in static BNNSNDArrayDescriptor.allocate<A, B>(randomIn:using:shape:batchSize:)()
{
  return static FixedWidthInteger.random<A>(in:using:)();
}

{
  return static BinaryFloatingPoint<>.random<A>(in:using:)();
}

uint64_t dispatch thunk of static BNNSScalar.bnnsDataType.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 8))();
}

uint64_t BNNS.RelationalOperator.value.setter(uint64_t result)
{
  _DWORD *v1;

  *v1 = result;
  return result;
}

uint64_t (*BNNS.RelationalOperator.value.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t static BNNS.RelationalOperator.equal.getter()
{
  return 0;
}

uint64_t static BNNS.RelationalOperator.less.getter()
{
  return 1;
}

uint64_t static BNNS.RelationalOperator.lessEqual.getter()
{
  return 2;
}

uint64_t static BNNS.RelationalOperator.greater.getter()
{
  return 3;
}

uint64_t static BNNS.RelationalOperator.greaterEqual.getter()
{
  return 4;
}

uint64_t static BNNS.RelationalOperator.notEqual.getter()
{
  return 5;
}

uint64_t static BNNS.RelationalOperator.and.getter()
{
  return 6;
}

uint64_t static BNNS.RelationalOperator.or.getter()
{
  return 7;
}

uint64_t static BNNS.RelationalOperator.not.getter()
{
  return 8;
}

uint64_t static BNNS.RelationalOperator.nand.getter()
{
  return 9;
}

uint64_t static BNNS.RelationalOperator.nor.getter()
{
  return 10;
}

uint64_t static BNNS.RelationalOperator.xor.getter()
{
  return 11;
}

uint64_t static BNNS.compare(_:_:using:output:)(_OWORD *a1, _OWORD *a2, BNNSRelationalOperator a3, _OWORD *a4)
{
  __int128 v4;
  __int128 v5;
  __int128 v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  uint64_t result;
  _BYTE *v20;
  BNNSNDArrayDescriptor v21;
  BNNSNDArrayDescriptor in1;
  BNNSNDArrayDescriptor in0;
  uint64_t v24;

  v24 = *MEMORY[0x1E0C80C00];
  v4 = a1[9];
  *(_OWORD *)&in0.stride[7] = a1[8];
  *(_OWORD *)&in0.data_type = v4;
  *(_OWORD *)&in0.table_data_type = a1[10];
  v5 = a1[5];
  *(_OWORD *)&in0.size[7] = a1[4];
  *(_OWORD *)&in0.stride[1] = v5;
  v6 = a1[7];
  *(_OWORD *)&in0.stride[3] = a1[6];
  *(_OWORD *)&in0.stride[5] = v6;
  v7 = a1[1];
  *(_OWORD *)&in0.flags = *a1;
  *(_OWORD *)&in0.size[1] = v7;
  v8 = a1[3];
  *(_OWORD *)&in0.size[3] = a1[2];
  *(_OWORD *)&in0.size[5] = v8;
  v9 = a2[9];
  *(_OWORD *)&in1.stride[7] = a2[8];
  *(_OWORD *)&in1.data_type = v9;
  *(_OWORD *)&in1.table_data_type = a2[10];
  v10 = a2[5];
  *(_OWORD *)&in1.size[7] = a2[4];
  *(_OWORD *)&in1.stride[1] = v10;
  v11 = a2[7];
  *(_OWORD *)&in1.stride[3] = a2[6];
  *(_OWORD *)&in1.stride[5] = v11;
  v12 = a2[1];
  *(_OWORD *)&in1.flags = *a2;
  *(_OWORD *)&in1.size[1] = v12;
  v13 = a2[3];
  *(_OWORD *)&in1.size[3] = a2[2];
  *(_OWORD *)&in1.size[5] = v13;
  v14 = a4[9];
  *(_OWORD *)&v21.stride[7] = a4[8];
  *(_OWORD *)&v21.data_type = v14;
  *(_OWORD *)&v21.table_data_type = a4[10];
  v15 = a4[5];
  *(_OWORD *)&v21.size[7] = a4[4];
  *(_OWORD *)&v21.stride[1] = v15;
  v16 = a4[7];
  *(_OWORD *)&v21.stride[3] = a4[6];
  *(_OWORD *)&v21.stride[5] = v16;
  v17 = a4[1];
  *(_OWORD *)&v21.flags = *a4;
  *(_OWORD *)&v21.size[1] = v17;
  v18 = a4[3];
  *(_OWORD *)&v21.size[3] = a4[2];
  *(_OWORD *)&v21.size[5] = v18;
  result = BNNSCompareTensor(&in0, &in1, a3, &v21);
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v20 = 0;
    return swift_willThrow();
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.RelationalOperator()
{
  return &type metadata for BNNS.RelationalOperator;
}

uint64_t vImage.PixelBuffer<>.applyLookup(_:destination:)(uint64_t a1, uint64_t a2)
{
  return vImage.PixelBuffer<>.applyLookup(_:destination:)(a1, a2, MEMORY[0x1E0C8D788]);
}

{
  return vImage.PixelBuffer<>.applyLookup(_:destination:)(a1, a2, MEMORY[0x1E0C8D3D8]);
}

{
  return vImage.PixelBuffer<>.applyLookup(_:destination:)(a1, a2, MEMORY[0x1E0C8D3F0]);
}

{
  return vImage.PixelBuffer<>.applyLookup(_:destination:)(a1, a2, MEMORY[0x1E0C8D3E8]);
}

vImage_Error vImage.PixelBuffer<>.applyLookup(_:destination:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  _QWORD *v3;
  vImagePixelCount v4;
  vImagePixelCount v5;
  _QWORD *v6;
  uint64_t v7;
  uint64_t v8;
  void *v9;
  size_t v10;
  void *v11;
  size_t v12;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v16;

  v16 = *MEMORY[0x1E0C80C00];
  if (*(_QWORD *)(a1 + 16) != 256)
  {
    __break(1u);
    goto LABEL_16;
  }
  v3 = *(_QWORD **)v2;
  if (!*(_QWORD *)(*(_QWORD *)v2 + 16))
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v4 = v3[6];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v5 = v3[5];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v4)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v5)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v6 = *(_QWORD **)a2;
  if (!*(_QWORD *)(*(_QWORD *)a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v7 = v6[6];
  if (v7 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v8 = v6[5];
  if (v8 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v8)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v4 != v7)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v5 != v8)
    goto LABEL_27;
  v9 = (void *)v3[4];
  v10 = v3[7];
  src.data = v9;
  src.height = v5;
  src.width = v4;
  src.rowBytes = v10;
  v11 = (void *)v6[4];
  v12 = v6[7];
  dest.data = v11;
  dest.height = v5;
  dest.width = v4;
  dest.rowBytes = v12;
  return vImageLookupTable_Planar8toPlanar24(&src, &dest, (const uint32_t *)(a1 + 32), 0);
}

{
  uint64_t v2;
  _QWORD *v3;
  vImagePixelCount v4;
  vImagePixelCount v5;
  _QWORD *v6;
  uint64_t v7;
  uint64_t v8;
  void *v9;
  size_t v10;
  void *v11;
  size_t v12;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v16;

  v16 = *MEMORY[0x1E0C80C00];
  if (*(_QWORD *)(a1 + 16) != 4096)
  {
    __break(1u);
    goto LABEL_16;
  }
  v3 = *(_QWORD **)v2;
  if (!*(_QWORD *)(*(_QWORD *)v2 + 16))
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v4 = v3[6];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v5 = v3[5];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v4)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v5)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v6 = *(_QWORD **)a2;
  if (!*(_QWORD *)(*(_QWORD *)a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v7 = v6[6];
  if (v7 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v8 = v6[5];
  if (v8 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v8)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v4 != v7)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v5 != v8)
    goto LABEL_27;
  v9 = (void *)v3[4];
  v10 = v3[7];
  src.data = v9;
  src.height = v5;
  src.width = v4;
  src.rowBytes = v10;
  v11 = (void *)v6[4];
  v12 = v6[7];
  dest.data = v11;
  dest.height = v5;
  dest.width = v4;
  dest.rowBytes = v12;
  return vImageLookupTable_PlanarFtoPlanar8(&src, &dest, (const Pixel_8 *)(a1 + 32), 0);
}

{
  uint64_t v2;
  vImagePixelCount v3;
  _QWORD *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  void *v10;
  size_t v11;
  void *v12;
  size_t v13;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v17;

  v17 = *MEMORY[0x1E0C80C00];
  v3 = *(_QWORD *)(a1 + 16);
  if (!v3)
  {
    __break(1u);
    goto LABEL_16;
  }
  v4 = *(_QWORD **)v2;
  if (!*(_QWORD *)(*(_QWORD *)v2 + 16))
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v5)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v6)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = *(_QWORD **)a2;
  if (!*(_QWORD *)(*(_QWORD *)a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v8)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v9)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v5 != v8)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v6 != v9)
    goto LABEL_27;
  v10 = (void *)v4[4];
  v11 = v4[7];
  src.data = v10;
  src.height = v6;
  src.width = v5;
  src.rowBytes = v11;
  v12 = (void *)v7[4];
  v13 = v7[7];
  dest.data = v12;
  dest.height = v6;
  dest.width = v5;
  dest.rowBytes = v13;
  return vImageInterpolatedLookupTable_PlanarF(&src, &dest, (const Pixel_F *)(a1 + 32), v3, 1.0, 0.0, 0);
}

{
  uint64_t v2;
  _QWORD *v3;
  vImagePixelCount v4;
  vImagePixelCount v5;
  _QWORD *v6;
  uint64_t v7;
  uint64_t v8;
  void *v9;
  size_t v10;
  void *v11;
  size_t v12;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v16;

  v16 = *MEMORY[0x1E0C80C00];
  if (*(_QWORD *)(a1 + 16) != 0x10000)
  {
    __break(1u);
    goto LABEL_16;
  }
  v3 = *(_QWORD **)v2;
  if (!*(_QWORD *)(*(_QWORD *)v2 + 16))
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v4 = v3[6];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v5 = v3[5];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v4)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v5)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v6 = *(_QWORD **)a2;
  if (!*(_QWORD *)(*(_QWORD *)a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v7 = v6[6];
  if (v7 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v8 = v6[5];
  if (v8 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v8)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v4 != v7)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v5 != v8)
    goto LABEL_27;
  v9 = (void *)v3[4];
  v10 = v3[7];
  src.data = v9;
  src.height = v5;
  src.width = v4;
  src.rowBytes = v10;
  v11 = (void *)v6[4];
  v12 = v6[7];
  dest.data = v11;
  dest.height = v5;
  dest.width = v4;
  dest.rowBytes = v12;
  return vImageLookupTable_Planar16(&src, &dest, (const Pixel_16U *)(a1 + 32), 0);
}

uint64_t vImage.PixelBuffer<>.applyLookup(_:destination:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(_QWORD *, _QWORD *, uint64_t, _QWORD))
{
  uint64_t v3;
  _QWORD *v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  _QWORD v15[4];
  _QWORD v16[5];

  v16[4] = *MEMORY[0x1E0C80C00];
  if (*(_QWORD *)(a1 + 16) != 256)
  {
    __break(1u);
    goto LABEL_16;
  }
  v4 = *(_QWORD **)v3;
  if (!*(_QWORD *)(*(_QWORD *)v3 + 16))
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v5 = v4[6];
  if (v5 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v6 = v4[5];
  if (v6 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v5)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v6)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = *(_QWORD **)a2;
  if (!*(_QWORD *)(*(_QWORD *)a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v8)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v9)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v5 != v8)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v6 != v9)
    goto LABEL_27;
  v10 = v4[4];
  v11 = v4[7];
  v16[0] = v10;
  v16[1] = v6;
  v16[2] = v5;
  v16[3] = v11;
  v12 = v7[4];
  v13 = v7[7];
  v15[0] = v12;
  v15[1] = v6;
  v15[2] = v5;
  v15[3] = v13;
  return a3(v16, v15, a1 + 32, 0);
}

vImage_Error vImage.PixelBuffer<>.applyLookup(alphaTable:redTable:greenTable:blueTable:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v5;
  _QWORD *v6;
  vImagePixelCount v7;
  vImagePixelCount v8;
  _QWORD *v9;
  uint64_t v10;
  uint64_t v11;
  void *v12;
  size_t v13;
  void *v14;
  size_t v15;
  const Pixel_8 *v16;
  const Pixel_8 *v17;
  const Pixel_8 *v18;
  const Pixel_8 *v19;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v23;

  v23 = *MEMORY[0x1E0C80C00];
  if (a1 && *(_QWORD *)(a1 + 16) != 256)
    goto LABEL_44;
  if (a2 && *(_QWORD *)(a2 + 16) != 256)
    goto LABEL_45;
  if (a3 && *(_QWORD *)(a3 + 16) != 256)
    goto LABEL_46;
  if (a4 && *(_QWORD *)(a4 + 16) != 256)
LABEL_47:
    __break(1u);
  v6 = *(_QWORD **)v5;
  if (!*(_QWORD *)(*(_QWORD *)v5 + 16))
  {
    __break(1u);
    goto LABEL_33;
  }
  v7 = v6[6];
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  v8 = v6[5];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (!v7)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (!v8)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  v9 = *(_QWORD **)a5;
  if (!*(_QWORD *)(*(_QWORD *)a5 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v10 = v9[6];
  if (v10 < 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v11 = v9[5];
  if (v11 < 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v10)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v11)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (v7 != v10)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (v8 != v11)
  {
LABEL_43:
    __break(1u);
LABEL_44:
    __break(1u);
LABEL_45:
    __break(1u);
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  v12 = (void *)v6[4];
  v13 = v6[7];
  src.data = v12;
  src.height = v8;
  src.width = v7;
  src.rowBytes = v13;
  v14 = (void *)v9[4];
  v15 = v9[7];
  dest.data = v14;
  dest.height = v8;
  v16 = (const Pixel_8 *)(a1 + 32);
  if (!a1)
    v16 = 0;
  v17 = (const Pixel_8 *)(a2 + 32);
  if (!a2)
    v17 = 0;
  if (a3)
    v18 = (const Pixel_8 *)(a3 + 32);
  else
    v18 = 0;
  if (a4)
    v19 = (const Pixel_8 *)(a4 + 32);
  else
    v19 = 0;
  dest.width = v7;
  dest.rowBytes = v15;
  return vImageTableLookUp_ARGB8888(&src, &dest, v16, v17, v18, v19, 0);
}

BOOL static BNNS.DescriptorType.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void BNNS.DescriptorType.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

uint64_t BNNS.ArithmeticUnaryFunction.bnnsArithmeticFunction.getter()
{
  char *v0;

  return dword_1CAB622FC[*v0];
}

void *static BNNS.ArithmeticUnaryFunction.allCases.getter()
{
  return &outlined read-only object #0 of static BNNS.ArithmeticUnaryFunction.allCases.getter;
}

void protocol witness for static CaseIterable.allCases.getter in conformance BNNS.ArithmeticUnaryFunction(_QWORD *a1@<X8>)
{
  *a1 = &outlined read-only object #0 of static BNNS.ArithmeticUnaryFunction.allCases.getter;
}

void *static BNNS.ArithmeticBinaryFunction.allCases.getter()
{
  return &outlined read-only object #0 of static BNNS.ArithmeticBinaryFunction.allCases.getter;
}

void protocol witness for static CaseIterable.allCases.getter in conformance BNNS.ArithmeticBinaryFunction(_QWORD *a1@<X8>)
{
  *a1 = &outlined read-only object #0 of static BNNS.ArithmeticBinaryFunction.allCases.getter;
}

uint64_t BNNS.UnaryArithmeticLayer.__allocating_init(input:inputDescriptorType:output:outputDescriptorType:function:activation:filterParameters:)(_OWORD *a1, unsigned __int8 *a2, _OWORD *a3, unsigned __int8 *a4, uint64_t a5, uint64_t *a6, int a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  int v22;
  uint64_t v23;
  int v24;
  int *v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  int v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  _DWORD v34[2];
  _OWORD *v35;
  int v36;
  uint64_t v37;
  uint64_t v38;
  int v39;
  __int128 v40;
  uint64_t v41;
  _OWORD v42[11];
  int v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  int v55;
  int v56;
  _BYTE v57[180];
  int v58;
  uint64_t v59;
  uint64_t v60;
  int v61;
  __int128 v62;
  uint64_t v63;
  uint64_t v64;

  v64 = *MEMORY[0x1E0C80C00];
  v12 = a1[9];
  v42[8] = a1[8];
  v42[9] = v12;
  v42[10] = a1[10];
  v13 = a1[5];
  v42[4] = a1[4];
  v42[5] = v13;
  v14 = a1[7];
  v42[6] = a1[6];
  v42[7] = v14;
  v15 = a1[1];
  v42[0] = *a1;
  v42[1] = v15;
  v16 = a1[3];
  v42[2] = a1[2];
  v42[3] = v16;
  v17 = a3[5];
  *(_OWORD *)&v57[68] = a3[4];
  v18 = a3[2];
  *(_OWORD *)&v57[52] = a3[3];
  v19 = a3[6];
  *(_OWORD *)&v57[116] = a3[7];
  v20 = a3[9];
  *(_OWORD *)&v57[132] = a3[8];
  *(_OWORD *)&v57[148] = v20;
  *(_OWORD *)&v57[164] = a3[10];
  *(_OWORD *)&v57[84] = v17;
  *(_OWORD *)&v57[100] = v19;
  v21 = a3[1];
  *(_OWORD *)&v57[4] = *a3;
  *(_OWORD *)&v57[20] = v21;
  *(_OWORD *)&v57[36] = v18;
  v53 = *(_OWORD *)&v57[144];
  v54 = *(_OWORD *)&v57[160];
  v49 = *(_OWORD *)&v57[80];
  v50 = *(_OWORD *)&v57[96];
  v51 = *(_OWORD *)&v57[112];
  v52 = *(_OWORD *)&v57[128];
  v48 = *(_OWORD *)&v57[64];
  v44 = *(_OWORD *)v57;
  v45 = *(_OWORD *)&v57[16];
  v46 = *(_OWORD *)&v57[32];
  v22 = *a4;
  v23 = *a6;
  v43 = *a2;
  v55 = *(_DWORD *)&v57[176];
  v47 = *(_OWORD *)&v57[48];
  v56 = v22;
  v24 = BNNS.ArithmeticUnaryFunction.bnnsArithmeticFunction.getter();
  v34[1] = HIDWORD(v23);
  BNNS.ActivationFunction.bnnsActivation.getter();
  v37 = v59;
  v34[0] = v24;
  v35 = v42;
  v36 = v58;
  v38 = v60;
  v39 = v61;
  v40 = v62;
  v41 = v63;
  if (a9 == 1)
  {
    v25 = 0;
  }
  else
  {
    v30 = a7;
    v31 = a8;
    v32 = a9;
    v33 = a10;
    v25 = &v30;
  }
  v26 = MEMORY[0x1D1794594](v34, v25);
  type metadata accessor for BNNS.UnaryArithmeticLayer();
  v27 = swift_allocObject();
  v28 = v27;
  if (v26)
  {
    *(_QWORD *)(v27 + 16) = v26;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v28;
}

uint64_t BNNS.UnaryArithmeticLayer.apply(batchSize:input:output:)(size_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;

  return specialized static BNNS.arithmeticLayerApply(_:batchSize:input:output:)(v3, a1, a2, a3);
}

uint64_t BNNS.UnaryArithmeticLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:)(size_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, __int128 *a5)
{
  uint64_t v5;

  return specialized static BNNS.arithmeticLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:)(v5, a1, a2, a3, a4, a5);
}

uint64_t type metadata accessor for BNNS.UnaryArithmeticLayer()
{
  return objc_opt_self();
}

uint64_t BNNS.UnaryArithmeticLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.BinaryArithmeticLayer.__allocating_init(inputA:inputADescriptorType:inputB:inputBDescriptorType:output:outputDescriptorType:function:activation:filterParameters:)(_OWORD *a1, unsigned __int8 *a2, _OWORD *a3, unsigned __int8 *a4, _OWORD *a5, unsigned __int8 *a6, char *a7, uint64_t *a8, int a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  int v27;
  int v28;
  int v29;
  uint64_t v30;
  uint64_t v31;
  int v32;
  int *v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  int v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  _DWORD v42[2];
  _OWORD *v43;
  int v44;
  uint64_t v45;
  uint64_t v46;
  int v47;
  __int128 v48;
  uint64_t v49;
  _OWORD v50[11];
  int v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  __int128 v61;
  __int128 v62;
  int v63;
  int v64;
  __int128 v65;
  __int128 v66;
  __int128 v67;
  __int128 v68;
  __int128 v69;
  __int128 v70;
  __int128 v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  int v76;
  int v77;
  _BYTE v78[180];
  _BYTE v79[180];
  int v80;
  uint64_t v81;
  uint64_t v82;
  int v83;
  __int128 v84;
  uint64_t v85;
  uint64_t v86;

  v86 = *MEMORY[0x1E0C80C00];
  v12 = a1[9];
  v50[8] = a1[8];
  v50[9] = v12;
  v50[10] = a1[10];
  v13 = a1[5];
  v50[4] = a1[4];
  v50[5] = v13;
  v14 = a1[7];
  v50[6] = a1[6];
  v50[7] = v14;
  v15 = a1[1];
  v50[0] = *a1;
  v50[1] = v15;
  v16 = a1[3];
  v50[2] = a1[2];
  v50[3] = v16;
  v17 = a3[6];
  *(_OWORD *)&v79[116] = a3[7];
  v18 = a3[9];
  *(_OWORD *)&v79[132] = a3[8];
  *(_OWORD *)&v79[148] = v18;
  *(_OWORD *)&v79[164] = a3[10];
  v19 = a3[2];
  *(_OWORD *)&v79[52] = a3[3];
  v20 = a3[5];
  *(_OWORD *)&v79[68] = a3[4];
  *(_OWORD *)&v79[84] = v20;
  *(_OWORD *)&v79[100] = v17;
  v21 = a3[1];
  *(_OWORD *)&v79[4] = *a3;
  *(_OWORD *)&v79[20] = v21;
  *(_OWORD *)&v79[36] = v19;
  v22 = a5[6];
  *(_OWORD *)&v78[116] = a5[7];
  v23 = a5[9];
  *(_OWORD *)&v78[132] = a5[8];
  *(_OWORD *)&v78[148] = v23;
  *(_OWORD *)&v78[164] = a5[10];
  v24 = a5[2];
  *(_OWORD *)&v78[52] = a5[3];
  v25 = a5[5];
  *(_OWORD *)&v78[68] = a5[4];
  *(_OWORD *)&v78[84] = v25;
  *(_OWORD *)&v78[100] = v22;
  v26 = a5[1];
  *(_OWORD *)&v78[4] = *a5;
  *(_OWORD *)&v78[20] = v26;
  *(_OWORD *)&v78[36] = v24;
  v61 = *(_OWORD *)&v79[144];
  v62 = *(_OWORD *)&v79[160];
  v57 = *(_OWORD *)&v79[80];
  v58 = *(_OWORD *)&v79[96];
  v27 = *a2;
  v28 = *a4;
  v29 = *a6;
  v30 = *a7;
  v31 = *a8;
  v59 = *(_OWORD *)&v79[112];
  v60 = *(_OWORD *)&v79[128];
  v51 = v27;
  v63 = *(_DWORD *)&v79[176];
  v56 = *(_OWORD *)&v79[64];
  v52 = *(_OWORD *)v79;
  v53 = *(_OWORD *)&v79[16];
  v54 = *(_OWORD *)&v79[32];
  v55 = *(_OWORD *)&v79[48];
  v64 = v28;
  v73 = *(_OWORD *)&v78[128];
  v74 = *(_OWORD *)&v78[144];
  v75 = *(_OWORD *)&v78[160];
  v69 = *(_OWORD *)&v78[64];
  v70 = *(_OWORD *)&v78[80];
  v71 = *(_OWORD *)&v78[96];
  v72 = *(_OWORD *)&v78[112];
  v65 = *(_OWORD *)v78;
  v66 = *(_OWORD *)&v78[16];
  v67 = *(_OWORD *)&v78[32];
  v68 = *(_OWORD *)&v78[48];
  v76 = *(_DWORD *)&v78[176];
  v77 = v29;
  v32 = dword_1CAB62368[v30];
  v42[1] = HIDWORD(v31);
  BNNS.ActivationFunction.bnnsActivation.getter();
  v45 = v81;
  v42[0] = v32;
  v43 = v50;
  v44 = v80;
  v46 = v82;
  v47 = v83;
  v48 = v84;
  v49 = v85;
  if (a11 == 1)
  {
    v33 = 0;
  }
  else
  {
    v38 = a9;
    v39 = a10;
    v40 = a11;
    v41 = a12;
    v33 = &v38;
  }
  v34 = MEMORY[0x1D1794594](v42, v33);
  type metadata accessor for BNNS.BinaryArithmeticLayer();
  v35 = swift_allocObject();
  v36 = v35;
  if (v34)
  {
    *(_QWORD *)(v35 + 16) = v34;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v36;
}

uint64_t BNNS.BinaryArithmeticLayer.apply(batchSize:inputA:inputB:output:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4;

  return specialized static BNNS.arithmeticLayerApply(_:batchSize:inputA:inputB:output:)(v4, a1, a2, a3, a4);
}

uint64_t BNNS.BinaryArithmeticLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _OWORD *a5, _OWORD *a6, __int128 *a7)
{
  uint64_t v7;

  return specialized static BNNS.arithmeticLayerApplyBackward(_:batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(v7, a1, a2, a3, a4, a5, a6, a7);
}

uint64_t closure #1 in closure #1 in static BNNS.arithmeticLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:)@<X0>(BNNSNDArrayDescriptor *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, size_t a4@<X3>, char **a5@<X4>, uint64_t a6@<X6>, uint64_t a7@<X7>, int *a8@<X8>)
{
  uint64_t v10;
  uint64_t v11;
  size_t out_delta_stride;
  char *v13;
  char isUniquelyReferenced_nonNull_native;
  int v15;
  uint64_t result;
  size_t out_stride;
  const void *v19;
  void *filter;
  BNNSNDArrayDescriptor **in_delta;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  _BYTE v35[17];
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  unint64_t v46;
  unint64_t v47;
  unint64_t v48;
  unint64_t v49;
  unint64_t v50;
  unint64_t v51;
  _BYTE v52[136];
  _BYTE v53[136];
  _BYTE v54[8];
  const void *v55;
  _BYTE v56[144];

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>>);
  v10 = swift_allocObject();
  *(_OWORD *)(v10 + 16) = xmmword_1CAB5E430;
  *(_QWORD *)(v10 + 32) = a2;
  in_delta = (BNNSNDArrayDescriptor **)(v10 + 32);
  filter = *(void **)(a3 + 16);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  v11 = swift_allocObject();
  *(_OWORD *)(v11 + 16) = xmmword_1CAB5E430;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v53);
  outlined init with take of BNNS.Shape((uint64_t)v53, (uint64_t)v56);
  outlined init with take of BNNS.Shape((uint64_t)v56, (uint64_t)v52);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v56, (uint64_t)v52);
  BNNS.Shape.stride.getter();
  *(_QWORD *)(v11 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v44, v45, v46, v47, v48, v49, v50, v51, v44, v45, v46, v47, v48, v49, v50, v51);
  outlined init with take of UnsafeMutableRawPointer?(a7 + 136, (uint64_t)v54);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v54, (uint64_t)&v55);
  v19 = v55;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v44);
  outlined init with take of BNNS.Shape((uint64_t)&v44, (uint64_t)v52);
  outlined init with take of BNNS.Shape((uint64_t)v52, (uint64_t)v53);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v52, (uint64_t)v53);
  BNNS.Shape.stride.getter();
  out_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v36, v37, v38, v39, v40, v41, v42, v43, v36, v37, v38, v39, v40, v41, v42, v43);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v36);
  outlined init with take of BNNS.Shape((uint64_t)&v36, (uint64_t)v53);
  outlined init with take of BNNS.Shape((uint64_t)v53, (uint64_t)v35);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v53, (uint64_t)v35);
  BNNS.Shape.stride.getter();
  out_delta_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v27, v28, v29, v30, v31, v32, v33, v34, v27, v28, v29, v30, v31, v32, v33, v34);
  v13 = *a5;
  isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
  *a5 = v13;
  if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    v13 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v13 + 2), 0, v13);
  *a5 = v13;
  swift_bridgeObjectRetain();
  v15 = BNNSArithmeticFilterApplyBackwardBatch(filter, a4, 1uLL, (const void **)v13 + 4, (const size_t *)(v11 + 32), in_delta, (const size_t *)(a6 + 32), v19, out_stride, a1, out_delta_stride);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  result = swift_bridgeObjectRelease();
  *a8 = v15;
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static BNNS.arithmeticLayerApplyBackward(_:batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)@<X0>(BNNSNDArrayDescriptor *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, size_t a5@<X4>, char **a6@<X5>, int *a7@<X8>, uint64_t a8, uint64_t a9)
{
  uint64_t v12;
  unint64_t v13;
  uint64_t v14;
  unint64_t v15;
  size_t out_delta_stride;
  char *v17;
  char isUniquelyReferenced_nonNull_native;
  int v19;
  uint64_t result;
  size_t out_stride;
  const void *v22;
  const size_t *in_stride;
  void *v25;
  BNNSNDArrayDescriptor **v26;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  _BYTE v38[136];
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  unint64_t v46;
  _BYTE v47[136];
  unint64_t v48;
  unint64_t v49;
  unint64_t v50;
  unint64_t v51;
  unint64_t v52;
  unint64_t v53;
  unint64_t v54;
  unint64_t v55;
  _BYTE v56[136];
  _BYTE v57[8];
  const void *v58;
  _BYTE v59[136];
  _BYTE v60[144];
  uint64_t v61;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>>);
  v12 = swift_allocObject();
  *(_OWORD *)(v12 + 16) = xmmword_1CAB5E440;
  *(_QWORD *)(v12 + 32) = a2;
  v26 = (BNNSNDArrayDescriptor **)(v12 + 32);
  *(_QWORD *)(v12 + 40) = a3;
  v25 = *(void **)(a4 + 16);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  v61 = swift_allocObject();
  *(_OWORD *)(v61 + 16) = xmmword_1CAB5E440;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v56);
  outlined init with take of BNNS.Shape((uint64_t)v56, (uint64_t)v59);
  outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v60);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v60);
  BNNS.Shape.stride.getter();
  v13 = specialized static BNNS.calculateBatchStride(size:stride:)(v48, v49, v50, v51, v52, v53, v54, v55, v48, v49, v50, v51, v52, v53, v54, v55);
  v14 = v61;
  *(_QWORD *)(v61 + 32) = v13;
  in_stride = (const size_t *)(v14 + 32);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v48);
  outlined init with take of BNNS.Shape((uint64_t)&v48, (uint64_t)v60);
  outlined init with take of BNNS.Shape((uint64_t)v60, (uint64_t)v47);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v60, (uint64_t)v47);
  BNNS.Shape.stride.getter();
  v15 = specialized static BNNS.calculateBatchStride(size:stride:)(v39, v40, v41, v42, v43, v44, v45, v46, v39, v40, v41, v42, v43, v44, v45, v46);
  *(_QWORD *)(v61 + 40) = v15;
  outlined init with take of UnsafeMutableRawPointer?(a9 + 136, (uint64_t)v57);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v57, (uint64_t)&v58);
  v22 = v58;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v47);
  outlined init with take of BNNS.Shape((uint64_t)v47, (uint64_t)&v48);
  outlined init with take of BNNS.Shape((uint64_t)&v48, (uint64_t)v56);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)&v48, (uint64_t)v56);
  BNNS.Shape.stride.getter();
  out_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v39, v40, v41, v42, v43, v44, v45, v46, v39, v40, v41, v42, v43, v44, v45, v46);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v39);
  outlined init with take of BNNS.Shape((uint64_t)&v39, (uint64_t)v56);
  outlined init with take of BNNS.Shape((uint64_t)v56, (uint64_t)v38);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v56, (uint64_t)v38);
  BNNS.Shape.stride.getter();
  out_delta_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v30, v31, v32, v33, v34, v35, v36, v37, v30, v31, v32, v33, v34, v35, v36, v37);
  v17 = *a6;
  isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
  *a6 = v17;
  if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    v17 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v17 + 2), 0, v17);
  *a6 = v17;
  swift_bridgeObjectRetain();
  v19 = BNNSArithmeticFilterApplyBackwardBatch(v25, a5, 2uLL, (const void **)v17 + 4, in_stride, v26, (const size_t *)(a8 + 32), v22, out_stride, a1, out_delta_stride);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  result = swift_bridgeObjectRelease();
  *a7 = v19;
  return result;
}

uint64_t specialized static BNNS.arithmeticLayerApply(_:batchSize:input:output:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4)
{
  void *v7;
  size_t v8;
  int v9;
  uint64_t result;
  _BYTE *v11;
  _BYTE *v12;
  void *v13;
  void *v14;
  uint64_t v15;
  unint64_t v16;
  unint64_t v17;
  unint64_t v18;
  unint64_t v19;
  unint64_t v20;
  unint64_t v21;
  unint64_t v22;
  unint64_t v23;
  void *in;
  _BYTE v25[136];
  _BYTE v26[136];
  _BYTE v27[136];
  _BYTE v28[136];
  _BYTE v29[8];
  _BYTE v30[8];
  void *v31;
  _QWORD v32[3];

  v32[1] = *MEMORY[0x1E0C80C00];
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v30);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v30, (uint64_t)&v31);
  v7 = v31;
  if (v31
    && (outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v29),
        outlined init with take of UnsafeMutableRawPointer?((uint64_t)v29, (uint64_t)v32),
        v32[0]))
  {
    in = v7;
    v13 = (void *)v32[0];
    v14 = *(void **)(a1 + 16);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
    v15 = swift_allocObject();
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v26);
    outlined init with take of BNNS.Shape((uint64_t)v26, (uint64_t)v27);
    outlined init with take of BNNS.Shape((uint64_t)v27, (uint64_t)v25);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v27, (uint64_t)v25);
    BNNS.Shape.stride.getter();
    *(_QWORD *)(v15 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v16, v17, v18, v19, v20, v21, v22, v23, v16, v17, v18, v19, v20, v21, v22, v23);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v26);
    outlined init with take of BNNS.Shape((uint64_t)v26, (uint64_t)v28);
    outlined init with take of BNNS.Shape((uint64_t)v28, (uint64_t)v25);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v28, (uint64_t)v25);
    BNNS.Shape.stride.getter();
    v8 = specialized static BNNS.calculateBatchStride(size:stride:)(v16, v17, v18, v19, v20, v21, v22, v23, v16, v17, v18, v19, v20, v21, v22, v23);
    v9 = BNNSArithmeticFilterApplyBatch(v14, a2, 1uLL, (const void **)&in, (const size_t *)(v15 + 32), v13, v8);
    swift_setDeallocating();
    result = swift_deallocClassInstance();
    if (!v9)
      return result;
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v11 = 0;
  }
  else
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v12 = 2;
  }
  return swift_willThrow();
}

uint64_t specialized static BNNS.arithmeticLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4, _OWORD *a5, __int128 *a6)
{
  uint64_t inited;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  _BYTE *v23;
  uint64_t v25;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  char *v37;
  BNNSNDArrayDescriptor v38;
  _OWORD v39[11];
  _BYTE v40[136];
  _BYTE v41[8];
  _QWORD v42[4];

  v42[1] = *MEMORY[0x1E0C80C00];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer?>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5E430;
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v41);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v41, (uint64_t)v42);
  *(_QWORD *)(inited + 32) = v42[0];
  v37 = (char *)inited;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  v25 = swift_initStackObject();
  *(_OWORD *)(v25 + 16) = xmmword_1CAB5E430;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v39);
  outlined init with take of BNNS.Shape((uint64_t)v39, (uint64_t)v40);
  outlined init with take of BNNS.Shape((uint64_t)v40, (uint64_t)&v38);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v40, (uint64_t)&v38);
  BNNS.Shape.stride.getter();
  *(_QWORD *)(v25 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v29, v30, v31, v32, v33, v34, v35, v36, v29, v30, v31, v32, v33, v34, v35, v36);
  v10 = a6[8];
  v11 = a6[9];
  v12 = a6[6];
  v39[7] = a6[7];
  v39[8] = v10;
  v13 = a6[10];
  v39[9] = v11;
  v39[10] = v13;
  v14 = a6[4];
  v15 = a6[5];
  v16 = a6[2];
  v39[3] = a6[3];
  v39[4] = v14;
  v39[5] = v15;
  v39[6] = v12;
  v17 = *a6;
  v39[1] = a6[1];
  v39[2] = v16;
  v18 = a5[9];
  *(_OWORD *)&v38.stride[7] = a5[8];
  *(_OWORD *)&v38.data_type = v18;
  *(_OWORD *)&v38.table_data_type = a5[10];
  v39[0] = v17;
  v19 = a5[5];
  *(_OWORD *)&v38.size[7] = a5[4];
  *(_OWORD *)&v38.stride[1] = v19;
  v20 = a5[7];
  *(_OWORD *)&v38.stride[3] = a5[6];
  *(_OWORD *)&v38.stride[5] = v20;
  v21 = a5[1];
  *(_OWORD *)&v38.flags = *a5;
  *(_OWORD *)&v38.size[1] = v21;
  v22 = a5[3];
  *(_OWORD *)&v38.size[3] = a5[2];
  *(_OWORD *)&v38.size[5] = v22;
  closure #1 in closure #1 in static BNNS.arithmeticLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:)(&v38, (uint64_t)v39, a1, a2, &v37, v25, a4, (int *)&v29);
  swift_setDeallocating();
  if ((_DWORD)v29)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v23 = 0;
    swift_willThrow();
  }
  return swift_bridgeObjectRelease();
}

uint64_t type metadata accessor for BNNS.BinaryArithmeticLayer()
{
  return objc_opt_self();
}

uint64_t specialized static BNNS.arithmeticLayerApply(_:batchSize:inputA:inputB:output:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  unint64_t v15;
  size_t v16;
  int v17;
  uint64_t result;
  _BYTE *v19;
  _BYTE *v20;
  unint64_t v21;
  void *v22;
  const void **v23;
  void *out;
  uint64_t v25;
  unint64_t v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  unint64_t v37;
  unint64_t v38;
  _BYTE v39[136];
  _BYTE v40[136];
  _BYTE v41[136];
  _BYTE v42[136];
  _BYTE v43[136];
  _BYTE v44[8];
  _BYTE v45[8];
  _BYTE v46[8];
  uint64_t v47;
  uint64_t v48;
  void *v49;

  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v46);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v46, (uint64_t)&v47);
  v9 = v47;
  if (v47
    && (outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v45),
        outlined init with take of UnsafeMutableRawPointer?((uint64_t)v45, (uint64_t)&v48),
        (v10 = v48) != 0)
    && (outlined init with take of UnsafeMutableRawPointer?(a5 + 136, (uint64_t)v44),
        outlined init with take of UnsafeMutableRawPointer?((uint64_t)v44, (uint64_t)&v49),
        v49))
  {
    out = v49;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
    v11 = swift_allocObject();
    *(_OWORD *)(v11 + 16) = xmmword_1CAB5E440;
    *(_QWORD *)(v11 + 32) = v9;
    v23 = (const void **)(v11 + 32);
    *(_QWORD *)(v11 + 40) = v10;
    v22 = *(void **)(a1 + 16);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
    v25 = swift_allocObject();
    *(_OWORD *)(v25 + 16) = xmmword_1CAB5E440;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v40);
    outlined init with take of BNNS.Shape((uint64_t)v40, (uint64_t)v41);
    outlined init with take of BNNS.Shape((uint64_t)v41, (uint64_t)v39);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v41, (uint64_t)v39);
    BNNS.Shape.stride.getter();
    *(_QWORD *)(v25 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v34, *((unint64_t *)&v34 + 1), v35, *((unint64_t *)&v35 + 1), v36, *((unint64_t *)&v36 + 1), v37, v38, v34, *((unint64_t *)&v34 + 1), v35, *((unint64_t *)&v35 + 1), v36, *((unint64_t *)&v36 + 1), v37, v38);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v39);
    outlined init with take of BNNS.Shape((uint64_t)v39, (uint64_t)v42);
    outlined init with take of BNNS.Shape((uint64_t)v42, (uint64_t)&v34);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v42, (uint64_t)&v34);
    BNNS.Shape.stride.getter();
    *(_QWORD *)(v25 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v26, v27, v28, v29, v30, v31, v32, v33, v26, v27, v28, v29, v30, v31, v32, v33);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v40);
    outlined init with take of BNNS.Shape((uint64_t)v40, (uint64_t)v43);
    outlined init with take of BNNS.Shape((uint64_t)v43, (uint64_t)v39);
    BNNS.Shape.size.getter();
    v12 = v34;
    v13 = v35;
    v14 = v36;
    v15 = v37;
    v21 = v38;
    outlined init with take of BNNS.Shape((uint64_t)v43, (uint64_t)v39);
    BNNS.Shape.stride.getter();
    v16 = specialized static BNNS.calculateBatchStride(size:stride:)(v12, *((unint64_t *)&v12 + 1), v13, *((unint64_t *)&v13 + 1), v14, *((unint64_t *)&v14 + 1), v15, v21, v34, *((unint64_t *)&v34 + 1), v35, *((unint64_t *)&v35 + 1), v36, *((unint64_t *)&v36 + 1), v37, v38);
    v17 = BNNSArithmeticFilterApplyBatch(v22, a2, 2uLL, v23, (const size_t *)(v25 + 32), out, v16);
    swift_bridgeObjectRelease();
    result = swift_bridgeObjectRelease();
    if (!v17)
      return result;
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v19 = 0;
  }
  else
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v20 = 2;
  }
  return swift_willThrow();
}

uint64_t specialized static BNNS.arithmeticLayerApplyBackward(_:batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4, uint64_t a5, _OWORD *a6, _OWORD *a7, __int128 *a8)
{
  uint64_t v13;
  unint64_t v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  _BYTE *v33;
  uint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  unint64_t v46;
  char *v47;
  BNNSNDArrayDescriptor v48;
  _OWORD v49[11];
  _OWORD v50[11];
  _BYTE v51[136];
  _BYTE v52[136];
  _BYTE v53[8];
  _BYTE v54[8];
  uint64_t v55;
  _QWORD v56[4];

  v56[1] = *MEMORY[0x1E0C80C00];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer?>);
  v13 = swift_allocObject();
  *(_OWORD *)(v13 + 16) = xmmword_1CAB5E440;
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v54);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v54, (uint64_t)&v55);
  *(_QWORD *)(v13 + 32) = v55;
  outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v53);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v53, (uint64_t)v56);
  *(_QWORD *)(v13 + 40) = v56[0];
  v47 = (char *)v13;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  v38 = swift_allocObject();
  *(_OWORD *)(v38 + 16) = xmmword_1CAB5E440;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v50);
  outlined init with take of BNNS.Shape((uint64_t)v50, (uint64_t)v51);
  outlined init with take of BNNS.Shape((uint64_t)v51, (uint64_t)v49);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v51, (uint64_t)v49);
  BNNS.Shape.stride.getter();
  *(_QWORD *)(v38 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(*(unint64_t *)&v48.flags, v48.size[0], v48.size[1], v48.size[2], v48.size[3], v48.size[4], v48.size[5], v48.size[6], *(unint64_t *)&v48.flags, v48.size[0], v48.size[1], v48.size[2], v48.size[3], v48.size[4], v48.size[5], v48.size[6]);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v49);
  outlined init with take of BNNS.Shape((uint64_t)v49, (uint64_t)v52);
  outlined init with take of BNNS.Shape((uint64_t)v52, (uint64_t)&v48);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v52, (uint64_t)&v48);
  BNNS.Shape.stride.getter();
  v14 = specialized static BNNS.calculateBatchStride(size:stride:)(v39, v40, v41, v42, v43, v44, v45, v46, v39, v40, v41, v42, v43, v44, v45, v46);
  v15 = a7[9];
  v50[8] = a7[8];
  v50[9] = v15;
  v50[10] = a7[10];
  v16 = a7[5];
  v50[4] = a7[4];
  v50[5] = v16;
  v17 = a7[7];
  v50[6] = a7[6];
  v50[7] = v17;
  v18 = a7[1];
  v50[0] = *a7;
  v50[1] = v18;
  v19 = a7[3];
  v50[2] = a7[2];
  v50[3] = v19;
  v20 = a8[8];
  v21 = a8[9];
  v22 = a8[6];
  v49[7] = a8[7];
  v49[8] = v20;
  v23 = a8[10];
  v49[9] = v21;
  v49[10] = v23;
  v24 = a8[4];
  v25 = a8[5];
  v26 = a8[2];
  v49[3] = a8[3];
  v49[4] = v24;
  *(_QWORD *)(v38 + 40) = v14;
  v49[5] = v25;
  v49[6] = v22;
  v27 = *a8;
  v49[1] = a8[1];
  v49[2] = v26;
  v28 = a6[9];
  *(_OWORD *)&v48.stride[7] = a6[8];
  *(_OWORD *)&v48.data_type = v28;
  *(_OWORD *)&v48.table_data_type = a6[10];
  v49[0] = v27;
  v29 = a6[5];
  *(_OWORD *)&v48.size[7] = a6[4];
  *(_OWORD *)&v48.stride[1] = v29;
  v30 = a6[7];
  *(_OWORD *)&v48.stride[3] = a6[6];
  *(_OWORD *)&v48.stride[5] = v30;
  v31 = a6[1];
  *(_OWORD *)&v48.flags = *a6;
  *(_OWORD *)&v48.size[1] = v31;
  v32 = a6[3];
  *(_OWORD *)&v48.size[3] = a6[2];
  *(_OWORD *)&v48.size[5] = v32;
  closure #1 in closure #1 in closure #1 in static BNNS.arithmeticLayerApplyBackward(_:batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(&v48, (uint64_t)v50, (uint64_t)v49, a1, a2, &v47, (int *)&v39, v38, a5);
  swift_setDeallocating();
  swift_deallocClassInstance();
  if ((_DWORD)v39)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v33 = 0;
    swift_willThrow();
  }
  return swift_bridgeObjectRelease();
}

unint64_t lazy protocol witness table accessor for type BNNS.DescriptorType and conformance BNNS.DescriptorType()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNS.DescriptorType and conformance BNNS.DescriptorType;
  if (!lazy protocol witness table cache variable for type BNNS.DescriptorType and conformance BNNS.DescriptorType)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNS.DescriptorType, &type metadata for BNNS.DescriptorType);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.DescriptorType and conformance BNNS.DescriptorType);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type BNNS.ArithmeticUnaryFunction and conformance BNNS.ArithmeticUnaryFunction()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNS.ArithmeticUnaryFunction and conformance BNNS.ArithmeticUnaryFunction;
  if (!lazy protocol witness table cache variable for type BNNS.ArithmeticUnaryFunction and conformance BNNS.ArithmeticUnaryFunction)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNS.ArithmeticUnaryFunction, &type metadata for BNNS.ArithmeticUnaryFunction);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.ArithmeticUnaryFunction and conformance BNNS.ArithmeticUnaryFunction);
  }
  return result;
}

uint64_t associated type witness table accessor for CaseIterable.AllCases : Collection in BNNS.ArithmeticUnaryFunction()
{
  return lazy protocol witness table accessor for type [BNNS.ArithmeticUnaryFunction] and conformance [A](&lazy protocol witness table cache variable for type [BNNS.ArithmeticUnaryFunction] and conformance [A], &demangling cache variable for type metadata for [BNNS.ArithmeticUnaryFunction]);
}

unint64_t lazy protocol witness table accessor for type BNNS.ArithmeticBinaryFunction and conformance BNNS.ArithmeticBinaryFunction()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNS.ArithmeticBinaryFunction and conformance BNNS.ArithmeticBinaryFunction;
  if (!lazy protocol witness table cache variable for type BNNS.ArithmeticBinaryFunction and conformance BNNS.ArithmeticBinaryFunction)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNS.ArithmeticBinaryFunction, &type metadata for BNNS.ArithmeticBinaryFunction);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.ArithmeticBinaryFunction and conformance BNNS.ArithmeticBinaryFunction);
  }
  return result;
}

uint64_t associated type witness table accessor for CaseIterable.AllCases : Collection in BNNS.ArithmeticBinaryFunction()
{
  return lazy protocol witness table accessor for type [BNNS.ArithmeticUnaryFunction] and conformance [A](&lazy protocol witness table cache variable for type [BNNS.ArithmeticBinaryFunction] and conformance [A], &demangling cache variable for type metadata for [BNNS.ArithmeticBinaryFunction]);
}

uint64_t lazy protocol witness table accessor for type [BNNS.ArithmeticUnaryFunction] and conformance [A](unint64_t *a1, uint64_t *a2)
{
  uint64_t result;
  uint64_t v4;

  result = *a1;
  if (!result)
  {
    v4 = __swift_instantiateConcreteTypeFromMangledNameAbstract(a2);
    result = MEMORY[0x1D1794D08](MEMORY[0x1E0DEAF50], v4);
    atomic_store(result, a1);
  }
  return result;
}

uint64_t storeEnumTagSinglePayload for BNNS.DescriptorType(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 2 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 2) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFE)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFD)
    return ((uint64_t (*)(void))((char *)&loc_1CAB3A488 + 4 * byte_1CAB61FD5[v4]))();
  *a1 = a2 + 2;
  return ((uint64_t (*)(void))((char *)sub_1CAB3A4BC + 4 * asc_1CAB61FD0[v4]))();
}

uint64_t sub_1CAB3A4BC(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB3A4C4(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB3A4CCLL);
  return result;
}

uint64_t sub_1CAB3A4D8(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB3A4E0);
  *(_BYTE *)result = a2 + 2;
  return result;
}

uint64_t sub_1CAB3A4E4(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB3A4EC(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.DescriptorType()
{
  return &type metadata for BNNS.DescriptorType;
}

uint64_t getEnumTagSinglePayload for BNNS.ArithmeticUnaryFunction(unsigned __int8 *a1, unsigned int a2)
{
  int v2;
  int v3;
  int v4;
  unsigned int v6;
  BOOL v7;
  int v8;

  if (!a2)
    return 0;
  if (a2 < 0xE6)
    goto LABEL_17;
  if (a2 + 26 >= 0xFFFF00)
    v2 = 4;
  else
    v2 = 2;
  if ((a2 + 26) >> 8 < 0xFF)
    v3 = 1;
  else
    v3 = v2;
  if (v3 == 4)
  {
    v4 = *(_DWORD *)(a1 + 1);
    if (v4)
      return (*a1 | (v4 << 8)) - 26;
  }
  else
  {
    if (v3 == 2)
    {
      v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1))
        goto LABEL_17;
      return (*a1 | (v4 << 8)) - 26;
    }
    v4 = a1[1];
    if (a1[1])
      return (*a1 | (v4 << 8)) - 26;
  }
LABEL_17:
  v6 = *a1;
  v7 = v6 >= 0x1B;
  v8 = v6 - 27;
  if (!v7)
    v8 = -1;
  return (v8 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.ArithmeticUnaryFunction(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 26 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 26) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xE6)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xE5)
    return ((uint64_t (*)(void))((char *)&loc_1CAB3A5E4 + 4 * byte_1CAB61FDF[v4]))();
  *a1 = a2 + 26;
  return ((uint64_t (*)(void))((char *)sub_1CAB3A618 + 4 * byte_1CAB61FDA[v4]))();
}

uint64_t sub_1CAB3A618(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB3A620(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB3A628);
  return result;
}

uint64_t sub_1CAB3A634(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB3A63CLL);
  *(_BYTE *)result = a2 + 26;
  return result;
}

uint64_t sub_1CAB3A640(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB3A648(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.ArithmeticUnaryFunction()
{
  return &type metadata for BNNS.ArithmeticUnaryFunction;
}

uint64_t getEnumTagSinglePayload for BNNS.ArithmeticBinaryFunction(unsigned __int8 *a1, unsigned int a2)
{
  int v2;
  int v3;
  int v4;
  unsigned int v6;
  BOOL v7;
  int v8;

  if (!a2)
    return 0;
  if (a2 < 0xF5)
    goto LABEL_17;
  if (a2 + 11 >= 0xFFFF00)
    v2 = 4;
  else
    v2 = 2;
  if ((a2 + 11) >> 8 < 0xFF)
    v3 = 1;
  else
    v3 = v2;
  if (v3 == 4)
  {
    v4 = *(_DWORD *)(a1 + 1);
    if (v4)
      return (*a1 | (v4 << 8)) - 11;
  }
  else
  {
    if (v3 == 2)
    {
      v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1))
        goto LABEL_17;
      return (*a1 | (v4 << 8)) - 11;
    }
    v4 = a1[1];
    if (a1[1])
      return (*a1 | (v4 << 8)) - 11;
  }
LABEL_17:
  v6 = *a1;
  v7 = v6 >= 0xC;
  v8 = v6 - 12;
  if (!v7)
    v8 = -1;
  return (v8 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.ArithmeticBinaryFunction(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 11 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 11) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xF5)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xF4)
    return ((uint64_t (*)(void))((char *)&loc_1CAB3A740 + 4 * byte_1CAB61FE9[v4]))();
  *a1 = a2 + 11;
  return ((uint64_t (*)(void))((char *)sub_1CAB3A774 + 4 * byte_1CAB61FE4[v4]))();
}

uint64_t sub_1CAB3A774(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB3A77C(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB3A784);
  return result;
}

uint64_t sub_1CAB3A790(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB3A798);
  *(_BYTE *)result = a2 + 11;
  return result;
}

uint64_t sub_1CAB3A79C(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB3A7A4(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.ArithmeticBinaryFunction()
{
  return &type metadata for BNNS.ArithmeticBinaryFunction;
}

uint64_t method lookup function for BNNS.UnaryArithmeticLayer()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of BNNS.UnaryArithmeticLayer.apply(batchSize:input:output:)(uint64_t a1, uint64_t *a2, uint64_t *a3)
{
  uint64_t v3;
  uint64_t v4;
  int v5;
  uint64_t v6;
  int v7;
  uint64_t v8;
  int v9;
  uint64_t v10;
  int v11;
  uint64_t (*v12)(uint64_t, uint64_t *, uint64_t *);
  uint64_t v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  uint64_t v23;
  int v24;
  uint64_t v25;
  int v26;
  uint64_t v27;
  uint64_t v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  uint64_t v37;
  int v38;
  uint64_t v39;
  int v40;
  uint64_t v41;

  v4 = a2[17];
  v5 = *((_DWORD *)a2 + 36);
  v6 = a2[19];
  v7 = *((_DWORD *)a2 + 40);
  v8 = a3[17];
  v9 = *((_DWORD *)a3 + 36);
  v10 = a3[19];
  v11 = *((_DWORD *)a3 + 40);
  v12 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *))(*(_QWORD *)v3 + 96);
  v28 = *a2;
  v29 = *(_OWORD *)(a2 + 1);
  v30 = *(_OWORD *)(a2 + 3);
  v31 = *(_OWORD *)(a2 + 5);
  v32 = *(_OWORD *)(a2 + 7);
  v33 = *(_OWORD *)(a2 + 9);
  v34 = *(_OWORD *)(a2 + 11);
  v35 = *(_OWORD *)(a2 + 13);
  v36 = *(_OWORD *)(a2 + 15);
  v37 = v4;
  v38 = v5;
  v39 = v6;
  v40 = v7;
  v41 = *(uint64_t *)((char *)a2 + 164);
  v14 = *a3;
  v15 = *(_OWORD *)(a3 + 1);
  v16 = *(_OWORD *)(a3 + 3);
  v17 = *(_OWORD *)(a3 + 5);
  v18 = *(_OWORD *)(a3 + 7);
  v19 = *(_OWORD *)(a3 + 9);
  v20 = *(_OWORD *)(a3 + 11);
  v21 = *(_OWORD *)(a3 + 13);
  v22 = *(_OWORD *)(a3 + 15);
  v23 = v8;
  v24 = v9;
  v25 = v10;
  v26 = v11;
  v27 = *(uint64_t *)((char *)a3 + 164);
  return v12(a1, &v28, &v14);
}

uint64_t dispatch thunk of BNNS.UnaryArithmeticLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5)
{
  uint64_t v5;
  uint64_t v6;
  int v7;
  uint64_t v8;
  int v9;
  uint64_t v10;
  int v11;
  uint64_t v12;
  int v13;
  uint64_t v14;
  int v15;
  uint64_t v16;
  int v17;
  uint64_t v18;
  int v19;
  uint64_t v20;
  int v21;
  uint64_t (*v22)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *);
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  uint64_t v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  uint64_t v40;
  __int128 v41;
  uint64_t v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  uint64_t v52;
  int v53;
  uint64_t v54;
  int v55;
  uint64_t v56;
  uint64_t v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  __int128 v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  __int128 v65;
  uint64_t v66;
  int v67;
  uint64_t v68;
  int v69;
  uint64_t v70;
  uint64_t v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  __int128 v76;
  __int128 v77;
  __int128 v78;
  __int128 v79;
  uint64_t v80;
  int v81;
  uint64_t v82;
  int v83;
  uint64_t v84;
  uint64_t v85;
  __int128 v86;
  __int128 v87;
  __int128 v88;
  __int128 v89;
  __int128 v90;
  __int128 v91;
  __int128 v92;
  __int128 v93;
  uint64_t v94;
  int v95;
  uint64_t v96;
  int v97;
  uint64_t v98;

  v6 = a2[17];
  v7 = *((_DWORD *)a2 + 36);
  v8 = a2[19];
  v9 = *((_DWORD *)a2 + 40);
  v10 = a3[17];
  v11 = *((_DWORD *)a3 + 36);
  v12 = a3[19];
  v13 = *((_DWORD *)a3 + 40);
  v14 = a4[17];
  v15 = *((_DWORD *)a4 + 36);
  v16 = a4[19];
  v17 = *((_DWORD *)a4 + 40);
  v18 = a5[17];
  v19 = *((_DWORD *)a5 + 36);
  v20 = a5[19];
  v21 = *((_DWORD *)a5 + 40);
  v22 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(_QWORD *)v5 + 104);
  v85 = *a2;
  v86 = *(_OWORD *)(a2 + 1);
  v87 = *(_OWORD *)(a2 + 3);
  v88 = *(_OWORD *)(a2 + 5);
  v89 = *(_OWORD *)(a2 + 7);
  v90 = *(_OWORD *)(a2 + 9);
  v91 = *(_OWORD *)(a2 + 11);
  v92 = *(_OWORD *)(a2 + 13);
  v93 = *(_OWORD *)(a2 + 15);
  v94 = v6;
  v95 = v7;
  v96 = v8;
  v97 = v9;
  v98 = *(uint64_t *)((char *)a2 + 164);
  v71 = *a3;
  v72 = *(_OWORD *)(a3 + 1);
  v73 = *(_OWORD *)(a3 + 3);
  v74 = *(_OWORD *)(a3 + 5);
  v75 = *(_OWORD *)(a3 + 7);
  v76 = *(_OWORD *)(a3 + 9);
  v77 = *(_OWORD *)(a3 + 11);
  v23 = *(_OWORD *)(a3 + 15);
  v78 = *(_OWORD *)(a3 + 13);
  v24 = *(_OWORD *)(a4 + 1);
  v25 = *(_OWORD *)(a4 + 3);
  v26 = *(_OWORD *)(a4 + 5);
  v27 = *(_OWORD *)(a4 + 7);
  v28 = *(_OWORD *)(a4 + 9);
  v29 = *(_OWORD *)(a4 + 11);
  v30 = *(_OWORD *)(a4 + 13);
  v31 = *a4;
  v32 = *(_OWORD *)(a4 + 15);
  v33 = *(_OWORD *)(a5 + 1);
  v34 = *(_OWORD *)(a5 + 3);
  v35 = *(_OWORD *)(a5 + 5);
  v36 = *(_OWORD *)(a5 + 7);
  v37 = *(_OWORD *)(a5 + 9);
  v38 = *(_OWORD *)(a5 + 11);
  v39 = *(_OWORD *)(a5 + 13);
  v40 = *a5;
  v41 = *(_OWORD *)(a5 + 15);
  v79 = v23;
  v80 = v10;
  v81 = v11;
  v82 = v12;
  v83 = v13;
  v84 = *(uint64_t *)((char *)a3 + 164);
  *(_QWORD *)&v23 = *(uint64_t *)((char *)a5 + 164);
  v57 = v31;
  v58 = v24;
  *(_QWORD *)&v24 = *(uint64_t *)((char *)a4 + 164);
  v59 = v25;
  v60 = v26;
  v61 = v27;
  v62 = v28;
  v63 = v29;
  v64 = v30;
  v65 = v32;
  v66 = v14;
  v67 = v15;
  v68 = v16;
  v69 = v17;
  v70 = v24;
  v43 = v40;
  v44 = v33;
  v45 = v34;
  v46 = v35;
  v47 = v36;
  v48 = v37;
  v49 = v38;
  v50 = v39;
  v51 = v41;
  v52 = v18;
  v53 = v19;
  v54 = v20;
  v55 = v21;
  v56 = v23;
  return v22(a1, &v85, &v71, &v57, &v43);
}

uint64_t method lookup function for BNNS.BinaryArithmeticLayer()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of BNNS.BinaryArithmeticLayer.apply(batchSize:inputA:inputB:output:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4)
{
  uint64_t v4;
  uint64_t v5;
  int v6;
  uint64_t v7;
  int v8;
  uint64_t v9;
  int v10;
  uint64_t v11;
  int v12;
  uint64_t v13;
  int v14;
  uint64_t v15;
  int v16;
  uint64_t (*v17)(uint64_t, uint64_t *, uint64_t *, uint64_t *);
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  uint64_t v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  uint64_t v33;
  int v34;
  uint64_t v35;
  int v36;
  uint64_t v37;
  uint64_t v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  uint64_t v47;
  int v48;
  uint64_t v49;
  int v50;
  uint64_t v51;
  uint64_t v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  uint64_t v61;
  int v62;
  uint64_t v63;
  int v64;
  uint64_t v65;

  v5 = a2[17];
  v6 = *((_DWORD *)a2 + 36);
  v7 = a2[19];
  v8 = *((_DWORD *)a2 + 40);
  v9 = a3[17];
  v10 = *((_DWORD *)a3 + 36);
  v11 = a3[19];
  v12 = *((_DWORD *)a3 + 40);
  v13 = a4[17];
  v14 = *((_DWORD *)a4 + 36);
  v15 = a4[19];
  v16 = *((_DWORD *)a4 + 40);
  v17 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *))(*(_QWORD *)v4 + 96);
  v52 = *a2;
  v53 = *(_OWORD *)(a2 + 1);
  v54 = *(_OWORD *)(a2 + 3);
  v55 = *(_OWORD *)(a2 + 5);
  v56 = *(_OWORD *)(a2 + 7);
  v57 = *(_OWORD *)(a2 + 9);
  v58 = *(_OWORD *)(a2 + 11);
  v59 = *(_OWORD *)(a2 + 13);
  v60 = *(_OWORD *)(a2 + 15);
  v61 = v5;
  v62 = v6;
  v63 = v7;
  v64 = v8;
  v65 = *(uint64_t *)((char *)a2 + 164);
  v38 = *a3;
  v39 = *(_OWORD *)(a3 + 1);
  v40 = *(_OWORD *)(a3 + 3);
  v41 = *(_OWORD *)(a3 + 5);
  v42 = *(_OWORD *)(a3 + 7);
  v43 = *(_OWORD *)(a3 + 9);
  v44 = *(_OWORD *)(a3 + 11);
  v45 = *(_OWORD *)(a3 + 13);
  v46 = *(_OWORD *)(a3 + 15);
  v47 = v9;
  v48 = v10;
  v49 = v11;
  v50 = v12;
  v51 = *(uint64_t *)((char *)a3 + 164);
  v24 = *a4;
  v25 = *(_OWORD *)(a4 + 1);
  v26 = *(_OWORD *)(a4 + 3);
  v18 = *(_OWORD *)(a4 + 7);
  v19 = *(_OWORD *)(a4 + 9);
  v20 = *(_OWORD *)(a4 + 11);
  v21 = *(_OWORD *)(a4 + 13);
  v22 = *(_OWORD *)(a4 + 15);
  v27 = *(_OWORD *)(a4 + 5);
  v28 = v18;
  v29 = v19;
  v30 = v20;
  v31 = v21;
  v32 = v22;
  v33 = v13;
  v34 = v14;
  v35 = v15;
  v36 = v16;
  v37 = *(uint64_t *)((char *)a4 + 164);
  return v17(a1, &v52, &v38, &v24);
}

uint64_t dispatch thunk of BNNS.BinaryArithmeticLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5, uint64_t *a6, uint64_t *a7)
{
  uint64_t v7;
  uint64_t v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  uint64_t v18;
  int v19;
  uint64_t v20;
  int v21;
  uint64_t v22;
  uint64_t v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  uint64_t v32;
  int v33;
  uint64_t v34;
  int v35;
  uint64_t v36;
  uint64_t v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  uint64_t v46;
  int v47;
  uint64_t v48;
  int v49;
  uint64_t v50;
  uint64_t v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  uint64_t v60;
  int v61;
  uint64_t v62;
  int v63;
  uint64_t v64;
  uint64_t v65;
  __int128 v66;
  __int128 v67;
  __int128 v68;
  __int128 v69;
  __int128 v70;
  __int128 v71;
  __int128 v72;
  __int128 v73;
  uint64_t v74;
  int v75;
  uint64_t v76;
  int v77;
  uint64_t v78;
  uint64_t v79;
  __int128 v80;
  __int128 v81;
  __int128 v82;
  __int128 v83;
  __int128 v84;
  __int128 v85;
  __int128 v86;
  __int128 v87;
  uint64_t v88;
  int v89;
  uint64_t v90;
  int v91;
  uint64_t v92;

  v85 = *(_OWORD *)(a2 + 11);
  v86 = *(_OWORD *)(a2 + 13);
  v87 = *(_OWORD *)(a2 + 15);
  v92 = *(uint64_t *)((char *)a2 + 164);
  v80 = *(_OWORD *)(a2 + 1);
  v81 = *(_OWORD *)(a2 + 3);
  v82 = *(_OWORD *)(a2 + 5);
  v83 = *(_OWORD *)(a2 + 7);
  v84 = *(_OWORD *)(a2 + 9);
  v71 = *(_OWORD *)(a3 + 11);
  v72 = *(_OWORD *)(a3 + 13);
  v73 = *(_OWORD *)(a3 + 15);
  v78 = *(uint64_t *)((char *)a3 + 164);
  v66 = *(_OWORD *)(a3 + 1);
  v67 = *(_OWORD *)(a3 + 3);
  v68 = *(_OWORD *)(a3 + 5);
  v69 = *(_OWORD *)(a3 + 7);
  v70 = *(_OWORD *)(a3 + 9);
  v57 = *(_OWORD *)(a4 + 11);
  v58 = *(_OWORD *)(a4 + 13);
  v59 = *(_OWORD *)(a4 + 15);
  v64 = *(uint64_t *)((char *)a4 + 164);
  v52 = *(_OWORD *)(a4 + 1);
  v53 = *(_OWORD *)(a4 + 3);
  v54 = *(_OWORD *)(a4 + 5);
  v55 = *(_OWORD *)(a4 + 7);
  v56 = *(_OWORD *)(a4 + 9);
  v43 = *(_OWORD *)(a5 + 11);
  v44 = *(_OWORD *)(a5 + 13);
  v45 = *(_OWORD *)(a5 + 15);
  v50 = *(uint64_t *)((char *)a5 + 164);
  v38 = *(_OWORD *)(a5 + 1);
  v39 = *(_OWORD *)(a5 + 3);
  v40 = *(_OWORD *)(a5 + 5);
  v41 = *(_OWORD *)(a5 + 7);
  v42 = *(_OWORD *)(a5 + 9);
  v29 = *(_OWORD *)(a6 + 11);
  v30 = *(_OWORD *)(a6 + 13);
  v31 = *(_OWORD *)(a6 + 15);
  v36 = *(uint64_t *)((char *)a6 + 164);
  v79 = *a2;
  v65 = *a3;
  v51 = *a4;
  v37 = *a5;
  v23 = *a6;
  v24 = *(_OWORD *)(a6 + 1);
  v25 = *(_OWORD *)(a6 + 3);
  v26 = *(_OWORD *)(a6 + 5);
  v27 = *(_OWORD *)(a6 + 7);
  v28 = *(_OWORD *)(a6 + 9);
  v9 = *a7;
  v10 = *(_OWORD *)(a7 + 1);
  v11 = *(_OWORD *)(a7 + 3);
  v12 = *(_OWORD *)(a7 + 5);
  v13 = *(_OWORD *)(a7 + 7);
  v14 = *(_OWORD *)(a7 + 9);
  v15 = *(_OWORD *)(a7 + 11);
  v16 = *(_OWORD *)(a7 + 13);
  v17 = *(_OWORD *)(a7 + 15);
  v22 = *(uint64_t *)((char *)a7 + 164);
  v88 = a2[17];
  v89 = *((_DWORD *)a2 + 36);
  v90 = a2[19];
  v91 = *((_DWORD *)a2 + 40);
  v74 = a3[17];
  v75 = *((_DWORD *)a3 + 36);
  v76 = a3[19];
  v77 = *((_DWORD *)a3 + 40);
  v60 = a4[17];
  v61 = *((_DWORD *)a4 + 36);
  v62 = a4[19];
  v63 = *((_DWORD *)a4 + 40);
  v46 = a5[17];
  v47 = *((_DWORD *)a5 + 36);
  v48 = a5[19];
  v49 = *((_DWORD *)a5 + 40);
  v32 = a6[17];
  v33 = *((_DWORD *)a6 + 36);
  v34 = a6[19];
  v35 = *((_DWORD *)a6 + 40);
  v18 = a7[17];
  v19 = *((_DWORD *)a7 + 36);
  v20 = a7[19];
  v21 = *((_DWORD *)a7 + 40);
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(_QWORD *)v7 + 104))(a1, &v79, &v65, &v51, &v37, &v23, &v9);
}

uint64_t static vDSP.linearInterpolate<A, B>(_:_:using:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v5;

  v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(_:_:using:));
}

{
  uint64_t v5;

  v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(_:_:using:));
}

uint64_t closure #1 in static vDSP.linearInterpolate<A, B>(_:_:using:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9)
{
  uint64_t v17;
  uint64_t v18;
  uint64_t result;

  v17 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v18 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(a3, a4, a1, a5, a6, v17, a7, a8, a9, v18);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  *a2 = result;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(_:_:using:)(uint64_t a1, uint64_t *a2)
{
  uint64_t v2;

  return closure #1 in static vDSP.linearInterpolate<A, B>(_:_:using:)(a1, a2, *(_QWORD *)(v2 + 48), *(_QWORD *)(v2 + 56), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), *(float *)(v2 + 64));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.linearInterpolate<A, B>(_:_:using:)(a1, a2, *(_QWORD *)(v2 + 48), *(_QWORD *)(v2 + 56), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), *(double *)(v2 + 64));
}

uint64_t static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, uint64_t a10)
{
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  void (*v22)(char *);
  uint64_t v23;
  uint64_t (*v24)(uint64_t, uint64_t);
  uint64_t v25;
  uint64_t v26;
  uint64_t (*v27)(uint64_t, uint64_t);
  uint64_t v28;
  uint64_t v29;
  uint64_t result;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t (*v50)(uint64_t, uint64_t);

  v46 = a2;
  v47 = a8;
  v49 = a6;
  v45 = *(_QWORD *)(a5 - 8);
  v15 = MEMORY[0x1E0C80A78](a1);
  v17 = (char *)&v40 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = *(_QWORD *)(v18 - 8);
  MEMORY[0x1E0C80A78](v15);
  v21 = (char *)&v40 - ((v20 + 15) & 0xFFFFFFFFFFFFFFF0);
  v22 = *(void (**)(char *))(v19 + 16);
  v43 = v23;
  v22(v21);
  v24 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  v44 = a7;
  v25 = v24(a4, a7);
  v42 = a10;
  v26 = *(_QWORD *)(a10 + 8);
  v27 = *(uint64_t (**)(uint64_t, uint64_t))(v26 + 16);
  v48 = a3;
  v28 = v49;
  v50 = v27;
  v29 = v27(v49, v26);
  result = (*(uint64_t (**)(char *, uint64_t))(v19 + 8))(v21, a4);
  if (v25 != v29)
  {
    __break(1u);
    goto LABEL_6;
  }
  v41 = a4;
  v31 = v45;
  v32 = v46;
  (*(void (**)(char *, uint64_t, uint64_t))(v45 + 16))(v17, v46, a5);
  v33 = v47;
  v34 = (*(uint64_t (**)(uint64_t, uint64_t))(v47 + 16))(a5, v47);
  v35 = v48;
  v36 = v50(v28, v26);
  result = (*(uint64_t (**)(char *, uint64_t))(v31 + 8))(v17, a5);
  if (v34 != v36)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  result = v50(v28, v26);
  if ((result & 0x8000000000000000) == 0)
  {
    v37 = MEMORY[0x1E0C80A78](result);
    v38 = v42;
    *(&v40 - 10) = v41;
    *(&v40 - 9) = a5;
    v39 = v44;
    *(&v40 - 8) = v28;
    *(&v40 - 7) = v39;
    *(&v40 - 6) = v33;
    *(&v40 - 5) = v38;
    *(&v40 - 4) = v32;
    *(&v40 - 3) = v35;
    *((float *)&v40 - 4) = a9;
    *(&v40 - 1) = v37;
    return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t)))(v39 + 24))(partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:));
  }
LABEL_7:
  __break(1u);
  return result;
}

void closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(float **a1, const float *__A, int a3, const float *__B, int a5, vDSP_Length __N, float a7)
{
  float v7;

  if (!__A)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!__B)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  v7 = a7;
  if (*a1)
  {
    vDSP_vintb(__A, 1, __B, 1, &v7, *a1, 1, __N);
    return;
  }
LABEL_7:
  __break(1u);
}

uint64_t closure #1 in static vDSP.linearInterpolate<A, B>(_:_:using:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9)
{
  uint64_t v17;
  uint64_t v18;
  uint64_t result;

  v17 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v18 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(a3, a4, a1, a5, a6, v17, a7, a8, a9, v18);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  *a2 = result;
  return result;
}

uint64_t static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, uint64_t a10)
{
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  void (*v22)(char *);
  uint64_t v23;
  uint64_t (*v24)(uint64_t, uint64_t);
  uint64_t v25;
  uint64_t v26;
  uint64_t (*v27)(uint64_t, uint64_t);
  uint64_t v28;
  uint64_t v29;
  uint64_t result;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t (*v50)(uint64_t, uint64_t);

  v46 = a2;
  v47 = a8;
  v49 = a6;
  v45 = *(_QWORD *)(a5 - 8);
  v15 = MEMORY[0x1E0C80A78](a1);
  v17 = (char *)&v40 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = *(_QWORD *)(v18 - 8);
  MEMORY[0x1E0C80A78](v15);
  v21 = (char *)&v40 - ((v20 + 15) & 0xFFFFFFFFFFFFFFF0);
  v22 = *(void (**)(char *))(v19 + 16);
  v43 = v23;
  v22(v21);
  v24 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  v44 = a7;
  v25 = v24(a4, a7);
  v42 = a10;
  v26 = *(_QWORD *)(a10 + 8);
  v27 = *(uint64_t (**)(uint64_t, uint64_t))(v26 + 16);
  v48 = a3;
  v28 = v49;
  v50 = v27;
  v29 = v27(v49, v26);
  result = (*(uint64_t (**)(char *, uint64_t))(v19 + 8))(v21, a4);
  if (v25 != v29)
  {
    __break(1u);
    goto LABEL_6;
  }
  v41 = a4;
  v31 = v45;
  v32 = v46;
  (*(void (**)(char *, uint64_t, uint64_t))(v45 + 16))(v17, v46, a5);
  v33 = v47;
  v34 = (*(uint64_t (**)(uint64_t, uint64_t))(v47 + 16))(a5, v47);
  v35 = v48;
  v36 = v50(v28, v26);
  result = (*(uint64_t (**)(char *, uint64_t))(v31 + 8))(v17, a5);
  if (v34 != v36)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  result = v50(v28, v26);
  if ((result & 0x8000000000000000) == 0)
  {
    v37 = MEMORY[0x1E0C80A78](result);
    v38 = v42;
    *(&v40 - 10) = v41;
    *(&v40 - 9) = a5;
    v39 = v44;
    *(&v40 - 8) = v28;
    *(&v40 - 7) = v39;
    *(&v40 - 6) = v33;
    *(&v40 - 5) = v38;
    *(&v40 - 4) = v32;
    *(&v40 - 3) = v35;
    *((double *)&v40 - 2) = a9;
    *(&v40 - 1) = v37;
    return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t)))(v39 + 24))(partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:));
  }
LABEL_7:
  __break(1u);
  return result;
}

void closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(double **a1, const double *__A, int a3, const double *__B, int a5, vDSP_Length __N, double a7)
{
  double v7;

  if (!__A)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!__B)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  v7 = a7;
  if (*a1)
  {
    vDSP_vintbD(__A, 1, __B, 1, &v7, *a1, 1, __N);
    return;
  }
LABEL_7:
  __break(1u);
}

uint64_t static vDSP.linearInterpolate<A, B>(elementsOf:using:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.linearInterpolate<A, B>(elementsOf:using:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(elementsOf:using:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.linearInterpolate<A, B>(elementsOf:using:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(elementsOf:using:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v14;
  uint64_t v15;
  char *v16;
  void (*v17)(char *);
  uint64_t v18;
  uint64_t (*v19)(uint64_t, uint64_t);
  uint64_t v20;
  uint64_t v21;
  uint64_t (*v22)(uint64_t, uint64_t);
  uint64_t v23;
  uint64_t result;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;

  v33 = a1;
  v34 = a4;
  v14 = *(_QWORD *)(a5 - 8);
  MEMORY[0x1E0C80A78](a1);
  v16 = (char *)&v30 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  v17 = *(void (**)(char *))(v14 + 16);
  v31 = v18;
  v17(v16);
  v19 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  v32 = a8;
  v20 = v19(a5, a8);
  v30 = a9;
  v21 = *(_QWORD *)(a9 + 8);
  v22 = *(uint64_t (**)(uint64_t, uint64_t))(v21 + 16);
  v23 = v22(a6, v21);
  result = (*(uint64_t (**)(char *, uint64_t))(v14 + 8))(v16, a5);
  if (v20 != v23)
  {
    __break(1u);
    goto LABEL_6;
  }
  result = v22(a6, v21);
  if (result < 0)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  v25 = result;
  v26 = v34;
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(v34, a7);
  if ((result & 0x8000000000000000) == 0)
  {
    v27 = MEMORY[0x1E0C80A78](result);
    *(&v30 - 10) = v26;
    *(&v30 - 9) = a5;
    *(&v30 - 8) = a6;
    *(&v30 - 7) = a7;
    v28 = v30;
    *(&v30 - 6) = v32;
    *(&v30 - 5) = v28;
    *(&v30 - 4) = v31;
    *(&v30 - 3) = a3;
    *(&v30 - 2) = v25;
    *(&v30 - 1) = v27;
    return (*(uint64_t (**)(uint64_t))(a7 + 24))(v29);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vDSP.linearInterpolate<A, B>(elementsOf:using:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, _QWORD *))
{
  uint64_t v16;
  _QWORD v18[10];

  v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  v18[2] = a3;
  v18[3] = a4;
  v18[4] = a5;
  v18[5] = a6;
  v18[6] = a1;
  v18[7] = a2;
  return a8(v16, a7, v18);
}

uint64_t closure #1 in static vDSP.linearInterpolate<A, B>(elementsOf:using:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, unint64_t *a10, void (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v17;
  uint64_t v18;
  uint64_t result;

  v17 = __swift_instantiateConcreteTypeFromMangledName(a9);
  v18 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a10, a9);
  a11(a3, a4, a1, a5, a6, v17, a7, a8, v18);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
  *a2 = result;
  return result;
}

_QWORD *closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(_QWORD *result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t, _QWORD, uint64_t))
{
  if (!a2)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!a4)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*result)
    return (_QWORD *)a8(a2, a4, 1, *result, 1);
LABEL_7:
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  int v5;
  uint64_t v6;
  _QWORD v8[3];
  __int128 v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  int v15;
  uint64_t v16;

  v3 = *(_QWORD *)(v2 + 40);
  v4 = *(_QWORD *)(v2 + 72);
  v5 = *(_DWORD *)(v2 + 80);
  v6 = *(_QWORD *)(v2 + 88);
  v8[2] = *(_QWORD *)(v2 + 16);
  v9 = *(_OWORD *)(v2 + 24);
  v10 = v3;
  v11 = *(_OWORD *)(v2 + 48);
  v12 = v4;
  v13 = a1;
  v14 = a2;
  v15 = v5;
  v16 = v6;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t), _QWORD *, uint64_t, _QWORD))(v11 + 24))(partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:), v8, MEMORY[0x1E0DEE9C0] + 8, v9);
}

{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD v8[3];
  __int128 v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;

  v3 = *(_QWORD *)(v2 + 40);
  v4 = *(_QWORD *)(v2 + 72);
  v5 = *(_QWORD *)(v2 + 80);
  v6 = *(_QWORD *)(v2 + 88);
  v8[2] = *(_QWORD *)(v2 + 16);
  v9 = *(_OWORD *)(v2 + 24);
  v10 = v3;
  v11 = *(_OWORD *)(v2 + 48);
  v12 = v4;
  v13 = a1;
  v14 = a2;
  v15 = v5;
  v16 = v6;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t), _QWORD *, uint64_t, _QWORD))(v11 + 24))(partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:), v8, MEMORY[0x1E0DEE9C0] + 8, v9);
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(elementsOf:using:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(elementsOf:using:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:));
}

{
  return partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(elementsOf:using:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:));
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:));
}

{
  return partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:));
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(elementsOf:using:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t *v5;

  return closure #1 in static vDSP.linearInterpolate<A, B>(elementsOf:using:)(a1, a2, v5[6], v5[7], v5[2], v5[3], v5[4], v5[5], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  _QWORD v7[3];
  __int128 v8;
  uint64_t v9;
  __int128 v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  __int128 v14;

  v4 = *(_QWORD *)(v3 + 40);
  v5 = *(_QWORD *)(v3 + 72);
  v7[2] = *(_QWORD *)(v3 + 16);
  v8 = *(_OWORD *)(v3 + 24);
  v9 = v4;
  v10 = *(_OWORD *)(v3 + 48);
  v11 = v5;
  v12 = a1;
  v13 = a2;
  v14 = *(_OWORD *)(v3 + 80);
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, _QWORD))(v10 + 24))(a3, v7, MEMORY[0x1E0DEE9C0] + 8, v8);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:));
}

_QWORD *partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(_QWORD *a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, MEMORY[0x1E0C8C698]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, MEMORY[0x1E0C8C690]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  _OWORD v7[2];
  uint64_t v8;
  uint64_t v9;
  __int128 v10;

  v4 = *(_QWORD *)(v3 + 32);
  v5 = *(_QWORD *)(v3 + 56);
  v7[1] = *(_OWORD *)(v3 + 72);
  v8 = a1;
  v9 = a2;
  v10 = *(_OWORD *)(v3 + 88);
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, uint64_t))(v5 + 16))(a3, v7, MEMORY[0x1E0DEE9C0] + 8, v4);
}

_QWORD *partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(_QWORD *a1, uint64_t (*a2)(uint64_t, uint64_t, uint64_t, _QWORD, uint64_t))
{
  uint64_t *v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, v2[2], v2[3], v2[4], v2[5], v2[6], v2[7], a2);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  _OWORD v7[2];
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;

  v3 = *(_QWORD *)(v2 + 32);
  v4 = *(_QWORD *)(v2 + 56);
  v5 = *(_QWORD *)(v2 + 88);
  v7[1] = *(_OWORD *)(v2 + 72);
  v8 = a1;
  v9 = a2;
  v10 = v5;
  return (*(uint64_t (**)(void (*)(double **), _OWORD *, uint64_t, uint64_t))(v4 + 16))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:), v7, MEMORY[0x1E0DEE9C0] + 8, v3);
}

{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  int v5;
  _OWORD v7[2];
  uint64_t v8;
  uint64_t v9;
  int v10;

  v3 = *(_QWORD *)(v2 + 32);
  v4 = *(_QWORD *)(v2 + 56);
  v5 = *(_DWORD *)(v2 + 88);
  v7[1] = *(_OWORD *)(v2 + 72);
  v8 = a1;
  v9 = a2;
  v10 = v5;
  return (*(uint64_t (**)(void (*)(float **), _OWORD *, uint64_t, uint64_t))(v4 + 16))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:), v7, MEMORY[0x1E0DEE9C0] + 8, v3);
}

void partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(double **a1)
{
  uint64_t v1;

  closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(a1, *(const double **)(v1 + 16), *(_QWORD *)(v1 + 24), *(const double **)(v1 + 32), *(_QWORD *)(v1 + 40), *(_QWORD *)(v1 + 56), *(double *)(v1 + 48));
}

void partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(float **a1)
{
  uint64_t v1;

  closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(a1, *(const float **)(v1 + 16), *(_QWORD *)(v1 + 24), *(const float **)(v1 + 32), *(_QWORD *)(v1 + 40), *(_QWORD *)(v1 + 56), *(float *)(v1 + 48));
}

BOOL static BNNS.InterpolationMethod.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void BNNS.InterpolationMethod.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

Swift::Int BNNS.InterpolationMethod.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t BNNS.ResizeLayer.__allocating_init(interpolationMethod:input:output:alignsCorners:filterParameters:)(_BYTE *a1, _OWORD *a2, __int128 *a3, char a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  int *v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  int v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  _BYTE v29[180];
  _BOOL4 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  int v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  __int128 v52;
  __int128 v53;
  char v54;
  uint64_t v55;

  v55 = *MEMORY[0x1E0C80C00];
  v8 = a3[8];
  v9 = a3[9];
  v10 = a3[6];
  v50 = a3[7];
  v51 = v8;
  v11 = a3[10];
  v52 = v9;
  v53 = v11;
  v12 = a3[4];
  v48 = a3[5];
  v49 = v10;
  v13 = a3[2];
  v46 = a3[3];
  v47 = v12;
  v14 = a3[1];
  v43 = *a3;
  v44 = v14;
  v45 = v13;
  v15 = a2[5];
  *(_OWORD *)&v29[68] = a2[4];
  v16 = a2[2];
  *(_OWORD *)&v29[52] = a2[3];
  v17 = a2[6];
  *(_OWORD *)&v29[116] = a2[7];
  v18 = a2[9];
  *(_OWORD *)&v29[132] = a2[8];
  *(_OWORD *)&v29[148] = v18;
  *(_OWORD *)&v29[164] = a2[10];
  *(_OWORD *)&v29[84] = v15;
  *(_OWORD *)&v29[100] = v17;
  v19 = a2[1];
  *(_OWORD *)&v29[4] = *a2;
  *(_OWORD *)&v29[20] = v19;
  *(_OWORD *)&v29[36] = v16;
  v39 = *(_OWORD *)&v29[128];
  v40 = *(_OWORD *)&v29[144];
  v41 = *(_OWORD *)&v29[160];
  v35 = *(_OWORD *)&v29[64];
  v36 = *(_OWORD *)&v29[80];
  v37 = *(_OWORD *)&v29[96];
  v38 = *(_OWORD *)&v29[112];
  v31 = *(_OWORD *)v29;
  v32 = *(_OWORD *)&v29[16];
  v33 = *(_OWORD *)&v29[32];
  v30 = (*a1 & 1) == 0;
  v42 = *(_DWORD *)&v29[176];
  v34 = *(_OWORD *)&v29[48];
  v54 = a4;
  if (a7 == 1)
  {
    v20 = 0;
  }
  else
  {
    v25 = a5;
    v26 = a6;
    v27 = a7;
    v28 = a8;
    v20 = &v25;
  }
  v21 = MEMORY[0x1D1794630](&v30, v20);
  type metadata accessor for BNNS.ResizeLayer();
  v22 = swift_allocObject();
  v23 = v22;
  if (v21)
  {
    *(_QWORD *)(v22 + 16) = v21;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v23;
}

uint64_t type metadata accessor for BNNS.ResizeLayer()
{
  return objc_opt_self();
}

uint64_t BNNS.ResizeLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.ResizeLayer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

unint64_t lazy protocol witness table accessor for type BNNS.InterpolationMethod and conformance BNNS.InterpolationMethod()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNS.InterpolationMethod and conformance BNNS.InterpolationMethod;
  if (!lazy protocol witness table cache variable for type BNNS.InterpolationMethod and conformance BNNS.InterpolationMethod)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNS.InterpolationMethod, &type metadata for BNNS.InterpolationMethod);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.InterpolationMethod and conformance BNNS.InterpolationMethod);
  }
  return result;
}

uint64_t storeEnumTagSinglePayload for BNNS.InterpolationMethod(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 1 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 1) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFF)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFE)
    return ((uint64_t (*)(void))((char *)&loc_1CAB3C344 + 4 * byte_1CAB623A5[v4]))();
  *a1 = a2 + 1;
  return ((uint64_t (*)(void))((char *)sub_1CAB3C378 + 4 * byte_1CAB623A0[v4]))();
}

uint64_t sub_1CAB3C378(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB3C380(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB3C388);
  return result;
}

uint64_t sub_1CAB3C394(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB3C39CLL);
  *(_BYTE *)result = a2 + 1;
  return result;
}

uint64_t sub_1CAB3C3A0(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB3C3A8(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.InterpolationMethod()
{
  return &type metadata for BNNS.InterpolationMethod;
}

uint64_t static vForce.ceil<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.ceil<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.ceil<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vForce.ceil<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.ceil<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.ceil<A, B>(_:result:));
}

uint64_t static vForce.ceil<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.ceil<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.ceil<A, B>(_:result:));
}

uint64_t static vForce.floor<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.floor<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.floor<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.floor<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.floor<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.floor<A, B>(_:result:));
}

uint64_t static vForce.copysign<A, B>(magnitudes:signs:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.copysign<A, B>(magnitudes:signs:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.copysign<A, B>(magnitudes:signs:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.copysign<A, B, C>(magnitudes:signs:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  void (*v24)(char *);
  uint64_t v25;
  void (*v26)(char *, uint64_t, uint64_t);
  uint64_t (*v27)(uint64_t, uint64_t);
  uint64_t v28;
  uint64_t (*v29)(uint64_t, uint64_t);
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  void (*v33)(char *, uint64_t);
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  void (*v40)(char *, uint64_t);
  uint64_t v41;
  uint64_t v42;
  uint64_t (*v43)(uint64_t, uint64_t);
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  int v50;
  uint64_t v51;

  v48 = a3;
  v49 = a6;
  v51 = *MEMORY[0x1E0C80C00];
  v14 = *(_QWORD *)(a5 - 8);
  v15 = MEMORY[0x1E0C80A78](a1);
  v17 = (char *)&v41 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  v18 = MEMORY[0x1E0C80A78](v15);
  v20 = (char *)&v41 - v19;
  MEMORY[0x1E0C80A78](v18);
  v22 = (char *)&v41 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  v47 = v23;
  v24 = *(void (**)(char *))(v23 + 16);
  v44 = v25;
  v24(v22);
  v46 = v14;
  v26 = *(void (**)(char *, uint64_t, uint64_t))(v14 + 16);
  v42 = a2;
  v26(v20, a2, a5);
  v27 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  v45 = a7;
  v43 = v27;
  v28 = v27(a4, a7);
  v29 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  v30 = v29(a5, a8);
  v26(v17, (uint64_t)v20, a5);
  if (v28 != v30)
  {
    v40 = *(void (**)(char *, uint64_t))(v46 + 8);
    v40(v17, a5);
    v40(v20, a5);
    (*(void (**)(char *, uint64_t))(v47 + 8))(v22, a4);
    goto LABEL_7;
  }
  v41 = a8;
  v31 = v29(a5, a8);
  v32 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a9 + 8) + 16))(v49);
  v33 = *(void (**)(char *, uint64_t))(v46 + 8);
  v33(v17, a5);
  v33(v20, a5);
  (*(void (**)(char *, uint64_t))(v47 + 8))(v22, a4);
  if (v31 != v32)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  v34 = v44;
  v35 = v45;
  v36 = v43(a4, v45);
  if (v36 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (v36 > 0x7FFFFFFF)
    goto LABEL_9;
  v50 = v36;
  MEMORY[0x1E0C80A78](v36);
  *(&v41 - 10) = a4;
  *(&v41 - 9) = a5;
  *(&v41 - 8) = v49;
  *(&v41 - 7) = v35;
  *(&v41 - 6) = v41;
  *(&v41 - 5) = a9;
  v37 = v42;
  *(&v41 - 4) = v34;
  *(&v41 - 3) = v37;
  *(&v41 - 2) = (uint64_t)&v50;
  return (*(uint64_t (**)(uint64_t))(a9 + 16))(v38);
}

uint64_t static vForce.copysign<A, B>(magnitudes:signs:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t))
{
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  char *v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  char *v19;
  void (*v20)(char *);
  uint64_t v21;
  void (*v22)(char *, uint64_t, uint64_t);
  uint64_t (*v23)(uint64_t, uint64_t);
  uint64_t v24;
  uint64_t v25;
  uint64_t result;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t (*v36)(uint64_t, uint64_t);
  uint64_t v37;

  v35 = a7;
  v36 = a8;
  v37 = a6;
  v12 = *(_QWORD *)(a4 - 8);
  v13 = MEMORY[0x1E0C80A78](a1);
  v15 = (char *)&v33 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0);
  v17 = *(_QWORD *)(v16 - 8);
  MEMORY[0x1E0C80A78](v13);
  v19 = (char *)&v33 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  v20 = *(void (**)(char *))(v17 + 16);
  v33 = v21;
  v20(v19);
  v22 = *(void (**)(char *, uint64_t, uint64_t))(v12 + 16);
  v34 = a2;
  v22(v15, a2, a4);
  v23 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  v24 = v23(a3, a5);
  v25 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(v37 + 8) + 16))(a4);
  (*(void (**)(char *, uint64_t))(v12 + 8))(v15, a4);
  result = (*(uint64_t (**)(char *, uint64_t))(v17 + 8))(v19, a3);
  if (v24 == v25)
  {
    v27 = v33;
    v28 = v23(a3, a5);
    v29 = MEMORY[0x1E0C80A78](v28);
    *(&v33 - 6) = a3;
    *(&v33 - 5) = a4;
    v30 = v37;
    *(&v33 - 4) = a5;
    *(&v33 - 3) = v30;
    v32 = v34;
    v31 = v35;
    *(&v33 - 2) = v27;
    *(&v33 - 1) = v32;
    return v36(v29, v31);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, unint64_t *a10, void (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t result;

  v17 = __swift_instantiateConcreteTypeFromMangledName(a9);
  v18 = *(_QWORD *)(a8 + 8);
  v19 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a10, a9);
  a11(a3, a4, a1, a5, a6, v17, a7, v18, v19);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  *a2 = result;
  return result;
}

uint64_t static vForce.truncatingRemainder<A, B>(dividends:divisors:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.truncatingRemainder<A, B>(dividends:divisors:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.truncatingRemainder<A, B>(dividends:divisors:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  void (*v24)(char *);
  uint64_t v25;
  void (*v26)(char *, uint64_t, uint64_t);
  uint64_t (*v27)(uint64_t, uint64_t);
  uint64_t v28;
  uint64_t (*v29)(uint64_t, uint64_t);
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t (*v33)(uint64_t, uint64_t);
  uint64_t v34;
  void (*v35)(char *, uint64_t);
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  void (*v43)(char *, uint64_t);
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  int v53;
  uint64_t v54;

  v51 = a3;
  v52 = a6;
  v54 = *MEMORY[0x1E0C80C00];
  v14 = *(_QWORD *)(a5 - 8);
  v15 = MEMORY[0x1E0C80A78](a1);
  v17 = (char *)&v44 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  v18 = MEMORY[0x1E0C80A78](v15);
  v20 = (char *)&v44 - v19;
  MEMORY[0x1E0C80A78](v18);
  v22 = (char *)&v44 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  v49 = v23;
  v24 = *(void (**)(char *))(v23 + 16);
  v46 = v25;
  v24(v22);
  v26 = *(void (**)(char *, uint64_t, uint64_t))(v14 + 16);
  v47 = a2;
  v26(v20, a2, a5);
  v27 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  v50 = a4;
  v48 = a7;
  v28 = v27(a4, a7);
  v29 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  v30 = v29(a5, a8);
  v26(v17, (uint64_t)v20, a5);
  if (v28 != v30)
  {
    v43 = *(void (**)(char *, uint64_t))(v14 + 8);
    v43(v17, a5);
    v43(v20, a5);
    (*(void (**)(char *, uint64_t))(v49 + 8))(v22, v50);
    goto LABEL_7;
  }
  v45 = a8;
  v31 = ((uint64_t (*)(uint64_t))v29)(a5);
  v32 = *(_QWORD *)(a9 + 8);
  v33 = *(uint64_t (**)(uint64_t, uint64_t))(v32 + 16);
  v34 = v33(v52, v32);
  v35 = *(void (**)(char *, uint64_t))(v14 + 8);
  v35(v17, a5);
  v35(v20, a5);
  v36 = v50;
  (*(void (**)(char *, uint64_t))(v49 + 8))(v22, v50);
  if (v31 != v34)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  v37 = v33(v52, v32);
  if (v37 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (v37 > 0x7FFFFFFF)
    goto LABEL_9;
  v53 = v37;
  MEMORY[0x1E0C80A78](v37);
  *(&v44 - 10) = v36;
  *(&v44 - 9) = a5;
  v38 = v48;
  *(&v44 - 8) = v39;
  *(&v44 - 7) = v38;
  *(&v44 - 6) = v45;
  *(&v44 - 5) = a9;
  v40 = v47;
  *(&v44 - 4) = v46;
  *(&v44 - 3) = v40;
  *(&v44 - 2) = (uint64_t)&v53;
  return (*(uint64_t (**)(uint64_t))(a9 + 16))(v41);
}

uint64_t static vForce.remainder<A, B>(dividends:divisors:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.remainder<A, B>(dividends:divisors:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.remainder<A, B>(dividends:divisors:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.remainder<A, B, C>(dividends:divisors:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

uint64_t closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(uint64_t a1, uint64_t a2, uint64_t *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(void))
{
  uint64_t result;

  result = *a3;
  if (!*a3)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!a4)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (a1)
    return a7();
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vForce.trunc<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.trunc<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.trunc<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.trunc<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.trunc<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.trunc<A, B>(_:result:));
}

uint64_t static vForce.nearestInteger<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.nearestInteger<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.nearestInteger<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.nearestInteger<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.nearestInteger<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.nearestInteger<A, B>(_:result:));
}

uint64_t static vForce.rsqrt<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.rsqrt<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.rsqrt<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.rsqrt<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.rsqrt<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.rsqrt<A, B>(_:result:));
}

uint64_t static vForce.sqrt<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.sqrt<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.sqrt<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.sqrt<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.sqrt<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.sqrt<A, B>(_:result:));
}

uint64_t static vForce.reciprocal<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.reciprocal<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.reciprocal<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.reciprocal<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.reciprocal<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.reciprocal<A, B>(_:result:));
}

uint64_t static vForce.exp<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.exp<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.exp<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.exp<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.exp<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.exp<A, B>(_:result:));
}

uint64_t static vForce.expm1<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.expm1<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.expm1<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.expm1<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.expm1<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.expm1<A, B>(_:result:));
}

uint64_t static vForce.exp2<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.exp2<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.exp2<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.exp2<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.exp2<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.exp2<A, B>(_:result:));
}

uint64_t static vForce.log2<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.log2<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.log2<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.log2<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.log2<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.log2<A, B>(_:result:));
}

uint64_t static vForce.log10<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.log10<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.log10<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.log10<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.log10<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.log10<A, B>(_:result:));
}

uint64_t static vForce.logb<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.logb<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.logb<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.logb<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.logb<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.logb<A, B>(_:result:));
}

uint64_t static vForce.pow<A, B>(bases:exponents:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.pow<A, B>(bases:exponents:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.pow<A, B>(bases:exponents:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.pow<A, B>(bases:exponents:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.pow<A, B>(bases:exponents:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.pow<A, B, C>(bases:exponents:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

uint64_t static vForce.pow<A, B>(bases:exponents:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t))
{
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  void (*v21)(char *);
  uint64_t v22;
  void (*v23)(char *, uint64_t, uint64_t);
  uint64_t (*v24)(uint64_t, uint64_t);
  uint64_t v25;
  uint64_t v26;
  uint64_t (*v27)(uint64_t, uint64_t);
  uint64_t v28;
  uint64_t result;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t (*v41)(uint64_t, uint64_t);

  v40 = a7;
  v41 = a8;
  v13 = *(_QWORD *)(a4 - 8);
  v14 = MEMORY[0x1E0C80A78](a1);
  v16 = (char *)&v35 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  v18 = *(_QWORD *)(v17 - 8);
  MEMORY[0x1E0C80A78](v14);
  v20 = (char *)&v35 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  v21 = *(void (**)(char *))(v18 + 16);
  v36 = v22;
  v21(v20);
  v23 = *(void (**)(char *, uint64_t, uint64_t))(v13 + 16);
  v38 = a2;
  v23(v16, a2, a4);
  v24 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  v37 = a5;
  v25 = v24(a3, a5);
  v39 = a6;
  v26 = *(_QWORD *)(a6 + 8);
  v27 = *(uint64_t (**)(uint64_t, uint64_t))(v26 + 16);
  v28 = v27(a4, v26);
  (*(void (**)(char *, uint64_t))(v13 + 8))(v16, a4);
  result = (*(uint64_t (**)(char *, uint64_t))(v18 + 8))(v20, a3);
  if (v25 == v28)
  {
    v30 = v38;
    v31 = v27(a4, v26);
    v32 = MEMORY[0x1E0C80A78](v31);
    *(&v35 - 6) = a3;
    *(&v35 - 5) = a4;
    v34 = v39;
    v33 = v40;
    *(&v35 - 4) = v37;
    *(&v35 - 3) = v34;
    *(&v35 - 2) = v36;
    *(&v35 - 1) = v30;
    return v41(v32, v33);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in static vForce.pow<A, B>(bases:exponents:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, unint64_t *a10, void (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t result;

  v16 = __swift_instantiateConcreteTypeFromMangledName(a9);
  v17 = *(_QWORD *)(a8 + 8);
  v18 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a10, a9);
  a11(a3, a4, a1, a5, a6, v16, a7, v17, v18);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(v17 + 16))(a6, v17);
  *a2 = result;
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(uint64_t a1, uint64_t a2, uint64_t *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(void))
{
  uint64_t result;

  result = *a3;
  if (!*a3)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!a1)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (a4)
    return a7();
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vForce.sin<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.sin<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.sin<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.sin<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.sin<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.sin<A, B>(_:result:));
}

uint64_t static vForce.sinPi<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.sinPi<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.sinPi<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.sinPi<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.sinPi<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.sinPi<A, B>(_:result:));
}

uint64_t static vForce.cos<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.cos<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.cos<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.cos<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.cos<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.cos<A, B>(_:result:));
}

uint64_t static vForce.cosPi<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.cosPi<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.cosPi<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.cosPi<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.cosPi<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.cosPi<A, B>(_:result:));
}

uint64_t static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v14;
  uint64_t v15;
  char *v16;
  void (*v17)(char *);
  uint64_t v18;
  uint64_t (*v19)(uint64_t, uint64_t);
  uint64_t v20;
  uint64_t v21;
  uint64_t (*v22)(uint64_t, uint64_t);
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  _QWORD v32[2];
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  int v38;
  uint64_t v39;

  v36 = a3;
  v37 = a6;
  v39 = *MEMORY[0x1E0C80C00];
  v14 = *(_QWORD *)(a4 - 8);
  MEMORY[0x1E0C80A78](a1);
  v16 = (char *)v32 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  v17 = *(void (**)(char *))(v14 + 16);
  v34 = v18;
  v17(v16);
  v19 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  v35 = a7;
  v20 = v19(a4, a7);
  v33 = a8;
  v21 = *(_QWORD *)(a8 + 8);
  v22 = *(uint64_t (**)(uint64_t, uint64_t))(v21 + 16);
  if (v20 != v22(a5, v21))
  {
LABEL_9:
    (*(void (**)(char *, uint64_t))(v14 + 8))(v16, a4);
    __break(1u);
  }
  v32[1] = a2;
  v23 = v22(a5, v21);
  v24 = v36;
  v25 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a9 + 8) + 16))(v37);
  (*(void (**)(char *, uint64_t))(v14 + 8))(v16, a4);
  if (v23 != v25)
  {
    __break(1u);
    goto LABEL_7;
  }
  v26 = v34;
  v14 = v35;
  v27 = v19(a4, v35);
  if (v27 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (v27 > 0x7FFFFFFF)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v38 = v27;
  MEMORY[0x1E0C80A78](v27);
  v32[-10] = a4;
  v32[-9] = a5;
  v32[-8] = v28;
  v32[-7] = v14;
  v29 = v33;
  v32[-6] = v33;
  v32[-5] = a9;
  v32[-4] = v24;
  v32[-3] = v26;
  v32[-2] = &v38;
  return (*(uint64_t (**)(uint64_t))(v29 + 16))(v30);
}

uint64_t closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1, uint64_t a2, uint64_t *a3, _QWORD *a4, uint64_t a5, uint64_t (*a6)(void))
{
  uint64_t result;

  result = *a3;
  if (!*a3)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!*a4)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (a1)
    return a6();
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vForce.tan<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.tan<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.tan<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.tan<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.tan<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.tan<A, B>(_:result:));
}

uint64_t static vForce.tanPi<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.tanPi<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.tanPi<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.tanPi<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.tanPi<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.tanPi<A, B>(_:result:));
}

uint64_t static vForce.asin<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.asin<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.asin<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.asin<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.asin<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.asin<A, B>(_:result:));
}

uint64_t static vForce.acos<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.acos<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.acos<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.acos<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.acos<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.acos<A, B>(_:result:));
}

uint64_t static vForce.atan<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.atan<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.atan<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.atan<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.atan<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.atan<A, B>(_:result:));
}

uint64_t static vForce.sinh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.sinh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.sinh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.sinh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.sinh<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.sinh<A, B>(_:result:));
}

uint64_t static vForce.cosh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.cosh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.cosh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.cosh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.cosh<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.cosh<A, B>(_:result:));
}

uint64_t static vForce.tanh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.tanh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.tanh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.tanh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.tanh<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.tanh<A, B>(_:result:));
}

uint64_t static vForce.asinh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.asinh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.asinh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.asinh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.asinh<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.asinh<A, B>(_:result:));
}

uint64_t static vForce.acosh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.acosh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.acosh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.acosh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.acosh<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.acosh<A, B>(_:result:));
}

uint64_t static vForce.atanh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.atanh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.atanh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.atanh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.atanh<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.atanh<A, B>(_:result:));
}

uint64_t static vForce.ceil<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v12;
  uint64_t v13;
  char *v14;
  uint64_t (*v15)(uint64_t, uint64_t);
  uint64_t v16;
  uint64_t (*v17)(uint64_t);
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  int v25;
  uint64_t v26;

  v23 = a7;
  v24 = a4;
  v26 = *MEMORY[0x1E0C80C00];
  v12 = *(_QWORD *)(a3 - 8);
  MEMORY[0x1E0C80A78](a1);
  v14 = (char *)&v22 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(char *, uint64_t))(v12 + 16))(v14, a1);
  v15 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  v16 = v15(a3, a5);
  v17 = *(uint64_t (**)(uint64_t))(*(_QWORD *)(a6 + 8) + 16);
  v22 = a2;
  v18 = v24;
  v19 = v17(v24);
  (*(void (**)(char *, uint64_t))(v12 + 8))(v14, a3);
  if (v16 != v19)
  {
    __break(1u);
    goto LABEL_6;
  }
  v20 = v15(a3, a5);
  if (v20 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  if (v20 > 0x7FFFFFFF)
    goto LABEL_7;
  v25 = v20;
  MEMORY[0x1E0C80A78](v20);
  *(&v22 - 6) = a3;
  *(&v22 - 5) = v18;
  *(&v22 - 4) = a5;
  *(&v22 - 3) = a6;
  *(&v22 - 2) = a1;
  *(&v22 - 1) = (uint64_t)&v25;
  return (*(uint64_t (**)(uint64_t))(a6 + 16))(v23);
}

uint64_t partial apply for closure #1 in static vForce.ceil<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.ceil<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.ceil<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.floor<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.floor<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.floor<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.floor<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.floor<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.floor<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vForce.copysign<A, B, C>(magnitudes:signs:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vForce.copysign<A, B>(magnitudes:signs:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vForce.copysign<A, B, C>(magnitudes:signs:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vForce.copysign<A, B>(magnitudes:signs:));
}

uint64_t partial apply for closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:));
}

uint64_t partial apply for closure #1 in static vForce.truncatingRemainder<A, B>(dividends:divisors:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vForce.copysign<A, B>(magnitudes:signs:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vForce.copysign<A, B>(magnitudes:signs:));
}

uint64_t partial apply for closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:));
}

uint64_t partial apply for closure #1 in static vForce.remainder<A, B>(dividends:divisors:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vForce.remainder<A, B, C>(dividends:divisors:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vForce.copysign<A, B>(magnitudes:signs:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vForce.remainder<A, B, C>(dividends:divisors:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vForce.copysign<A, B>(magnitudes:signs:));
}

uint64_t partial apply for closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:));
}

uint64_t partial apply for closure #1 in static vForce.trunc<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.trunc<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.trunc<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.trunc<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.trunc<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.trunc<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.nearestInteger<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.nearestInteger<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.nearestInteger<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.nearestInteger<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.nearestInteger<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.nearestInteger<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.rsqrt<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.rsqrt<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.rsqrt<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.rsqrt<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.rsqrt<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.rsqrt<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sqrt<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.sqrt<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.sqrt<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sqrt<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sqrt<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sqrt<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.reciprocal<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.reciprocal<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.reciprocal<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.reciprocal<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.reciprocal<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.reciprocal<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.exp<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.exp<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.exp<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.exp<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.exp<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.exp<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.expm1<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.expm1<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.expm1<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.expm1<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.expm1<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.expm1<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.exp2<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.exp2<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.exp2<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.exp2<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.exp2<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.exp2<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.log2<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.log2<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.log2<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.log2<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.log2<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.log2<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.log10<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.log10<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.log10<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.log10<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.log10<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.log10<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.logb<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.logb<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.logb<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.logb<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.logb<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.logb<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.pow<A, B>(bases:exponents:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vForce.pow<A, B, C>(bases:exponents:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vForce.pow<A, B>(bases:exponents:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vForce.pow<A, B, C>(bases:exponents:result:), (uint64_t (*)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, uint64_t))closure #1 in static vForce.pow<A, B>(bases:exponents:));
}

uint64_t partial apply for closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:));
}

uint64_t partial apply for closure #1 in static vForce.sin<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.sin<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.sin<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sin<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sin<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sin<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sinPi<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.sinPi<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.sinPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sinPi<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sinPi<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sinPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cos<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.cos<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.cos<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cos<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.cos<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.cos<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cosPi<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.cosPi<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.cosPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cosPi<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.cosPi<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.cosPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1)
{
  return partial apply for closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:));
}

{
  return partial apply for closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:));
}

uint64_t partial apply for closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  __int128 v7;
  _OWORD v9[3];
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;

  v4 = *(_QWORD *)(v2 + 48);
  v3 = *(_QWORD *)(v2 + 56);
  v5 = *(_QWORD *)(v2 + 72);
  v6 = *(_QWORD *)(v2 + 80);
  v7 = *(_OWORD *)(v2 + 32);
  v9[1] = *(_OWORD *)(v2 + 16);
  v9[2] = v7;
  v10 = v4;
  v11 = v3;
  v12 = v5;
  v13 = a1;
  v14 = v6;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, _QWORD))(v3 + 16))(a2, v9, MEMORY[0x1E0DEE9C0] + 8, v7);
}

uint64_t partial apply for closure #1 in static vForce.tan<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.tan<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.tan<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.tan<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.tan<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.tan<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.tanPi<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.tanPi<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.tanPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.tanPi<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.tanPi<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.tanPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.asin<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.asin<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.asin<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.asin<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.asin<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.asin<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.acos<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.acos<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.acos<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.acos<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.acos<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.acos<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.atan<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.atan<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.atan<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.atan<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.atan<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.atan<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sinh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.sinh<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.sinh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sinh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sinh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sinh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cosh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.cosh<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.cosh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cosh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.cosh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.cosh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.tanh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.tanh<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.tanh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.tanh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.tanh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.tanh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.asinh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.asinh<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.asinh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.asinh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.asinh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.asinh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.acosh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.acosh<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.acosh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.acosh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.acosh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.acosh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.atanh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.atanh<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.atanh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.atanh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D918]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D920]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.acosh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D8A8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D8B8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.asinh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D8D8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D8E8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.tanh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DBA8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DBB0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.cosh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D970]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D980]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sinh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DB50]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DB58]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.atan<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D8F0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D910]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.acos<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D890]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D898]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.asin<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D8C0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D8C8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.tanPi<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DBC0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DBC8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.tan<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DB90]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DBA0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, a2, *(uint64_t **)(v2 + 16), *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8DB30]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, a2, *(uint64_t **)(v2 + 16), *(_QWORD **)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8DB38]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1, uint64_t a2)
{
  _QWORD *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  _QWORD v7[6];

  v3 = v2[2];
  v4 = v2[5];
  v5 = v2[10];
  v7[2] = v2[9];
  v7[3] = a1;
  v7[4] = v5;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v4 + 24))(a2, v7, MEMORY[0x1E0DEE9C0] + 8, v3);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.cosPi<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D988]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D990]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.cos<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D958]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D968]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sinPi<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DB68]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DB70]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sin<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DB28]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DB48]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8DAC0]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8DAC8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.logb<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DA88]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DA90]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.log10<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DA50]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DA58]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.log2<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DA78]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DA80]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.exp2<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D9B8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D9C8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.expm1<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D9E0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D9E8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.exp<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D9B0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D9D8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.reciprocal<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DAE8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DAF0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sqrt<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DB78]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DB80]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.rsqrt<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DB10]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DB20]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.nearestInteger<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DAA8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DAB8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.trunc<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DA38]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DA40]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8DB00]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8DB08]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8DA20]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8DA30]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8D948]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8D950]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  _QWORD *v3;
  uint64_t v4;
  uint64_t v5;
  _QWORD v7[5];

  v4 = v3[3];
  v5 = v3[6];
  v7[2] = v3[9];
  v7[3] = a1;
  v7[4] = a2;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v5 + 24))(a3, v7, MEMORY[0x1E0DEE9C0] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.floor<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DA08]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DA18]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.ceil<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D930]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8D938]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(void))
{
  uint64_t v3;
  uint64_t result;

  result = **(_QWORD **)(v3 + 16);
  if (result)
  {
    if (a1)
      return a3();
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t static BNNS.SparsityType.== infix(_:_:)()
{
  return 1;
}

void BNNS.SparsityType.hash(into:)()
{
  Hasher._combine(_:)(0);
}

Swift::Int BNNS.SparsityType.hashValue.getter()
{
  Hasher.init(_seed:)();
  Hasher._combine(_:)(0);
  return Hasher._finalize()();
}

void BNNS.SparseParameters.init(type:ratio:targetSystem:)(int a1@<W1>, int a2@<W2>, int a3@<W3>, _DWORD *a4@<X8>)
{
  *a4 = a1;
  a4[1] = a2;
  a4[2] = a3;
}

uint64_t BNNS.SparseParameters.ratio.getter()
{
  uint64_t v0;

  return *(_QWORD *)v0;
}

uint64_t BNNS.SparseParameters.targetSystem.getter()
{
  uint64_t v0;

  return *(unsigned int *)(v0 + 8);
}

uint64_t static BNNS.FullyConnectedLayer.sparsify(batchSize:inputLayout:inputDenseShape:inputValues:output:sparseParameters:workspace:filterParameters:)(size_t a1, void *__src, __int128 *a3, _OWORD *a4, BNNSNDArrayDescriptor *a5, uint64_t *a6, void *a7, uint64_t a8, char a9, uint64_t a10, size_t a11, int (__cdecl *a12)(void **, size_t, size_t), void (__cdecl *a13)(void *))
{
  uint64_t v16;
  int v17;
  char v18;
  size_t v19;
  int v20;
  __int128 *v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  void *v36;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  void *v43;
  uint64_t v48;
  int v49;
  char v50;
  _BYTE v51[352];
  _OWORD v52[11];
  __int128 v53[11];
  _OWORD v54[11];
  _BYTE __dst[360];

  outlined init with take of BNNS.SparseLayout(__src, __dst);
  v16 = *a6;
  v17 = *((_DWORD *)a6 + 2);
  v18 = *((_BYTE *)a6 + 12);
  if (a9 & 1 | (a7 == 0))
    v19 = 0;
  else
    v19 = a8 - (_QWORD)a7;
  outlined init with take of BNNS.SparseLayout(__dst, v51);
  v20 = _s10Accelerate4BNNSO12SparseLayoutOWOg((uint64_t)v51);
  v21 = (__int128 *)_s10Accelerate4BNNSO12SparseLayoutOWOj0_((uint64_t)v51);
  if (v20 == 1)
  {
    v22 = v21[9];
    v53[8] = v21[8];
    v53[9] = v22;
    v23 = v21[11];
    v53[10] = v21[10];
    v24 = v21[5];
    v53[4] = v21[4];
    v53[5] = v24;
    v25 = v21[7];
    v53[6] = v21[6];
    v53[7] = v25;
    v26 = v21[1];
    v53[0] = *v21;
    v53[1] = v26;
    v27 = v21[3];
    v53[2] = v21[2];
    v53[3] = v27;
    v28 = v21[18];
    v29 = v21[20];
    v30 = v21[21];
    v54[8] = v21[19];
    v54[9] = v29;
    v54[10] = v30;
    v31 = v21[14];
    v32 = v21[16];
    v33 = v21[17];
    v54[4] = v21[15];
    v54[5] = v32;
    v54[6] = v33;
    v54[7] = v28;
    v34 = v21[12];
    v35 = v21[13];
    v54[0] = v23;
    v54[1] = v34;
    v54[2] = v35;
    v54[3] = v31;
    v48 = v16;
    v49 = v17;
    if ((a9 & 1) != 0)
      v36 = 0;
    else
      v36 = a7;
    v50 = v18;
    return specialized static BNNS.FullyConnectedLayer.convertCSRtoOpaque(batchSize:inputDenseShape:inputColumnIndices:inputRowStarts:inputValues:output:sparseParameters:workspace:workspaceSize:filterParameters:)(a1, a3, v53, v54, a4, a5, (uint64_t)&v48, v36, v19, a10, a11, a12, a13);
  }
  else
  {
    v38 = v21[9];
    v52[8] = v21[8];
    v52[9] = v38;
    v52[10] = v21[10];
    v39 = v21[5];
    v52[4] = v21[4];
    v52[5] = v39;
    v40 = v21[7];
    v52[6] = v21[6];
    v52[7] = v40;
    v41 = v21[1];
    v52[0] = *v21;
    v52[1] = v41;
    v42 = v21[3];
    v52[2] = v21[2];
    v52[3] = v42;
    v48 = 0;
    v49 = 0;
    if ((a9 & 1) != 0)
      v43 = 0;
    else
      v43 = a7;
    v50 = 1;
    return specialized static BNNS.FullyConnectedLayer.convertCOOtoOpaque(batchSize:inputDenseShape:inputIndices:inputValues:output:sparseParameters:workspace:workspaceSize:filterParameters:)(a1, a3, v52, a4, a5, (uint64_t)&v48, v43, v19, a10, a11, a12, a13);
  }
}

void *outlined init with take of BNNS.SparseLayout(void *__src, void *__dst)
{
  return memcpy(__dst, __src, 0x160uLL);
}

uint64_t _s10Accelerate4BNNSO12SparseLayoutOWOg(uint64_t a1)
{
  return *(_QWORD *)(a1 + 344) >> 63;
}

uint64_t _s10Accelerate4BNNSO12SparseLayoutOWOj0_(uint64_t result)
{
  *(_QWORD *)(result + 344) &= ~0x8000000000000000;
  return result;
}

uint64_t specialized static BNNS.FullyConnectedLayer.convertCOOtoOpaque(batchSize:inputDenseShape:inputIndices:inputValues:output:sparseParameters:workspace:workspaceSize:filterParameters:)(size_t batch_size, __int128 *a2, _OWORD *a3, _OWORD *a4, BNNSNDArrayDescriptor *out, uint64_t a6, void *a7, size_t a8, uint64_t a9, size_t a10, int (__cdecl *a11)(void **, size_t, size_t), void (__cdecl *a12)(void *))
{
  uint32_t v12;
  uint32_t v13;
  BNNSTargetSystem v14;
  BOOL v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  uint64_t result;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  _BYTE *v40;
  BNNSSparsityParameters sparse_params;
  BNNSFilterParameters filter_params;
  BNNSNDArrayDescriptor in_values;
  BNNSNDArrayDescriptor in_indices;
  BNNSNDArrayDescriptor in_dense_shape;
  uint64_t v46;

  v46 = *MEMORY[0x1E0C80C00];
  v13 = *(_DWORD *)a6;
  v12 = *(_DWORD *)(a6 + 4);
  v14 = *(_DWORD *)(a6 + 8);
  v15 = *(_BYTE *)(a6 + 12) == 0;
  if (*(_BYTE *)(a6 + 12))
    v13 = 0;
  sparse_params.flags = 0;
  if (!v15)
  {
    v12 = 0;
    v14 = BNNSTargetSystemGeneric;
  }
  sparse_params.sparsity_ratio[0] = v13;
  sparse_params.sparsity_ratio[1] = v12;
  sparse_params.sparsity_type = BNNSSparsityTypeUnstructured;
  sparse_params.target_system = v14;
  v16 = a2[8];
  v17 = a2[9];
  v18 = a2[6];
  *(_OWORD *)&in_dense_shape.stride[5] = a2[7];
  *(_OWORD *)&in_dense_shape.stride[7] = v16;
  v19 = a2[10];
  *(_OWORD *)&in_dense_shape.data_type = v17;
  *(_OWORD *)&in_dense_shape.table_data_type = v19;
  v20 = a2[4];
  v21 = a2[5];
  v22 = a2[2];
  *(_OWORD *)&in_dense_shape.size[5] = a2[3];
  *(_OWORD *)&in_dense_shape.size[7] = v20;
  *(_OWORD *)&in_dense_shape.stride[1] = v21;
  *(_OWORD *)&in_dense_shape.stride[3] = v18;
  v23 = *a2;
  *(_OWORD *)&in_dense_shape.size[1] = a2[1];
  *(_OWORD *)&in_dense_shape.size[3] = v22;
  v24 = a3[9];
  *(_OWORD *)&in_indices.stride[7] = a3[8];
  *(_OWORD *)&in_indices.data_type = v24;
  *(_OWORD *)&in_indices.table_data_type = a3[10];
  *(_OWORD *)&in_dense_shape.flags = v23;
  v25 = a3[5];
  *(_OWORD *)&in_indices.size[7] = a3[4];
  *(_OWORD *)&in_indices.stride[1] = v25;
  v26 = a3[7];
  *(_OWORD *)&in_indices.stride[3] = a3[6];
  *(_OWORD *)&in_indices.stride[5] = v26;
  v27 = a3[1];
  *(_OWORD *)&in_indices.flags = *a3;
  *(_OWORD *)&in_indices.size[1] = v27;
  v28 = a3[3];
  *(_OWORD *)&in_indices.size[3] = a3[2];
  *(_OWORD *)&in_indices.size[5] = v28;
  v29 = a4[8];
  v30 = a4[9];
  v31 = a4[6];
  *(_OWORD *)&in_values.stride[5] = a4[7];
  *(_OWORD *)&in_values.stride[7] = v29;
  v32 = a4[10];
  *(_OWORD *)&in_values.data_type = v30;
  *(_OWORD *)&in_values.table_data_type = v32;
  if (a11 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    v33 = a4[4];
    *(_OWORD *)&in_values.stride[1] = a4[5];
    *(_OWORD *)&in_values.stride[3] = v31;
    v34 = a4[1];
    *(_OWORD *)&in_values.flags = *a4;
    *(_OWORD *)&in_values.size[1] = v34;
    v35 = a4[2];
    *(_OWORD *)&in_values.size[5] = a4[3];
    *(_OWORD *)&in_values.size[7] = v33;
    *(_OWORD *)&in_values.size[3] = v35;
    result = BNNSNDArrayFullyConnectedSparsifySparseCOO(&in_dense_shape, &in_indices, &in_values, out, &sparse_params, batch_size, a7, a8, 0);
  }
  else
  {
    v37 = a4[5];
    *(_OWORD *)&in_values.size[7] = a4[4];
    filter_params.flags = a9;
    filter_params.n_threads = a10;
    filter_params.alloc_memory = a11;
    filter_params.free_memory = a12;
    *(_OWORD *)&in_values.stride[1] = v37;
    *(_OWORD *)&in_values.stride[3] = v31;
    v38 = a4[1];
    *(_OWORD *)&in_values.flags = *a4;
    *(_OWORD *)&in_values.size[1] = v38;
    v39 = a4[3];
    *(_OWORD *)&in_values.size[3] = a4[2];
    *(_OWORD *)&in_values.size[5] = v39;
    result = BNNSNDArrayFullyConnectedSparsifySparseCOO(&in_dense_shape, &in_indices, &in_values, out, &sparse_params, batch_size, a7, a8, &filter_params);
  }
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v40 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t specialized static BNNS.FullyConnectedLayer.convertCSRtoOpaque(batchSize:inputDenseShape:inputColumnIndices:inputRowStarts:inputValues:output:sparseParameters:workspace:workspaceSize:filterParameters:)(size_t batch_size, _OWORD *a2, __int128 *a3, _OWORD *a4, _OWORD *a5, BNNSNDArrayDescriptor *out, uint64_t a7, void *a8, size_t workspace_size, uint64_t a10, size_t a11, int (__cdecl *a12)(void **, size_t, size_t), void (__cdecl *a13)(void *))
{
  uint32_t v13;
  uint32_t v14;
  BNNSTargetSystem v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  uint64_t result;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  _BYTE *v45;
  BNNSSparsityParameters sparse_params;
  BNNSFilterParameters filter_params;
  BNNSNDArrayDescriptor in_values;
  BNNSNDArrayDescriptor in_row_starts;
  BNNSNDArrayDescriptor in_column_indices;
  BNNSNDArrayDescriptor in_dense_shape;
  uint64_t v52;

  v52 = *MEMORY[0x1E0C80C00];
  v14 = *(_DWORD *)a7;
  v13 = *(_DWORD *)(a7 + 4);
  v15 = *(_DWORD *)(a7 + 8);
  if (*(_BYTE *)(a7 + 12))
  {
    v14 = 0;
    v13 = 0;
    v15 = BNNSTargetSystemGeneric;
  }
  sparse_params.flags = 0;
  sparse_params.sparsity_ratio[0] = v14;
  sparse_params.sparsity_ratio[1] = v13;
  sparse_params.sparsity_type = BNNSSparsityTypeUnstructured;
  sparse_params.target_system = v15;
  v16 = a2[9];
  *(_OWORD *)&in_dense_shape.stride[7] = a2[8];
  *(_OWORD *)&in_dense_shape.data_type = v16;
  *(_OWORD *)&in_dense_shape.table_data_type = a2[10];
  v17 = a2[5];
  *(_OWORD *)&in_dense_shape.size[7] = a2[4];
  *(_OWORD *)&in_dense_shape.stride[1] = v17;
  v18 = a2[7];
  *(_OWORD *)&in_dense_shape.stride[3] = a2[6];
  *(_OWORD *)&in_dense_shape.stride[5] = v18;
  v19 = a2[1];
  *(_OWORD *)&in_dense_shape.flags = *a2;
  *(_OWORD *)&in_dense_shape.size[1] = v19;
  v20 = a2[3];
  *(_OWORD *)&in_dense_shape.size[3] = a2[2];
  *(_OWORD *)&in_dense_shape.size[5] = v20;
  v21 = a3[8];
  v22 = a3[9];
  v23 = a3[6];
  *(_OWORD *)&in_column_indices.stride[5] = a3[7];
  *(_OWORD *)&in_column_indices.stride[7] = v21;
  v24 = a3[10];
  *(_OWORD *)&in_column_indices.data_type = v22;
  *(_OWORD *)&in_column_indices.table_data_type = v24;
  v25 = a3[4];
  v26 = a3[5];
  v27 = a3[2];
  *(_OWORD *)&in_column_indices.size[5] = a3[3];
  *(_OWORD *)&in_column_indices.size[7] = v25;
  *(_OWORD *)&in_column_indices.stride[1] = v26;
  *(_OWORD *)&in_column_indices.stride[3] = v23;
  v28 = *a3;
  *(_OWORD *)&in_column_indices.size[1] = a3[1];
  *(_OWORD *)&in_column_indices.size[3] = v27;
  v29 = a4[9];
  *(_OWORD *)&in_row_starts.stride[7] = a4[8];
  *(_OWORD *)&in_row_starts.data_type = v29;
  *(_OWORD *)&in_row_starts.table_data_type = a4[10];
  *(_OWORD *)&in_column_indices.flags = v28;
  v30 = a4[5];
  *(_OWORD *)&in_row_starts.size[7] = a4[4];
  *(_OWORD *)&in_row_starts.stride[1] = v30;
  v31 = a4[7];
  *(_OWORD *)&in_row_starts.stride[3] = a4[6];
  *(_OWORD *)&in_row_starts.stride[5] = v31;
  v32 = a4[1];
  *(_OWORD *)&in_row_starts.flags = *a4;
  *(_OWORD *)&in_row_starts.size[1] = v32;
  v33 = a4[3];
  *(_OWORD *)&in_row_starts.size[3] = a4[2];
  *(_OWORD *)&in_row_starts.size[5] = v33;
  v34 = a5[8];
  v35 = a5[9];
  v36 = a5[6];
  *(_OWORD *)&in_values.stride[5] = a5[7];
  *(_OWORD *)&in_values.stride[7] = v34;
  v37 = a5[10];
  *(_OWORD *)&in_values.data_type = v35;
  *(_OWORD *)&in_values.table_data_type = v37;
  if (a12 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    v38 = a5[4];
    *(_OWORD *)&in_values.stride[1] = a5[5];
    *(_OWORD *)&in_values.stride[3] = v36;
    v39 = a5[1];
    *(_OWORD *)&in_values.flags = *a5;
    *(_OWORD *)&in_values.size[1] = v39;
    v40 = a5[2];
    *(_OWORD *)&in_values.size[5] = a5[3];
    *(_OWORD *)&in_values.size[7] = v38;
    *(_OWORD *)&in_values.size[3] = v40;
    result = BNNSNDArrayFullyConnectedSparsifySparseCSR(&in_dense_shape, &in_column_indices, &in_row_starts, &in_values, out, &sparse_params, batch_size, a8, workspace_size, 0);
  }
  else
  {
    v42 = a5[5];
    *(_OWORD *)&in_values.size[7] = a5[4];
    filter_params.flags = a10;
    filter_params.n_threads = a11;
    filter_params.alloc_memory = a12;
    filter_params.free_memory = a13;
    *(_OWORD *)&in_values.stride[1] = v42;
    *(_OWORD *)&in_values.stride[3] = v36;
    v43 = a5[1];
    *(_OWORD *)&in_values.flags = *a5;
    *(_OWORD *)&in_values.size[1] = v43;
    v44 = a5[3];
    *(_OWORD *)&in_values.size[3] = a5[2];
    *(_OWORD *)&in_values.size[5] = v44;
    result = BNNSNDArrayFullyConnectedSparsifySparseCSR(&in_dense_shape, &in_column_indices, &in_row_starts, &in_values, out, &sparse_params, batch_size, a8, workspace_size, &filter_params);
  }
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v45 = 0;
    return swift_willThrow();
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type BNNS.SparsityType and conformance BNNS.SparsityType()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNS.SparsityType and conformance BNNS.SparsityType;
  if (!lazy protocol witness table cache variable for type BNNS.SparsityType and conformance BNNS.SparsityType)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNS.SparsityType, &type metadata for BNNS.SparsityType);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.SparsityType and conformance BNNS.SparsityType);
  }
  return result;
}

uint64_t storeEnumTagSinglePayload for BNNS.SparsityType(uint64_t a1, int a2, int a3)
{
  int v3;
  uint64_t v4;

  if ((a3 + 1) >= 0x10000)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 1) < 0x100)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3)
    v4 = v4;
  else
    v4 = 0;
  if (a2)
    return ((uint64_t (*)(void))((char *)sub_1CAB411C4 + 4 * byte_1CAB62460[v4]))();
  else
    return ((uint64_t (*)(void))((char *)sub_1CAB411E4 + 4 * byte_1CAB62465[v4]))();
}

_BYTE *sub_1CAB411C4(_BYTE *result, char a2)
{
  *result = a2;
  return result;
}

_BYTE *sub_1CAB411E4(_BYTE *result)
{
  *result = 0;
  return result;
}

_DWORD *sub_1CAB411EC(_DWORD *result, int a2)
{
  *result = a2;
  return result;
}

_WORD *sub_1CAB411F4(_WORD *result, __int16 a2)
{
  *result = a2;
  return result;
}

_WORD *sub_1CAB411FC(_WORD *result)
{
  *result = 0;
  return result;
}

_DWORD *sub_1CAB41204(_DWORD *result)
{
  *result = 0;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.SparsityType()
{
  return &type metadata for BNNS.SparsityType;
}

uint64_t __swift_memcpy12_4(uint64_t result, uint64_t *a2)
{
  uint64_t v2;

  v2 = *a2;
  *(_DWORD *)(result + 8) = *((_DWORD *)a2 + 2);
  *(_QWORD *)result = v2;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.SparseParameters(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 12))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for BNNS.SparseParameters(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_DWORD *)(result + 8) = 0;
    *(_QWORD *)result = (a2 - 1);
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 12) = v3;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.SparseParameters()
{
  return &type metadata for BNNS.SparseParameters;
}

void *__swift_memcpy352_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x160uLL);
}

uint64_t getEnumTagSinglePayload for BNNS.SparseLayout(uint64_t a1, int a2)
{
  unsigned int v2;
  int v3;

  if (!a2)
    return 0;
  if (a2 < 0 && *(_BYTE *)(a1 + 352))
    return *(_DWORD *)a1 + 0x80000000;
  v2 = *(_DWORD *)(a1 + 148);
  if (v2 > 0x80000000)
    v3 = ~v2;
  else
    v3 = -1;
  return (v3 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.SparseLayout(uint64_t result, int a2, int a3)
{
  if (a2 < 0)
  {
    *(_QWORD *)(result + 344) = 0;
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(_QWORD *)result = a2 ^ 0x80000000;
    if (a3 < 0)
      *(_BYTE *)(result + 352) = 1;
  }
  else
  {
    if ((a3 & 0x80000000) == 0)
    {
      if (!a2)
        return result;
LABEL_8:
      *(_OWORD *)(result + 112) = 0u;
      *(_OWORD *)(result + 128) = 0u;
      *(_OWORD *)(result + 80) = 0u;
      *(_OWORD *)(result + 96) = 0u;
      *(_OWORD *)(result + 48) = 0u;
      *(_OWORD *)(result + 64) = 0u;
      *(_OWORD *)(result + 16) = 0u;
      *(_OWORD *)(result + 32) = 0u;
      *(_OWORD *)result = 0u;
      *(_QWORD *)(result + 144) = (unint64_t)-a2 << 32;
      *(_OWORD *)(result + 168) = 0u;
      *(_OWORD *)(result + 184) = 0u;
      *(_OWORD *)(result + 200) = 0u;
      *(_OWORD *)(result + 216) = 0u;
      *(_OWORD *)(result + 232) = 0u;
      *(_OWORD *)(result + 248) = 0u;
      *(_QWORD *)(result + 344) = 0;
      *(_OWORD *)(result + 152) = 0u;
      result += 152;
      *(_OWORD *)(result + 112) = 0u;
      *(_OWORD *)(result + 128) = 0u;
      *(_OWORD *)(result + 144) = 0u;
      *(_OWORD *)(result + 160) = 0u;
      *(_OWORD *)(result + 176) = 0u;
      return result;
    }
    *(_BYTE *)(result + 352) = 0;
    if (a2)
      goto LABEL_8;
  }
  return result;
}

uint64_t destructiveInjectEnumTag for BNNS.SparseLayout(uint64_t result, int a2)
{
  uint64_t v2;
  uint64_t v3;

  v2 = *(unsigned int *)(result + 168);
  v3 = *(unsigned int *)(result + 320);
  *(_QWORD *)(result + 144) = *(unsigned int *)(result + 144);
  *(_QWORD *)(result + 168) = v2;
  *(_QWORD *)(result + 320) = v3;
  *(_DWORD *)(result + 348) = a2 << 31;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.SparseLayout()
{
  return &type metadata for BNNS.SparseLayout;
}

double BNNS.FusedUnaryArithmeticParameters.layerParameters(input:output:)@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, uint64_t *a3@<X8>)
{
  uint64_t v3;
  int v6;
  int v7;
  uint64_t v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  int v14;
  uint64_t v15;
  uint64_t v16;
  double result;
  _BYTE v18[180];

  *(_OWORD *)&v18[100] = a2[6];
  *(_OWORD *)&v18[116] = a2[7];
  *(_OWORD *)&v18[132] = a2[8];
  *(_OWORD *)&v18[148] = a2[9];
  *(_OWORD *)&v18[164] = a2[10];
  *(_OWORD *)&v18[52] = a2[3];
  *(_OWORD *)&v18[68] = a2[4];
  *(_OWORD *)&v18[84] = a2[5];
  *(_OWORD *)&v18[4] = *a2;
  *(_OWORD *)&v18[20] = a2[1];
  v6 = *(unsigned __int8 *)(v3 + 8);
  v7 = *(unsigned __int8 *)(v3 + 9);
  *(_OWORD *)&v18[36] = a2[2];
  v8 = swift_slowAlloc();
  v9 = a1[9];
  *(_OWORD *)(v8 + 128) = a1[8];
  *(_OWORD *)(v8 + 144) = v9;
  *(_OWORD *)(v8 + 160) = a1[10];
  v10 = a1[5];
  *(_OWORD *)(v8 + 64) = a1[4];
  *(_OWORD *)(v8 + 80) = v10;
  v11 = a1[7];
  *(_OWORD *)(v8 + 96) = a1[6];
  *(_OWORD *)(v8 + 112) = v11;
  v12 = a1[1];
  *(_OWORD *)v8 = *a1;
  *(_OWORD *)(v8 + 16) = v12;
  v13 = a1[3];
  *(_OWORD *)(v8 + 32) = a1[2];
  *(_OWORD *)(v8 + 48) = v13;
  *(_OWORD *)(v8 + 324) = *(_OWORD *)&v18[144];
  *(_OWORD *)(v8 + 340) = *(_OWORD *)&v18[160];
  *(_OWORD *)(v8 + 244) = *(_OWORD *)&v18[64];
  *(_OWORD *)(v8 + 260) = *(_OWORD *)&v18[80];
  *(_OWORD *)(v8 + 276) = *(_OWORD *)&v18[96];
  *(_OWORD *)(v8 + 292) = *(_OWORD *)&v18[112];
  *(_OWORD *)(v8 + 308) = *(_OWORD *)&v18[128];
  *(_OWORD *)(v8 + 180) = *(_OWORD *)v18;
  *(_OWORD *)(v8 + 196) = *(_OWORD *)&v18[16];
  *(_OWORD *)(v8 + 212) = *(_OWORD *)&v18[32];
  *(_QWORD *)v3 = v8;
  *(_DWORD *)(v8 + 176) = v6;
  *(_DWORD *)(v8 + 356) = *(_DWORD *)&v18[176];
  *(_OWORD *)(v8 + 228) = *(_OWORD *)&v18[48];
  *(_DWORD *)(v8 + 360) = v7;
  v14 = BNNS.ArithmeticUnaryFunction.bnnsArithmeticFunction.getter();
  type metadata accessor for BNNSLayerParametersArithmetic(0);
  a3[3] = v15;
  a3[4] = (uint64_t)&protocol witness table for BNNSLayerParametersArithmetic;
  v16 = swift_allocObject();
  *a3 = v16;
  *(_DWORD *)(v16 + 16) = v14;
  *(_QWORD *)(v16 + 24) = v8;
  *(_DWORD *)(v16 + 32) = 0;
  *(int32x2_t *)(v16 + 36) = vdup_n_s32(0x7FC00000u);
  *(_DWORD *)(v16 + 44) = 1;
  result = 0.0;
  *(_OWORD *)(v16 + 48) = 0u;
  *(_OWORD *)(v16 + 64) = 0u;
  return result;
}

void BNNS.FusedUnaryArithmeticParameters.inputDescriptorType.getter(_BYTE *a1@<X8>)
{
  uint64_t v1;

  *a1 = *(_BYTE *)(v1 + 8);
}

_BYTE *BNNS.FusedUnaryArithmeticParameters.inputDescriptorType.setter(_BYTE *result)
{
  uint64_t v1;

  *(_BYTE *)(v1 + 8) = *result;
  return result;
}

uint64_t (*BNNS.FusedUnaryArithmeticParameters.inputDescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedUnaryArithmeticParameters.outputDescriptorType.getter(_BYTE *a1@<X8>)
{
  uint64_t v1;

  *a1 = *(_BYTE *)(v1 + 9);
}

_BYTE *BNNS.FusedUnaryArithmeticParameters.outputDescriptorType.setter(_BYTE *result)
{
  uint64_t v1;

  *(_BYTE *)(v1 + 9) = *result;
  return result;
}

uint64_t (*BNNS.FusedUnaryArithmeticParameters.outputDescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedUnaryArithmeticParameters.function.getter(_BYTE *a1@<X8>)
{
  uint64_t v1;

  *a1 = *(_BYTE *)(v1 + 10);
}

_BYTE *BNNS.FusedUnaryArithmeticParameters.function.setter(_BYTE *result)
{
  uint64_t v1;

  *(_BYTE *)(v1 + 10) = *result;
  return result;
}

uint64_t (*BNNS.FusedUnaryArithmeticParameters.function.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

char *BNNS.FusedUnaryArithmeticParameters.init(inputDescriptorType:outputDescriptorType:function:)@<X0>(char *result@<X0>, char *a2@<X1>, char *a3@<X2>, uint64_t a4@<X8>)
{
  char v4;
  char v5;
  char v6;

  v4 = *result;
  v5 = *a2;
  v6 = *a3;
  *(_QWORD *)a4 = 0;
  *(_BYTE *)(a4 + 8) = v4;
  *(_BYTE *)(a4 + 9) = v5;
  *(_BYTE *)(a4 + 10) = v6;
  return result;
}

uint64_t protocol witness for FusableLayerParametersWrapper.filterType.getter in conformance BNNS.FusedUnaryArithmeticParameters()
{
  return 8;
}

uint64_t __swift_memcpy11_8(uint64_t result, uint64_t *a2)
{
  uint64_t v2;

  v2 = *a2;
  *(_DWORD *)(result + 7) = *(_DWORD *)((char *)a2 + 7);
  *(_QWORD *)result = v2;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.FusedUnaryArithmeticParameters(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  BOOL v4;
  int v5;

  if (!a2)
    return 0;
  if (a2 >= 0xFE && *(_BYTE *)(a1 + 11))
    return (*(_DWORD *)a1 + 254);
  v3 = *(unsigned __int8 *)(a1 + 8);
  v4 = v3 >= 3;
  v5 = v3 - 3;
  if (!v4)
    v5 = -1;
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.FusedUnaryArithmeticParameters(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFD)
  {
    *(_BYTE *)(result + 10) = 0;
    *(_WORD *)(result + 8) = 0;
    *(_QWORD *)result = a2 - 254;
    if (a3 >= 0xFE)
      *(_BYTE *)(result + 11) = 1;
  }
  else
  {
    if (a3 >= 0xFE)
      *(_BYTE *)(result + 11) = 0;
    if (a2)
      *(_BYTE *)(result + 8) = a2 + 2;
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.FusedUnaryArithmeticParameters()
{
  return &type metadata for BNNS.FusedUnaryArithmeticParameters;
}

uint64_t sub_1CAB416DC()
{
  return swift_deallocObject();
}

uint64_t static vImage.Planar8.channelCount.getter()
{
  return 1;
}

uint64_t static vImage.Interleaved8x2.channelCount.getter()
{
  return 2;
}

uint64_t static vImage.Interleaved8x3.channelCount.getter()
{
  return 3;
}

uint64_t static vImage.Interleaved8x4.channelCount.getter()
{
  return 4;
}

uint64_t static vImage.PlanarF.channelCount.getter()
{
  return 1;
}

uint64_t static vImage.InterleavedFx2.channelCount.getter()
{
  return 2;
}

uint64_t static vImage.InterleavedFx3.channelCount.getter()
{
  return 3;
}

uint64_t static vImage.InterleavedFx4.channelCount.getter()
{
  return 4;
}

uint64_t _ss15withUnsafeBytes2of_q0_x_q0_SWq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v8;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  char *v14;
  uint64_t v15;
  uint64_t (*v16)(uint64_t, uint64_t, char *);
  uint64_t result;
  uint64_t v18;

  v11 = *(_QWORD *)(a5 - 8);
  v12 = MEMORY[0x1E0C80A78](a1);
  v14 = (char *)&v18 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  result = v16(v12, v12 + *(_QWORD *)(*(_QWORD *)(v15 - 8) + 64), v14);
  if (v8)
    return (*(uint64_t (**)(uint64_t, char *, uint64_t))(v11 + 32))(a8, v14, a5);
  return result;
}

uint64_t static vImage.Planar8x2.bitCountPerPlanarPixel.getter()
{
  return 8;
}

uint64_t static vImage.Planar8x2.planeCount.getter()
{
  return 2;
}

uint64_t protocol witness for static MultiplePlanePixelFormat.planeCount.getter in conformance vImage.Planar8x2()
{
  return 2;
}

uint64_t static vImage.Planar8x3.bitCountPerPlanarPixel.getter()
{
  return 8;
}

uint64_t static vImage.Planar8x3.planeCount.getter()
{
  return 3;
}

uint64_t protocol witness for static MultiplePlanePixelFormat.planeCount.getter in conformance vImage.Planar8x3()
{
  return 3;
}

uint64_t static vImage.Planar8x4.bitCountPerPlanarPixel.getter()
{
  return 8;
}

uint64_t static vImage.Planar8x4.planeCount.getter()
{
  return 4;
}

uint64_t protocol witness for static MultiplePlanePixelFormat.planeCount.getter in conformance vImage.Planar8x4()
{
  return 4;
}

uint64_t static vImage.PlanarFx2.bitCountPerPlanarPixel.getter()
{
  return 32;
}

uint64_t static vImage.PlanarFx2.planeCount.getter()
{
  return 2;
}

uint64_t protocol witness for static MultiplePlanePixelFormat.bitCountPerPlanarPixel.getter in conformance vImage.PlanarFx2()
{
  return 32;
}

uint64_t static vImage.PlanarFx3.bitCountPerPlanarPixel.getter()
{
  return 32;
}

uint64_t static vImage.PlanarFx3.planeCount.getter()
{
  return 3;
}

uint64_t static vImage.PlanarFx4.bitCountPerPlanarPixel.getter()
{
  return 32;
}

uint64_t static vImage.PlanarFx4.planeCount.getter()
{
  return 4;
}

uint64_t static vImage.Planar8.bitCountPerComponent.getter()
{
  return 8;
}

uint64_t static vImage.Planar8.bitCountPerPixel.getter()
{
  return 8;
}

uint64_t static vImage.Interleaved8x2.bitCountPerPixel.getter()
{
  return 16;
}

uint64_t protocol witness for static StaticPixelFormat.bitCountPerPixel.getter in conformance vImage.Interleaved8x2()
{
  return 16;
}

uint64_t static vImage.Interleaved8x3.bitCountPerComponent.getter()
{
  return 8;
}

uint64_t static vImage.Interleaved8x3.bitCountPerPixel.getter()
{
  return 24;
}

uint64_t protocol witness for static StaticPixelFormat.bitCountPerPixel.getter in conformance vImage.Interleaved8x3()
{
  return 24;
}

uint64_t static vImage.Interleaved8x4.bitCountPerComponent.getter()
{
  return 8;
}

uint64_t static vImage.Interleaved8x4.bitCountPerPixel.getter()
{
  return 32;
}

uint64_t static vImage.Planar16F.channelCount.getter()
{
  return 1;
}

uint64_t static vImage.Planar16F.bitCountPerComponent.getter()
{
  return 16;
}

uint64_t static vImage.Planar16F.bitCountPerPixel.getter()
{
  return 16;
}

uint64_t static vImage.Interleaved16Fx2.channelCount.getter()
{
  return 2;
}

uint64_t static vImage.Interleaved16Fx2.bitCountPerPixel.getter()
{
  return 32;
}

uint64_t static vImage.Interleaved16Fx4.channelCount.getter()
{
  return 4;
}

uint64_t static vImage.Interleaved16Fx4.bitCountPerComponent.getter()
{
  return 16;
}

uint64_t static vImage.Interleaved16Fx4.bitCountPerPixel.getter()
{
  return 64;
}

uint64_t protocol witness for static StaticPixelFormat.bitCountPerPixel.getter in conformance vImage.Interleaved16Fx4()
{
  return 64;
}

uint64_t static vImage.PlanarF.bitCountPerComponent.getter()
{
  return 32;
}

uint64_t static vImage.PlanarF.bitCountPerPixel.getter()
{
  return 32;
}

uint64_t static vImage.InterleavedFx2.bitCountPerPixel.getter()
{
  return 64;
}

uint64_t static vImage.InterleavedFx3.bitCountPerComponent.getter()
{
  return 32;
}

uint64_t static vImage.InterleavedFx3.bitCountPerPixel.getter()
{
  return 96;
}

uint64_t protocol witness for static StaticPixelFormat.bitCountPerPixel.getter in conformance vImage.InterleavedFx3()
{
  return 96;
}

uint64_t static vImage.InterleavedFx4.bitCountPerComponent.getter()
{
  return 32;
}

uint64_t static vImage.InterleavedFx4.bitCountPerPixel.getter()
{
  return 128;
}

uint64_t protocol witness for static StaticPixelFormat.bitCountPerPixel.getter in conformance vImage.InterleavedFx4()
{
  return 128;
}

uint64_t static vImage.Planar16U.channelCount.getter()
{
  return 1;
}

uint64_t static vImage.Planar16U.bitCountPerPixel.getter()
{
  return 16;
}

uint64_t static vImage.Interleaved16Ux2.channelCount.getter()
{
  return 2;
}

uint64_t static vImage.Interleaved16Ux2.bitCountPerComponent.getter()
{
  return 16;
}

uint64_t static vImage.Interleaved16Ux2.bitCountPerPixel.getter()
{
  return 32;
}

uint64_t static vImage.Interleaved16Ux4.channelCount.getter()
{
  return 4;
}

uint64_t static vImage.Interleaved16Ux4.bitCountPerComponent.getter()
{
  return 16;
}

uint64_t static vImage.Interleaved16Ux4.bitCountPerPixel.getter()
{
  return 64;
}

uint64_t static vImage.Planar16F.makePixel(_:)(float a1)
{
  float v2;
  unsigned __int16 v3;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v6;

  v6 = *MEMORY[0x1E0C80C00];
  v3 = 0;
  v2 = a1;
  src.data = &v2;
  *(int64x2_t *)&src.height = vdupq_n_s64(1uLL);
  src.rowBytes = 4;
  dest.data = &v3;
  *(_OWORD *)&dest.height = *(_OWORD *)&src.height;
  dest.rowBytes = 2;
  vImageConvert_PlanarFtoPlanar16F(&src, &dest, 0);
  return v3;
}

uint64_t convert<A, B>(src:dest:convertFunc:channelCount:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  _QWORD v14[12];

  if (a5 < 0)
  {
    __break(1u);
  }
  else
  {
    v5 = MEMORY[0x1E0C80A78](result);
    v14[2] = v6;
    v14[3] = v7;
    v14[4] = v8;
    v14[5] = v9;
    v14[6] = v5;
    v14[7] = v10;
    v14[8] = v11;
    v14[9] = v12;
    return _ss15withUnsafeBytes2of_q0_x_q0_SWq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(v5, (uint64_t)partial apply for closure #1 in convert<A, B>(src:dest:convertFunc:channelCount:), (uint64_t)v14, v6, MEMORY[0x1E0DEDCE8], MEMORY[0x1E0DEE9C0] + 8, MEMORY[0x1E0DEDD18], v13);
  }
  return result;
}

uint64_t static vImage.Interleaved16Fx2.makePixel(_:)(float a1, float a2)
{
  _DWORD v3[2];
  unsigned int v4;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v7;

  v7 = *MEMORY[0x1E0C80C00];
  v4 = 0;
  *(float *)v3 = a1;
  *(float *)&v3[1] = a2;
  src.data = v3;
  *(_OWORD *)&src.height = xmmword_1CAB5E430;
  src.rowBytes = 8;
  dest.data = &v4;
  *(_OWORD *)&dest.height = xmmword_1CAB5E430;
  dest.rowBytes = 4;
  vImageConvert_PlanarFtoPlanar16F(&src, &dest, 0);
  return v4;
}

uint64_t static vImage.Interleaved16Fx4.makePixel(_:)(float a1, float a2, float a3, float a4)
{
  _DWORD v5[4];
  uint64_t v6;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v9;

  v9 = *MEMORY[0x1E0C80C00];
  *(float *)v5 = a1;
  *(float *)&v5[1] = a2;
  *(float *)&v5[2] = a3;
  *(float *)&v5[3] = a4;
  src.data = v5;
  *(_OWORD *)&src.height = xmmword_1CAB62590;
  src.rowBytes = 16;
  v6 = 0;
  dest.data = &v6;
  *(_OWORD *)&dest.height = xmmword_1CAB62590;
  dest.rowBytes = 8;
  vImageConvert_PlanarFtoPlanar16F(&src, &dest, 0);
  return v6;
}

float static vImage.PlanarF.makePixel(_:)(__int16 a1)
{
  __int16 v2;
  float v3;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v6;

  v6 = *MEMORY[0x1E0C80C00];
  v3 = 0.0;
  v2 = a1;
  src.data = &v2;
  *(int64x2_t *)&src.height = vdupq_n_s64(1uLL);
  src.rowBytes = 2;
  dest.data = &v3;
  *(_OWORD *)&dest.height = *(_OWORD *)&src.height;
  dest.rowBytes = 4;
  vImageConvert_Planar16FtoPlanarF(&src, &dest, 0);
  return v3;
}

float static vImage.InterleavedFx2.makePixel(_:)(__int16 a1, __int16 a2)
{
  _WORD v3[2];
  uint64_t v4;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v7;

  v7 = *MEMORY[0x1E0C80C00];
  v3[0] = a1;
  v3[1] = a2;
  src.data = v3;
  *(_OWORD *)&src.height = xmmword_1CAB5E430;
  src.rowBytes = 4;
  v4 = 0;
  dest.data = &v4;
  *(_OWORD *)&dest.height = xmmword_1CAB5E430;
  dest.rowBytes = 8;
  vImageConvert_Planar16FtoPlanarF(&src, &dest, 0);
  return *(float *)&v4;
}

float static vImage.InterleavedFx4.makePixel(_:)(__int16 a1, __int16 a2, __int16 a3, __int16 a4)
{
  _WORD v5[4];
  _QWORD v6[2];
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v9;

  v9 = *MEMORY[0x1E0C80C00];
  v6[0] = 0;
  v6[1] = 0;
  v5[0] = a1;
  v5[1] = a2;
  v5[2] = a3;
  v5[3] = a4;
  src.data = v5;
  *(_OWORD *)&src.height = xmmword_1CAB62590;
  src.rowBytes = 8;
  dest.data = v6;
  *(_OWORD *)&dest.height = xmmword_1CAB62590;
  dest.rowBytes = 16;
  vImageConvert_Planar16FtoPlanarF(&src, &dest, 0);
  return *(float *)v6;
}

uint64_t partial apply for closure #1 in convert<A, B>(src:dest:convertFunc:channelCount:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v12;
  _QWORD v14[6];
  __int128 v15;
  uint64_t v16;
  __int128 v17;

  v9 = *(_QWORD *)(v8 + 24);
  v10 = *(_QWORD *)(v8 + 32);
  v11 = *(_OWORD *)(v8 + 40);
  v12 = *(_QWORD *)(v8 + 56);
  v14[2] = *(_QWORD *)(v8 + 16);
  v14[3] = v9;
  v14[4] = a1;
  v14[5] = a2;
  v15 = v11;
  v16 = v12;
  v17 = *(_OWORD *)(v8 + 64);
  return _ss15withUnsafeBytes2of_q0_x_q0_SWq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(v10, (uint64_t)partial apply for closure #1 in closure #1 in convert<A, B>(src:dest:convertFunc:channelCount:), (uint64_t)v14, v9, MEMORY[0x1E0DEDCE8], MEMORY[0x1E0DEE9C0] + 8, MEMORY[0x1E0DEDD18], a8);
}

uint64_t associated type witness table accessor for PixelFormat.ComponentType : Equatable in vImage.Planar8x2()
{
  return MEMORY[0x1E0DEDE88];
}

_UNKNOWN **associated type witness table accessor for MultiplePlanePixelFormat.PlanarPixelFormat : StaticPixelFormat in vImage.Planar8x2()
{
  return &protocol witness table for vImage.Planar8;
}

uint64_t associated type witness table accessor for PixelFormat.ComponentType : Equatable in vImage.PlanarFx2()
{
  return MEMORY[0x1E0DEB1B8];
}

_UNKNOWN **associated type witness table accessor for MultiplePlanePixelFormat.PlanarPixelFormat : StaticPixelFormat in vImage.PlanarFx2()
{
  return &protocol witness table for vImage.PlanarF;
}

uint64_t associated type witness table accessor for PixelFormat.ComponentType : Equatable in vImage.DynamicPixelFormat()
{
  return MEMORY[0x1E0DEDD00];
}

uint64_t associated type witness table accessor for PixelFormat.ComponentType : Equatable in vImage.Planar16F()
{
  return MEMORY[0x1E0DEE088];
}

uint64_t dispatch thunk of static MultiplePlanePixelFormat.planeCount.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 32))();
}

uint64_t dispatch thunk of static MultiplePlanePixelFormat.bitCountPerPlanarPixel.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 40))();
}

uint64_t dispatch thunk of static StaticPixelFormat.bitCountPerPixel.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

uint64_t dispatch thunk of static StaticPixelFormat.channelCount.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 24))();
}

ValueMetadata *type metadata accessor for vImage.Planar8x2()
{
  return &type metadata for vImage.Planar8x2;
}

ValueMetadata *type metadata accessor for vImage.Planar8x3()
{
  return &type metadata for vImage.Planar8x3;
}

ValueMetadata *type metadata accessor for vImage.Planar8x4()
{
  return &type metadata for vImage.Planar8x4;
}

ValueMetadata *type metadata accessor for vImage.PlanarFx2()
{
  return &type metadata for vImage.PlanarFx2;
}

ValueMetadata *type metadata accessor for vImage.PlanarFx3()
{
  return &type metadata for vImage.PlanarFx3;
}

ValueMetadata *type metadata accessor for vImage.PlanarFx4()
{
  return &type metadata for vImage.PlanarFx4;
}

ValueMetadata *type metadata accessor for vImage.DynamicPixelFormat()
{
  return &type metadata for vImage.DynamicPixelFormat;
}

ValueMetadata *type metadata accessor for vImage.Planar8()
{
  return &type metadata for vImage.Planar8;
}

ValueMetadata *type metadata accessor for vImage.Interleaved8x2()
{
  return &type metadata for vImage.Interleaved8x2;
}

ValueMetadata *type metadata accessor for vImage.Interleaved8x3()
{
  return &type metadata for vImage.Interleaved8x3;
}

ValueMetadata *type metadata accessor for vImage.Interleaved8x4()
{
  return &type metadata for vImage.Interleaved8x4;
}

ValueMetadata *type metadata accessor for vImage.Planar16F()
{
  return &type metadata for vImage.Planar16F;
}

ValueMetadata *type metadata accessor for vImage.Interleaved16Fx2()
{
  return &type metadata for vImage.Interleaved16Fx2;
}

ValueMetadata *type metadata accessor for vImage.Interleaved16Fx4()
{
  return &type metadata for vImage.Interleaved16Fx4;
}

ValueMetadata *type metadata accessor for vImage.PlanarF()
{
  return &type metadata for vImage.PlanarF;
}

ValueMetadata *type metadata accessor for vImage.InterleavedFx2()
{
  return &type metadata for vImage.InterleavedFx2;
}

ValueMetadata *type metadata accessor for vImage.InterleavedFx3()
{
  return &type metadata for vImage.InterleavedFx3;
}

ValueMetadata *type metadata accessor for vImage.InterleavedFx4()
{
  return &type metadata for vImage.InterleavedFx4;
}

ValueMetadata *type metadata accessor for vImage.Planar16U()
{
  return &type metadata for vImage.Planar16U;
}

ValueMetadata *type metadata accessor for vImage.Interleaved16Ux2()
{
  return &type metadata for vImage.Interleaved16Ux2;
}

ValueMetadata *type metadata accessor for vImage.Interleaved16Ux4()
{
  return &type metadata for vImage.Interleaved16Ux4;
}

uint64_t partial apply for closure #1 in closure #1 in convert<A, B>(src:dest:convertFunc:channelCount:)(uint64_t result)
{
  _QWORD *v1;
  uint64_t v2;
  uint64_t (*v3)(_QWORD *, _QWORD *, _QWORD);
  uint64_t v4;
  uint64_t v5;
  _QWORD v6[4];
  _QWORD v7[4];

  if (v1[4])
  {
    v2 = v1[6];
    v4 = v1[8];
    v3 = (uint64_t (*)(_QWORD *, _QWORD *, _QWORD))v1[9];
    v5 = *(_QWORD *)(*(_QWORD *)(v1[2] - 8) + 72);
    v7[0] = v1[4];
    v7[1] = 1;
    v7[2] = v2;
    v7[3] = v5;
    if (result)
    {
      v6[0] = result;
      v6[1] = 1;
      v6[2] = v2;
      v6[3] = v4;
      return v3(v7, v6, 0);
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t static BNNS.copy(_:to:filterParameters:)(_OWORD *a1, _OWORD *a2, uint32_t a3, size_t a4, int (__cdecl *a5)(void **, size_t, size_t), void (__cdecl *a6)(void *))
{
  __int128 v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  uint64_t result;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  _BYTE *v27;
  BNNSFilterParameters v28;
  BNNSNDArrayDescriptor dest;
  BNNSNDArrayDescriptor src;
  uint64_t v31;

  v31 = *MEMORY[0x1E0C80C00];
  if (a5 != (int (__cdecl *)(void **, size_t, size_t))1)
  {
    v28.flags = a3;
    v28.n_threads = a4;
    v28.alloc_memory = a5;
    v28.free_memory = a6;
    v17 = a1[9];
    *(_OWORD *)&src.stride[7] = a1[8];
    *(_OWORD *)&src.data_type = v17;
    *(_OWORD *)&src.table_data_type = a1[10];
    v18 = a1[5];
    *(_OWORD *)&src.size[7] = a1[4];
    *(_OWORD *)&src.stride[1] = v18;
    v19 = a1[7];
    *(_OWORD *)&src.stride[3] = a1[6];
    *(_OWORD *)&src.stride[5] = v19;
    v20 = a1[1];
    *(_OWORD *)&src.flags = *a1;
    *(_OWORD *)&src.size[1] = v20;
    v21 = a1[3];
    *(_OWORD *)&src.size[3] = a1[2];
    *(_OWORD *)&src.size[5] = v21;
    v22 = a2[9];
    *(_OWORD *)&dest.stride[7] = a2[8];
    *(_OWORD *)&dest.data_type = v22;
    *(_OWORD *)&dest.table_data_type = a2[10];
    v23 = a2[5];
    *(_OWORD *)&dest.size[7] = a2[4];
    *(_OWORD *)&dest.stride[1] = v23;
    v24 = a2[7];
    *(_OWORD *)&dest.stride[3] = a2[6];
    *(_OWORD *)&dest.stride[5] = v24;
    v25 = a2[1];
    *(_OWORD *)&dest.flags = *a2;
    *(_OWORD *)&dest.size[1] = v25;
    v26 = a2[3];
    *(_OWORD *)&dest.size[3] = a2[2];
    *(_OWORD *)&dest.size[5] = v26;
    result = BNNSCopy(&dest, &src, &v28);
    if (!(_DWORD)result)
      return result;
    goto LABEL_5;
  }
  v6 = a1[9];
  *(_OWORD *)&src.stride[7] = a1[8];
  *(_OWORD *)&src.data_type = v6;
  *(_OWORD *)&src.table_data_type = a1[10];
  v7 = a1[5];
  *(_OWORD *)&src.size[7] = a1[4];
  *(_OWORD *)&src.stride[1] = v7;
  v8 = a1[7];
  *(_OWORD *)&src.stride[3] = a1[6];
  *(_OWORD *)&src.stride[5] = v8;
  v9 = a1[1];
  *(_OWORD *)&src.flags = *a1;
  *(_OWORD *)&src.size[1] = v9;
  v10 = a1[3];
  *(_OWORD *)&src.size[3] = a1[2];
  *(_OWORD *)&src.size[5] = v10;
  v11 = a2[9];
  *(_OWORD *)&dest.stride[7] = a2[8];
  *(_OWORD *)&dest.data_type = v11;
  *(_OWORD *)&dest.table_data_type = a2[10];
  v12 = a2[5];
  *(_OWORD *)&dest.size[7] = a2[4];
  *(_OWORD *)&dest.stride[1] = v12;
  v13 = a2[7];
  *(_OWORD *)&dest.stride[3] = a2[6];
  *(_OWORD *)&dest.stride[5] = v13;
  v14 = a2[1];
  *(_OWORD *)&dest.flags = *a2;
  *(_OWORD *)&dest.size[1] = v14;
  v15 = a2[3];
  *(_OWORD *)&dest.size[3] = a2[2];
  *(_OWORD *)&dest.size[5] = v15;
  result = BNNSCopy(&dest, &src, 0);
  if ((_DWORD)result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v27 = 0;
    return swift_willThrow();
  }
  return result;
}

void vImage.PixelBuffer<>.applyMorphology(operation:destination:)(uint64_t a1, _QWORD **a2)
{
  _QWORD **v2;

  specialized vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(*(_QWORD *)a1, *(_QWORD *)(a1 + 8), *(_QWORD *)(a1 + 16), *(_BYTE *)(a1 + 24), *a2, (uint64_t)@nonobjc vImageErode_ARGB8888(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageDilate_ARGB8888(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageMin_ARGB8888(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageMax_ARGB8888(_:_:_:_:_:_:_:_:), 0, *v2);
}

{
  _QWORD **v2;

  specialized vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(*(_QWORD *)a1, *(_QWORD *)(a1 + 8), *(_QWORD *)(a1 + 16), *(_BYTE *)(a1 + 24), *a2, (uint64_t)@nonobjc vImageErode_Planar8(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageDilate_Planar8(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageMin_Planar8(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageMax_Planar8(_:_:_:_:_:_:_:_:), 0, *v2);
}

{
  _QWORD **v2;

  specialized vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(*(_QWORD *)a1, *(_QWORD *)(a1 + 8), *(_QWORD *)(a1 + 16), *(_BYTE *)(a1 + 24), *a2, (uint64_t)@nonobjc vImageErode_ARGBFFFF(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageDilate_ARGBFFFF(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageMin_ARGBFFFF(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageMax_ARGBFFFF(_:_:_:_:_:_:_:_:), 0, *v2);
}

{
  _QWORD **v2;

  specialized vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(*(_QWORD *)a1, *(_QWORD *)(a1 + 8), *(_QWORD *)(a1 + 16), *(_BYTE *)(a1 + 24), *a2, (uint64_t)@nonobjc vImageErode_PlanarF(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageDilate_PlanarF(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageMin_PlanarF(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageMax_PlanarF(_:_:_:_:_:_:_:_:), 0, *v2);
}

void specialized vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, char a4, _QWORD *a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, _QWORD *a14)
{
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;

  if (a14[2])
  {
    if (a5[2])
    {
      v14 = a14[4];
      v15 = a5[4];
      if (v14)
      {
        if (!v15 || v14 != v15)
          goto LABEL_8;
        __break(1u);
      }
      if (!v15)
      {
        __break(1u);
        JUMPOUT(0x1CAB42480);
      }
LABEL_8:
      v16 = a14[6];
      if ((v16 & 0x8000000000000000) == 0)
      {
        v17 = a14[5];
        if ((v17 & 0x8000000000000000) == 0)
        {
          if (v16)
          {
            if (v17)
            {
              v18 = a5[6];
              if ((v18 & 0x8000000000000000) == 0)
              {
                v19 = a5[5];
                if ((v19 & 0x8000000000000000) == 0)
                {
                  if (v18)
                  {
                    if (v19)
                    {
                      if (v16 == v18)
                      {
                        if (v17 == v19)
                        {
                          if ((a4 & 0xFE) != 0)
                            LODWORD(v20) = a2;
                          else
                            v20 = a2 & ~(a2 >> 63);
                          if ((a4 & 0xFE) != 0)
                            LODWORD(v21) = a1;
                          else
                            v21 = a1 & ~(a1 >> 63);
                          if ((v20 & v21 & 1) != 0)
                            __asm { BR              X11 }
LABEL_38:
                          __break(1u);
                          JUMPOUT(0x1CAB42478);
                        }
LABEL_37:
                        __break(1u);
                        goto LABEL_38;
                      }
LABEL_36:
                      __break(1u);
                      goto LABEL_37;
                    }
LABEL_35:
                    __break(1u);
                    goto LABEL_36;
                  }
LABEL_34:
                  __break(1u);
                  goto LABEL_35;
                }
LABEL_33:
                __break(1u);
                goto LABEL_34;
              }
LABEL_32:
              __break(1u);
              goto LABEL_33;
            }
LABEL_31:
            __break(1u);
            goto LABEL_32;
          }
LABEL_30:
          __break(1u);
          goto LABEL_31;
        }
LABEL_29:
        __break(1u);
        goto LABEL_30;
      }
LABEL_28:
      __break(1u);
      goto LABEL_29;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  goto LABEL_28;
}

void vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15)
{
  uint64_t *v15;
  uint64_t v17;
  unsigned int v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  BOOL v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v35;
  uint64_t v39;
  _QWORD v40[2];
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;

  v45 = *MEMORY[0x1E0C80C00];
  v17 = *(_QWORD *)a1;
  v39 = *(_QWORD *)(a1 + 8);
  v35 = *(_QWORD *)(a1 + 16);
  v18 = *(unsigned __int8 *)(a1 + 24);
  v19 = *a2;
  v20 = *v15;
  v21 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v41 = v19;
  type metadata accessor for vImage.PixelBuffer(0, a12, *(_QWORD *)(*(_QWORD *)(a15 + 8) + 8), v22);
  v23 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (v21)
  {
    if (v23)
      v24 = v21 == v23;
    else
      v24 = 0;
    if (!v24)
    {
LABEL_9:
      v32 = a6;
      v40[0] = v20;
      swift_bridgeObjectRetain();
      vImage.PixelBuffer.size.getter(&v41);
      v26 = v41;
      v25 = v42;
      vImage.PixelBuffer.size.getter(v40);
      swift_bridgeObjectRelease();
      if (v26 == v40[0] && v25 == v40[1])
      {
        if (v18 >= 2)
          v27 = v17;
        else
          v27 = v17 & ~(v17 >> 63);
        if (v18 >= 2)
          v28 = v39;
        else
          v28 = v39 & ~(v39 >> 63);
        if ((v28 & v27 & 1) != 0)
        {
          v41 = v20;
          v41 = vImage.PixelBuffer<>.vImageBuffer.getter();
          v42 = v29;
          v43 = v30;
          v44 = v31;
          closure #1 in vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)((uint64_t)&v41, v19, v17, v39, v35, v18, a9, a10, v28, v27, a7, a8, a5, v32, a3, a4, *(_QWORD *)(a11 + 16), a12, a13,
            a14,
            a15);
          return;
        }
      }
      else
      {
        __break(1u);
      }
      __break(1u);
    }
    __break(1u);
  }
  if (v23)
    goto LABEL_9;
  __break(1u);
}

uint64_t vImage.PixelBuffer<>.applyMorphology(operation:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4;
  uint64_t result;
  uint64_t v7;
  uint64_t v8;
  unint64_t v9;
  unint64_t v10;
  uint64_t v11;
  uint64_t AssociatedTypeWitness;
  uint64_t AssociatedConformanceWitness;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;

  result = (*(uint64_t (**)(_QWORD, uint64_t))(a4 + 32))(*(_QWORD *)(a3 + 16), a4);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    v7 = result;
    if (result)
    {
      v8 = a1;
      v9 = 0;
      do
      {
        v10 = v9 + 1;
        v11 = v4;
        vImage.PixelBuffer<>.subscript.getter(v9, &v17);
        vImage.PixelBuffer<>.subscript.getter(v9, &v16);
        AssociatedTypeWitness = swift_getAssociatedTypeWitness();
        AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
        v15 = type metadata accessor for vImage.PixelBuffer(0, AssociatedTypeWitness, *(_QWORD *)(*(_QWORD *)(AssociatedConformanceWitness + 8) + 8), v14);
        vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(v8, &v16, (uint64_t)@nonobjc vImageErode_Planar8(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageDilate_Planar8(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageMin_Planar8(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageMax_Planar8(_:_:_:_:_:_:_:_:), 0, v15, AssociatedTypeWitness, MEMORY[0x1E0DEDE70], AssociatedConformanceWitness, AssociatedConformanceWitness);
        v4 = v11;
        swift_bridgeObjectRelease();
        result = swift_bridgeObjectRelease();
        v9 = v10;
      }
      while (v7 != v10);
    }
  }
  return result;
}

{
  uint64_t v4;
  uint64_t result;
  uint64_t v7;
  uint64_t v8;
  unint64_t v9;
  unint64_t v10;
  uint64_t v11;
  uint64_t AssociatedTypeWitness;
  uint64_t AssociatedConformanceWitness;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;

  result = (*(uint64_t (**)(_QWORD, uint64_t))(a4 + 32))(*(_QWORD *)(a3 + 16), a4);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    v7 = result;
    if (result)
    {
      v8 = a1;
      v9 = 0;
      do
      {
        v10 = v9 + 1;
        v11 = v4;
        vImage.PixelBuffer<>.subscript.getter(v9, &v17);
        vImage.PixelBuffer<>.subscript.getter(v9, &v16);
        AssociatedTypeWitness = swift_getAssociatedTypeWitness();
        AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
        v15 = type metadata accessor for vImage.PixelBuffer(0, AssociatedTypeWitness, *(_QWORD *)(*(_QWORD *)(AssociatedConformanceWitness + 8) + 8), v14);
        vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(v8, &v16, (uint64_t)@nonobjc vImageErode_PlanarF(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageDilate_PlanarF(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageMin_PlanarF(_:_:_:_:_:_:_:_:), 0, (uint64_t)@nonobjc vImageMax_PlanarF(_:_:_:_:_:_:_:_:), 0, v15, AssociatedTypeWitness, MEMORY[0x1E0DEB188], AssociatedConformanceWitness, AssociatedConformanceWitness);
        v4 = v11;
        swift_bridgeObjectRelease();
        result = swift_bridgeObjectRelease();
        v9 = v10;
      }
      while (v7 != v10);
    }
  }
  return result;
}

uint64_t vImage.MorphologyOperation.width.getter()
{
  unsigned __int8 *v0;

  if (v0[24] >= 2u)
    return *(_QWORD *)v0;
  else
    return *(_QWORD *)v0 & ~(*(uint64_t *)v0 >> 63);
}

uint64_t type metadata accessor for vImage.MorphologyOperation(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return __swift_instantiateGenericMetadata(a1, a2, a3, a4, (uint64_t)&nominal type descriptor for vImage.MorphologyOperation);
}

uint64_t vImage.MorphologyOperation.height.getter()
{
  uint64_t v0;

  if (*(unsigned __int8 *)(v0 + 24) >= 2u)
    return *(_QWORD *)(v0 + 8);
  else
    return *(_QWORD *)(v0 + 8) & ~(*(uint64_t *)(v0 + 8) >> 63);
}

__n128 closure #1 in vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, char a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16, uint64_t a17, uint64_t a18, uint64_t a19, uint64_t a20,uint64_t a21)
{
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  __n128 result;
  _QWORD v29[6];

  v29[4] = *MEMORY[0x1E0C80C00];
  v29[0] = a2;
  type metadata accessor for vImage.PixelBuffer(0, a18, *(_QWORD *)(*(_QWORD *)(a21 + 8) + 8), a4);
  v29[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v29[1] = v22;
  v29[2] = v23;
  v29[3] = v24;
  closure #1 in closure #1 in vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)((uint64_t)v29, a3, a4, a5, a6);
  return result;
}

void closure #1 in closure #1 in vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, char a5)
{
  __asm { BR              X14 }
}

uint64_t sub_1CAB42D34@<X0>(uint64_t a1@<X1>, uint64_t a2@<X5>, uint64_t a3@<X6>, uint64_t a4@<X7>, uint64_t (*a5)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, uint64_t, uint64_t, _QWORD)@<X8>)
{
  return a5(a4, a1, 0, 0, 0, a2, a3, 0);
}

uint64_t vImage.MorphologyOperation.structuringElement.getter@<X0>(uint64_t *a1@<X8>)
{
  uint64_t v1;
  unsigned int v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t result;

  v3 = *(unsigned __int8 *)(v1 + 24);
  if (v3 >= 2)
  {
    v5 = *(_QWORD *)(v1 + 8);
    v6 = *(_QWORD *)(v1 + 16);
    v4 = *(_QWORD *)v1;
    result = outlined copy of vImage.MorphologyOperation<A><A>(*(_QWORD *)v1, v5, v6, v3);
  }
  else
  {
    v4 = 0;
    v5 = 0;
    v6 = 0;
  }
  *a1 = v4;
  a1[1] = v5;
  a1[2] = v6;
  return result;
}

uint64_t outlined copy of vImage.MorphologyOperation<A><A>(uint64_t a1, uint64_t a2, uint64_t a3, char a4)
{
  uint64_t result;

  if ((a4 & 0xFE) == 2)
    return swift_bridgeObjectRetain();
  return result;
}

uint64_t destroy for vImage.MorphologyOperation(uint64_t a1)
{
  return outlined consume of vImage.MorphologyOperation<Float>(*(_QWORD *)a1, *(_QWORD *)(a1 + 8), *(_QWORD *)(a1 + 16), *(_BYTE *)(a1 + 24));
}

uint64_t initializeWithCopy for vImage.MorphologyOperation(uint64_t a1, uint64_t a2)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  char v6;

  v3 = *(_QWORD *)a2;
  v4 = *(_QWORD *)(a2 + 8);
  v5 = *(_QWORD *)(a2 + 16);
  v6 = *(_BYTE *)(a2 + 24);
  outlined copy of vImage.MorphologyOperation<A><A>(*(_QWORD *)a2, v4, v5, v6);
  *(_QWORD *)a1 = v3;
  *(_QWORD *)(a1 + 8) = v4;
  *(_QWORD *)(a1 + 16) = v5;
  *(_BYTE *)(a1 + 24) = v6;
  return a1;
}

uint64_t assignWithCopy for vImage.MorphologyOperation(uint64_t a1, uint64_t a2)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  char v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  char v10;

  v3 = *(_QWORD *)a2;
  v4 = *(_QWORD *)(a2 + 8);
  v5 = *(_QWORD *)(a2 + 16);
  v6 = *(_BYTE *)(a2 + 24);
  outlined copy of vImage.MorphologyOperation<A><A>(*(_QWORD *)a2, v4, v5, v6);
  v7 = *(_QWORD *)a1;
  v8 = *(_QWORD *)(a1 + 8);
  v9 = *(_QWORD *)(a1 + 16);
  *(_QWORD *)a1 = v3;
  *(_QWORD *)(a1 + 8) = v4;
  *(_QWORD *)(a1 + 16) = v5;
  v10 = *(_BYTE *)(a1 + 24);
  *(_BYTE *)(a1 + 24) = v6;
  outlined consume of vImage.MorphologyOperation<Float>(v7, v8, v9, v10);
  return a1;
}

__n128 __swift_memcpy25_8(uint64_t a1, uint64_t a2)
{
  __n128 result;

  result = *(__n128 *)a2;
  *(_OWORD *)(a1 + 9) = *(_OWORD *)(a2 + 9);
  *(__n128 *)a1 = result;
  return result;
}

uint64_t assignWithTake for vImage.MorphologyOperation(uint64_t a1, uint64_t a2)
{
  uint64_t v3;
  char v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  char v8;

  v3 = *(_QWORD *)(a2 + 16);
  v4 = *(_BYTE *)(a2 + 24);
  v5 = *(_QWORD *)a1;
  v7 = *(_QWORD *)(a1 + 8);
  v6 = *(_QWORD *)(a1 + 16);
  *(_OWORD *)a1 = *(_OWORD *)a2;
  *(_QWORD *)(a1 + 16) = v3;
  v8 = *(_BYTE *)(a1 + 24);
  *(_BYTE *)(a1 + 24) = v4;
  outlined consume of vImage.MorphologyOperation<Float>(v5, v7, v6, v8);
  return a1;
}

uint64_t getEnumTagSinglePayload for vImage.MorphologyOperation(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  int v4;

  if (!a2)
    return 0;
  if (a2 >= 0xFD && *(_BYTE *)(a1 + 25))
    return (*(_DWORD *)a1 + 253);
  v3 = *(unsigned __int8 *)(a1 + 24);
  if (v3 <= 3)
    v4 = -1;
  else
    v4 = v3 ^ 0xFF;
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for vImage.MorphologyOperation(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFC)
  {
    *(_QWORD *)(result + 8) = 0;
    *(_QWORD *)(result + 16) = 0;
    *(_BYTE *)(result + 24) = 0;
    *(_QWORD *)result = a2 - 253;
    if (a3 >= 0xFD)
      *(_BYTE *)(result + 25) = 1;
  }
  else
  {
    if (a3 >= 0xFD)
      *(_BYTE *)(result + 25) = 0;
    if (a2)
      *(_BYTE *)(result + 24) = -(char)a2;
  }
  return result;
}

uint64_t getEnumTag for vImage.MorphologyOperation(uint64_t a1)
{
  return *(unsigned __int8 *)(a1 + 24);
}

uint64_t destructiveInjectEnumTag for vImage.MorphologyOperation(uint64_t result, char a2)
{
  *(_BYTE *)(result + 24) = a2;
  return result;
}

uint64_t outlined consume of vImage.MorphologyOperation<Float>(uint64_t a1, uint64_t a2, uint64_t a3, char a4)
{
  uint64_t result;

  if ((a4 & 0xFE) == 2)
    return swift_bridgeObjectRelease();
  return result;
}

uint64_t specialized _ArrayBuffer._nonNative.getter(uint64_t result)
{
  if (result >= 0)
    return result & 0xFFFFFFFFFFFFFF8;
  return result;
}

unint64_t static BNNS.matrixMultiplicationWorkspaceSize(inputA:transposed:inputB:transposed:output:alpha:filterParameters:)(_OWORD *a1, char a2, _OWORD *a3, char a4, _OWORD *a5, int a6, uint64_t a7, uint64_t a8, float a9, uint64_t a10)
{
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  BOOL v25;
  BOOL v26;
  const BNNSFilterParameters *v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  unint64_t result;
  int v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  BNNSNDArrayDescriptor output;
  BNNSNDArrayDescriptor inputB;
  BNNSNDArrayDescriptor inputA;
  uint64_t v51;

  v51 = *MEMORY[0x1E0C80C00];
  if (a8 == 1)
  {
    v10 = a1[9];
    *(_OWORD *)&inputA.stride[7] = a1[8];
    *(_OWORD *)&inputA.data_type = v10;
    *(_OWORD *)&inputA.table_data_type = a1[10];
    v11 = a1[5];
    *(_OWORD *)&inputA.size[7] = a1[4];
    *(_OWORD *)&inputA.stride[1] = v11;
    v12 = a1[7];
    *(_OWORD *)&inputA.stride[3] = a1[6];
    *(_OWORD *)&inputA.stride[5] = v12;
    v13 = a1[1];
    *(_OWORD *)&inputA.flags = *a1;
    *(_OWORD *)&inputA.size[1] = v13;
    v14 = a1[3];
    *(_OWORD *)&inputA.size[3] = a1[2];
    *(_OWORD *)&inputA.size[5] = v14;
    v15 = a3[9];
    *(_OWORD *)&inputB.stride[7] = a3[8];
    *(_OWORD *)&inputB.data_type = v15;
    *(_OWORD *)&inputB.table_data_type = a3[10];
    v16 = a3[5];
    *(_OWORD *)&inputB.size[7] = a3[4];
    *(_OWORD *)&inputB.stride[1] = v16;
    v17 = a3[7];
    *(_OWORD *)&inputB.stride[3] = a3[6];
    *(_OWORD *)&inputB.stride[5] = v17;
    v18 = a3[1];
    *(_OWORD *)&inputB.flags = *a3;
    *(_OWORD *)&inputB.size[1] = v18;
    v19 = a3[3];
    *(_OWORD *)&inputB.size[3] = a3[2];
    *(_OWORD *)&inputB.size[5] = v19;
    v20 = a5[9];
    *(_OWORD *)&output.stride[7] = a5[8];
    *(_OWORD *)&output.data_type = v20;
    *(_OWORD *)&output.table_data_type = a5[10];
    v21 = a5[5];
    *(_OWORD *)&output.size[7] = a5[4];
    *(_OWORD *)&output.stride[1] = v21;
    v22 = a5[7];
    *(_OWORD *)&output.stride[3] = a5[6];
    *(_OWORD *)&output.stride[5] = v22;
    v23 = a5[1];
    *(_OWORD *)&output.flags = *a5;
    *(_OWORD *)&output.size[1] = v23;
    v24 = a5[3];
    *(_OWORD *)&output.size[3] = a5[2];
    *(_OWORD *)&output.size[5] = v24;
    v25 = a2 & 1;
    v26 = a4 & 1;
    v27 = 0;
  }
  else
  {
    v44 = a6;
    v45 = a7;
    v46 = a8;
    v47 = a10;
    v28 = a1[9];
    *(_OWORD *)&inputA.stride[7] = a1[8];
    *(_OWORD *)&inputA.data_type = v28;
    *(_OWORD *)&inputA.table_data_type = a1[10];
    v29 = a1[5];
    *(_OWORD *)&inputA.size[7] = a1[4];
    *(_OWORD *)&inputA.stride[1] = v29;
    v30 = a1[7];
    *(_OWORD *)&inputA.stride[3] = a1[6];
    *(_OWORD *)&inputA.stride[5] = v30;
    v31 = a1[1];
    *(_OWORD *)&inputA.flags = *a1;
    *(_OWORD *)&inputA.size[1] = v31;
    v32 = a1[3];
    *(_OWORD *)&inputA.size[3] = a1[2];
    *(_OWORD *)&inputA.size[5] = v32;
    v33 = a3[9];
    *(_OWORD *)&inputB.stride[7] = a3[8];
    *(_OWORD *)&inputB.data_type = v33;
    *(_OWORD *)&inputB.table_data_type = a3[10];
    v34 = a3[5];
    *(_OWORD *)&inputB.size[7] = a3[4];
    *(_OWORD *)&inputB.stride[1] = v34;
    v35 = a3[7];
    *(_OWORD *)&inputB.stride[3] = a3[6];
    *(_OWORD *)&inputB.stride[5] = v35;
    v36 = a3[1];
    *(_OWORD *)&inputB.flags = *a3;
    *(_OWORD *)&inputB.size[1] = v36;
    v37 = a3[3];
    *(_OWORD *)&inputB.size[3] = a3[2];
    *(_OWORD *)&inputB.size[5] = v37;
    v38 = a5[9];
    *(_OWORD *)&output.stride[7] = a5[8];
    *(_OWORD *)&output.data_type = v38;
    *(_OWORD *)&output.table_data_type = a5[10];
    v39 = a5[5];
    *(_OWORD *)&output.size[7] = a5[4];
    *(_OWORD *)&output.stride[1] = v39;
    v40 = a5[7];
    *(_OWORD *)&output.stride[3] = a5[6];
    *(_OWORD *)&output.stride[5] = v40;
    v41 = a5[1];
    *(_OWORD *)&output.flags = *a5;
    *(_OWORD *)&output.size[1] = v41;
    v42 = a5[3];
    *(_OWORD *)&output.size[3] = a5[2];
    *(_OWORD *)&output.size[5] = v42;
    v25 = a2 & 1;
    v26 = a4 & 1;
    v27 = (const BNNSFilterParameters *)&v44;
  }
  result = BNNSMatMulWorkspaceSize(v25, v26, a9, &inputA, &inputB, &output, v27);
  if (result > 0x7FFFFFFFFFFFFFFELL)
    return 0;
  return result;
}

uint64_t static BNNS.applyMatrixMultiplication(inputA:transposed:inputB:transposed:output:alpha:workspace:filterParameters:)(_OWORD *a1, char a2, _OWORD *a3, char a4, _OWORD *a5, void *a6, float a7, uint64_t a8, char a9, uint32_t a10, size_t a11, int (__cdecl *a12)(void **, size_t, size_t), void (__cdecl *a13)(void *))
{
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  uint64_t result;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  _BYTE *v44;
  BNNSFilterParameters v45;
  BNNSNDArrayDescriptor output;
  BNNSNDArrayDescriptor inputB;
  BNNSNDArrayDescriptor inputA;
  uint64_t v49;

  v49 = *MEMORY[0x1E0C80C00];
  if (a12 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    v13 = a1[9];
    *(_OWORD *)&inputA.stride[7] = a1[8];
    *(_OWORD *)&inputA.data_type = v13;
    *(_OWORD *)&inputA.table_data_type = a1[10];
    v14 = a1[5];
    *(_OWORD *)&inputA.size[7] = a1[4];
    *(_OWORD *)&inputA.stride[1] = v14;
    v15 = a1[7];
    *(_OWORD *)&inputA.stride[3] = a1[6];
    *(_OWORD *)&inputA.stride[5] = v15;
    v16 = a1[1];
    *(_OWORD *)&inputA.flags = *a1;
    *(_OWORD *)&inputA.size[1] = v16;
    v17 = a1[3];
    *(_OWORD *)&inputA.size[3] = a1[2];
    *(_OWORD *)&inputA.size[5] = v17;
    v18 = a3[9];
    *(_OWORD *)&inputB.stride[7] = a3[8];
    *(_OWORD *)&inputB.data_type = v18;
    *(_OWORD *)&inputB.table_data_type = a3[10];
    v19 = a3[5];
    *(_OWORD *)&inputB.size[7] = a3[4];
    *(_OWORD *)&inputB.stride[1] = v19;
    v20 = a3[7];
    *(_OWORD *)&inputB.stride[3] = a3[6];
    *(_OWORD *)&inputB.stride[5] = v20;
    v21 = a3[1];
    *(_OWORD *)&inputB.flags = *a3;
    *(_OWORD *)&inputB.size[1] = v21;
    v22 = a3[3];
    *(_OWORD *)&inputB.size[3] = a3[2];
    *(_OWORD *)&inputB.size[5] = v22;
    v23 = a5[9];
    *(_OWORD *)&output.stride[7] = a5[8];
    *(_OWORD *)&output.data_type = v23;
    *(_OWORD *)&output.table_data_type = a5[10];
    v24 = a5[5];
    *(_OWORD *)&output.size[7] = a5[4];
    *(_OWORD *)&output.stride[1] = v24;
    v25 = a5[7];
    *(_OWORD *)&output.stride[3] = a5[6];
    *(_OWORD *)&output.stride[5] = v25;
    v26 = a5[1];
    *(_OWORD *)&output.flags = *a5;
    *(_OWORD *)&output.size[1] = v26;
    v27 = a5[3];
    if ((a9 & 1) != 0)
      a6 = 0;
    *(_OWORD *)&output.size[3] = a5[2];
    *(_OWORD *)&output.size[5] = v27;
    result = BNNSMatMul(a2 & 1, a4 & 1, a7, &inputA, &inputB, &output, a6, 0);
    if (!(_DWORD)result)
      return result;
LABEL_9:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v44 = 0;
    return swift_willThrow();
  }
  v45.flags = a10;
  v45.n_threads = a11;
  v45.alloc_memory = a12;
  v45.free_memory = a13;
  v29 = a1[9];
  *(_OWORD *)&inputA.stride[7] = a1[8];
  *(_OWORD *)&inputA.data_type = v29;
  *(_OWORD *)&inputA.table_data_type = a1[10];
  v30 = a1[5];
  *(_OWORD *)&inputA.size[7] = a1[4];
  *(_OWORD *)&inputA.stride[1] = v30;
  v31 = a1[7];
  *(_OWORD *)&inputA.stride[3] = a1[6];
  *(_OWORD *)&inputA.stride[5] = v31;
  v32 = a1[1];
  *(_OWORD *)&inputA.flags = *a1;
  *(_OWORD *)&inputA.size[1] = v32;
  v33 = a1[3];
  *(_OWORD *)&inputA.size[3] = a1[2];
  *(_OWORD *)&inputA.size[5] = v33;
  v34 = a3[9];
  *(_OWORD *)&inputB.stride[7] = a3[8];
  *(_OWORD *)&inputB.data_type = v34;
  *(_OWORD *)&inputB.table_data_type = a3[10];
  v35 = a3[5];
  *(_OWORD *)&inputB.size[7] = a3[4];
  *(_OWORD *)&inputB.stride[1] = v35;
  v36 = a3[7];
  *(_OWORD *)&inputB.stride[3] = a3[6];
  *(_OWORD *)&inputB.stride[5] = v36;
  v37 = a3[1];
  *(_OWORD *)&inputB.flags = *a3;
  *(_OWORD *)&inputB.size[1] = v37;
  v38 = a3[3];
  *(_OWORD *)&inputB.size[3] = a3[2];
  *(_OWORD *)&inputB.size[5] = v38;
  v39 = a5[9];
  *(_OWORD *)&output.stride[7] = a5[8];
  *(_OWORD *)&output.data_type = v39;
  *(_OWORD *)&output.table_data_type = a5[10];
  v40 = a5[5];
  *(_OWORD *)&output.size[7] = a5[4];
  *(_OWORD *)&output.stride[1] = v40;
  v41 = a5[7];
  *(_OWORD *)&output.stride[3] = a5[6];
  *(_OWORD *)&output.stride[5] = v41;
  v42 = a5[1];
  *(_OWORD *)&output.flags = *a5;
  *(_OWORD *)&output.size[1] = v42;
  v43 = a5[3];
  if ((a9 & 1) != 0)
    a6 = 0;
  *(_OWORD *)&output.size[3] = a5[2];
  *(_OWORD *)&output.size[5] = v43;
  result = BNNSMatMul(a2 & 1, a4 & 1, a7, &inputA, &inputB, &output, a6, &v45);
  if ((_DWORD)result)
    goto LABEL_9;
  return result;
}

double BNNS.FusedQuantizationParameters.layerParameters(input:output:)@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, uint64_t *a3@<X8>)
{
  double result;

  *(_QWORD *)&result = BNNS.FusedQuantizationParameters.layerParameters(input:output:)(a1, a2, 0, a3).n128_u64[0];
  return result;
}

uint64_t BNNS.FusedQuantizationParameters.axis.getter()
{
  uint64_t v0;

  return *(_QWORD *)v0;
}

uint64_t BNNS.FusedQuantizationParameters.axis.setter(uint64_t result, char a2)
{
  uint64_t v2;

  *(_QWORD *)v2 = result;
  *(_BYTE *)(v2 + 8) = a2 & 1;
  return result;
}

uint64_t (*BNNS.FusedQuantizationParameters.axis.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedQuantizationParameters.scale.setter(uint64_t a1)
{
  uint64_t v1;

  return outlined init with take of BNNSNDArrayDescriptor?(a1, v1 + 16);
}

uint64_t (*BNNS.FusedQuantizationParameters.scale.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedQuantizationParameters.bias.setter(uint64_t a1)
{
  uint64_t v1;

  return outlined init with take of BNNSNDArrayDescriptor?(a1, v1 + 200);
}

uint64_t (*BNNS.FusedQuantizationParameters.bias.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

double protocol witness for FusableLayerParametersWrapper.layerParameters(input:output:) in conformance BNNS.FusedQuantizationParameters@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, uint64_t *a3@<X8>)
{
  double result;

  *(_QWORD *)&result = BNNS.FusedQuantizationParameters.layerParameters(input:output:)(a1, a2, 0, a3).n128_u64[0];
  return result;
}

uint64_t protocol witness for FusableLayerParametersWrapper.filterType.getter in conformance BNNS.FusedQuantizationParameters()
{
  return 7;
}

double BNNS.FusedDequantizationParameters.layerParameters(input:output:)@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, uint64_t *a3@<X8>)
{
  double result;

  *(_QWORD *)&result = BNNS.FusedQuantizationParameters.layerParameters(input:output:)(a1, a2, 1, a3).n128_u64[0];
  return result;
}

__n128 BNNS.FusedQuantizationParameters.layerParameters(input:output:)@<Q0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, int a3@<W2>, uint64_t *a4@<X8>)
{
  uint64_t v4;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  int v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  __n128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  int v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __n128 result;
  int v39;
  int v40;
  __n128 v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;
  uint64_t v56;
  uint64_t v57;
  uint64_t v58;
  uint64_t v59;
  uint64_t v60;
  uint64_t v61;
  uint64_t v62;
  uint64_t v63;
  uint64_t v64;
  uint64_t v65;
  uint64_t v66;
  uint64_t v67;
  uint64_t v68;
  int v69;
  int v70;
  uint64_t v71;
  uint64_t v72;
  uint64_t v73;
  uint64_t v74;
  uint64_t v75;
  uint64_t v76;
  uint64_t v77;
  _BYTE v79[180];
  _BYTE v80[184];
  _BYTE v81[184];
  _BYTE v82[184];
  _BYTE v83[192];

  v8 = *(_QWORD *)v4;
  v69 = *(unsigned __int8 *)(v4 + 8);
  outlined init with take of BNNSNDArrayDescriptor?(v4 + 16, (uint64_t)v82);
  outlined init with take of BNNSNDArrayDescriptor?(v4 + 200, (uint64_t)v83);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v82, (uint64_t)v81);
  v9 = 0;
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v81) == 1)
  {
    v66 = 0;
    v67 = 0;
    v64 = 0;
    v65 = 0;
    v62 = 0;
    v63 = 0;
    v60 = 0;
    v61 = 0;
    v58 = 0;
    v59 = 0;
    v56 = 0;
    v57 = 0;
    v75 = 0;
    v76 = 0;
    v73 = 0;
    v74 = 0;
    v71 = 0;
    v72 = 0;
    v70 = 0;
    v77 = 0;
    v10 = 0;
    v68 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v82, (uint64_t)v79);
    v77 = *(_QWORD *)v79;
    v66 = *(_QWORD *)&v79[16];
    v67 = *(_QWORD *)&v79[8];
    v64 = *(_QWORD *)&v79[32];
    v65 = *(_QWORD *)&v79[24];
    v62 = *(_QWORD *)&v79[48];
    v63 = *(_QWORD *)&v79[40];
    v60 = *(_QWORD *)&v79[64];
    v61 = *(_QWORD *)&v79[56];
    v58 = *(_QWORD *)&v79[80];
    v59 = *(_QWORD *)&v79[72];
    v56 = *(_QWORD *)&v79[96];
    v57 = *(_QWORD *)&v79[88];
    v75 = *(_QWORD *)&v79[112];
    v76 = *(_QWORD *)&v79[104];
    v73 = *(_QWORD *)&v79[128];
    v74 = *(_QWORD *)&v79[120];
    v71 = *(_QWORD *)&v79[152];
    v68 = *(_QWORD *)&v79[144];
    v72 = *(_QWORD *)&v79[136];
    v10 = *(_QWORD *)&v79[164];
    v70 = *(_DWORD *)&v79[160];
    v40 = *(_DWORD *)&v79[172];
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v83, (uint64_t)v80);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v80) == 1)
  {
    v11 = 0;
    v12 = 0;
    v13 = 0;
    v14 = 0;
    v15 = 0;
    v16 = 0;
    v42 = 0;
    v43 = 0;
    v44 = 0;
    v45 = 0;
    v46 = 0;
    v47 = 0;
    v48 = 0;
    v49 = 0;
    v50 = 0;
    v51 = 0;
    v52 = 0;
    v53 = 0;
    v55 = 0;
    v54 = 0;
    v41 = 0u;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v83, (uint64_t)v79);
    v55 = *(_QWORD *)&v79[8];
    v54 = *(_QWORD *)v79;
    v52 = *(_QWORD *)&v79[24];
    v53 = *(_QWORD *)&v79[16];
    v50 = *(_QWORD *)&v79[40];
    v51 = *(_QWORD *)&v79[32];
    v48 = *(_QWORD *)&v79[56];
    v49 = *(_QWORD *)&v79[48];
    v46 = *(_QWORD *)&v79[72];
    v47 = *(_QWORD *)&v79[64];
    v44 = *(_QWORD *)&v79[88];
    v45 = *(_QWORD *)&v79[80];
    v16 = *(_QWORD *)&v79[112];
    v42 = *(_QWORD *)&v79[104];
    v43 = *(_QWORD *)&v79[96];
    v15 = *(_QWORD *)&v79[120];
    v13 = *(_QWORD *)&v79[136];
    v14 = *(_QWORD *)&v79[128];
    v17.n128_u64[0] = *(_QWORD *)&v79[144];
    v41 = v17;
    v12 = *(_QWORD *)&v79[152];
    v9 = *(_QWORD *)&v79[164];
    v11 = *(_DWORD *)&v79[160];
    v39 = *(_DWORD *)&v79[172];
  }
  v18 = a1[6];
  *(_OWORD *)&v79[116] = a1[7];
  v19 = a1[9];
  *(_OWORD *)&v79[132] = a1[8];
  *(_OWORD *)&v79[148] = v19;
  *(_OWORD *)&v79[164] = a1[10];
  v20 = a1[2];
  *(_OWORD *)&v79[52] = a1[3];
  v21 = a1[5];
  *(_OWORD *)&v79[68] = a1[4];
  *(_OWORD *)&v79[84] = v21;
  *(_OWORD *)&v79[100] = v18;
  v22 = a1[1];
  *(_OWORD *)&v79[4] = *a1;
  *(_OWORD *)&v79[20] = v22;
  v23 = v69;
  if ((unint64_t)(v8 - 65) < 0xFFFFFFFFFFFFFF7FLL)
    v23 = 1;
  if (v8 < 0)
    v23 = 1;
  v24 = 1 << v8;
  if ((unint64_t)v8 >= 0x40)
    v24 = 0;
  if (v23)
    v25 = 0;
  else
    v25 = v24;
  *(_OWORD *)&v79[36] = v20;
  type metadata accessor for BNNSLayerParametersQuantization(0);
  a4[3] = v26;
  a4[4] = (uint64_t)&protocol witness table for BNNSLayerParametersQuantization;
  v27 = swift_allocObject();
  *a4 = v27;
  *(_QWORD *)(v27 + 16) = v25;
  *(_DWORD *)(v27 + 24) = a3;
  *(_QWORD *)(v27 + 392) = v67;
  *(_QWORD *)(v27 + 400) = v66;
  *(_QWORD *)(v27 + 408) = v65;
  *(_QWORD *)(v27 + 416) = v64;
  *(_QWORD *)(v27 + 424) = v63;
  *(_QWORD *)(v27 + 432) = v62;
  *(_QWORD *)(v27 + 440) = v61;
  *(_QWORD *)(v27 + 448) = v60;
  *(_QWORD *)(v27 + 456) = v59;
  *(_QWORD *)(v27 + 464) = v58;
  *(_QWORD *)(v27 + 472) = v57;
  *(_QWORD *)(v27 + 480) = v56;
  *(_DWORD *)(v27 + 204) = *(_DWORD *)&v79[176];
  v28 = *(_OWORD *)&v79[144];
  *(_OWORD *)(v27 + 156) = *(_OWORD *)&v79[128];
  *(_OWORD *)(v27 + 172) = v28;
  *(_OWORD *)(v27 + 188) = *(_OWORD *)&v79[160];
  v29 = *(_OWORD *)&v79[80];
  *(_OWORD *)(v27 + 92) = *(_OWORD *)&v79[64];
  *(_OWORD *)(v27 + 108) = v29;
  v30 = *(_OWORD *)&v79[112];
  *(_OWORD *)(v27 + 124) = *(_OWORD *)&v79[96];
  *(_OWORD *)(v27 + 140) = v30;
  v31 = *(_OWORD *)&v79[16];
  *(_OWORD *)(v27 + 28) = *(_OWORD *)v79;
  *(_OWORD *)(v27 + 44) = v31;
  v32 = *(_OWORD *)&v79[48];
  *(_OWORD *)(v27 + 60) = *(_OWORD *)&v79[32];
  *(_OWORD *)(v27 + 76) = v32;
  v33 = a2[9];
  *(_OWORD *)(v27 + 336) = a2[8];
  *(_OWORD *)(v27 + 352) = v33;
  *(_OWORD *)(v27 + 368) = a2[10];
  v34 = a2[5];
  *(_OWORD *)(v27 + 272) = a2[4];
  *(_OWORD *)(v27 + 288) = v34;
  v35 = a2[7];
  *(_OWORD *)(v27 + 304) = a2[6];
  *(_OWORD *)(v27 + 320) = v35;
  v36 = a2[1];
  *(_OWORD *)(v27 + 208) = *a2;
  *(_OWORD *)(v27 + 224) = v36;
  v37 = a2[3];
  *(_OWORD *)(v27 + 240) = a2[2];
  *(_OWORD *)(v27 + 256) = v37;
  *(_QWORD *)(v27 + 384) = v77;
  *(_QWORD *)(v27 + 488) = v76;
  *(_QWORD *)(v27 + 496) = v75;
  *(_QWORD *)(v27 + 504) = v74;
  *(_QWORD *)(v27 + 512) = v73;
  *(_QWORD *)(v27 + 520) = v72;
  *(_QWORD *)(v27 + 528) = v68;
  *(_QWORD *)(v27 + 536) = v71;
  *(_DWORD *)(v27 + 544) = v70;
  *(_QWORD *)(v27 + 548) = v10;
  *(_DWORD *)(v27 + 556) = v40;
  *(_QWORD *)(v27 + 560) = v54;
  *(_QWORD *)(v27 + 568) = v55;
  *(_QWORD *)(v27 + 576) = v53;
  *(_QWORD *)(v27 + 584) = v52;
  *(_QWORD *)(v27 + 592) = v51;
  *(_QWORD *)(v27 + 600) = v50;
  *(_QWORD *)(v27 + 608) = v49;
  *(_QWORD *)(v27 + 616) = v48;
  *(_QWORD *)(v27 + 624) = v47;
  *(_QWORD *)(v27 + 632) = v46;
  *(_QWORD *)(v27 + 640) = v45;
  *(_QWORD *)(v27 + 648) = v44;
  *(_QWORD *)(v27 + 656) = v43;
  *(_QWORD *)(v27 + 664) = v42;
  *(_QWORD *)(v27 + 672) = v16;
  *(_QWORD *)(v27 + 680) = v15;
  *(_QWORD *)(v27 + 688) = v14;
  *(_QWORD *)(v27 + 696) = v13;
  result = v41;
  *(_QWORD *)(v27 + 704) = v41.n128_u64[0];
  *(_QWORD *)(v27 + 712) = v12;
  *(_DWORD *)(v27 + 720) = v11;
  *(_QWORD *)(v27 + 724) = v9;
  *(_DWORD *)(v27 + 732) = v39;
  return result;
}

void *BNNS.FusedQuantizationParameters.init(scale:bias:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, void *a3@<X8>)
{
  _BYTE v6[184];
  _BYTE v7[184];
  _BYTE v8[184];
  _BYTE v9[184];
  _QWORD __src[48];

  outlined init with take of BNNSNDArrayDescriptor?(a2, (uint64_t)v7);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v7, (uint64_t)v8);
  outlined init with take of BNNSNDArrayDescriptor?(a1, (uint64_t)v6);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v6, (uint64_t)v9);
  __src[0] = 0;
  LOBYTE(__src[1]) = 1;
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v9, (uint64_t)&__src[2]);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v8, (uint64_t)&__src[25]);
  return memcpy(a3, __src, 0x179uLL);
}

uint64_t (*BNNS.FusedDequantizationParameters.axis.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedQuantizationParameters.scale.getter@<X0>(uint64_t a1@<X8>)
{
  uint64_t v1;
  _BYTE v4[184];

  outlined init with take of BNNSNDArrayDescriptor?(v1 + 16, (uint64_t)v4);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v4, a1);
}

uint64_t (*BNNS.FusedDequantizationParameters.scale.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedQuantizationParameters.bias.getter@<X0>(uint64_t a1@<X8>)
{
  uint64_t v1;
  _BYTE v4[184];

  outlined init with take of BNNSNDArrayDescriptor?(v1 + 200, (uint64_t)v4);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v4, a1);
}

uint64_t (*BNNS.FusedDequantizationParameters.bias.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

double protocol witness for FusableLayerParametersWrapper.layerParameters(input:output:) in conformance BNNS.FusedDequantizationParameters@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, uint64_t *a3@<X8>)
{
  double result;

  *(_QWORD *)&result = BNNS.FusedQuantizationParameters.layerParameters(input:output:)(a1, a2, 1, a3).n128_u64[0];
  return result;
}

void *__swift_memcpy377_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x179uLL);
}

uint64_t getEnumTagSinglePayload for BNNS.FusedQuantizationParameters(uint64_t a1, int a2)
{
  if (a2 && *(_BYTE *)(a1 + 377))
    return (*(_DWORD *)a1 + 1);
  else
    return 0;
}

uint64_t storeEnumTagSinglePayload for BNNS.FusedQuantizationParameters(uint64_t result, int a2, int a3)
{
  char v3;

  if (a2)
  {
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_BYTE *)(result + 376) = 0;
    *(_OWORD *)(result + 360) = 0u;
    *(_OWORD *)(result + 344) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(_QWORD *)result = (a2 - 1);
    if (!a3)
      return result;
    v3 = 1;
  }
  else
  {
    if (!a3)
      return result;
    v3 = 0;
  }
  *(_BYTE *)(result + 377) = v3;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.FusedQuantizationParameters()
{
  return &type metadata for BNNS.FusedQuantizationParameters;
}

ValueMetadata *type metadata accessor for BNNS.FusedDequantizationParameters()
{
  return &type metadata for BNNS.FusedDequantizationParameters;
}

uint64_t sub_1CAB43EE8()
{
  return swift_deallocObject();
}

uint64_t vImage.PixelBuffer<>.histogram()()
{
  uint64_t *v0;
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  __int128 v6;
  uint64_t inited;
  vImage_Buffer src;
  uint64_t v10;

  v10 = *MEMORY[0x1E0C80C00];
  v1 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(_QWORD *)(v1 + 16) = 256;
  bzero((void *)(v1 + 32), 0x800uLL);
  v2 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(_QWORD *)(v2 + 16) = 256;
  bzero((void *)(v2 + 32), 0x800uLL);
  v3 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(_QWORD *)(v3 + 16) = 256;
  bzero((void *)(v3 + 32), 0x800uLL);
  v4 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(_QWORD *)(v4 + 16) = 256;
  bzero((void *)(v4 + 32), 0x800uLL);
  v5 = *v0;
  if (!*(_QWORD *)(*v0 + 16))
    __break(1u);
  v6 = *(_OWORD *)(v5 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v5 + 32);
  *(_OWORD *)&src.width = v6;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<UInt>?>);
  inited = swift_initStackObject();
  *(_QWORD *)(inited + 32) = v1 + 32;
  *(_QWORD *)(inited + 40) = v2 + 32;
  *(_QWORD *)(inited + 48) = v3 + 32;
  *(_QWORD *)(inited + 56) = v4 + 32;
  vImageHistogramCalculation_ARGB8888(&src, (vImagePixelCount **)(inited + 32), 0);
  swift_setDeallocating();
  return v1;
}

uint64_t vImage.PixelBuffer<>.histogram(binCount:)@<X0>(unint64_t a1@<X0>, unint64_t *a2@<X8>)
{
  uint64_t *v2;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  __int128 v10;
  uint64_t inited;
  uint64_t result;
  vImage_Buffer src;
  uint64_t v14;

  v14 = *MEMORY[0x1E0C80C00];
  if ((a1 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
  }
  if (a1)
  {
    v5 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v5 + 16) = a1;
    bzero((void *)(v5 + 32), 8 * a1);
    v6 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v6 + 16) = a1;
    bzero((void *)(v6 + 32), 8 * a1);
    v7 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v7 + 16) = a1;
    bzero((void *)(v7 + 32), 8 * a1);
    v8 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v8 + 16) = a1;
    bzero((void *)(v8 + 32), 8 * a1);
  }
  else
  {
    v8 = MEMORY[0x1E0DEE9D8];
    v7 = MEMORY[0x1E0DEE9D8];
    v6 = MEMORY[0x1E0DEE9D8];
    v5 = MEMORY[0x1E0DEE9D8];
  }
  v9 = *v2;
  if (!*(_QWORD *)(*v2 + 16))
    goto LABEL_9;
  v10 = *(_OWORD *)(v9 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v9 + 32);
  *(_OWORD *)&src.width = v10;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<UInt>?>);
  inited = swift_initStackObject();
  *(_QWORD *)(inited + 32) = v5 + 32;
  *(_QWORD *)(inited + 40) = v6 + 32;
  *(_QWORD *)(inited + 48) = v7 + 32;
  *(_QWORD *)(inited + 56) = v8 + 32;
  if (HIDWORD(a1))
    goto LABEL_10;
  vImageHistogramCalculation_ARGBFFFF(&src, (vImagePixelCount **)(inited + 32), a1, 0.0, 1.0, 0);
  result = swift_setDeallocating();
  *a2 = a1;
  a2[1] = v5;
  a2[2] = v6;
  a2[3] = v7;
  a2[4] = v8;
  return result;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(unint64_t a1, unint64_t a2, unint64_t a3, unint64_t a4, unint64_t a5, uint64_t a6)
{
  uint64_t v6;
  _QWORD *v7;
  vImagePixelCount v8;
  vImagePixelCount v9;
  _QWORD *v10;
  uint64_t v11;
  uint64_t v12;
  void *v13;
  size_t v14;
  void *v15;
  size_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  unint64_t v22[6];
  vImage_Buffer v23;
  vImage_Buffer v24;
  uint64_t v25;

  v25 = *MEMORY[0x1E0C80C00];
  v7 = *(_QWORD **)v6;
  if (!*(_QWORD *)(*(_QWORD *)v6 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v8 = v7[6];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v9 = v7[5];
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v8)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v9)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v10 = *(_QWORD **)a6;
  if (!*(_QWORD *)(*(_QWORD *)a6 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v11)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v12)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v8 != v11)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v9 != v12)
    goto LABEL_25;
  v13 = (void *)v7[4];
  v14 = v7[7];
  v24.data = v13;
  v24.height = v9;
  v24.width = v8;
  v24.rowBytes = v14;
  v15 = (void *)v10[4];
  v16 = v10[7];
  v23.data = v15;
  v23.height = v9;
  v23.width = v8;
  v23.rowBytes = v16;
  v17 = *(_QWORD *)(a2 + 16);
  v18 = *(_QWORD *)(a3 + 16);
  v19 = *(_QWORD *)(a4 + 16);
  v20 = *(_QWORD *)(a5 + 16);
  v22[0] = a1;
  v22[1] = a2;
  v22[2] = a3;
  v22[3] = a4;
  v22[4] = a5;
  return closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in vImage.PixelBuffer<>.specifyHistogram(_:destination:)(a5 + 32, v20, a2 + 32, v17, a3 + 32, v18, a4 + 32, v19, &v24, &v23, v22);
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in vImage.PixelBuffer<>.specifyHistogram(_:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, const vImage_Buffer *a9, const vImage_Buffer *a10, unint64_t *a11)
{
  unint64_t v15;
  uint64_t result;

  v15 = *a11;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<UInt>?>);
  result = swift_initStackObject();
  *(_QWORD *)(result + 32) = a3;
  *(_QWORD *)(result + 40) = a5;
  *(_QWORD *)(result + 48) = a7;
  *(_QWORD *)(result + 56) = a1;
  if ((v15 & 0x8000000000000000) != 0)
  {
    __break(1u);
  }
  else if (!HIDWORD(v15))
  {
    vImageHistogramSpecification_ARGBFFFF(a9, a10, 0, (const vImagePixelCount **)(result + 32), v15, 0.0, 1.0, 0);
    return swift_setDeallocating();
  }
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v5;
  _QWORD *v6;
  vImagePixelCount v7;
  vImagePixelCount v8;
  _QWORD *v9;
  uint64_t v10;
  uint64_t v11;
  void *v12;
  size_t v13;
  void *v14;
  size_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t inited;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v24;

  v24 = *MEMORY[0x1E0C80C00];
  v6 = *(_QWORD **)v5;
  if (!*(_QWORD *)(*(_QWORD *)v5 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  v7 = v6[6];
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v8 = v6[5];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v7)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v8)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v9 = *(_QWORD **)a5;
  if (!*(_QWORD *)(*(_QWORD *)a5 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v10 = v9[6];
  if (v10 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v11 = v9[5];
  if (v11 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v10)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v11)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v7 != v10)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v8 != v11)
    goto LABEL_25;
  v12 = (void *)v6[4];
  v13 = v6[7];
  src.data = v12;
  src.height = v8;
  src.width = v7;
  src.rowBytes = v13;
  v14 = (void *)v9[4];
  v15 = v9[7];
  dest.data = v14;
  dest.height = v8;
  v16 = a1 + 32;
  v17 = a2 + 32;
  v18 = a3 + 32;
  v19 = a4 + 32;
  dest.width = v7;
  dest.rowBytes = v15;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<UInt>?>);
  inited = swift_initStackObject();
  *(_QWORD *)(inited + 32) = v16;
  *(_QWORD *)(inited + 40) = v17;
  *(_QWORD *)(inited + 48) = v18;
  *(_QWORD *)(inited + 56) = v19;
  vImageHistogramSpecification_ARGB8888(&src, &dest, (const vImagePixelCount **)(inited + 32), 0);
  return swift_setDeallocating();
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1)
{
  return vImage.PixelBuffer<>.convert(to:)(a1, MEMORY[0x1E0C8D250]);
}

{
  return vImage.PixelBuffer<>.convert(to:)(a1, MEMORY[0x1E0C8D240]);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return vImage.PixelBuffer<>.equalizeHistogram(destination:)(a1, a2, a3, MEMORY[0x1E0C8D250]);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t a2)
{
  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, MEMORY[0x1E0C8D258]);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, a3, a4, MEMORY[0x1E0C8D258]);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t *a2)
{
  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, MEMORY[0x1E0C8D248]);
}

uint64_t vImage.PixelBuffer<>.contrastStretch(destination:)(uint64_t a1)
{
  return vImage.PixelBuffer<>.convert(to:)(a1, MEMORY[0x1E0C8CD18]);
}

{
  return vImage.PixelBuffer<>.convert(to:)(a1, MEMORY[0x1E0C8CD08]);
}

uint64_t vImage.PixelBuffer<>.contrastStretch(destination:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return vImage.PixelBuffer<>.equalizeHistogram(destination:)(a1, a2, a3, MEMORY[0x1E0C8CD18]);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(uint64_t, uint64_t, _QWORD))
{
  uint64_t v4;
  uint64_t v7;
  uint64_t result;
  uint64_t v9;
  uint64_t v10;
  unint64_t v11;
  uint64_t v12;
  uint64_t v13;
  __int128 v15;
  __int128 v16;
  _OWORD v17[2];
  uint64_t v18;

  v18 = *MEMORY[0x1E0C80C00];
  v7 = *(_QWORD *)(a2 + 16);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 32))(v7, a3);
  if (result < 0)
    goto LABEL_8;
  v9 = result;
  if (result)
  {
    v10 = 0;
    v11 = 0;
    while (1)
    {
      v12 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v11 >= *(_QWORD *)(v12 + 16))
        break;
      v15 = *(_OWORD *)(v12 + v10 + 48);
      v16 = *(_OWORD *)(v12 + v10 + 32);
      swift_bridgeObjectRelease();
      v17[0] = v16;
      v17[1] = v15;
      result = closure #1 in vImage.PixelBuffer<>.equalizeHistogram(destination:)((uint64_t)v17, a1, v11, v4, v7, a3, v13, a4);
      v10 += 32;
      if (v9 == ++v11)
        return result;
    }
    __break(1u);
LABEL_8:
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1, uint64_t a2, unint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, _QWORD))
{
  uint64_t v15;
  unint64_t v16;
  uint64_t v17;
  __int128 v19;
  __int128 v20;
  _OWORD v21[2];
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  type metadata accessor for vImage.PixelBuffer(0, a5, *(_QWORD *)(a6 + 8), a4);
  v15 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  if (*(_QWORD *)(v15 + 16) <= a3)
    goto LABEL_5;
  v16 = v15 + 32 * a3;
  v19 = *(_OWORD *)(v16 + 48);
  v20 = *(_OWORD *)(v16 + 32);
  swift_bridgeObjectRelease();
  v21[0] = v20;
  v21[1] = v19;
  return closure #1 in closure #1 in vImage.PixelBuffer<>.equalizeHistogram(destination:)((uint64_t)v21, a4, a3, a2, a1, a5, a6, v17, a8);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1, uint64_t a2, unint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t (*a9)(uint64_t, uint64_t, _QWORD))
{
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t result;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;

  type metadata accessor for vImage.PixelBuffer(0, a6, *(_QWORD *)(a7 + 8), a4);
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  v12 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_10;
  }
  if (*(_QWORD *)(v12 + 16) <= a3)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  v19 = a1;
  v13 = *(_QWORD *)(v12 + 32 * a3 + 48);
  swift_bridgeObjectRelease();
  v14 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(_QWORD *)(v14 + 16) <= a3)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  v15 = *(_QWORD *)(v14 + 32 * a3 + 48);
  swift_bridgeObjectRelease();
  if (v13 != v15)
  {
LABEL_12:
    swift_bridgeObjectRelease();
    result = swift_bridgeObjectRelease();
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  result = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(_QWORD *)(result + 16) <= a3)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  v17 = *(_QWORD *)(result + 32 * a3 + 40);
  swift_bridgeObjectRelease();
  result = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(_QWORD *)(result + 16) > a3)
  {
    v18 = *(_QWORD *)(result + 32 * a3 + 40);
    swift_bridgeObjectRelease_n();
    swift_bridgeObjectRelease_n();
    result = swift_bridgeObjectRelease();
    if (v17 == v18)
      return a9(a5, v19, 0);
    goto LABEL_13;
  }
LABEL_15:
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.contrastStretch(binCount:destination:)(unint64_t a1, uint64_t a2)
{
  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, MEMORY[0x1E0C8CD20]);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t a2, uint64_t (*a3)(_QWORD *, _QWORD *, _QWORD, unint64_t, _QWORD, float, float))
{
  uint64_t v3;
  _QWORD *v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  _QWORD v15[4];
  _QWORD v16[5];

  v16[4] = *MEMORY[0x1E0C80C00];
  v4 = *(_QWORD **)v3;
  if (!*(_QWORD *)(*(_QWORD *)v3 + 16))
  {
    __break(1u);
    goto LABEL_17;
  }
  v5 = v4[6];
  if (v5 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v6 = v4[5];
  if (v6 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v5)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v6)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v7 = *(_QWORD **)a2;
  if (!*(_QWORD *)(*(_QWORD *)a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v8)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v9)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v5 != v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v6 != v9)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v10 = v4[4];
  v11 = v4[7];
  v16[0] = v10;
  v16[1] = v6;
  v16[2] = v5;
  v16[3] = v11;
  v12 = v7[4];
  v13 = v7[7];
  v15[0] = v12;
  v15[1] = v6;
  v15[2] = v5;
  v15[3] = v13;
  if ((a1 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
LABEL_29:
    __break(1u);
  }
  if (HIDWORD(a1))
    goto LABEL_29;
  return a3(v16, v15, 0, a1, 0, 0.0, 1.0);
}

uint64_t vImage.PixelBuffer<>.contrastStretch(binCount:destination:)(unint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, a3, a4, MEMORY[0x1E0C8CD20]);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(uint64_t, uint64_t, _QWORD, unint64_t, _QWORD, float, float))
{
  uint64_t v5;
  uint64_t v8;
  uint64_t result;
  uint64_t v10;
  uint64_t v11;
  unint64_t v12;
  uint64_t v13;
  uint64_t v14;
  __int128 v17;
  __int128 v18;
  _OWORD v19[2];
  uint64_t v20;

  v20 = *MEMORY[0x1E0C80C00];
  v8 = *(_QWORD *)(a3 + 16);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 32))(v8, a4);
  if (result < 0)
    goto LABEL_8;
  v10 = result;
  if (result)
  {
    v11 = 0;
    v12 = 0;
    while (1)
    {
      v13 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v12 >= *(_QWORD *)(v13 + 16))
        break;
      v17 = *(_OWORD *)(v13 + v11 + 48);
      v18 = *(_OWORD *)(v13 + v11 + 32);
      swift_bridgeObjectRelease();
      v19[0] = v18;
      v19[1] = v17;
      result = closure #1 in vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)((uint64_t)v19, a2, v12, v5, a1, v8, a4, v14, a5);
      v11 += 32;
      if (v10 == ++v12)
        return result;
    }
    __break(1u);
LABEL_8:
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(uint64_t a1, uint64_t a2, unint64_t a3, uint64_t a4, unint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t (*a9)(uint64_t, uint64_t, _QWORD, unint64_t, _QWORD, float, float))
{
  uint64_t v16;
  unint64_t v17;
  uint64_t v19;
  __int128 v20;
  __int128 v21;
  _OWORD v22[2];
  uint64_t v23;

  v23 = *MEMORY[0x1E0C80C00];
  type metadata accessor for vImage.PixelBuffer(0, a6, *(_QWORD *)(a7 + 8), a4);
  v16 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  if (*(_QWORD *)(v16 + 16) <= a3)
    goto LABEL_5;
  v17 = v16 + 32 * a3;
  v20 = *(_OWORD *)(v17 + 48);
  v21 = *(_OWORD *)(v17 + 32);
  swift_bridgeObjectRelease();
  v22[0] = v21;
  v22[1] = v20;
  return closure #1 in closure #1 in vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)((uint64_t)v22, a4, a3, a2, a1, a5, a6, a7, v19, a9);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(uint64_t a1, uint64_t a2, unint64_t a3, uint64_t a4, uint64_t a5, unint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t (*a10)(uint64_t, uint64_t, _QWORD, unint64_t, _QWORD, float, float))
{
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t result;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;

  type metadata accessor for vImage.PixelBuffer(0, a7, *(_QWORD *)(a8 + 8), a4);
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  v13 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_12;
  }
  if (*(_QWORD *)(v13 + 16) <= a3)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v20 = a5;
  v14 = *(_QWORD *)(v13 + 32 * a3 + 48);
  swift_bridgeObjectRelease();
  v15 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(_QWORD *)(v15 + 16) <= a3)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  v16 = *(_QWORD *)(v15 + 32 * a3 + 48);
  swift_bridgeObjectRelease();
  if (v14 != v16)
  {
LABEL_14:
    swift_bridgeObjectRelease();
    result = swift_bridgeObjectRelease();
    goto LABEL_15;
  }
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  result = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(_QWORD *)(result + 16) <= a3)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  v18 = *(_QWORD *)(result + 32 * a3 + 40);
  swift_bridgeObjectRelease();
  result = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(_QWORD *)(result + 16) <= a3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v19 = *(_QWORD *)(result + 32 * a3 + 40);
  swift_bridgeObjectRelease_n();
  swift_bridgeObjectRelease_n();
  result = swift_bridgeObjectRelease();
  if (v18 != v19)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  if ((a6 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!HIDWORD(a6))
    return a10(v20, a1, 0, a6, 0, 0.0, 1.0);
LABEL_19:
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.contrastStretch(binCount:destination:)(unint64_t a1, uint64_t *a2)
{
  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, MEMORY[0x1E0C8CD10]);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t *a2, uint64_t (*a3)(_OWORD *, _OWORD *, _QWORD, unint64_t, _QWORD, float, float))
{
  uint64_t *v3;
  uint64_t v4;
  __int128 v5;
  uint64_t v6;
  __int128 v7;
  _OWORD v9[2];
  _OWORD v10[2];
  uint64_t v11;

  v11 = *MEMORY[0x1E0C80C00];
  v4 = *v3;
  if (!*(_QWORD *)(*v3 + 16))
  {
    __break(1u);
    goto LABEL_7;
  }
  v5 = *(_OWORD *)(v4 + 48);
  v10[0] = *(_OWORD *)(v4 + 32);
  v10[1] = v5;
  v6 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  v7 = *(_OWORD *)(v6 + 48);
  v9[0] = *(_OWORD *)(v6 + 32);
  v9[1] = v7;
  if ((a1 & 0x8000000000000000) != 0)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (HIDWORD(a1))
    goto LABEL_9;
  return a3(v10, v9, 0, a1, 0, 0.0, 1.0);
}

_QWORD *vImage.PixelBuffer<>.histogram()()
{
  uint64_t *v0;
  _QWORD *result;
  unint64_t v2;
  uint64_t v3;

  result = specialized vImage.PixelBuffer<>._calculateHistogram()(*v0);
  v2 = result[2];
  if (!v2)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (v2 == 1)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (v2 >= 3)
  {
    v3 = result[4];
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRelease();
    return (_QWORD *)v3;
  }
LABEL_7:
  __break(1u);
  return result;
}

{
  uint64_t *v0;
  _QWORD *result;
  unint64_t v2;
  uint64_t v3;

  result = specialized vImage.PixelBuffer<>._calculateHistogram()(*v0);
  v2 = result[2];
  if (!v2)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (v2 == 1)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (v2 < 3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (v2 != 3)
  {
    v3 = result[4];
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRelease();
    return (_QWORD *)v3;
  }
LABEL_9:
  __break(1u);
  return result;
}

_QWORD *specialized vImage.PixelBuffer<>._calculateHistogram()(uint64_t a1)
{
  _QWORD *data;
  uint64_t v3;
  unint64_t v4;
  unint64_t v5;
  int64_t v6;
  uint64_t v7;
  unint64_t v8;
  unint64_t v9;
  unint64_t v10;
  unint64_t v11;
  unint64_t v12;
  _QWORD *v13;
  unint64_t v14;
  uint64_t v15;
  __int128 *v16;
  __int128 v17;
  __int128 v18;
  unint64_t v19;
  _QWORD *v20;
  _QWORD *v21;
  char *v22;
  char isUniquelyReferenced_nonNull_native;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  vImage_Buffer src;
  uint64_t v30;

  v30 = *MEMORY[0x1E0C80C00];
  src.data = (void *)MEMORY[0x1E0DEE9D8];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 3, 0);
  data = src.data;
  v3 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(_QWORD *)(v3 + 16) = 256;
  bzero((void *)(v3 + 32), 0x800uLL);
  v5 = data[2];
  v4 = data[3];
  v6 = v5 + 1;
  if (v5 >= v4 >> 1)
  {
LABEL_24:
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v4 > 1), v6, 1);
    data = src.data;
  }
  data[2] = v6;
  data[v5 + 4] = v3;
  v7 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(_QWORD *)(v7 + 16) = 256;
  bzero((void *)(v7 + 32), 0x800uLL);
  src.data = data;
  v9 = data[2];
  v8 = data[3];
  if (v9 >= v8 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v8 > 1), v9 + 1, 1);
    data = src.data;
  }
  data[2] = v9 + 1;
  data[v9 + 4] = v7;
  v3 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(_QWORD *)(v3 + 16) = 256;
  bzero((void *)(v3 + 32), 0x800uLL);
  src.data = data;
  v11 = data[2];
  v10 = data[3];
  v6 = v11 + 1;
  if (v11 >= v10 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v10 > 1), v11 + 1, 1);
    data = src.data;
  }
  v12 = 0;
  data[2] = v6;
  data[v11 + 4] = v3;
  v5 = *(_QWORD *)(a1 + 16);
  do
  {
    if (v5)
    {
      src.data = (void *)MEMORY[0x1E0DEE9D8];
      swift_bridgeObjectRetain();
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v5, 0);
      v13 = src.data;
      v14 = *((_QWORD *)src.data + 2);
      v15 = 4 * v14;
      v16 = (__int128 *)(a1 + 48);
      v3 = v5;
      do
      {
        v17 = *(v16 - 1);
        v18 = *v16;
        src.data = v13;
        v19 = v13[3];
        v6 = v14 + 1;
        if (v14 >= v19 >> 1)
        {
          v25 = v18;
          v27 = v17;
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v19 > 1), v14 + 1, 1);
          v18 = v25;
          v17 = v27;
          v13 = src.data;
        }
        v13[2] = v6;
        v20 = &v13[v15];
        *((_OWORD *)v20 + 2) = v17;
        *((_OWORD *)v20 + 3) = v18;
        v15 += 4;
        v16 = (__int128 *)((char *)v16 + 40);
        v14 = v6;
        --v3;
      }
      while (v3);
      swift_bridgeObjectRelease();
    }
    else
    {
      v13 = (_QWORD *)MEMORY[0x1E0DEE9D8];
    }
    v4 = v13[2];
    if (v12 >= v4)
    {
      __break(1u);
LABEL_23:
      __break(1u);
      goto LABEL_24;
    }
    v21 = &v13[4 * v12];
    v26 = *((_OWORD *)v21 + 3);
    v28 = *((_OWORD *)v21 + 2);
    swift_bridgeObjectRelease();
    *(_OWORD *)&src.data = v28;
    *(_OWORD *)&src.width = v26;
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
    v4 = data[2];
    if (v12 >= v4)
      goto LABEL_23;
    v3 = (uint64_t)(data + 4);
    v22 = (char *)data[v12 + 4];
    isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
    data[v12 + 4] = v22;
    if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    {
      v22 = specialized _ArrayBuffer._consumeAndCreateNew()((uint64_t)v22);
      *(_QWORD *)(v3 + 8 * v12) = v22;
    }
    vImageHistogramCalculation_Planar8(&src, (vImagePixelCount *)v22 + 4, 0);
    *(_QWORD *)(v3 + 8 * v12++) = v22;
    v6 = v12;
  }
  while (v12 != 3);
  return data;
}

{
  _QWORD *data;
  uint64_t v3;
  unint64_t v4;
  unint64_t v5;
  int64_t v6;
  uint64_t v7;
  unint64_t v8;
  unint64_t v9;
  uint64_t v10;
  unint64_t v11;
  unint64_t v12;
  unint64_t v13;
  unint64_t v14;
  unint64_t v15;
  _QWORD *v16;
  unint64_t v17;
  uint64_t v18;
  __int128 *v19;
  __int128 v20;
  __int128 v21;
  unint64_t v22;
  _QWORD *v23;
  _QWORD *v24;
  char *v25;
  char isUniquelyReferenced_nonNull_native;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  vImage_Buffer src;
  uint64_t v33;

  v33 = *MEMORY[0x1E0C80C00];
  src.data = (void *)MEMORY[0x1E0DEE9D8];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 4, 0);
  data = src.data;
  v3 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(_QWORD *)(v3 + 16) = 256;
  bzero((void *)(v3 + 32), 0x800uLL);
  v5 = data[2];
  v4 = data[3];
  v6 = v5 + 1;
  if (v5 >= v4 >> 1)
  {
LABEL_26:
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v4 > 1), v6, 1);
    data = src.data;
  }
  data[2] = v6;
  data[v5 + 4] = v3;
  v7 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(_QWORD *)(v7 + 16) = 256;
  bzero((void *)(v7 + 32), 0x800uLL);
  src.data = data;
  v9 = data[2];
  v8 = data[3];
  if (v9 >= v8 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v8 > 1), v9 + 1, 1);
    data = src.data;
  }
  data[2] = v9 + 1;
  data[v9 + 4] = v7;
  v10 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(_QWORD *)(v10 + 16) = 256;
  bzero((void *)(v10 + 32), 0x800uLL);
  src.data = data;
  v12 = data[2];
  v11 = data[3];
  if (v12 >= v11 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v11 > 1), v12 + 1, 1);
    data = src.data;
  }
  data[2] = v12 + 1;
  data[v12 + 4] = v10;
  v3 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(_QWORD *)(v3 + 16) = 256;
  bzero((void *)(v3 + 32), 0x800uLL);
  src.data = data;
  v14 = data[2];
  v13 = data[3];
  v6 = v14 + 1;
  if (v14 >= v13 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v13 > 1), v14 + 1, 1);
    data = src.data;
  }
  v15 = 0;
  data[2] = v6;
  data[v14 + 4] = v3;
  v5 = *(_QWORD *)(a1 + 16);
  do
  {
    if (v5)
    {
      src.data = (void *)MEMORY[0x1E0DEE9D8];
      swift_bridgeObjectRetain();
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v5, 0);
      v16 = src.data;
      v17 = *((_QWORD *)src.data + 2);
      v18 = 4 * v17;
      v19 = (__int128 *)(a1 + 48);
      v3 = v5;
      do
      {
        v20 = *(v19 - 1);
        v21 = *v19;
        src.data = v16;
        v22 = v16[3];
        v6 = v17 + 1;
        if (v17 >= v22 >> 1)
        {
          v28 = v21;
          v30 = v20;
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v22 > 1), v17 + 1, 1);
          v21 = v28;
          v20 = v30;
          v16 = src.data;
        }
        v16[2] = v6;
        v23 = &v16[v18];
        *((_OWORD *)v23 + 2) = v20;
        *((_OWORD *)v23 + 3) = v21;
        v18 += 4;
        v19 = (__int128 *)((char *)v19 + 40);
        v17 = v6;
        --v3;
      }
      while (v3);
      swift_bridgeObjectRelease();
    }
    else
    {
      v16 = (_QWORD *)MEMORY[0x1E0DEE9D8];
    }
    v4 = v16[2];
    if (v15 >= v4)
    {
      __break(1u);
LABEL_25:
      __break(1u);
      goto LABEL_26;
    }
    v24 = &v16[4 * v15];
    v29 = *((_OWORD *)v24 + 3);
    v31 = *((_OWORD *)v24 + 2);
    swift_bridgeObjectRelease();
    *(_OWORD *)&src.data = v31;
    *(_OWORD *)&src.width = v29;
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
    v4 = data[2];
    if (v15 >= v4)
      goto LABEL_25;
    v3 = (uint64_t)(data + 4);
    v25 = (char *)data[v15 + 4];
    isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
    data[v15 + 4] = v25;
    if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    {
      v25 = specialized _ArrayBuffer._consumeAndCreateNew()((uint64_t)v25);
      *(_QWORD *)(v3 + 8 * v15) = v25;
    }
    vImageHistogramCalculation_Planar8(&src, (vImagePixelCount *)v25 + 4, 0);
    *(_QWORD *)(v3 + 8 * v15++) = v25;
    v6 = v15;
  }
  while (v15 != 4);
  return data;
}

_QWORD *vImage.PixelBuffer<>._calculateHistogram()(uint64_t a1, uint64_t a2)
{
  void **v2;
  void *v4;
  uint64_t v5;
  uint64_t (*v6)(uint64_t);
  int64_t v7;
  int64_t v8;
  _QWORD *data;
  uint64_t v10;
  unint64_t v11;
  unint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  unint64_t v16;
  uint64_t v17;
  _QWORD *v18;
  char *v19;
  char isUniquelyReferenced_nonNull_native;
  __int128 v22;
  __int128 v23;
  vImage_Buffer src;
  uint64_t v25;

  v25 = *MEMORY[0x1E0C80C00];
  v4 = *v2;
  v5 = *(_QWORD *)(a1 + 16);
  v6 = *(uint64_t (**)(uint64_t))(a2 + 32);
  v7 = v6(v5);
  if (v7 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v8 = v7;
  data = (_QWORD *)MEMORY[0x1E0DEE9D8];
  if (v7)
  {
    src.data = (void *)MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v7, 0);
    data = src.data;
    do
    {
      v10 = static Array._allocateBufferUninitialized(minimumCapacity:)();
      *(_QWORD *)(v10 + 16) = 256;
      bzero((void *)(v10 + 32), 0x800uLL);
      src.data = data;
      v12 = data[2];
      v11 = data[3];
      if (v12 >= v11 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v11 > 1), v12 + 1, 1);
        data = src.data;
      }
      data[2] = v12 + 1;
      data[v12 + 4] = v10;
      --v8;
    }
    while (v8);
  }
  v13 = ((uint64_t (*)(uint64_t, uint64_t))v6)(v5, a2);
  if (v13 < 0)
LABEL_21:
    __break(1u);
  v14 = v13;
  if (v13)
  {
    v15 = 0;
    v16 = 0;
    while (1)
    {
      src.data = v4;
      v17 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v16 >= *(_QWORD *)(v17 + 16))
        break;
      v22 = *(_OWORD *)(v17 + v15 + 48);
      v23 = *(_OWORD *)(v17 + v15 + 32);
      swift_bridgeObjectRelease();
      *(_OWORD *)&src.data = v23;
      *(_OWORD *)&src.width = v22;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
        data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
      if (v16 >= data[2])
        goto LABEL_19;
      v18 = &data[v16];
      v19 = (char *)v18[4];
      isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
      v18[4] = v19;
      if ((isUniquelyReferenced_nonNull_native & 1) == 0)
      {
        v19 = specialized _ArrayBuffer._consumeAndCreateNew()((uint64_t)v19);
        v18[4] = v19;
      }
      ++v16;
      vImageHistogramCalculation_Planar8(&src, (vImagePixelCount *)v19 + 4, 0);
      v18[4] = v19;
      v15 += 32;
      if (v14 == v16)
        return data;
    }
    __break(1u);
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  return data;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, _QWORD **a4)
{
  _QWORD **v4;
  _QWORD *v8;
  _QWORD *v9;
  uint64_t inited;

  v8 = *a4;
  v9 = *v4;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<[UInt]>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5EF70;
  *(_QWORD *)(inited + 32) = a1;
  *(_QWORD *)(inited + 40) = a2;
  *(_QWORD *)(inited + 48) = a3;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  specialized vImage.PixelBuffer<>._specifyHistogram(_:destination:)(inited, v8, v9);
  swift_setDeallocating();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [UInt]);
  return swift_arrayDestroy();
}

uint64_t specialized vImage.PixelBuffer<>._specifyHistogram(_:destination:)(uint64_t a1, _QWORD *a2, _QWORD *a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  unint64_t v7;
  void *v8;
  int64_t v9;
  _QWORD *data;
  unint64_t v11;
  uint64_t v12;
  __int128 *v13;
  __int128 v14;
  __int128 v15;
  unint64_t v16;
  unint64_t v17;
  _QWORD *v18;
  unint64_t v19;
  __int128 *v20;
  void *v21;
  int64_t v22;
  _QWORD *v23;
  unint64_t v24;
  uint64_t v25;
  __int128 *v26;
  __int128 v27;
  __int128 v28;
  unint64_t v29;
  unint64_t v30;
  _QWORD *v31;
  __int128 *v32;
  uint64_t v33;
  uint64_t result;
  uint64_t v35;
  uint64_t v36;
  __int128 *v37;
  int64_t v38;
  __int128 *v39;
  int64_t v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v51;

  v51 = *MEMORY[0x1E0C80C00];
  v40 = a3[2];
  if (!v40)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v3 = a3[6];
  if (v3 < 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v4 = a3[5];
  if (v4 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v3)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v4)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v38 = a2[2];
  if (!v38)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v5 = a2[6];
  if (v5 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  v6 = a2[5];
  if (v6 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v5)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v6)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v3 != v5)
  {
LABEL_40:
    __break(1u);
LABEL_41:
    __break(1u);
  }
  if (v4 != v6)
    goto LABEL_41;
  v7 = 0;
  v35 = a1 + 32;
  v36 = *(_QWORD *)(a1 + 16);
  v39 = (__int128 *)(a3 + 6);
  v37 = (__int128 *)(a2 + 6);
  v8 = (void *)MEMORY[0x1E0DEE9D8];
  do
  {
    src.data = v8;
    swift_bridgeObjectRetain();
    v9 = v40;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v40, 0);
    data = src.data;
    v11 = *((_QWORD *)src.data + 2);
    v12 = 4 * v11;
    v13 = v39;
    do
    {
      v14 = *(v13 - 1);
      v15 = *v13;
      src.data = data;
      v16 = data[3];
      v17 = v11 + 1;
      if (v11 >= v16 >> 1)
      {
        v41 = v15;
        v45 = v14;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v16 > 1), v11 + 1, 1);
        v15 = v41;
        v14 = v45;
        data = src.data;
      }
      data[2] = v17;
      v18 = &data[v12];
      *((_OWORD *)v18 + 2) = v14;
      *((_OWORD *)v18 + 3) = v15;
      v12 += 4;
      v13 = (__int128 *)((char *)v13 + 40);
      v11 = v17;
      --v9;
    }
    while (v9);
    swift_bridgeObjectRelease();
    if (v7 >= data[2])
    {
      __break(1u);
LABEL_28:
      __break(1u);
LABEL_29:
      __break(1u);
      goto LABEL_30;
    }
    v19 = v7 + 1;
    v20 = (__int128 *)&data[4 * v7 + 4];
    v42 = v20[1];
    v46 = *v20;
    swift_bridgeObjectRelease();
    *(_OWORD *)&src.data = v46;
    *(_OWORD *)&src.width = v42;
    v21 = v8;
    dest.data = v8;
    swift_bridgeObjectRetain();
    v22 = v38;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v38, 0);
    v23 = dest.data;
    v24 = *((_QWORD *)dest.data + 2);
    v25 = 4 * v24;
    v26 = v37;
    do
    {
      v27 = *(v26 - 1);
      v28 = *v26;
      dest.data = v23;
      v29 = v23[3];
      v30 = v24 + 1;
      if (v24 >= v29 >> 1)
      {
        v43 = v28;
        v47 = v27;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v29 > 1), v24 + 1, 1);
        v28 = v43;
        v27 = v47;
        v23 = dest.data;
      }
      v23[2] = v30;
      v31 = &v23[v25];
      *((_OWORD *)v31 + 2) = v27;
      *((_OWORD *)v31 + 3) = v28;
      v25 += 4;
      v26 = (__int128 *)((char *)v26 + 40);
      v24 = v30;
      --v22;
    }
    while (v22);
    swift_bridgeObjectRelease();
    if (v7 >= v23[2])
      goto LABEL_28;
    v32 = (__int128 *)&v23[4 * v7 + 4];
    v44 = v32[1];
    v48 = *v32;
    swift_bridgeObjectRelease();
    *(_OWORD *)&dest.data = v48;
    *(_OWORD *)&dest.width = v44;
    if (v7 == v36)
      goto LABEL_29;
    v33 = *(_QWORD *)(v35 + 8 * v7);
    swift_bridgeObjectRetain();
    vImageHistogramSpecification_Planar8(&src, &dest, (const vImagePixelCount *)(v33 + 32), 0);
    result = swift_bridgeObjectRelease();
    ++v7;
    v8 = v21;
  }
  while (v19 != 3);
  return result;
}

{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  unint64_t v7;
  void *v8;
  int64_t v9;
  _QWORD *data;
  unint64_t v11;
  uint64_t v12;
  __int128 *v13;
  __int128 v14;
  __int128 v15;
  unint64_t v16;
  unint64_t v17;
  _QWORD *v18;
  unint64_t v19;
  __int128 *v20;
  void *v21;
  int64_t v22;
  _QWORD *v23;
  unint64_t v24;
  uint64_t v25;
  __int128 *v26;
  __int128 v27;
  __int128 v28;
  unint64_t v29;
  unint64_t v30;
  _QWORD *v31;
  __int128 *v32;
  uint64_t v33;
  uint64_t result;
  uint64_t v35;
  uint64_t v36;
  __int128 *v37;
  int64_t v38;
  __int128 *v39;
  int64_t v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v51;

  v51 = *MEMORY[0x1E0C80C00];
  v40 = a3[2];
  if (!v40)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v3 = a3[6];
  if (v3 < 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v4 = a3[5];
  if (v4 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v3)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v4)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v38 = a2[2];
  if (!v38)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v5 = a2[6];
  if (v5 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  v6 = a2[5];
  if (v6 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v5)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v6)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v3 != v5)
  {
LABEL_40:
    __break(1u);
LABEL_41:
    __break(1u);
  }
  if (v4 != v6)
    goto LABEL_41;
  v7 = 0;
  v35 = a1 + 32;
  v36 = *(_QWORD *)(a1 + 16);
  v39 = (__int128 *)(a3 + 6);
  v37 = (__int128 *)(a2 + 6);
  v8 = (void *)MEMORY[0x1E0DEE9D8];
  do
  {
    src.data = v8;
    swift_bridgeObjectRetain();
    v9 = v40;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v40, 0);
    data = src.data;
    v11 = *((_QWORD *)src.data + 2);
    v12 = 4 * v11;
    v13 = v39;
    do
    {
      v14 = *(v13 - 1);
      v15 = *v13;
      src.data = data;
      v16 = data[3];
      v17 = v11 + 1;
      if (v11 >= v16 >> 1)
      {
        v41 = v15;
        v45 = v14;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v16 > 1), v11 + 1, 1);
        v15 = v41;
        v14 = v45;
        data = src.data;
      }
      data[2] = v17;
      v18 = &data[v12];
      *((_OWORD *)v18 + 2) = v14;
      *((_OWORD *)v18 + 3) = v15;
      v12 += 4;
      v13 = (__int128 *)((char *)v13 + 40);
      v11 = v17;
      --v9;
    }
    while (v9);
    swift_bridgeObjectRelease();
    if (v7 >= data[2])
    {
      __break(1u);
LABEL_28:
      __break(1u);
LABEL_29:
      __break(1u);
      goto LABEL_30;
    }
    v19 = v7 + 1;
    v20 = (__int128 *)&data[4 * v7 + 4];
    v42 = v20[1];
    v46 = *v20;
    swift_bridgeObjectRelease();
    *(_OWORD *)&src.data = v46;
    *(_OWORD *)&src.width = v42;
    v21 = v8;
    dest.data = v8;
    swift_bridgeObjectRetain();
    v22 = v38;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v38, 0);
    v23 = dest.data;
    v24 = *((_QWORD *)dest.data + 2);
    v25 = 4 * v24;
    v26 = v37;
    do
    {
      v27 = *(v26 - 1);
      v28 = *v26;
      dest.data = v23;
      v29 = v23[3];
      v30 = v24 + 1;
      if (v24 >= v29 >> 1)
      {
        v43 = v28;
        v47 = v27;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v29 > 1), v24 + 1, 1);
        v28 = v43;
        v27 = v47;
        v23 = dest.data;
      }
      v23[2] = v30;
      v31 = &v23[v25];
      *((_OWORD *)v31 + 2) = v27;
      *((_OWORD *)v31 + 3) = v28;
      v25 += 4;
      v26 = (__int128 *)((char *)v26 + 40);
      v24 = v30;
      --v22;
    }
    while (v22);
    swift_bridgeObjectRelease();
    if (v7 >= v23[2])
      goto LABEL_28;
    v32 = (__int128 *)&v23[4 * v7 + 4];
    v44 = v32[1];
    v48 = *v32;
    swift_bridgeObjectRelease();
    *(_OWORD *)&dest.data = v48;
    *(_OWORD *)&dest.width = v44;
    if (v7 == v36)
      goto LABEL_29;
    v33 = *(_QWORD *)(v35 + 8 * v7);
    swift_bridgeObjectRetain();
    vImageHistogramSpecification_Planar8(&src, &dest, (const vImagePixelCount *)(v33 + 32), 0);
    result = swift_bridgeObjectRelease();
    ++v7;
    v8 = v21;
  }
  while (v19 != 4);
  return result;
}

uint64_t vImage.PixelBuffer<>._specifyHistogram(_:destination:)(uint64_t a1, void **a2, uint64_t a3, uint64_t a4)
{
  void **v4;
  void *v7;
  void *v8;
  __int128 v9;
  uint64_t v11;
  uint64_t result;
  uint64_t v13;
  uint64_t v14;
  unint64_t v15;
  uint64_t v16;
  __int128 v18;
  __int128 v19;
  _QWORD v20[2];
  vImage_Buffer v21;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  v7 = *a2;
  v8 = *v4;
  v20[0] = *v4;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&v21);
  v9 = *(_OWORD *)&v21.data;
  vImage.PixelBuffer.size.getter(v20);
  swift_bridgeObjectRelease();
  if ((_QWORD)v9 != v20[0] || *((_QWORD *)&v9 + 1) != v20[1])
  {
LABEL_13:
    __break(1u);
LABEL_14:
    __break(1u);
  }
  v11 = *(_QWORD *)(a3 + 16);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 32))(v11, a4);
  if (result < 0)
    goto LABEL_14;
  v13 = result;
  if (result)
  {
    v14 = 0;
    v15 = 0;
    while (1)
    {
      v21.data = v8;
      v16 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v15 >= *(_QWORD *)(v16 + 16))
        break;
      v18 = *(_OWORD *)(v16 + v14 + 48);
      v19 = *(_OWORD *)(v16 + v14 + 32);
      swift_bridgeObjectRelease();
      *(_OWORD *)&v21.data = v19;
      *(_OWORD *)&v21.width = v18;
      result = closure #1 in vImage.PixelBuffer<>._specifyHistogram(_:destination:)(&v21, v7, v15, a1, v11, a4);
      v14 += 32;
      if (v13 == ++v15)
        return result;
    }
    __break(1u);
    goto LABEL_13;
  }
  return result;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _QWORD **a5)
{
  _QWORD **v5;
  uint64_t inited;
  _QWORD *v12;
  _QWORD *v13;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<[UInt]>);
  inited = swift_initStackObject();
  *(_QWORD *)(inited + 32) = a1;
  *(_OWORD *)(inited + 16) = xmmword_1CAB5F170;
  *(_QWORD *)(inited + 40) = a2;
  *(_QWORD *)(inited + 48) = a3;
  *(_QWORD *)(inited + 56) = a4;
  v12 = *a5;
  v13 = *v5;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  specialized vImage.PixelBuffer<>._specifyHistogram(_:destination:)(inited, v12, v13);
  swift_setDeallocating();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [UInt]);
  return swift_arrayDestroy();
}

uint64_t closure #1 in vImage.PixelBuffer<>._specifyHistogram(_:destination:)(const vImage_Buffer *a1, void *a2, unint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  __int128 v13;
  __int128 v14;
  vImage_Buffer dest;
  uint64_t v16;

  v16 = *MEMORY[0x1E0C80C00];
  dest.data = a2;
  type metadata accessor for vImage.PixelBuffer(0, a5, *(_QWORD *)(a6 + 8), a4);
  v9 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (*(_QWORD *)(v9 + 16) <= a3)
  {
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  v10 = v9 + 32 * a3;
  v13 = *(_OWORD *)(v10 + 48);
  v14 = *(_OWORD *)(v10 + 32);
  swift_bridgeObjectRelease();
  *(_OWORD *)&dest.data = v14;
  *(_OWORD *)&dest.width = v13;
  if (*(_QWORD *)(a4 + 16) <= a3)
    goto LABEL_7;
  v11 = *(_QWORD *)(a4 + 8 * a3 + 32);
  swift_bridgeObjectRetain();
  vImageHistogramSpecification_Planar8(a1, &dest, (const vImagePixelCount *)(v11 + 32), 0);
  return swift_bridgeObjectRelease();
}

_QWORD *vImage.PixelBuffer<>.histogram(binCount:)(unint64_t a1)
{
  uint64_t *v1;
  _QWORD *result;
  unint64_t v4;

  result = specialized vImage.PixelBuffer<>._calculateHistogram(binCount:)(a1, *v1);
  v4 = result[2];
  if (!v4)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (v4 == 1)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (v4 >= 3)
  {
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRelease();
    return (_QWORD *)a1;
  }
LABEL_7:
  __break(1u);
  return result;
}

_QWORD *specialized vImage.PixelBuffer<>._calculateHistogram(binCount:)(unint64_t a1, uint64_t a2)
{
  uint64_t v2;
  unint64_t v3;
  unint64_t v4;
  unint64_t v5;
  unint64_t v8;
  _QWORD *data;
  uint64_t v10;
  unint64_t v11;
  void *v12;
  unint64_t v13;
  void *v14;
  uint64_t v15;
  _QWORD *v16;
  unint64_t v17;
  uint64_t v18;
  __int128 *v19;
  __int128 v20;
  __int128 v21;
  unint64_t v22;
  _QWORD *v23;
  _QWORD *v24;
  _QWORD *v25;
  char *v26;
  char isUniquelyReferenced_nonNull_native;
  __int128 *v29;
  uint64_t v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  vImage_Buffer src;
  uint64_t v36;

  v36 = *MEMORY[0x1E0C80C00];
  src.data = (void *)MEMORY[0x1E0DEE9D8];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 3, 0);
  if ((a1 & 0x8000000000000000) != 0)
  {
LABEL_35:
    __break(1u);
  }
  else
  {
    data = src.data;
    v2 = 8 * a1;
    if (a1)
    {
      v4 = static Array._allocateBufferUninitialized(minimumCapacity:)();
      *(_QWORD *)(v4 + 16) = a1;
      bzero((void *)(v4 + 32), 8 * a1);
    }
    else
    {
      v4 = MEMORY[0x1E0DEE9D8];
    }
    v5 = data[2];
    v8 = data[3];
    v3 = v5 + 1;
    if (v5 < v8 >> 1)
      goto LABEL_6;
  }
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v8 > 1), v3, 1);
  data = src.data;
LABEL_6:
  data[2] = v3;
  data[v5 + 4] = v4;
  if (a1)
  {
    v10 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v10 + 16) = a1;
    bzero((void *)(v10 + 32), v2);
    v3 = data[2];
  }
  else
  {
    v10 = MEMORY[0x1E0DEE9D8];
  }
  src.data = data;
  v11 = data[3];
  if (v3 >= v11 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v11 > 1), v3 + 1, 1);
    data = src.data;
  }
  data[2] = v3 + 1;
  data[v3 + 4] = v10;
  v12 = (void *)MEMORY[0x1E0DEE9D8];
  if (a1)
  {
    v3 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v3 + 16) = a1;
    bzero((void *)(v3 + 32), v2);
  }
  else
  {
    v3 = MEMORY[0x1E0DEE9D8];
  }
  src.data = data;
  v4 = data[2];
  v13 = data[3];
  if (v4 >= v13 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v13 > 1), v4 + 1, 1);
    data = src.data;
  }
  data[2] = v4 + 1;
  data[v4 + 4] = v3;
  if (HIDWORD(a1))
    __break(1u);
  v5 = 0;
  v2 = *(_QWORD *)(a2 + 16);
  v29 = (__int128 *)(a2 + 48);
  v30 = v2;
  do
  {
    if (v2)
    {
      v4 = (unint64_t)data;
      v14 = v12;
      src.data = v12;
      v15 = a2;
      swift_bridgeObjectRetain();
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v2, 0);
      v16 = src.data;
      v17 = *((_QWORD *)src.data + 2);
      v18 = 4 * v17;
      v19 = v29;
      do
      {
        v20 = *(v19 - 1);
        v21 = *v19;
        src.data = v16;
        v22 = v16[3];
        v3 = v17 + 1;
        if (v17 >= v22 >> 1)
        {
          v31 = v21;
          v33 = v20;
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v22 > 1), v17 + 1, 1);
          v21 = v31;
          v20 = v33;
          v16 = src.data;
        }
        v16[2] = v3;
        v23 = &v16[v18];
        *((_OWORD *)v23 + 2) = v20;
        *((_OWORD *)v23 + 3) = v21;
        v18 += 4;
        v19 = (__int128 *)((char *)v19 + 40);
        v17 = v3;
        --v2;
      }
      while (v2);
      a2 = v15;
      swift_bridgeObjectRelease();
      v12 = v14;
      data = (_QWORD *)v4;
    }
    else
    {
      v16 = v12;
    }
    v8 = v16[2];
    if (v5 >= v8)
    {
      __break(1u);
LABEL_34:
      __break(1u);
      goto LABEL_35;
    }
    v24 = &v16[4 * v5];
    v32 = *((_OWORD *)v24 + 3);
    v34 = *((_OWORD *)v24 + 2);
    swift_bridgeObjectRelease();
    *(_OWORD *)&src.data = v34;
    *(_OWORD *)&src.width = v32;
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
    v8 = data[2];
    if (v5 >= v8)
      goto LABEL_34;
    v25 = data + 4;
    v26 = (char *)data[v5 + 4];
    isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
    data[v5 + 4] = v26;
    if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    {
      v26 = specialized _ArrayBuffer._consumeAndCreateNew()((uint64_t)v26);
      v25[v5] = v26;
    }
    vImageHistogramCalculation_PlanarF(&src, (vImagePixelCount *)v26 + 4, a1, 0.0, 1.0, 0);
    v25[v5++] = v26;
    v3 = v5;
    v2 = v30;
  }
  while (v5 != 3);
  return data;
}

{
  unint64_t v2;
  uint64_t v3;
  unint64_t v4;
  unint64_t v5;
  unint64_t v8;
  _QWORD *data;
  uint64_t v10;
  unint64_t v11;
  uint64_t v12;
  unint64_t v13;
  void *v14;
  uint64_t v15;
  unint64_t v16;
  unint64_t v17;
  void *v18;
  uint64_t v19;
  _QWORD *v20;
  unint64_t v21;
  uint64_t v22;
  __int128 *v23;
  __int128 v24;
  __int128 v25;
  unint64_t v26;
  _QWORD *v27;
  _QWORD *v28;
  _QWORD *v29;
  char *v30;
  char isUniquelyReferenced_nonNull_native;
  __int128 *v33;
  uint64_t v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  vImage_Buffer src;
  uint64_t v40;

  v40 = *MEMORY[0x1E0C80C00];
  src.data = (void *)MEMORY[0x1E0DEE9D8];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 4, 0);
  if ((a1 & 0x8000000000000000) != 0)
  {
LABEL_40:
    __break(1u);
  }
  else
  {
    data = src.data;
    v3 = 8 * a1;
    if (a1)
    {
      v5 = static Array._allocateBufferUninitialized(minimumCapacity:)();
      *(_QWORD *)(v5 + 16) = a1;
      bzero((void *)(v5 + 32), 8 * a1);
    }
    else
    {
      v5 = MEMORY[0x1E0DEE9D8];
    }
    v2 = data[2];
    v8 = data[3];
    v4 = v2 + 1;
    if (v2 < v8 >> 1)
      goto LABEL_6;
  }
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v8 > 1), v4, 1);
  data = src.data;
LABEL_6:
  data[2] = v4;
  data[v2 + 4] = v5;
  if (a1)
  {
    v10 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v10 + 16) = a1;
    bzero((void *)(v10 + 32), v3);
    v4 = data[2];
  }
  else
  {
    v10 = MEMORY[0x1E0DEE9D8];
  }
  src.data = data;
  v11 = data[3];
  if (v4 >= v11 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v11 > 1), v4 + 1, 1);
    data = src.data;
  }
  data[2] = v4 + 1;
  data[v4 + 4] = v10;
  if (a1)
  {
    v12 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v12 + 16) = a1;
    bzero((void *)(v12 + 32), v3);
  }
  else
  {
    v12 = MEMORY[0x1E0DEE9D8];
  }
  src.data = data;
  v2 = data[2];
  v13 = data[3];
  v4 = v2 + 1;
  if (v2 >= v13 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v13 > 1), v2 + 1, 1);
    data = src.data;
  }
  data[2] = v4;
  data[v2 + 4] = v12;
  v14 = (void *)MEMORY[0x1E0DEE9D8];
  if (a1)
  {
    v15 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(_QWORD *)(v15 + 16) = a1;
    bzero((void *)(v15 + 32), v3);
    v4 = data[2];
  }
  else
  {
    v15 = MEMORY[0x1E0DEE9D8];
  }
  src.data = data;
  v16 = data[3];
  if (v4 >= v16 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v16 > 1), v4 + 1, 1);
    data = src.data;
  }
  data[2] = v4 + 1;
  data[v4 + 4] = v15;
  if (HIDWORD(a1))
    __break(1u);
  v5 = 0;
  v3 = *(_QWORD *)(a2 + 16);
  v33 = (__int128 *)(a2 + 48);
  v34 = v3;
  do
  {
    if (v3)
    {
      v2 = (unint64_t)data;
      v17 = a1;
      v18 = v14;
      src.data = v14;
      v19 = a2;
      swift_bridgeObjectRetain();
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v3, 0);
      v20 = src.data;
      v21 = *((_QWORD *)src.data + 2);
      v22 = 4 * v21;
      v23 = v33;
      do
      {
        v24 = *(v23 - 1);
        v25 = *v23;
        src.data = v20;
        v26 = v20[3];
        v4 = v21 + 1;
        if (v21 >= v26 >> 1)
        {
          v35 = v25;
          v37 = v24;
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v26 > 1), v21 + 1, 1);
          v25 = v35;
          v24 = v37;
          v20 = src.data;
        }
        v20[2] = v4;
        v27 = &v20[v22];
        *((_OWORD *)v27 + 2) = v24;
        *((_OWORD *)v27 + 3) = v25;
        v22 += 4;
        v23 = (__int128 *)((char *)v23 + 40);
        v21 = v4;
        --v3;
      }
      while (v3);
      a2 = v19;
      swift_bridgeObjectRelease();
      a1 = v17;
      v14 = v18;
      data = (_QWORD *)v2;
    }
    else
    {
      v20 = v14;
    }
    v8 = v20[2];
    if (v5 >= v8)
    {
      __break(1u);
LABEL_39:
      __break(1u);
      goto LABEL_40;
    }
    v28 = &v20[4 * v5];
    v36 = *((_OWORD *)v28 + 3);
    v38 = *((_OWORD *)v28 + 2);
    swift_bridgeObjectRelease();
    *(_OWORD *)&src.data = v38;
    *(_OWORD *)&src.width = v36;
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
    v8 = data[2];
    if (v5 >= v8)
      goto LABEL_39;
    v29 = data + 4;
    v30 = (char *)data[v5 + 4];
    isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
    data[v5 + 4] = v30;
    if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    {
      v30 = specialized _ArrayBuffer._consumeAndCreateNew()((uint64_t)v30);
      v29[v5] = v30;
    }
    vImageHistogramCalculation_PlanarF(&src, (vImagePixelCount *)v30 + 4, a1, 0.0, 1.0, 0);
    v29[v5++] = v30;
    v4 = v5;
    v3 = v34;
  }
  while (v5 != 4);
  return data;
}

_QWORD *vImage.PixelBuffer<>._calculateHistogram(binCount:)(unint64_t a1, uint64_t a2, uint64_t a3)
{
  void **v3;
  uint64_t v6;
  uint64_t (*v7)(uint64_t, uint64_t);
  int64_t v8;
  int64_t v9;
  _QWORD *data;
  uint64_t v11;
  uint64_t v12;
  unint64_t v13;
  unint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  unint64_t v18;
  uint64_t v19;
  _QWORD *v20;
  char *v21;
  char isUniquelyReferenced_nonNull_native;
  void *v24;
  __int128 v25;
  __int128 v26;
  vImage_Buffer src;
  uint64_t v28;

  v28 = *MEMORY[0x1E0C80C00];
  v24 = *v3;
  v6 = *(_QWORD *)(a2 + 16);
  v7 = *(uint64_t (**)(uint64_t, uint64_t))(a3 + 32);
  v8 = v7(v6, a3);
  if (v8 < 0)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  v9 = v8;
  data = (_QWORD *)MEMORY[0x1E0DEE9D8];
  if (v8)
  {
    src.data = (void *)MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v8, 0);
    if ((a1 & 0x8000000000000000) != 0)
    {
LABEL_28:
      __break(1u);
      goto LABEL_29;
    }
    data = src.data;
    v11 = MEMORY[0x1E0DEE9D8];
    do
    {
      if (a1)
      {
        v12 = static Array._allocateBufferUninitialized(minimumCapacity:)();
        *(_QWORD *)(v12 + 16) = a1;
        bzero((void *)(v12 + 32), 8 * a1);
      }
      else
      {
        v12 = v11;
      }
      src.data = data;
      v14 = data[2];
      v13 = data[3];
      if (v14 >= v13 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((_QWORD *)(v13 > 1), v14 + 1, 1);
        data = src.data;
      }
      data[2] = v14 + 1;
      data[v14 + 4] = v12;
      --v9;
    }
    while (v9);
  }
  v15 = v7(v6, a3);
  if (v15 < 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v16 = v15;
  if (v15)
  {
    if ((a1 & 0x8000000000000000) != 0)
    {
LABEL_29:
      __break(1u);
LABEL_30:
      __break(1u);
    }
    if (HIDWORD(a1))
      goto LABEL_30;
    v17 = 0;
    v18 = 0;
    while (1)
    {
      src.data = v24;
      v19 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v18 >= *(_QWORD *)(v19 + 16))
        break;
      v25 = *(_OWORD *)(v19 + v17 + 48);
      v26 = *(_OWORD *)(v19 + v17 + 32);
      swift_bridgeObjectRelease();
      *(_OWORD *)&src.data = v26;
      *(_OWORD *)&src.width = v25;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
        data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
      if (v18 >= data[2])
        goto LABEL_25;
      v20 = &data[v18];
      v21 = (char *)v20[4];
      isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
      v20[4] = v21;
      if ((isUniquelyReferenced_nonNull_native & 1) == 0)
      {
        v21 = specialized _ArrayBuffer._consumeAndCreateNew()((uint64_t)v21);
        v20[4] = v21;
      }
      ++v18;
      vImageHistogramCalculation_PlanarF(&src, (vImagePixelCount *)v21 + 4, a1, 0.0, 1.0, 0);
      v20[4] = v21;
      v17 += 32;
      if (v16 == v18)
        return data;
    }
    __break(1u);
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  return data;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(unint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _QWORD **a5)
{
  _QWORD **v5;
  uint64_t inited;
  _QWORD *v12;
  _QWORD *v13;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<[UInt]>);
  inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1CAB5EF70;
  *(_QWORD *)(inited + 32) = a2;
  *(_QWORD *)(inited + 40) = a3;
  *(_QWORD *)(inited + 48) = a4;
  v12 = *a5;
  v13 = *v5;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  specialized vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(inited, a1, v12, v13);
  swift_setDeallocating();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [UInt]);
  return swift_arrayDestroy();
}

uint64_t specialized vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(uint64_t a1, unint64_t a2, _QWORD *a3, _QWORD *a4)
{
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  unint64_t v8;
  unint64_t v9;
  int64_t v10;
  _QWORD *data;
  unint64_t v12;
  uint64_t v13;
  __int128 *v14;
  __int128 v15;
  __int128 v16;
  unint64_t v17;
  unint64_t v18;
  _QWORD *v19;
  unint64_t v20;
  unint64_t v21;
  __int128 *v22;
  int64_t v23;
  _QWORD *v24;
  unint64_t v25;
  uint64_t v26;
  __int128 *v27;
  __int128 v28;
  __int128 v29;
  unint64_t v30;
  unint64_t v31;
  _QWORD *v32;
  __int128 *v33;
  uint64_t v34;
  uint64_t result;
  uint64_t v36;
  uint64_t v37;
  __int128 *v38;
  int64_t v39;
  __int128 *v40;
  int64_t v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v52;

  v52 = *MEMORY[0x1E0C80C00];
  v41 = a4[2];
  if (!v41)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v4 = a4[6];
  if (v4 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v5 = a4[5];
  if (v5 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v4)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (!v5)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v39 = a3[2];
  if (!v39)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  v6 = a3[6];
  if (v6 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v7 = a3[5];
  if (v7 < 1)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v4 != v6)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (v5 != v7)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v8 = a2;
  if ((a2 & 0x8000000000000000) != 0)
  {
LABEL_42:
    __break(1u);
LABEL_43:
    __break(1u);
  }
  if (HIDWORD(a2))
    goto LABEL_43;
  v9 = 0;
  v36 = a1 + 32;
  v37 = *(_QWORD *)(a1 + 16);
  v40 = (__int128 *)(a4 + 6);
  v38 = (__int128 *)(a3 + 6);
  do
  {
    src.data = (void *)MEMORY[0x1E0DEE9D8];
    swift_bridgeObjectRetain();
    v10 = v41;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v41, 0);
    data = src.data;
    v12 = *((_QWORD *)src.data + 2);
    v13 = 4 * v12;
    v14 = v40;
    do
    {
      v15 = *(v14 - 1);
      v16 = *v14;
      src.data = data;
      v17 = data[3];
      v18 = v12 + 1;
      if (v12 >= v17 >> 1)
      {
        v42 = v16;
        v46 = v15;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v17 > 1), v12 + 1, 1);
        v16 = v42;
        v15 = v46;
        data = src.data;
      }
      data[2] = v18;
      v19 = &data[v13];
      *((_OWORD *)v19 + 2) = v15;
      *((_OWORD *)v19 + 3) = v16;
      v13 += 4;
      v14 = (__int128 *)((char *)v14 + 40);
      v12 = v18;
      --v10;
    }
    while (v10);
    swift_bridgeObjectRelease();
    if (v9 >= data[2])
    {
      __break(1u);
LABEL_29:
      __break(1u);
LABEL_30:
      __break(1u);
      goto LABEL_31;
    }
    v20 = v8;
    v21 = v9 + 1;
    v22 = (__int128 *)&data[4 * v9 + 4];
    v43 = v22[1];
    v47 = *v22;
    swift_bridgeObjectRelease();
    *(_OWORD *)&src.data = v47;
    *(_OWORD *)&src.width = v43;
    dest.data = (void *)MEMORY[0x1E0DEE9D8];
    swift_bridgeObjectRetain();
    v23 = v39;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v39, 0);
    v24 = dest.data;
    v25 = *((_QWORD *)dest.data + 2);
    v26 = 4 * v25;
    v27 = v38;
    do
    {
      v28 = *(v27 - 1);
      v29 = *v27;
      dest.data = v24;
      v30 = v24[3];
      v31 = v25 + 1;
      if (v25 >= v30 >> 1)
      {
        v44 = v29;
        v48 = v28;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v30 > 1), v25 + 1, 1);
        v29 = v44;
        v28 = v48;
        v24 = dest.data;
      }
      v24[2] = v31;
      v32 = &v24[v26];
      *((_OWORD *)v32 + 2) = v28;
      *((_OWORD *)v32 + 3) = v29;
      v26 += 4;
      v27 = (__int128 *)((char *)v27 + 40);
      v25 = v31;
      --v23;
    }
    while (v23);
    swift_bridgeObjectRelease();
    if (v9 >= v24[2])
      goto LABEL_29;
    v33 = (__int128 *)&v24[4 * v9 + 4];
    v45 = v33[1];
    v49 = *v33;
    swift_bridgeObjectRelease();
    *(_OWORD *)&dest.data = v49;
    *(_OWORD *)&dest.width = v45;
    if (v9 == v37)
      goto LABEL_30;
    v34 = *(_QWORD *)(v36 + 8 * v9);
    swift_bridgeObjectRetain();
    v8 = v20;
    vImageHistogramSpecification_PlanarF(&src, &dest, 0, (const vImagePixelCount *)(v34 + 32), v20, 0.0, 1.0, 0);
    result = swift_bridgeObjectRelease();
    ++v9;
  }
  while (v21 != 3);
  return result;
}

{
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  unint64_t v8;
  unint64_t v9;
  int64_t v10;
  _QWORD *data;
  unint64_t v12;
  uint64_t v13;
  __int128 *v14;
  __int128 v15;
  __int128 v16;
  unint64_t v17;
  unint64_t v18;
  _QWORD *v19;
  unint64_t v20;
  unint64_t v21;
  __int128 *v22;
  int64_t v23;
  _QWORD *v24;
  unint64_t v25;
  uint64_t v26;
  __int128 *v27;
  __int128 v28;
  __int128 v29;
  unint64_t v30;
  unint64_t v31;
  _QWORD *v32;
  __int128 *v33;
  uint64_t v34;
  uint64_t result;
  uint64_t v36;
  uint64_t v37;
  __int128 *v38;
  int64_t v39;
  __int128 *v40;
  int64_t v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v52;

  v52 = *MEMORY[0x1E0C80C00];
  v41 = a4[2];
  if (!v41)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v4 = a4[6];
  if (v4 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v5 = a4[5];
  if (v5 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v4)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (!v5)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v39 = a3[2];
  if (!v39)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  v6 = a3[6];
  if (v6 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v7 = a3[5];
  if (v7 < 1)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v4 != v6)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (v5 != v7)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v8 = a2;
  if ((a2 & 0x8000000000000000) != 0)
  {
LABEL_42:
    __break(1u);
LABEL_43:
    __break(1u);
  }
  if (HIDWORD(a2))
    goto LABEL_43;
  v9 = 0;
  v36 = a1 + 32;
  v37 = *(_QWORD *)(a1 + 16);
  v40 = (__int128 *)(a4 + 6);
  v38 = (__int128 *)(a3 + 6);
  do
  {
    src.data = (void *)MEMORY[0x1E0DEE9D8];
    swift_bridgeObjectRetain();
    v10 = v41;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v41, 0);
    data = src.data;
    v12 = *((_QWORD *)src.data + 2);
    v13 = 4 * v12;
    v14 = v40;
    do
    {
      v15 = *(v14 - 1);
      v16 = *v14;
      src.data = data;
      v17 = data[3];
      v18 = v12 + 1;
      if (v12 >= v17 >> 1)
      {
        v42 = v16;
        v46 = v15;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v17 > 1), v12 + 1, 1);
        v16 = v42;
        v15 = v46;
        data = src.data;
      }
      data[2] = v18;
      v19 = &data[v13];
      *((_OWORD *)v19 + 2) = v15;
      *((_OWORD *)v19 + 3) = v16;
      v13 += 4;
      v14 = (__int128 *)((char *)v14 + 40);
      v12 = v18;
      --v10;
    }
    while (v10);
    swift_bridgeObjectRelease();
    if (v9 >= data[2])
    {
      __break(1u);
LABEL_29:
      __break(1u);
LABEL_30:
      __break(1u);
      goto LABEL_31;
    }
    v20 = v8;
    v21 = v9 + 1;
    v22 = (__int128 *)&data[4 * v9 + 4];
    v43 = v22[1];
    v47 = *v22;
    swift_bridgeObjectRelease();
    *(_OWORD *)&src.data = v47;
    *(_OWORD *)&src.width = v43;
    dest.data = (void *)MEMORY[0x1E0DEE9D8];
    swift_bridgeObjectRetain();
    v23 = v39;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v39, 0);
    v24 = dest.data;
    v25 = *((_QWORD *)dest.data + 2);
    v26 = 4 * v25;
    v27 = v38;
    do
    {
      v28 = *(v27 - 1);
      v29 = *v27;
      dest.data = v24;
      v30 = v24[3];
      v31 = v25 + 1;
      if (v25 >= v30 >> 1)
      {
        v44 = v29;
        v48 = v28;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v30 > 1), v25 + 1, 1);
        v29 = v44;
        v28 = v48;
        v24 = dest.data;
      }
      v24[2] = v31;
      v32 = &v24[v26];
      *((_OWORD *)v32 + 2) = v28;
      *((_OWORD *)v32 + 3) = v29;
      v26 += 4;
      v27 = (__int128 *)((char *)v27 + 40);
      v25 = v31;
      --v23;
    }
    while (v23);
    swift_bridgeObjectRelease();
    if (v9 >= v24[2])
      goto LABEL_29;
    v33 = (__int128 *)&v24[4 * v9 + 4];
    v45 = v33[1];
    v49 = *v33;
    swift_bridgeObjectRelease();
    *(_OWORD *)&dest.data = v49;
    *(_OWORD *)&dest.width = v45;
    if (v9 == v37)
      goto LABEL_30;
    v34 = *(_QWORD *)(v36 + 8 * v9);
    swift_bridgeObjectRetain();
    v8 = v20;
    vImageHistogramSpecification_PlanarF(&src, &dest, 0, (const vImagePixelCount *)(v34 + 32), v20, 0.0, 1.0, 0);
    result = swift_bridgeObjectRelease();
    ++v9;
  }
  while (v21 != 4);
  return result;
}

uint64_t vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(uint64_t a1, unint64_t a2, void **a3, uint64_t a4, uint64_t a5)
{
  void **v5;
  void *v8;
  void *v9;
  __int128 v10;
  uint64_t v12;
  uint64_t result;
  uint64_t v14;
  uint64_t v15;
  unint64_t v16;
  uint64_t v17;
  __int128 v20;
  __int128 v21;
  _QWORD v22[2];
  vImage_Buffer v23;
  uint64_t v24;

  v24 = *MEMORY[0x1E0C80C00];
  v8 = *a3;
  v9 = *v5;
  v22[0] = *v5;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&v23);
  v10 = *(_OWORD *)&v23.data;
  vImage.PixelBuffer.size.getter(v22);
  swift_bridgeObjectRelease();
  if ((_QWORD)v10 != v22[0] || *((_QWORD *)&v10 + 1) != v22[1])
  {
LABEL_13:
    __break(1u);
LABEL_14:
    __break(1u);
  }
  v12 = *(_QWORD *)(a4 + 16);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 32))(v12, a5);
  if (result < 0)
    goto LABEL_14;
  v14 = result;
  if (result)
  {
    v15 = 0;
    v16 = 0;
    while (1)
    {
      v23.data = v9;
      v17 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v16 >= *(_QWORD *)(v17 + 16))
        break;
      v20 = *(_OWORD *)(v17 + v15 + 48);
      v21 = *(_OWORD *)(v17 + v15 + 32);
      swift_bridgeObjectRelease();
      *(_OWORD *)&v23.data = v21;
      *(_OWORD *)&v23.width = v20;
      result = closure #1 in vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(&v23, v8, v16, a1, a2, v12, a5);
      v15 += 32;
      if (v14 == ++v16)
        return result;
    }
    __break(1u);
    goto LABEL_13;
  }
  return result;
}

_QWORD *vImage.PixelBuffer<>.histogram(binCount:)@<X0>(unint64_t a1@<X0>, unint64_t *a2@<X8>)
{
  uint64_t *v2;
  _QWORD *result;
  unint64_t v6;
  unint64_t v7;
  unint64_t v8;
  unint64_t v9;
  unint64_t v10;

  result = specialized vImage.PixelBuffer<>._calculateHistogram(binCount:)(a1, *v2);
  v6 = result[2];
  if (!v6)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (v6 == 1)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (v6 < 3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (v6 != 3)
  {
    v7 = result[4];
    v8 = result[5];
    v9 = result[6];
    v10 = result[7];
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    result = (_QWORD *)swift_bridgeObjectRelease();
    *a2 = a1;
    a2[1] = v7;
    a2[2] = v8;
    a2[3] = v9;
    a2[4] = v10;
    return result;
  }
LABEL_9:
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(unint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, _QWORD **a6)
{
  _QWORD **v6;
  uint64_t inited;
  _QWORD *v14;
  _QWORD *v15;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<[UInt]>);
  inited = swift_initStackObject();
  *(_QWORD *)(inited + 32) = a2;
  *(_OWORD *)(inited + 16) = xmmword_1CAB5F170;
  *(_QWORD *)(inited + 40) = a3;
  *(_QWORD *)(inited + 48) = a4;
  *(_QWORD *)(inited + 56) = a5;
  v14 = *a6;
  v15 = *v6;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  specialized vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(inited, a1, v14, v15);
  swift_setDeallocating();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [UInt]);
  return swift_arrayDestroy();
}

uint64_t closure #1 in vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(const vImage_Buffer *a1, void *a2, unint64_t a3, uint64_t a4, unint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  __int128 v15;
  __int128 v16;
  vImage_Buffer dest;
  uint64_t v18;

  v18 = *MEMORY[0x1E0C80C00];
  dest.data = a2;
  type metadata accessor for vImage.PixelBuffer(0, a6, *(_QWORD *)(a7 + 8), a4);
  v11 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_8;
  }
  if (*(_QWORD *)(v11 + 16) <= a3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v12 = v11 + 32 * a3;
  v15 = *(_OWORD *)(v12 + 48);
  v16 = *(_OWORD *)(v12 + 32);
  swift_bridgeObjectRelease();
  *(_OWORD *)&dest.data = v16;
  *(_OWORD *)&dest.width = v15;
  if (*(_QWORD *)(a4 + 16) <= a3)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if ((a5 & 0x8000000000000000) != 0)
  {
LABEL_10:
    __break(1u);
LABEL_11:
    __break(1u);
  }
  if (HIDWORD(a5))
    goto LABEL_11;
  v13 = *(_QWORD *)(a4 + 8 * a3 + 32);
  swift_bridgeObjectRetain();
  vImageHistogramSpecification_PlanarF(a1, &dest, 0, (const vImagePixelCount *)(v13 + 32), a5, 0.0, 1.0, 0);
  return swift_bridgeObjectRelease();
}

void vImage.PixelBuffer.size.getter(_QWORD *a1@<X8>)
{
  uint64_t *v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  BOOL v5;

  v2 = *v1;
  if (!*(_QWORD *)(*v1 + 16))
  {
    __break(1u);
    goto LABEL_10;
  }
  v3 = *(_QWORD *)(v2 + 48);
  if (v3 < 0)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  v4 = *(_QWORD *)(v2 + 40);
  if (v4 < 0)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  if (v3)
    v5 = v4 == 0;
  else
    v5 = 1;
  if (!v5)
  {
    *a1 = v3;
    a1[1] = v4;
    return;
  }
LABEL_12:
  __break(1u);
}

BOOL static vImage.Size.== infix(_:_:)(_QWORD *a1, _QWORD *a2)
{
  return *a1 == *a2 && a1[1] == a2[1];
}

_QWORD *vImage.PixelBuffer<>.init(size:pixelFormat:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X2>, uint64_t a3@<X3>, uint64_t *a4@<X8>)
{
  void *v7;
  vImagePixelCount v8;
  uint64_t v9;
  uint64_t v10;
  void *v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  _QWORD *result;

  v7 = *(void **)a1;
  v8 = *(_QWORD *)(a1 + 8);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v9 = swift_allocObject();
  *(_OWORD *)(v9 + 16) = xmmword_1CAB5E430;
  v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  if (v10 < 1)
  {
    __break(1u);
  }
  else if (!HIDWORD(v10))
  {
    v11 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)((vImagePixelCount)v7, v8, v10);
    v13 = v12;
    v15 = v14;
    v17 = v16;
    type metadata accessor for vImage.BufferReference();
    result = (_QWORD *)swift_allocObject();
    result[2] = v11;
    result[3] = v13;
    result[4] = v15;
    result[5] = v17;
    *(_QWORD *)(v9 + 32) = v11;
    *(_QWORD *)(v9 + 40) = v13;
    *(_QWORD *)(v9 + 48) = v15;
    *(_QWORD *)(v9 + 56) = v17;
    *(_QWORD *)(v9 + 64) = result;
    *a4 = v9;
    return result;
  }
  __break(1u);

  result = (_QWORD *)_assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.vImageBuffer.getter()
{
  uint64_t v0;
  uint64_t result;

  if (*(_QWORD *)(*(_QWORD *)v0 + 16))
    return *(_QWORD *)(*(_QWORD *)v0 + 32);
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer.width.getter()
{
  uint64_t v0;
  uint64_t result;

  if (!*(_QWORD *)(*(_QWORD *)v0 + 16))
  {
    __break(1u);
    goto LABEL_5;
  }
  result = *(_QWORD *)(*(_QWORD *)v0 + 48);
  if (result < 0)
LABEL_5:
    __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer.height.getter()
{
  uint64_t v0;
  uint64_t result;

  if (!*(_QWORD *)(*(_QWORD *)v0 + 16))
  {
    __break(1u);
    goto LABEL_5;
  }
  result = *(_QWORD *)(*(_QWORD *)v0 + 40);
  if (result < 0)
LABEL_5:
    __break(1u);
  return result;
}

uint64_t _ss17withUnsafePointer2to_q0_x_q0_SPyxGq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v8;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  char *v14;
  uint64_t (*v15)(uint64_t, char *);
  uint64_t result;
  uint64_t v17;

  v11 = *(_QWORD *)(a5 - 8);
  v12 = MEMORY[0x1E0C80A78](a1);
  v14 = (char *)&v17 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  result = v15(v12, v14);
  if (v8)
    return (*(uint64_t (**)(uint64_t, char *, uint64_t))(v11 + 32))(a8, v14, a5);
  return result;
}

uint64_t vImage.PixelBuffer<>.vImageBuffers.getter()
{
  uint64_t *v0;
  uint64_t v1;
  int64_t v2;
  uint64_t v3;
  unint64_t v4;
  uint64_t v5;
  __int128 *v6;
  __int128 v7;
  __int128 v8;
  unint64_t v9;
  unint64_t v10;
  uint64_t v11;
  __int128 v13;
  __int128 v14;
  uint64_t v15;

  v1 = *v0;
  v2 = *(_QWORD *)(*v0 + 16);
  v3 = MEMORY[0x1E0DEE9D8];
  if (v2)
  {
    v15 = MEMORY[0x1E0DEE9D8];
    swift_bridgeObjectRetain();
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v2, 0);
    v3 = v15;
    v4 = *(_QWORD *)(v15 + 16);
    v5 = 32 * v4;
    v6 = (__int128 *)(v1 + 48);
    do
    {
      v7 = *(v6 - 1);
      v8 = *v6;
      v9 = *(_QWORD *)(v15 + 24);
      v10 = v4 + 1;
      if (v4 >= v9 >> 1)
      {
        v13 = *v6;
        v14 = *(v6 - 1);
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v9 > 1), v4 + 1, 1);
        v8 = v13;
        v7 = v14;
      }
      *(_QWORD *)(v15 + 16) = v10;
      v11 = v15 + v5;
      *(_OWORD *)(v11 + 32) = v7;
      *(_OWORD *)(v11 + 48) = v8;
      v5 += 32;
      v6 = (__int128 *)((char *)v6 + 40);
      v4 = v10;
      --v2;
    }
    while (v2);
    swift_bridgeObjectRelease();
  }
  return v3;
}

uint64_t vImage.PixelBuffer<>.withUnsafePixelBuffers<A>(_:)(void (*a1)(void), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  vImage.PixelBuffer<>.pixelBuffers.getter(a3, a5);
  a1();
  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.init(data:width:height:byteCountPerRow:pixelFormat:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t *a5@<X8>)
{
  uint64_t result;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  result = swift_allocObject();
  *(_OWORD *)(result + 16) = xmmword_1CAB5E430;
  if ((a3 | a2) < 0)
  {
    __break(1u);
  }
  else
  {
    *(_QWORD *)(result + 32) = a1;
    *(_QWORD *)(result + 40) = a3;
    *(_QWORD *)(result + 48) = a2;
    *(_QWORD *)(result + 56) = a4;
    *(_QWORD *)(result + 64) = 0;
    *a5 = result;
  }
  return result;
}

uint64_t vImage.PixelBuffer<>.withUnsafePointerToVImageBuffer<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t *v4;
  uint64_t v5;
  __int128 v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  _QWORD v17[9];
  uint64_t v18;
  _OWORD v19[2];
  uint64_t v20;

  v20 = *MEMORY[0x1E0C80C00];
  v5 = *v4;
  if (!*(_QWORD *)(*v4 + 16))
    __break(1u);
  v7 = *(_OWORD *)(v5 + 48);
  v19[0] = *(_OWORD *)(v5 + 32);
  v19[1] = v7;
  v8 = MEMORY[0x1E0C80A78](a1);
  v17[2] = *(_QWORD *)(v9 + 16);
  v17[3] = v10;
  v17[4] = v11;
  v17[5] = v8;
  v17[6] = v12;
  type metadata accessor for vImage_Buffer(0);
  v14 = v13;
  v15 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Error);
  return _ss17withUnsafePointer2to_q0_x_q0_SPyxGq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF((uint64_t)v19, (uint64_t)partial apply for closure #1 in vImage.PixelBuffer<>.withUnsafePointerToVImageBuffer<A>(_:), (uint64_t)v17, v14, v15, a4, MEMORY[0x1E0DEDB38], (uint64_t)&v18);
}

uint64_t vImage.Size.width.getter()
{
  uint64_t v0;

  return *(_QWORD *)v0;
}

uint64_t vImage.Size.height.getter()
{
  uint64_t v0;

  return *(_QWORD *)(v0 + 8);
}

unint64_t vImage.PixelBuffer<>.subscript.getter@<X0>(unint64_t result@<X0>, uint64_t *a2@<X8>)
{
  uint64_t v2;
  uint64_t v4;
  __int128 v5;
  __int128 v6;
  uint64_t v7;
  __int128 v8;
  __int128 v9;
  uint64_t v10;
  _BYTE v11[8];

  if ((result & 0x8000000000000000) != 0)
  {
    __break(1u);
  }
  else if (*(_QWORD *)(*(_QWORD *)v2 + 16) > result)
  {
    v4 = *(_QWORD *)v2 + 40 * result;
    v5 = *(_OWORD *)(v4 + 32);
    v6 = *(_OWORD *)(v4 + 48);
    v10 = *(_QWORD *)(v4 + 64);
    v8 = v5;
    v9 = v6;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    v7 = swift_allocObject();
    *(_OWORD *)(v7 + 16) = xmmword_1CAB5E430;
    *(_OWORD *)(v7 + 32) = v8;
    *(_OWORD *)(v7 + 48) = v9;
    *(_QWORD *)(v7 + 64) = v10;
    *a2 = v7;
    outlined init with take of vImage.BufferReference?((uint64_t)&v10, (uint64_t)v11);
    return outlined retain of vImage.BufferReference?((uint64_t)v11);
  }
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.count.getter(uint64_t a1, uint64_t a2)
{
  uint64_t *v2;
  uint64_t v4;
  uint64_t result;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;

  v4 = *v2;
  result = vImage.PixelBuffer<>.rowStride.getter(a1, a2);
  if (!*(_QWORD *)(v4 + 16))
  {
    __break(1u);
    goto LABEL_7;
  }
  v6 = *(_QWORD *)(v4 + 40);
  if (v6 < 0)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  v7 = result * v6;
  if ((unsigned __int128)(result * (__int128)v6) >> 64 != (result * v6) >> 63)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v8 = (*(uint64_t (**)(void))(a2 + 24))();
  result = v7 * v8;
  if ((unsigned __int128)(v7 * (__int128)v8) >> 64 != (v7 * v8) >> 63)
LABEL_9:
    __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.rowStride.getter(uint64_t result, uint64_t a2)
{
  uint64_t v2;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;

  if (!*(_QWORD *)(*(_QWORD *)v2 + 16))
  {
    __break(1u);
    goto LABEL_14;
  }
  v4 = *(_QWORD *)(*(_QWORD *)v2 + 56);
  v5 = *(_QWORD *)(result + 16);
  result = swift_getAssociatedTypeWitness();
  v6 = *(_QWORD *)(*(_QWORD *)(result - 8) + 72);
  if (!v6)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  if (v4 == 0x8000000000000000 && v6 == -1)
    goto LABEL_16;
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a2 + 24))(v5, a2);
  if (!result)
  {
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (v4 / v6 != 0x8000000000000000 || result != -1)
    return v4 / v6 / result;
LABEL_17:
  __break(1u);
  return result;
}

uint64_t protocol witness for AccelerateMatrixBuffer.withUnsafeBufferPointer<A>(_:) in conformance <> vImage.PixelBuffer<A>(uint64_t (*a1)(uint64_t), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return vImage.PixelBuffer<>.withUnsafeBufferPointer<A>(_:)(a1, a2, a4, a4, *(_QWORD *)(a5 - 8));
}

uint64_t vImage.PixelBuffer<>.withUnsafeBufferPointer<A>(_:)(uint64_t (*a1)(uint64_t), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  _QWORD *v5;
  _QWORD *v9;
  uint64_t result;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;

  v9 = (_QWORD *)*v5;
  result = vImage.PixelBuffer<>.rowStride.getter(a3, a5);
  if (!v9[2])
  {
    __break(1u);
    goto LABEL_9;
  }
  v11 = v9[5];
  if (v11 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  v12 = result * v11;
  if ((unsigned __int128)(result * (__int128)v11) >> 64 != (result * v11) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  result = (*(uint64_t (**)(_QWORD, uint64_t))(a5 + 24))(*(_QWORD *)(a3 + 16), a5);
  if ((unsigned __int128)(v12 * (__int128)result) >> 64 != (v12 * result) >> 63)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  if (!v9[2])
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  if (v9[4])
  {
    swift_getAssociatedTypeWitness();
    v13 = UnsafeBufferPointer.init(start:count:)();
    return a1(v13);
  }
LABEL_13:
  __break(1u);
  return result;
}

uint64_t protocol witness for AccelerateMutableMatrixBuffer.withUnsafeMutableBufferPointer<A>(_:) in conformance <> vImage.PixelBuffer<A>(uint64_t (*a1)(_QWORD *), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return vImage.PixelBuffer<>.withUnsafeMutableBufferPointer<A>(_:)(a1, a2, a4, a4, *(_QWORD *)(a5 - 8));
}

uint64_t vImage.PixelBuffer<>.withUnsafeMutableBufferPointer<A>(_:)(uint64_t (*a1)(_QWORD *), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  _QWORD *v5;
  uint64_t result;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  _QWORD v13[2];

  v13[0] = *v5;
  result = vImage.PixelBuffer<>.rowStride.getter(a3, a5);
  if (!*(_QWORD *)(v13[0] + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  v10 = *(_QWORD *)(v13[0] + 40);
  if (v10 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  v11 = result * v10;
  if ((unsigned __int128)(result * (__int128)v10) >> 64 != (result * v10) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  result = (*(uint64_t (**)(_QWORD, uint64_t))(a5 + 24))(*(_QWORD *)(a3 + 16), a5);
  if ((unsigned __int128)(v11 * (__int128)result) >> 64 != (v11 * result) >> 63)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  if (!*(_QWORD *)(v13[0] + 16))
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  if (*(_QWORD *)(v13[0] + 32))
  {
    swift_getAssociatedTypeWitness();
    v13[0] = UnsafeMutableBufferPointer.init(start:count:)();
    v13[1] = v12;
    return a1(v13);
  }
LABEL_13:
  __break(1u);
  return result;
}

uint64_t vImage.Size.init(width:height:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X1>, uint64_t *a3@<X8>)
{
  if (result < 1 || a2 < 1)
  {
    __break(1u);
  }
  else
  {
    *a3 = result;
    a3[1] = a2;
  }
  return result;
}

uint64_t (*vImage.PixelBuffer<>.withUnsafeVImageBuffer<A>(_:)(uint64_t (*result)(_QWORD, _QWORD, _QWORD, _QWORD)))(_QWORD, _QWORD, _QWORD, _QWORD)
{
  uint64_t v1;

  if (*(_QWORD *)(*(_QWORD *)v1 + 16))
    return (uint64_t (*)(_QWORD, _QWORD, _QWORD, _QWORD))result(*(_QWORD *)(*(_QWORD *)v1 + 32), *(_QWORD *)(*(_QWORD *)v1 + 40), *(_QWORD *)(*(_QWORD *)v1 + 48), *(_QWORD *)(*(_QWORD *)v1 + 56));
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.withUnsafeVImageBuffers<A>(_:)(void (*a1)(void))
{
  vImage.PixelBuffer<>.vImageBuffers.getter();
  a1();
  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.pixelBuffers.getter(uint64_t a1, uint64_t a2)
{
  _QWORD *v2;
  uint64_t v3;
  uint64_t AssociatedTypeWitness;
  uint64_t AssociatedConformanceWitness;
  uint64_t v6;
  char *v7;
  unint64_t v8;
  uint64_t v9;
  uint64_t v10;
  _QWORD v12[6];

  v12[5] = *v2;
  v12[2] = *(_QWORD *)(a1 + 16);
  v12[3] = a2;
  swift_bridgeObjectRetain();
  v3 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [vImage.BufferWrapper]);
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  v7 = (char *)type metadata accessor for vImage.PixelBuffer(0, AssociatedTypeWitness, *(_QWORD *)(*(_QWORD *)(AssociatedConformanceWitness + 8) + 8), v6);
  v8 = lazy protocol witness table accessor for type [vImage.BufferWrapper] and conformance [A]();
  v10 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in vImage.PixelBuffer<>.pixelBuffers.getter, (uint64_t)v12, v3, v7, MEMORY[0x1E0DEDCE8], v8, MEMORY[0x1E0DEDD18], v9);
  swift_bridgeObjectRelease();
  return v10;
}

uint64_t vImage.PixelBuffer<>.withUnsafePixelBuffer<A>(at:_:)(unint64_t a1, void (*a2)(uint64_t *))
{
  uint64_t v4;
  uint64_t v5;

  vImage.PixelBuffer<>.subscript.getter(a1, &v5);
  v4 = v5;
  a2(&v4);
  return swift_bridgeObjectRelease();
}

uint64_t closure #1 in vImage.PixelBuffer<>.pixelBuffers.getter@<X0>(__int128 *a1@<X0>, uint64_t *a2@<X8>)
{
  uint64_t v4;
  __int128 v5;
  __int128 v6;
  uint64_t v8;
  _BYTE v9[8];

  v8 = *((_QWORD *)a1 + 4);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v4 = swift_allocObject();
  v5 = *a1;
  v6 = a1[1];
  *(_OWORD *)(v4 + 16) = xmmword_1CAB5E430;
  *(_OWORD *)(v4 + 32) = v5;
  *(_OWORD *)(v4 + 48) = v6;
  *(_QWORD *)(v4 + 64) = *((_QWORD *)a1 + 4);
  *a2 = v4;
  outlined init with take of vImage.BufferReference?((uint64_t)&v8, (uint64_t)v9);
  return outlined retain of vImage.BufferReference?((uint64_t)v9);
}

double vImage.PixelBuffer<>.init(bufferWrapper:pixelFormat:)@<D0>(__int128 *a1@<X0>, uint64_t *a2@<X8>)
{
  uint64_t v4;
  double result;
  __int128 v6;
  __int128 v7;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v4 = swift_allocObject();
  *(_QWORD *)&result = 1;
  v6 = *a1;
  v7 = a1[1];
  *(_OWORD *)(v4 + 16) = xmmword_1CAB5E430;
  *(_OWORD *)(v4 + 32) = v6;
  *(_OWORD *)(v4 + 48) = v7;
  *(_QWORD *)(v4 + 64) = *((_QWORD *)a1 + 4);
  *a2 = v4;
  return result;
}

uint64_t vImage.PixelBuffer<>.init(data:width:height:byteCountPerRow:pixelFormat:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, char a5@<W4>, uint64_t a6@<X6>, uint64_t a7@<X7>, uint64_t *a8@<X8>)
{
  uint64_t result;
  uint64_t v16;
  uint64_t v17;

  if ((a5 & 1) == 0)
    goto LABEL_4;
  result = swift_getAssociatedTypeWitness();
  v16 = *(_QWORD *)(*(_QWORD *)(result - 8) + 72);
  v17 = a2 * v16;
  if ((unsigned __int128)(a2 * (__int128)v16) >> 64 != (a2 * v16) >> 63)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 24))(a6, a7);
  a4 = v17 * result;
  if ((unsigned __int128)(v17 * (__int128)result) >> 64 == (v17 * result) >> 63)
  {
LABEL_4:
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    result = swift_allocObject();
    *(_OWORD *)(result + 16) = xmmword_1CAB5E430;
    if (((a3 | a2) & 0x8000000000000000) == 0)
    {
      *(_QWORD *)(result + 32) = a1;
      *(_QWORD *)(result + 40) = a3;
      *(_QWORD *)(result + 48) = a2;
      *(_QWORD *)(result + 56) = a4;
      *(_QWORD *)(result + 64) = 0;
      *a8 = result;
      return result;
    }
    __break(1u);
    goto LABEL_7;
  }
LABEL_8:
  __break(1u);
  return result;
}

int64_t vImage.PixelBuffer<>.init(size:pixelFormat:)@<X0>(vImagePixelCount *a1@<X0>, void *a2@<X2>, uint64_t a3@<X3>, _QWORD *a4@<X8>)
{
  vImagePixelCount v7;
  vImagePixelCount v8;
  int64_t result;
  int64_t v10;
  uint64_t v11;
  __int128 v12;
  __int128 v13;
  uint64_t v14;
  unint64_t v15;
  unint64_t v16;
  uint64_t v17;
  __int128 v18;
  __int128 v19;
  _QWORD *v20;
  __int128 v21;
  __int128 v22;
  uint64_t v23;
  uint64_t v24;

  v7 = *a1;
  v8 = a1[1];
  result = (*(uint64_t (**)(void *, uint64_t))(a3 + 32))(a2, a3);
  if ((result & 0x8000000000000000) == 0)
  {
    v10 = result;
    v11 = MEMORY[0x1E0DEE9D8];
    if (!result)
    {
LABEL_9:
      *a4 = v11;
      return result;
    }
    v20 = a4;
    v24 = MEMORY[0x1E0DEE9D8];
    result = (int64_t)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, result, 0);
    v11 = v24;
    while (v10)
    {
      result = (int64_t)closure #1 in vImage.PixelBuffer<>.init(size:pixelFormat:)(v7, v8, a2, a3, &v21);
      v12 = v21;
      v13 = v22;
      v14 = v23;
      v24 = v11;
      v16 = *(_QWORD *)(v11 + 16);
      v15 = *(_QWORD *)(v11 + 24);
      if (v16 >= v15 >> 1)
      {
        v18 = v22;
        v19 = v21;
        result = (int64_t)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v15 > 1), v16 + 1, 1);
        v13 = v18;
        v12 = v19;
        v11 = v24;
      }
      *(_QWORD *)(v11 + 16) = v16 + 1;
      v17 = v11 + 40 * v16;
      *(_OWORD *)(v17 + 32) = v12;
      *(_OWORD *)(v17 + 48) = v13;
      *(_QWORD *)(v17 + 64) = v14;
      if (!--v10)
      {
        a4 = v20;
        goto LABEL_9;
      }
    }
    __break(1u);
  }
  __break(1u);
  return result;
}

_QWORD *closure #1 in vImage.PixelBuffer<>.init(size:pixelFormat:)@<X0>(vImagePixelCount a1@<X1>, vImagePixelCount a2@<X2>, void *a3@<X3>, uint64_t a4@<X4>, _QWORD *a5@<X8>)
{
  void *v5;
  void *v9;
  uint64_t v10;
  void *v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  void *v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  _QWORD *result;

  v9 = a3;
  v10 = (*(uint64_t (**)(void *, uint64_t))(a4 + 40))(a3, a4);
  if (v10 < 1)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (HIDWORD(v10))
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  v11 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(a1, a2, v10);
  v9 = v5;
  if (!v5)
  {
    v15 = v11;
    v16 = v12;
    v17 = v13;
    v18 = v14;
    type metadata accessor for vImage.BufferReference();
    result = (_QWORD *)swift_allocObject();
    result[2] = v15;
    result[3] = v16;
    result[4] = v17;
    result[5] = v18;
    *a5 = v15;
    a5[1] = v16;
    a5[2] = v17;
    a5[3] = v18;
    a5[4] = result;
    return result;
  }
LABEL_7:

  result = (_QWORD *)_assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X2>, uint64_t a3@<X3>, char **a4@<X8>)
{
  uint64_t AssociatedTypeWitness;
  uint64_t AssociatedConformanceWitness;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t (*v13)(uint64_t, uint64_t);
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  char *v28;
  uint64_t result;
  uint64_t v30;
  _QWORD v31[2];
  uint64_t v32;
  uint64_t v33;
  char **v34;
  uint64_t (*v35)(uint64_t, uint64_t);
  uint64_t v36;

  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  v11 = type metadata accessor for vImage.PixelBuffer(0, AssociatedTypeWitness, *(_QWORD *)(*(_QWORD *)(AssociatedConformanceWitness + 8) + 8), v10);
  v12 = MEMORY[0x1D1794114](a1, v11);
  v13 = *(uint64_t (**)(uint64_t, uint64_t))(a3 + 32);
  v14 = v13(a2, a3);
  if (v12 != v14)
  {
    __break(1u);
    goto LABEL_7;
  }
  v34 = a4;
  v35 = v13;
  v36 = a1;
  MEMORY[0x1E0C80A78](v14);
  v32 = a2;
  v33 = a3;
  v15 = type metadata accessor for Array();
  v16 = MEMORY[0x1D1794D08](MEMORY[0x1E0DEAF50], v15);
  v18 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:), (uint64_t)v31, v15, MEMORY[0x1E0DEB418], MEMORY[0x1E0DEDCE8], v16, MEMORY[0x1E0DEDD18], v17);
  v19 = specialized Set.init<A>(_:)(v18);
  v20 = swift_bridgeObjectRelease();
  v36 = a1;
  MEMORY[0x1E0C80A78](v20);
  v32 = a2;
  v33 = a3;
  v22 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:), (uint64_t)v31, v15, MEMORY[0x1E0DEB418], MEMORY[0x1E0DEDCE8], v16, MEMORY[0x1E0DEDD18], v21);
  v23 = specialized Set.init<A>(_:)(v22);
  swift_bridgeObjectRelease();
  v24 = *(_QWORD *)(v19 + 16);
  swift_bridgeObjectRelease();
  if (v24 == 1)
  {
    v25 = *(_QWORD *)(v23 + 16);
    swift_bridgeObjectRelease();
    if (v25 == 1)
    {
      v26 = v35(a2, a3);
      if ((v26 & 0x8000000000000000) == 0)
      {
        MEMORY[0x1E0C80A78](v26);
        v31[0] = a2;
        v31[1] = a3;
        v32 = a1;
        v28 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO13BufferWrapperVs5NeverOTg5((char *)partial apply for closure #3 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:), (uint64_t)&v30, 0, v27);
        result = swift_bridgeObjectRelease();
        *v34 = v28;
        return result;
      }
      goto LABEL_8;
    }
LABEL_7:
    __break(1u);
LABEL_8:
    __break(1u);
  }
  result = swift_bridgeObjectRelease();
  __break(1u);
  return result;
}

_QWORD *closure #3 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)@<X0>(uint64_t a1@<X2>, uint64_t a2@<X3>, uint64_t a3@<X8>)
{
  uint64_t AssociatedTypeWitness;
  uint64_t AssociatedConformanceWitness;
  uint64_t v8;
  _QWORD *result;
  void *v10;
  vImagePixelCount v11;
  vImagePixelCount v12;
  size_t v13;
  unint64_t v14;
  __int128 v15;
  _QWORD *v16;
  _OWORD v17[2];
  uint64_t v18;

  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  type metadata accessor for vImage.PixelBuffer(0, AssociatedTypeWitness, *(_QWORD *)(*(_QWORD *)(AssociatedConformanceWitness + 8) + 8), v8);
  Array.subscript.getter();
  result = v16;
  if (v16[2])
  {
    v10 = (void *)v16[4];
    v11 = v16[5];
    v12 = v16[6];
    v13 = v16[7];
    swift_bridgeObjectRelease();
    v14 = (*(uint64_t (**)(uint64_t, uint64_t))(a2 + 40))(a1, a2);
    result = specialized vImage.BufferWrapper.init(copying:bitsPerPixel:)(v10, v11, v12, v13, v14, v17);
    v15 = v17[1];
    *(_OWORD *)a3 = v17[0];
    *(_OWORD *)(a3 + 16) = v15;
    *(_QWORD *)(a3 + 32) = v18;
  }
  else
  {
    __break(1u);
  }
  return result;
}

_QWORD *vImage.PixelBuffer<>.init(width:height:pixelFormat:)@<X0>(int64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X3>, uint64_t a4@<X4>, uint64_t *a5@<X8>)
{
  void *v5;
  uint64_t v10;
  uint64_t v11;
  void *v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  _QWORD *result;

  if (a1 < 1 || (v5 = (void *)a2, a2 < 1))
  {
    __break(1u);
    goto LABEL_7;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v10 = swift_allocObject();
  *(_OWORD *)(v10 + 16) = xmmword_1CAB5E430;
  v11 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  if (v11 < 1)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (!HIDWORD(v11))
  {
    v12 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(a1, (vImagePixelCount)v5, v11);
    v14 = v13;
    v16 = v15;
    v18 = v17;
    type metadata accessor for vImage.BufferReference();
    result = (_QWORD *)swift_allocObject();
    result[2] = v12;
    result[3] = v14;
    result[4] = v16;
    result[5] = v18;
    *(_QWORD *)(v10 + 32) = v12;
    *(_QWORD *)(v10 + 40) = v14;
    *(_QWORD *)(v10 + 48) = v16;
    *(_QWORD *)(v10 + 56) = v18;
    *(_QWORD *)(v10 + 64) = result;
    *a5 = v10;
    return result;
  }
LABEL_8:
  __break(1u);

  result = (_QWORD *)_assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.channelCount.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 24))();
}

uint64_t vImage.PixelBuffer<>.byteCountPerPixel.getter(uint64_t a1, uint64_t a2)
{
  return specialized vImage.PixelBuffer<>.byteCountPerPixel.getter(*(_QWORD *)(a1 + 16), a2);
}

vImage_Error vImage.PixelBuffer<>.copy(to:)(uint64_t *a1, uint64_t a2, uint64_t a3)
{
  uint64_t *v3;
  uint64_t v5;
  uint64_t v6;
  __int128 v7;
  vImage_Buffer v9;
  uint64_t v10;

  v10 = *MEMORY[0x1E0C80C00];
  v5 = *v3;
  if (!*(_QWORD *)(*v3 + 16))
    __break(1u);
  v6 = *a1;
  v7 = *(_OWORD *)(v5 + 48);
  *(_OWORD *)&v9.data = *(_OWORD *)(v5 + 32);
  *(_OWORD *)&v9.width = v7;
  return closure #1 in vImage.PixelBuffer<>.copy(to:)(&v9, v6, v5, *(_QWORD *)(a2 + 16), a3);
}

vImage_Error closure #1 in vImage.PixelBuffer<>.copy(to:)(const vImage_Buffer *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  __int128 v6;
  uint64_t v7;
  vImage_Buffer v9;
  uint64_t v10;

  v10 = *MEMORY[0x1E0C80C00];
  if (!*(_QWORD *)(a2 + 16))
    __break(1u);
  v6 = *(_OWORD *)(a2 + 48);
  *(_OWORD *)&v9.data = *(_OWORD *)(a2 + 32);
  *(_OWORD *)&v9.width = v6;
  v7 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  return vImageCopyBuffer(a1, &v9, v7 / 8, 0);
}

uint64_t vImage.PixelBuffer<>.cropped(to:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X1>, uint64_t *a3@<X8>, double a4@<D0>)
{
  uint64_t *v7;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v13;
  CGFloat v14;
  CGFloat v15;
  double x;
  double y;
  double height;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  char v23;
  CGRect v24;
  CGRect v25;
  CGRect v26;

  v9 = *v7;
  if (!*(_QWORD *)(*v7 + 16))
  {
    __break(1u);
    goto LABEL_12;
  }
  v10 = *(_QWORD *)(v9 + 48);
  if (v10 < 0)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v11 = *(_QWORD *)(v9 + 40);
  if (v11 < 0)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  v13 = result;
  v14 = (double)v10;
  v15 = (double)v11;
  v26 = CGRectIntegral(*(CGRect *)&a4);
  v24.origin.x = 0.0;
  v24.origin.y = 0.0;
  v24.size.width = v14;
  v24.size.height = v15;
  v25 = CGRectIntersection(v24, v26);
  x = v25.origin.x;
  y = v25.origin.y;
  height = v25.size.height;
  result = specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&v21, v25.size.width);
  if ((result & 1) == 0 || (v22 & 1) != 0)
    goto LABEL_16;
  v19 = v21;
  if (v21 >= 1)
  {
    result = specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&v21, height);
    if ((result & 1) == 0 || v22 == 1)
      goto LABEL_17;
    v20 = v21;
    if (v21 >= 1)
    {
      v21 = v19;
      v22 = v20;
      v23 = 0;
      vImage.PixelBuffer<>.init(size:pixelFormat:)((uint64_t)&v21, *(_QWORD *)(v13 + 16), a2, a3);
      return vImage.PixelBuffer<>.crop(at:destination:)(a3, v13, a2, x, y);
    }
    goto LABEL_15;
  }
LABEL_14:
  __break(1u);
LABEL_15:
  __break(1u);
LABEL_16:
  __break(1u);
LABEL_17:
  __break(1u);
  return result;
}

BOOL vImage.Size.init(exactly:)@<W0>(uint64_t a1@<X8>, double a2@<D0>, double a3@<D1>)
{
  _BOOL8 result;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  char v9;

  result = specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&v8, a2);
  if (!result
    || v9 == 1
    || (v6 = v8, v8 < 1)
    || !(result = specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&v8, a3))
    || v9 == 1
    || (v7 = v8, v8 < 1))
  {
    *(_QWORD *)a1 = 0;
    *(_QWORD *)(a1 + 8) = 0;
    *(_BYTE *)(a1 + 16) = 1;
  }
  else
  {
    *(_QWORD *)a1 = v6;
    *(_QWORD *)(a1 + 8) = v7;
    *(_BYTE *)(a1 + 16) = 0;
  }
  return result;
}

uint64_t vImage.PixelBuffer<>.crop(at:destination:)(uint64_t *a1, uint64_t a2, uint64_t a3, double a4, double a5)
{
  uint64_t v5;
  uint64_t v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  CGFloat v16;
  CGFloat v17;
  double v18;
  double v19;
  double x;
  double y;
  CGFloat width;
  CGFloat height;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  BOOL v29;
  uint64_t v30;
  uint64_t v31;
  double v32;
  double v33;
  __int128 v34;
  uint64_t v36;
  vImage_Buffer v37;
  uint64_t v38;
  CGRect v39;
  CGRect v40;
  CGRect v41;
  CGRect v42;
  CGRect v43;

  v6 = v5;
  v38 = *MEMORY[0x1E0C80C00];
  v7 = *(_QWORD **)v5;
  if (!*(_QWORD *)(*(_QWORD *)v5 + 16))
  {
    __break(1u);
    goto LABEL_30;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v11 = *a1;
  if (!*(_QWORD *)(*a1 + 16))
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v12 = *(_QWORD *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  v13 = *(_QWORD *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v16 = (double)v8;
  v17 = (double)v9;
  v18 = (double)v12;
  v19 = (double)v13;
  v43 = CGRectIntegral(*(CGRect *)&a4);
  v39.origin.x = 0.0;
  v39.origin.y = 0.0;
  v39.size.width = v16;
  v39.size.height = v17;
  v40 = CGRectIntersection(v39, v43);
  x = v40.origin.x;
  y = v40.origin.y;
  width = v40.size.width;
  height = v40.size.height;
  if (CGRectIsEmpty(v40))
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if ((~*(_QWORD *)&y & 0x7FF0000000000000) == 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (y <= -9.22337204e18)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (y >= 9.22337204e18)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v7[2])
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v24 = v7[7];
  v25 = (uint64_t)y * v24;
  if ((unsigned __int128)((uint64_t)y * (__int128)v24) >> 64 != v25 >> 63)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if ((~*(_QWORD *)&x & 0x7FF0000000000000) == 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (x <= -9.22337204e18)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (x >= 9.22337204e18)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  v26 = *(_QWORD *)(a2 + 16);
  v27 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(v26, a3);
  v28 = (uint64_t)x * (v27 / 8);
  if ((unsigned __int128)((uint64_t)x * (__int128)(v27 / 8)) >> 64 != v28 >> 63)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  v29 = __OFADD__(v25, v28);
  v30 = v25 + v28;
  if (v29)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7[2])
  {
LABEL_46:
    __break(1u);
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  v31 = v7[4];
  if (!v31)
    goto LABEL_55;
  v41.origin.x = x;
  v41.origin.y = y;
  v41.size.width = width;
  v41.size.height = height;
  v32 = CGRectGetWidth(v41);
  if ((~*(_QWORD *)&v32 & 0x7FF0000000000000) == 0)
    goto LABEL_47;
  if (v32 <= -9.22337204e18)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (v32 >= 9.22337204e18)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  v42.origin.x = x;
  v42.origin.y = y;
  v42.size.width = width;
  v42.size.height = height;
  v33 = CGRectGetHeight(v42);
  if ((~*(_QWORD *)&v33 & 0x7FF0000000000000) == 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (v33 <= -9.22337204e18)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v33 >= 9.22337204e18)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v7[2])
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  vImage.PixelBuffer<>.init(data:width:height:byteCountPerRow:pixelFormat:)(v31 + v30, (uint64_t)v32, (uint64_t)v33, v7[7], &v36);
  if (!*(_QWORD *)(v36 + 16))
  {
LABEL_54:
    __break(1u);
LABEL_55:
    __break(1u);
  }
  v34 = *(_OWORD *)(v36 + 48);
  *(_OWORD *)&v37.data = *(_OWORD *)(v36 + 32);
  *(_OWORD *)&v37.width = v34;
  closure #1 in vImage.PixelBuffer<>.crop(at:destination:)(&v37, a1, v6, v26, a3);
  return swift_bridgeObjectRelease();
}

vImage_Error closure #1 in vImage.PixelBuffer<>.crop(at:destination:)(const vImage_Buffer *a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v5;
  __int128 v7;
  uint64_t v8;
  vImage_Buffer v10;
  uint64_t v11;

  v11 = *MEMORY[0x1E0C80C00];
  v5 = *a2;
  if (!*(_QWORD *)(*a2 + 16))
    __break(1u);
  v7 = *(_OWORD *)(v5 + 48);
  *(_OWORD *)&v10.data = *(_OWORD *)(v5 + 32);
  *(_OWORD *)&v10.width = v7;
  v8 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  return vImageCopyBuffer(a1, &v10, v8 / 8, 0);
}

void vImage.PixelBuffer<>.withUnsafeRegionOfInterest<A>(_:_:)(void (*a1)(uint64_t *), double a2, double a3, double a4, double a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  uint64_t v9;
  _QWORD *v10;
  uint64_t v11;
  uint64_t v12;
  CGFloat v15;
  CGFloat v16;
  double x;
  double y;
  CGFloat width;
  CGFloat height;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  double v27;
  double v28;
  uint64_t v29;
  CGRect v30;
  CGRect v31;
  CGRect v32;
  CGRect v33;
  CGRect v34;

  v10 = *(_QWORD **)v9;
  if (!*(_QWORD *)(*(_QWORD *)v9 + 16))
  {
    __break(1u);
    goto LABEL_26;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v15 = (double)v11;
  v16 = (double)v12;
  v34 = CGRectIntegral(*(CGRect *)&a2);
  v30.origin.x = 0.0;
  v30.origin.y = 0.0;
  v30.size.width = v15;
  v30.size.height = v16;
  v31 = CGRectIntersection(v30, v34);
  x = v31.origin.x;
  y = v31.origin.y;
  width = v31.size.width;
  height = v31.size.height;
  if (CGRectIsEmpty(v31))
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if ((~*(_QWORD *)&y & 0x7FF0000000000000) == 0)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (y <= -9.22337204e18)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (y >= 9.22337204e18)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v10[2])
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v21 = v10[7];
  v22 = (uint64_t)y * v21;
  if ((unsigned __int128)((uint64_t)y * (__int128)v21) >> 64 != v22 >> 63)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if ((~*(_QWORD *)&x & 0x7FF0000000000000) == 0)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (x <= -9.22337204e18)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (x >= 9.22337204e18)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  v23 = (*(uint64_t (**)(void))(a9 + 16))();
  v24 = (uint64_t)x * (v23 / 8);
  if ((unsigned __int128)((uint64_t)x * (__int128)(v23 / 8)) >> 64 != v24 >> 63)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v25 = v22 + v24;
  if (__OFADD__(v22, v24))
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v10[2])
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v26 = v10[4];
  if (v26)
  {
    v32.origin.x = x;
    v32.origin.y = y;
    v32.size.width = width;
    v32.size.height = height;
    v27 = CGRectGetWidth(v32);
    if ((~*(_QWORD *)&v27 & 0x7FF0000000000000) != 0)
    {
      if (v27 > -9.22337204e18)
      {
        if (v27 < 9.22337204e18)
        {
          v33.origin.x = x;
          v33.origin.y = y;
          v33.size.width = width;
          v33.size.height = height;
          v28 = CGRectGetHeight(v33);
          if ((~*(_QWORD *)&v28 & 0x7FF0000000000000) != 0)
          {
            if (v28 > -9.22337204e18)
            {
              if (v28 < 9.22337204e18)
              {
                if (v10[2])
                {
                  vImage.PixelBuffer<>.init(data:width:height:byteCountPerRow:pixelFormat:)(v26 + v25, (uint64_t)v27, (uint64_t)v28, v10[7], &v29);
                  a1(&v29);
                  swift_bridgeObjectRelease();
                  return;
                }
                goto LABEL_46;
              }
LABEL_45:
              __break(1u);
LABEL_46:
              __break(1u);
              goto LABEL_47;
            }
LABEL_44:
            __break(1u);
            goto LABEL_45;
          }
LABEL_43:
          __break(1u);
          goto LABEL_44;
        }
LABEL_42:
        __break(1u);
        goto LABEL_43;
      }
LABEL_41:
      __break(1u);
      goto LABEL_42;
    }
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
LABEL_47:
  __break(1u);
}

double static vImage.PixelBuffer<>.makeDynamicPixelBufferAndCGImageFormat(cgImage:)@<D0>(uint64_t *a1@<X0>, void *a2@<X1>, uint64_t a3@<X8>)
{
  uint64_t v3;
  CGImage *v6;
  double result;
  uint64_t v8;
  __int128 v9;
  __int128 v10;
  CGColorSpaceRef colorSpace;
  const CGFloat *decode;
  CGColorRenderingIntent renderingIntent;
  vImage_CGImageFormat v14;
  _OWORD v15[2];
  uint64_t v16;
  uint64_t v17;

  v17 = *MEMORY[0x1E0C80C00];
  memset(&v14, 0, 36);
  v6 = a2;
  specialized vImage.BufferWrapper.init(cgImage:format:)(v6, &v14, v15);

  if (!v3)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    v8 = swift_allocObject();
    v9 = v15[0];
    v10 = v15[1];
    *(_OWORD *)(v8 + 16) = xmmword_1CAB5E430;
    *(_OWORD *)(v8 + 32) = v9;
    *(_OWORD *)(v8 + 48) = v10;
    *(_QWORD *)(v8 + 64) = v16;
    *a1 = v8;
    colorSpace = v14.colorSpace;
    decode = v14.decode;
    renderingIntent = v14.renderingIntent;
    result = *(double *)&v14.bitsPerComponent;
    *(_QWORD *)&v9 = *(_QWORD *)&v14.bitmapInfo;
    *(_QWORD *)a3 = *(_QWORD *)&v14.bitsPerComponent;
    *(_QWORD *)(a3 + 8) = colorSpace;
    *(_QWORD *)(a3 + 16) = v9;
    *(_QWORD *)(a3 + 24) = decode;
    *(_DWORD *)(a3 + 32) = renderingIntent;
  }
  return result;
}

double static vImage.PixelBuffer<>.makePixelBufferAndCGImageFormat(cgImage:pixelFormat:)@<D0>(_QWORD *a1@<X0>, void *a2@<X1>, uint64_t a3@<X3>, uint64_t a4@<X4>, size_t a5@<X5>, uint64_t a6@<X8>)
{
  uint64_t v6;
  double result;
  CGColorSpaceRef colorSpace;
  const CGFloat *decode;
  CGColorRenderingIntent renderingIntent;
  uint64_t v13;
  uint64_t v14;
  vImage_CGImageFormat v15;
  uint64_t v16;

  v16 = *MEMORY[0x1E0C80C00];
  memset(&v15, 0, 36);
  vImage.PixelBuffer<>.init(cgImage:cgImageFormat:pixelFormat:)((CGImage *)a2, &v15, a3, a4, a5, &v14);
  if (!v6)
  {
    *a1 = v14;
    colorSpace = v15.colorSpace;
    decode = v15.decode;
    renderingIntent = v15.renderingIntent;
    result = *(double *)&v15.bitsPerComponent;
    v13 = *(_QWORD *)&v15.bitmapInfo;
    *(_QWORD *)a6 = *(_QWORD *)&v15.bitsPerComponent;
    *(_QWORD *)(a6 + 8) = colorSpace;
    *(_QWORD *)(a6 + 16) = v13;
    *(_QWORD *)(a6 + 24) = decode;
    *(_DWORD *)(a6 + 32) = renderingIntent;
  }
  return result;
}

void vImage.PixelBuffer<>.init(cgImage:cgImageFormat:pixelFormat:)(CGImage *a1@<X0>, vImage_CGImageFormat *a2@<X1>, uint64_t a3@<X3>, uint64_t a4@<X4>, size_t a5@<X5>, uint64_t *a6@<X8>)
{
  uint64_t v6;
  size_t bitsPerComponent;
  uint64_t bitsPerPixel;
  CGColorSpace *colorSpace;
  const CGFloat *decode;
  CGColorRenderingIntent renderingIntent;
  CGImage *v18;
  uint64_t v19;
  __int128 v20;
  __int128 v21;
  vImage_CGImageFormat v22;
  vImage_CGImageFormat f1;
  _OWORD v24[2];
  uint64_t v25;
  uint64_t v26;

  v26 = *MEMORY[0x1E0C80C00];
  bitsPerComponent = a2->bitsPerComponent;
  bitsPerPixel = a2->bitsPerPixel;
  colorSpace = a2->colorSpace;
  decode = a2->decode;
  renderingIntent = a2->renderingIntent;
  f1.bitsPerComponent = a2->bitsPerComponent;
  f1.bitsPerPixel = bitsPerPixel;
  f1.colorSpace = colorSpace;
  *(_QWORD *)&f1.bitmapInfo = *(_QWORD *)&a2->bitmapInfo;
  f1.decode = decode;
  f1.renderingIntent = renderingIntent;
  memset(&v22, 0, 36);
  if (vImageCGImageFormat_IsEqual(&f1, &v22))
  {
    bitsPerComponent = CGImageGetBitsPerPixel(a1);
    if (bitsPerComponent != (*(uint64_t (**)(uint64_t, size_t))(a5 + 16))(a3, a5))
    {
      __break(1u);
      goto LABEL_12;
    }
    a5 = CGImageGetBitsPerComponent(a1);
    if (a5 == (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4))
      goto LABEL_7;
    __break(1u);
  }
  if ((*(uint64_t (**)(uint64_t, size_t))(a5 + 16))(a3, a5) != bitsPerPixel)
  {
LABEL_12:
    __break(1u);
LABEL_13:
    __break(1u);
  }
  if ((*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4) != bitsPerComponent)
    goto LABEL_13;
LABEL_7:
  v18 = a1;
  specialized vImage.BufferWrapper.init(cgImage:format:)(v18, a2, v24);

  if (v6)
  {

  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    v19 = swift_allocObject();
    v20 = v24[0];
    v21 = v24[1];
    *(_OWORD *)(v19 + 16) = xmmword_1CAB5E430;
    *(_OWORD *)(v19 + 32) = v20;
    *(_OWORD *)(v19 + 48) = v21;
    *(_QWORD *)(v19 + 64) = v25;

    *a6 = v19;
  }
}

CGImageRef vImage.PixelBuffer<>.makeCGImage(cgImageFormat:)(CGImageRef result)
{
  uint64_t v1;
  _QWORD *v2;
  void *v3;
  vImagePixelCount v4;
  vImagePixelCount v5;
  size_t v6;
  vImage_Flags v7;

  v2 = *(_QWORD **)v1;
  if (*(_QWORD *)(*(_QWORD *)v1 + 16))
  {
    v3 = (void *)v2[4];
    v4 = v2[5];
    v5 = v2[6];
    v6 = v2[7];
    v7 = 0;
    return vImage_Buffer.createCGImage(format:flags:)((uint64_t)result, &v7, v3, v4, v5, v6);
  }
  else
  {
    __break(1u);
  }
  return result;
}

void vImage.PixelBuffer<>.init(referencing:planeIndex:overrideSize:pixelFormat:)(__CVBuffer *a1@<X0>, size_t a2@<X1>, uint64_t a3@<X2>, _QWORD *a4@<X8>)
{
  size_t HeightOfPlane;
  size_t WidthOfPlane;
  int v9;
  void *BaseAddressOfPlane;
  size_t BytesPerRowOfPlane;
  uint64_t v12;

  WidthOfPlane = *(_QWORD *)a3;
  HeightOfPlane = *(_QWORD *)(a3 + 8);
  v9 = *(unsigned __int8 *)(a3 + 16);
  BaseAddressOfPlane = CVPixelBufferGetBaseAddressOfPlane(a1, a2);
  BytesPerRowOfPlane = CVPixelBufferGetBytesPerRowOfPlane(a1, a2);
  if (v9 == 1)
  {
    HeightOfPlane = CVPixelBufferGetHeightOfPlane(a1, a2);
    WidthOfPlane = CVPixelBufferGetWidthOfPlane(a1, a2);
  }
  if (BaseAddressOfPlane)
  {
    vImage.PixelBuffer<>.init(data:width:height:byteCountPerRow:pixelFormat:)((uint64_t)BaseAddressOfPlane, WidthOfPlane, HeightOfPlane, BytesPerRowOfPlane, &v12);

    *a4 = v12;
  }
  else
  {
    __break(1u);
  }
}

void vImage.PixelBuffer<>.init(copying:cvImageFormat:cgImageFormat:pixelFormat:)(__CVBuffer *a1@<X0>, vImageCVImageFormat *a2@<X1>, vImage_CGImageFormat *a3@<X2>, uint64_t *a4@<X8>)
{
  uint64_t v4;
  uint64_t v9;
  __int128 v10;
  _OWORD v11[2];
  uint64_t v12;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v9 = swift_allocObject();
  *(_OWORD *)(v9 + 16) = xmmword_1CAB5E430;
  specialized vImage.BufferWrapper.init(cvPixelBuffer:cvImageFormat:cgImageFormat:)(a1, a2, a3, v11);
  if (v4)
  {
    *(_QWORD *)(v9 + 16) = 0;

    swift_release();
  }
  else
  {
    v10 = v11[1];
    *(_OWORD *)(v9 + 32) = v11[0];
    *(_OWORD *)(v9 + 48) = v10;
    *(_QWORD *)(v9 + 64) = v12;

    *a4 = v9;
  }
}

void vImage.PixelBuffer<>.init(referencing:converter:destinationPixelFormat:)(__CVBuffer *a1@<X0>, vImageConverter *a2@<X1>, uint64_t *a3@<X8>)
{
  uint64_t v6;
  __int128 v7;
  vImage_Buffer v8;
  uint64_t v9;

  v9 = *MEMORY[0x1E0C80C00];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v6 = swift_allocObject();
  *(_OWORD *)(v6 + 16) = xmmword_1CAB5E430;
  memset(&v8, 0, sizeof(v8));
  vImageBuffer_InitForCopyFromCVPixelBuffer(&v8, a2, a1, 0x200u);
  v7 = *(_OWORD *)&v8.width;
  *(_OWORD *)(v6 + 32) = *(_OWORD *)&v8.data;
  *(_OWORD *)(v6 + 48) = v7;
  *(_QWORD *)(v6 + 64) = 0;

  *a3 = v6;
}

vImage_Error vImage.PixelBuffer<>.copy(to:cvImageFormat:cgImageFormat:)(__CVBuffer *a1, vImageCVImageFormat *a2, uint64_t a3)
{
  uint64_t *v3;
  uint64_t v7;
  size_t Width;
  uint64_t v9;
  size_t Height;
  uint64_t v11;
  size_t v12;
  __int128 v13;
  __int128 v14;
  vImage_Error result;
  uint64_t v16;
  char *v17;
  char *v18;
  char bitsPerComponent;
  vImage_CGImageFormat v20;
  vImage_Buffer buffer;
  uint64_t v22;

  v22 = *MEMORY[0x1E0C80C00];
  v7 = *v3;
  if (CVPixelBufferGetPlaneCount(a1))
  {
    __break(1u);
    goto LABEL_15;
  }
  swift_bridgeObjectRetain();
  Width = CVPixelBufferGetWidth(a1);
  if (!*(_QWORD *)(v7 + 16))
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  v9 = *(_QWORD *)(v7 + 48);
  if (v9 < 0)
  {
LABEL_16:
    __break(1u);
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (Width != v9)
    goto LABEL_21;
  Height = CVPixelBufferGetHeight(a1);
  if (!*(_QWORD *)(v7 + 16))
    goto LABEL_17;
  v11 = *(_QWORD *)(v7 + 40);
  if (v11 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v12 = Height;
  swift_bridgeObjectRelease();
  if (v12 != v11)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!*(_QWORD *)(v7 + 16))
  {
LABEL_20:
    __break(1u);
LABEL_21:
    swift_bridgeObjectRelease();
    __break(1u);
  }
  v13 = *(_OWORD *)(v7 + 48);
  *(_OWORD *)&buffer.data = *(_OWORD *)(v7 + 32);
  *(_OWORD *)&buffer.width = v13;
  v14 = *(_OWORD *)(a3 + 16);
  *(_OWORD *)&v20.bitsPerComponent = *(_OWORD *)a3;
  *(_OWORD *)&v20.bitmapInfo = v14;
  *(_QWORD *)&v20.renderingIntent = *(_QWORD *)(a3 + 32);
  result = vImageBuffer_CopyToCVPixelBuffer(&buffer, &v20, a1, a2, 0, 0);
  if (result)
  {
    v16 = result;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v18 = v17;
    vImage.Error.init(rawValue:)(v16, (char *)&v20);
    bitsPerComponent = v20.bitsPerComponent;
    if (LOBYTE(v20.bitsPerComponent) == 20)
      bitsPerComponent = 11;
    *v18 = bitsPerComponent;
    return swift_willThrow();
  }
  return result;
}

uint64_t vImage.PixelBuffer<>.init<A>(pixelValues:size:pixelFormat:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X3>, uint64_t a3@<X4>, uint64_t a4@<X5>, uint64_t a5@<X6>, uint64_t *a6@<X8>)
{
  uint64_t v11;
  uint64_t v12;
  char *v13;
  uint64_t *v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t (*v19)(uint64_t, uint64_t);
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t (*v23)(uint64_t, uint64_t);
  uint64_t v24;
  uint64_t v25;
  void (*v26)(char *, uint64_t);
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  unint64_t v32;
  uint64_t (*v33)(uint64_t, uint64_t);
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  void *v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  _QWORD *v45;
  uint64_t result;
  void **v47;
  uint64_t v48;
  uint64_t *v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  void *v54;
  uint64_t v55;
  uint64_t v56;
  uint64_t v57;
  uint64_t v58;

  v58 = *MEMORY[0x1E0C80C00];
  v11 = *(_QWORD *)(a3 - 8);
  MEMORY[0x1E0C80A78](a1);
  v13 = (char *)&v48 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  v15 = *v14;
  v16 = v14[1];
  v52 = v17;
  v53 = v16;
  (*(void (**)(char *, uint64_t, uint64_t))(v11 + 16))(v13, v17, v18);
  v19 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  v51 = a5;
  v20 = v19(a3, a5);
  v21 = v15 * v16;
  v50 = v15;
  if ((unsigned __int128)(v15 * (__int128)v16) >> 64 != (v15 * v16) >> 63)
  {
    __break(1u);
    goto LABEL_10;
  }
  v22 = v20;
  v49 = a6;
  v23 = *(uint64_t (**)(uint64_t, uint64_t))(a4 + 24);
  v24 = a4;
  v25 = v23(a2, a4);
  v26 = *(void (**)(char *, uint64_t))(v11 + 8);
  v26(v13, a3);
  if ((unsigned __int128)(v21 * (__int128)v25) >> 64 != (v21 * v25) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (v22 != v21 * v25)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  v27 = a3;
  v28 = v24;
  v29 = *(_QWORD *)(*(_QWORD *)(swift_getAssociatedTypeWitness() - 8) + 72);
  if ((unint64_t)(v29 - 0x1000000000000000) >> 61 != 7)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v30 = 8 * v29;
  v31 = v23(a2, v24);
  v32 = v30 * v31;
  if ((unsigned __int128)(v30 * (__int128)v31) >> 64 != (v30 * v31) >> 63)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  if ((v32 & 0x8000000000000000) != 0)
  {
LABEL_14:
    __break(1u);
LABEL_15:
    __break(1u);
  }
  if (HIDWORD(v32))
    goto LABEL_15;
  v33 = (uint64_t (*)(uint64_t, uint64_t))v26;
  v34 = v50;
  v54 = specialized vImage_Buffer.init(size:bitsPerPixel:)(v32, (double)v50, (double)v53);
  v55 = v35;
  v56 = v36;
  v57 = v37;
  MEMORY[0x1E0C80A78](v54);
  *(&v48 - 8) = a2;
  *(&v48 - 7) = a3;
  v38 = v51;
  v39 = v52;
  *(&v48 - 6) = v28;
  *(&v48 - 5) = v38;
  *(&v48 - 4) = v53;
  *(&v48 - 3) = v34;
  v47 = &v54;
  (*(void (**)(vImage_Error (*)(uint64_t, uint64_t)))(v38 + 24))(partial apply for closure #1 in vImage.PixelBuffer<>.init<A>(pixelValues:size:pixelFormat:));
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v40 = swift_allocObject();
  *(_OWORD *)(v40 + 16) = xmmword_1CAB5E430;
  v41 = v54;
  v42 = v55;
  v43 = v56;
  v44 = v57;
  type metadata accessor for vImage.BufferReference();
  v45 = (_QWORD *)swift_allocObject();
  v45[2] = v41;
  v45[3] = v42;
  v45[4] = v43;
  v45[5] = v44;
  *(_QWORD *)(v40 + 32) = v41;
  *(_QWORD *)(v40 + 40) = v42;
  *(_QWORD *)(v40 + 48) = v43;
  *(_QWORD *)(v40 + 56) = v44;
  *(_QWORD *)(v40 + 64) = v45;
  result = v33(v39, v27);
  *v49 = v40;
  return result;
}

vImage_Error closure #1 in vImage.PixelBuffer<>.init<A>(pixelValues:size:pixelFormat:)(uint64_t a1, uint64_t a2, vImagePixelCount a3, int64_t a4, vImage_Buffer *a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t AssociatedTypeWitness;
  uint64_t v14;
  uint64_t v15;
  int64_t v16;
  void *v17;
  uint64_t (*v18)(uint64_t, uint64_t);
  uint64_t v19;
  uint64_t v20;
  vImage_Buffer *dest;
  vImage_Buffer src;
  uint64_t v24;

  v24 = *MEMORY[0x1E0C80C00];
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v14 = UnsafeBufferPointer.baseAddress.getter();
  if (!v14)
LABEL_11:
    __break(1u);
  if (((a4 | a3) & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_8;
  }
  dest = a5;
  v15 = *(_QWORD *)(*(_QWORD *)(AssociatedTypeWitness - 8) + 72);
  v16 = a4 * v15;
  if ((unsigned __int128)(a4 * (__int128)v15) >> 64 != (a4 * v15) >> 63)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v17 = (void *)v14;
  v18 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 24);
  v19 = v18(a6, a8);
  if ((unsigned __int128)(v16 * (__int128)v19) >> 64 != (v16 * v19) >> 63)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  src.data = v17;
  src.height = a3;
  src.width = a4;
  src.rowBytes = v16 * v19;
  v20 = v18(a6, a8);
  if ((unsigned __int128)(v15 * (__int128)v20) >> 64 != (v15 * v20) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  return vImageCopyBuffer(&src, dest, v15 * v20, 0);
}

uint64_t vImage.PixelBuffer<>.array.getter(uint64_t result, uint64_t a2)
{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;

  v3 = *v2;
  if (!*(_QWORD *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  v4 = *(_QWORD *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v5 = *(_QWORD *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  v6 = v4 * v5;
  if ((unsigned __int128)(v4 * (__int128)v5) >> 64 != (v4 * v5) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  result = (*(uint64_t (**)(_QWORD))(a2 + 24))(*(_QWORD *)(result + 16));
  if ((unsigned __int128)(v6 * (__int128)result) >> 64 == (v6 * result) >> 63)
  {
    MEMORY[0x1E0C80A78](result);
    swift_getAssociatedTypeWitness();
    return Array.init(unsafeUninitializedCapacity:initializingWith:)();
  }
LABEL_11:
  __break(1u);
  return result;
}

vImage_Error closure #1 in vImage.PixelBuffer<>.array.getter(uint64_t a1, _QWORD *a2, _QWORD *a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v6;
  uint64_t AssociatedTypeWitness;
  uint64_t v13;
  vImagePixelCount v14;
  int64_t v15;
  uint64_t v16;
  int64_t v17;
  void *v18;
  uint64_t (*v19)(uint64_t, uint64_t);
  uint64_t v20;
  void *v21;
  vImagePixelCount v22;
  size_t v23;
  vImagePixelCount v24;
  uint64_t v25;
  vImage_Error result;
  uint64_t v27;
  _QWORD *v28;
  vImage_Flags v29;
  vImage_Buffer dest;
  uint64_t v31;

  v31 = *MEMORY[0x1E0C80C00];
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v13 = UnsafeBufferPointer.baseAddress.getter();
  if (!a3[2])
  {
    __break(1u);
    goto LABEL_11;
  }
  v14 = a3[5];
  if ((v14 & 0x8000000000000000) != 0)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  v15 = a3[6];
  if (v15 < 0)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v28 = a2;
  v16 = *(_QWORD *)(*(_QWORD *)(AssociatedTypeWitness - 8) + 72);
  v17 = v15 * v16;
  if ((unsigned __int128)(v15 * (__int128)v16) >> 64 != (v15 * v16) >> 63)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  v18 = (void *)v13;
  v27 = a4;
  v19 = *(uint64_t (**)(uint64_t, uint64_t))(a6 + 24);
  v20 = v19(a5, a6);
  if ((unsigned __int128)(v17 * (__int128)v20) >> 64 != (v17 * v20) >> 63)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  dest.data = v18;
  dest.height = v14;
  dest.width = v15;
  dest.rowBytes = v17 * v20;
  if (!a3[2])
  {
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
  }
  v21 = (void *)a3[4];
  v22 = a3[5];
  v24 = a3[6];
  v23 = a3[7];
  v25 = v19(a5, a6);
  if ((unsigned __int128)(v16 * (__int128)v25) >> 64 != (v16 * v25) >> 63)
    goto LABEL_16;
  v29 = 0;
  result = vImage_Buffer.copy(destinationBuffer:pixelSize:flags:)(&dest, v16 * v25, &v29, v21, v22, v24, v23);
  if (v6)
  {
    result = swift_unexpectedError();
    __break(1u);
  }
  else
  {
    *v28 = v27;
  }
  return result;
}

uint64_t protocol witness for AccelerateBuffer.count.getter in conformance <> vImage.PixelBuffer<A>(uint64_t a1, uint64_t a2)
{
  return vImage.PixelBuffer<>.count.getter(a1, *(_QWORD *)(a2 - 8));
}

vImageCVImageFormatRef static vImageCVImageFormatRef.make(format:colorSpace:alphaIsOpaqueHint:)(char *a1, CGColorSpaceRef baseColorspace, char a3)
{
  return vImageCVImageFormat_Create(dword_1CAB62D90[*a1], 0, 0, baseColorspace, a3 & 1);
}

uint64_t vImage.Size.init<A>(exactWidth:height:)@<X0>(uint64_t a1@<X0>, char *a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X8>)
{
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  char *v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  char *v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  char *v36;
  uint64_t v37;
  uint64_t v38;
  char *v39;
  uint64_t v40;
  char *v41;
  void (*v42)(char *, uint64_t, uint64_t);
  _BOOL4 v43;
  void (*v44)(char *, uint64_t);
  void (*v45)(char *, char *, uint64_t);
  char *v46;
  uint64_t v47;
  char *v48;
  char *v49;
  char v50;
  char *v51;
  uint64_t v52;
  char v53;
  uint64_t v54;
  char *v55;
  char v56;
  char *v57;
  uint64_t v58;
  char v59;
  char v60;
  uint64_t v61;
  char *v62;
  char v63;
  uint64_t v64;
  uint64_t v65;
  uint64_t v66;
  uint64_t v67;
  char *v68;
  char *v69;
  uint64_t v70;
  void (*v71)(char *, char *, uint64_t);
  char v72;
  char *v73;
  uint64_t v74;
  char *v75;
  char *v76;
  uint64_t v77;
  char *v78;
  char *v79;
  uint64_t v80;
  char *v81;
  void (*v82)(char *, char *, uint64_t);
  char *v83;
  char *v84;
  char v85;
  uint64_t v86;
  char *v87;
  char v88;
  char *v89;
  char *v90;
  uint64_t v91;
  char v92;
  uint64_t v93;
  uint64_t result;
  uint64_t v95;
  uint64_t AssociatedConformanceWitness;
  char *v97;
  char *v98;
  char *v99;
  char v100;
  uint64_t v101;
  char *v102;
  char *v103;
  char v104;
  char *v105;
  int64_t v106;
  uint64_t v107;
  char *v108;
  char v109;
  char *v110;
  uint64_t v111;
  uint64_t v112;
  char *v113;
  char *v114;
  char *v115;
  char *v116;
  int64_t v117;
  uint64_t v118;
  _QWORD v119[2];
  uint64_t AssociatedTypeWitness;
  uint64_t v121;
  char *v122;
  uint64_t v123;
  char *v124;
  char *v125;
  char *v126;
  char *v127;
  char *v128;
  char *v129;
  uint64_t v130;
  uint64_t v131;
  char *v132;
  char *v133;
  void (*v134)(char *, uint64_t, uint64_t);
  uint64_t v135;
  int64_t v136;

  v133 = a2;
  v131 = a5;
  v121 = *(_QWORD *)(*(_QWORD *)(a4 + 24) + 16);
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v7 = MEMORY[0x1E0C80A78](AssociatedTypeWitness);
  v119[1] = (char *)v119 - v8;
  v9 = *(_QWORD *)(a3 - 8);
  v10 = MEMORY[0x1E0C80A78](v7);
  v119[0] = (char *)v119 - ((v11 + 15) & 0xFFFFFFFFFFFFFFF0);
  v12 = MEMORY[0x1E0C80A78](v10);
  v124 = (char *)v119 - v13;
  v14 = MEMORY[0x1E0C80A78](v12);
  v125 = (char *)v119 - v15;
  v16 = MEMORY[0x1E0C80A78](v14);
  v128 = (char *)v119 - v17;
  v18 = MEMORY[0x1E0C80A78](v16);
  v126 = (char *)v119 - v19;
  v20 = MEMORY[0x1E0C80A78](v18);
  v127 = (char *)v119 - v21;
  v22 = MEMORY[0x1E0C80A78](v20);
  v24 = (char *)v119 - v23;
  v25 = MEMORY[0x1E0C80A78](v22);
  v122 = (char *)v119 - v26;
  v27 = MEMORY[0x1E0C80A78](v25);
  v132 = (char *)v119 - v28;
  v29 = MEMORY[0x1E0C80A78](v27);
  v31 = (char *)v119 - v30;
  v32 = MEMORY[0x1E0C80A78](v29);
  v129 = (char *)v119 - v33;
  v34 = MEMORY[0x1E0C80A78](v32);
  v36 = (char *)v119 - v35;
  v37 = MEMORY[0x1E0C80A78](v34);
  v39 = (char *)v119 - v38;
  MEMORY[0x1E0C80A78](v37);
  v41 = (char *)v119 - v40;
  v42 = *(void (**)(char *, uint64_t, uint64_t))(v9 + 16);
  v130 = a1;
  v42((char *)v119 - v40, a1, a3);
  LOBYTE(a1) = dispatch thunk of static BinaryInteger.isSigned.getter();
  v134 = v42;
  v42(v39, (uint64_t)v41, a3);
  v43 = (a1 & 1) != 0 && dispatch thunk of BinaryInteger.bitWidth.getter() > 64;
  v123 = v9;
  v44 = *(void (**)(char *, uint64_t))(v9 + 8);
  v44(v39, a3);
  v45 = (void (*)(char *, char *, uint64_t))v134;
  v134(v36, (uint64_t)v41, a3);
  v46 = v132;
  if (!v43)
  {
    v44(v36, a3);
    v48 = v133;
    goto LABEL_10;
  }
  v136 = 0x8000000000000000;
  if ((dispatch thunk of static BinaryInteger.isSigned.getter() & 1) != 0)
  {
    v47 = dispatch thunk of BinaryInteger.bitWidth.getter();
    v48 = v133;
    if (v47 >= 64)
    {
      lazy protocol witness table accessor for type Int and conformance Int();
      v49 = v129;
      dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
      v50 = dispatch thunk of static Comparable.< infix(_:_:)();
      v51 = v49;
      v45 = (void (*)(char *, char *, uint64_t))v134;
      v44(v51, a3);
      v44(v36, a3);
      if ((v50 & 1) != 0)
        goto LABEL_30;
      goto LABEL_10;
    }
  }
  else
  {
    v60 = dispatch thunk of static BinaryInteger.isSigned.getter();
    v61 = dispatch thunk of BinaryInteger.bitWidth.getter();
    if ((v60 & 1) != 0)
    {
      if (v61 <= 64)
      {
        v95 = AssociatedTypeWitness;
        AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
        MEMORY[0x1D17943A8](&unk_1CAB61788, 256, v95, AssociatedConformanceWitness);
        v97 = v129;
        dispatch thunk of ExpressibleByFloatLiteral.init(floatLiteral:)();
        LOBYTE(v95) = dispatch thunk of static Comparable.< infix(_:_:)();
        v44(v97, a3);
        (*(void (**)(char *, char *, uint64_t))(v123 + 32))(v24, v36, a3);
        if ((v95 & 1) != 0)
        {
          v44(v24, a3);
          v66 = v130;
          v65 = v131;
          v48 = v133;
          goto LABEL_77;
        }
        v106 = v136;
        v107 = dispatch thunk of BinaryInteger._lowWord.getter();
        v44(v24, a3);
        v48 = v133;
        v45 = (void (*)(char *, char *, uint64_t))v134;
        v46 = v132;
        if (v107 < v106)
          goto LABEL_30;
      }
      else
      {
        lazy protocol witness table accessor for type Int and conformance Int();
        v62 = v129;
        dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
        v63 = dispatch thunk of static Comparable.< infix(_:_:)();
        v44(v62, a3);
        v44(v36, a3);
        v48 = v133;
        v45 = (void (*)(char *, char *, uint64_t))v134;
        if ((v63 & 1) != 0)
          goto LABEL_30;
      }
      goto LABEL_10;
    }
    v48 = v133;
    v45 = (void (*)(char *, char *, uint64_t))v134;
    if (v61 >= 64)
    {
      v44(v36, a3);
      goto LABEL_10;
    }
  }
  v67 = dispatch thunk of BinaryInteger._lowWord.getter();
  v44(v36, a3);
  if (v67 < v136)
  {
LABEL_30:
    v66 = v130;
    v65 = v131;
LABEL_77:
    v44(v41, a3);
    goto LABEL_78;
  }
LABEL_10:
  v52 = dispatch thunk of BinaryInteger.bitWidth.getter();
  v45(v31, v41, a3);
  if (v52 >= 65)
  {
    v44(v31, a3);
    v45(v46, v41, a3);
    goto LABEL_12;
  }
  v58 = dispatch thunk of BinaryInteger.bitWidth.getter();
  v44(v31, a3);
  if (v58 != 64)
  {
    v45(v46, v41, a3);
LABEL_26:
    v44(v46, a3);
    v66 = v130;
    v65 = v131;
    goto LABEL_33;
  }
  v59 = dispatch thunk of static BinaryInteger.isSigned.getter();
  v45(v46, v41, a3);
  if ((v59 & 1) != 0)
    goto LABEL_26;
LABEL_12:
  v136 = 0x7FFFFFFFFFFFFFFFLL;
  v53 = dispatch thunk of static BinaryInteger.isSigned.getter();
  v54 = dispatch thunk of BinaryInteger.bitWidth.getter();
  if ((v53 & 1) != 0)
  {
    if (v54 > 64)
    {
      lazy protocol witness table accessor for type Int and conformance Int();
      v55 = v129;
      dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
      v56 = dispatch thunk of static Comparable.< infix(_:_:)();
      v44(v55, a3);
      v57 = v46;
      goto LABEL_32;
    }
LABEL_23:
    v64 = dispatch thunk of BinaryInteger._lowWord.getter();
    v44(v46, a3);
    v66 = v130;
    v65 = v131;
    if (v136 < v64)
      goto LABEL_77;
    goto LABEL_33;
  }
  if (v54 <= 63)
    goto LABEL_23;
  v135 = 0x7FFFFFFFFFFFFFFFLL;
  v68 = v129;
  (*(void (**)(char *, char *, uint64_t))(v123 + 32))(v129, v46, a3);
  lazy protocol witness table accessor for type Int and conformance Int();
  v69 = v122;
  dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
  v56 = dispatch thunk of static Comparable.< infix(_:_:)();
  v44(v69, a3);
  v57 = v68;
LABEL_32:
  v44(v57, a3);
  v66 = v130;
  v65 = v131;
  if ((v56 & 1) != 0)
    goto LABEL_77;
LABEL_33:
  v70 = dispatch thunk of BinaryInteger._lowWord.getter();
  v44(v41, a3);
  if (v70 <= 0)
  {
LABEL_78:
    v44(v48, a3);
    result = ((uint64_t (*)(uint64_t, uint64_t))v44)(v66, a3);
    goto LABEL_79;
  }
  v132 = (char *)v70;
  v41 = v127;
  v71 = (void (*)(char *, char *, uint64_t))v134;
  v134(v127, (uint64_t)v48, a3);
  v72 = dispatch thunk of static BinaryInteger.isSigned.getter();
  v73 = v126;
  v71(v126, v41, a3);
  if ((v72 & 1) == 0)
  {
    v44(v73, a3);
    v75 = v128;
    v71(v128, v41, a3);
    goto LABEL_40;
  }
  v74 = dispatch thunk of BinaryInteger.bitWidth.getter();
  v44(v73, a3);
  v75 = v128;
  v71(v128, v41, a3);
  if (v74 < 65)
  {
LABEL_40:
    v79 = v75;
    goto LABEL_41;
  }
  v136 = 0x8000000000000000;
  if ((dispatch thunk of static BinaryInteger.isSigned.getter() & 1) == 0)
  {
    v100 = dispatch thunk of static BinaryInteger.isSigned.getter();
    v101 = dispatch thunk of BinaryInteger.bitWidth.getter();
    if ((v100 & 1) != 0)
    {
      if (v101 <= 64)
      {
        v111 = AssociatedTypeWitness;
        v112 = swift_getAssociatedConformanceWitness();
        MEMORY[0x1D17943A8](&unk_1CAB61788, 256, v111, v112);
        v113 = v129;
        dispatch thunk of ExpressibleByFloatLiteral.init(floatLiteral:)();
        v114 = v128;
        LOBYTE(v111) = dispatch thunk of static Comparable.< infix(_:_:)();
        v44(v113, a3);
        v115 = (char *)v119[0];
        (*(void (**)(_QWORD, char *, uint64_t))(v123 + 32))(v119[0], v114, a3);
        if ((v111 & 1) != 0)
        {
          v44(v115, a3);
          goto LABEL_77;
        }
        v116 = v115;
        v117 = v136;
        v118 = dispatch thunk of BinaryInteger._lowWord.getter();
        v44(v116, a3);
        if (v118 < v117)
          goto LABEL_77;
      }
      else
      {
        lazy protocol witness table accessor for type Int and conformance Int();
        v102 = v129;
        dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
        v103 = v128;
        v104 = dispatch thunk of static Comparable.< infix(_:_:)();
        v105 = v102;
        v48 = v133;
        v44(v105, a3);
        v44(v103, a3);
        if ((v104 & 1) != 0)
          goto LABEL_77;
      }
      goto LABEL_42;
    }
    if (v101 < 64)
    {
      v110 = v128;
      v77 = dispatch thunk of BinaryInteger._lowWord.getter();
      v78 = v110;
LABEL_72:
      v44(v78, a3);
      if (v77 < v136)
        goto LABEL_77;
      goto LABEL_42;
    }
    v79 = v128;
LABEL_41:
    v44(v79, a3);
    goto LABEL_42;
  }
  v76 = v128;
  if (dispatch thunk of BinaryInteger.bitWidth.getter() < 64)
  {
    v77 = dispatch thunk of BinaryInteger._lowWord.getter();
    v78 = v76;
    goto LABEL_72;
  }
  lazy protocol witness table accessor for type Int and conformance Int();
  v108 = v129;
  dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
  v109 = dispatch thunk of static Comparable.< infix(_:_:)();
  v44(v108, a3);
  v44(v76, a3);
  if ((v109 & 1) != 0)
    goto LABEL_77;
LABEL_42:
  v80 = dispatch thunk of BinaryInteger.bitWidth.getter();
  v81 = v125;
  v82 = (void (*)(char *, char *, uint64_t))v134;
  v134(v125, (uint64_t)v41, a3);
  if (v80 < 65)
  {
    v91 = dispatch thunk of BinaryInteger.bitWidth.getter();
    v44(v81, a3);
    if (v91 != 64)
    {
      v84 = v124;
      v134(v124, (uint64_t)v41, a3);
      goto LABEL_53;
    }
    v92 = dispatch thunk of static BinaryInteger.isSigned.getter();
    v84 = v124;
    v134(v124, (uint64_t)v41, a3);
    if ((v92 & 1) != 0)
      goto LABEL_53;
  }
  else
  {
    v44(v81, a3);
    v83 = v124;
    v82(v124, v41, a3);
    v84 = v83;
  }
  v136 = 0x7FFFFFFFFFFFFFFFLL;
  v85 = dispatch thunk of static BinaryInteger.isSigned.getter();
  v86 = dispatch thunk of BinaryInteger.bitWidth.getter();
  if ((v85 & 1) != 0)
  {
    if (v86 > 64)
    {
      lazy protocol witness table accessor for type Int and conformance Int();
      v87 = v129;
      dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
      v88 = dispatch thunk of static Comparable.< infix(_:_:)();
      v89 = v87;
      v48 = v133;
      v44(v89, a3);
      v90 = v84;
      goto LABEL_59;
    }
    goto LABEL_51;
  }
  if (v86 <= 63)
  {
LABEL_51:
    dispatch thunk of BinaryInteger._lowWord.getter();
LABEL_53:
    v44(v84, a3);
    goto LABEL_54;
  }
  v135 = 0x7FFFFFFFFFFFFFFFLL;
  v98 = v129;
  (*(void (**)(char *, char *, uint64_t))(v123 + 32))(v129, v84, a3);
  lazy protocol witness table accessor for type Int and conformance Int();
  v99 = v122;
  dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
  v88 = dispatch thunk of static Comparable.< infix(_:_:)();
  v44(v99, a3);
  v90 = v98;
  v48 = v133;
LABEL_59:
  v44(v90, a3);
  if ((v88 & 1) != 0)
    goto LABEL_77;
LABEL_54:
  v93 = dispatch thunk of BinaryInteger._lowWord.getter();
  v44(v41, a3);
  v44(v48, a3);
  result = ((uint64_t (*)(uint64_t, uint64_t))v44)(v66, a3);
  if (v93 > 0)
  {
    *(_QWORD *)v65 = v132;
    *(_QWORD *)(v65 + 8) = v93;
    *(_BYTE *)(v65 + 16) = 0;
    return result;
  }
LABEL_79:
  *(_QWORD *)v65 = 0;
  *(_QWORD *)(v65 + 8) = 0;
  *(_BYTE *)(v65 + 16) = 1;
  return result;
}

uint64_t vImage.Size.init<A>(exactWidth:height:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X8>)
{
  uint64_t v8;
  uint64_t v9;
  char *v10;
  void (*v11)(char *, uint64_t);
  void (*v12)(uint64_t, uint64_t);
  uint64_t result;
  uint64_t v14;
  void (*v15)(uint64_t, uint64_t);
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  char v19;

  v8 = *(_QWORD *)(a3 - 8);
  MEMORY[0x1E0C80A78](a1);
  v10 = (char *)&v16 - ((v9 + 15) & 0xFFFFFFFFFFFFFFF0);
  v11 = *(void (**)(char *, uint64_t))(v8 + 16);
  v11(v10, a1);
  lazy protocol witness table accessor for type Int and conformance Int();
  FixedWidthInteger.init<A>(exactly:)();
  if (v19 == 1 || v18 < 1)
  {
    v15 = *(void (**)(uint64_t, uint64_t))(v8 + 8);
    v15(a2, a3);
    result = ((uint64_t (*)(uint64_t, uint64_t))v15)(a1, a3);
  }
  else
  {
    v17 = v18;
    ((void (*)(char *, uint64_t, uint64_t))v11)(v10, a2, a3);
    FixedWidthInteger.init<A>(exactly:)();
    v12 = *(void (**)(uint64_t, uint64_t))(v8 + 8);
    v12(a2, a3);
    result = ((uint64_t (*)(uint64_t, uint64_t))v12)(a1, a3);
    if ((v19 & 1) == 0)
    {
      v14 = v18;
      if (v18 >= 1)
      {
        *(_QWORD *)a4 = v17;
        *(_QWORD *)(a4 + 8) = v14;
        *(_BYTE *)(a4 + 16) = 0;
        return result;
      }
    }
  }
  *(_QWORD *)a4 = 0;
  *(_QWORD *)(a4 + 8) = 0;
  *(_BYTE *)(a4 + 16) = 1;
  return result;
}

uint64_t vImage.Size.init(width:height:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X1>, _QWORD *a3@<X8>)
{
  if ((a2 | result) < 0)
  {
    __break(1u);
  }
  else
  {
    *a3 = result;
    a3[1] = a2;
  }
  return result;
}

void vImage.Size.init(cvPixelBuffer:)(__CVBuffer *a1@<X0>, int64_t *a2@<X8>)
{
  int64_t Width;
  int64_t Height;

  Width = CVPixelBufferGetWidth(a1);
  Height = CVPixelBufferGetHeight(a1);

  if (Width < 1 || Height < 1)
  {
    __break(1u);
  }
  else
  {
    *a2 = Width;
    a2[1] = Height;
  }
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance vImage.Size(_QWORD *a1, _QWORD *a2)
{
  return *a1 == *a2 && a1[1] == a2[1];
}

char *specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(char *a1, int64_t a2, char a3)
{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<Float>?>);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<vImage_Buffer>?>);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<UInt32>);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>?>);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<UInt64>);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<BNNSNDArrayDescriptor>>);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>>);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<Int32>);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<UInt>);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3);
  *v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3);
  *v3 = result;
  return result;
}

_QWORD *specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(_QWORD *a1, int64_t a2, char a3)
{
  _QWORD **v3;
  _QWORD *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<[Int16]>, &demangling cache variable for type metadata for [Int16]);
  *v3 = result;
  return result;
}

{
  _QWORD **v3;
  _QWORD *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.PixelBuffer<vImage.DynamicPixelFormat>>, &demangling cache variable for type metadata for vImage.PixelBuffer<vImage.DynamicPixelFormat>);
  *v3 = result;
  return result;
}

{
  _QWORD **v3;
  _QWORD *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<[UInt]>, &demangling cache variable for type metadata for [UInt]);
  *v3 = result;
  return result;
}

{
  _QWORD **v3;
  _QWORD *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.PixelBuffer<vImage.Planar16U>>, &demangling cache variable for type metadata for vImage.PixelBuffer<vImage.Planar16U>);
  *v3 = result;
  return result;
}

{
  _QWORD **v3;
  _QWORD *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.PixelBuffer<vImage.PlanarF>>, &demangling cache variable for type metadata for vImage.PixelBuffer<vImage.PlanarF>);
  *v3 = result;
  return result;
}

{
  _QWORD **v3;
  _QWORD *result;

  result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.PixelBuffer<vImage.Planar8>>, &demangling cache variable for type metadata for vImage.PixelBuffer<vImage.Planar8>);
  *v3 = result;
  return result;
}

char *specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(char *result, int64_t a2, char a3, char *a4)
{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  int64_t v12;
  char *v13;
  char *v14;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int16>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    v12 = v11 - 32;
    if (v11 < 32)
      v12 = v11 - 31;
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = v12 & 0xFFFFFFFFFFFFFFFELL;
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v13 = v10 + 32;
  v14 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v13 >= &v14[2 * v8])
      memmove(v13, v14, 2 * v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 2 * v8);
  }
  swift_release();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<BNNSGraph.Shape>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    v12 = v11 - 32;
    if (v11 < 32)
      v12 = v11 - 17;
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * (v12 >> 4);
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v13 = v10 + 32;
  v14 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v13 >= &v14[16 * v8])
      memmove(v13, v14, 16 * v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 16 * v8);
  }
  swift_release();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<bnns_graph_shape_t>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    v12 = v11 - 32;
    if (v11 < 32)
      v12 = v11 - 17;
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * (v12 >> 4);
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v13 = v10 + 32;
  v14 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v13 >= &v14[16 * v8])
      memmove(v13, v14, 16 * v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 16 * v8);
  }
  swift_release();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<String>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    v12 = v11 - 32;
    if (v11 < 32)
      v12 = v11 - 17;
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * (v12 >> 4);
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v13 = v10 + 32;
  v14 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v13 >= &v14[16 * v8])
      memmove(v13, v14, 16 * v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    swift_arrayInitWithCopy();
  }
  swift_release();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  int64_t v8;
  int64_t v9;
  char *v10;
  size_t v11;
  char *v12;
  char *v13;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * v11 - 64;
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v12 = v10 + 32;
  v13 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v12 >= &v13[v8])
      memmove(v12, v13, v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v12, v13, v8);
  }
  swift_release();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  int64_t v8;
  int64_t v9;
  char *v10;
  size_t v11;
  char *v12;
  char *v13;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferType?>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * v11 - 64;
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v12 = v10 + 32;
  v13 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v12 >= &v13[v8])
      memmove(v12, v13, v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v12, v13, v8);
  }
  swift_release();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  size_t v11;
  char *v12;
  char *v13;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * ((uint64_t)(v11 - 32) / 40);
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v12 = v10 + 32;
  v13 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v12 >= &v13[40 * v8])
      memmove(v12, v13, 40 * v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    swift_arrayInitWithCopy();
  }
  swift_release();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  v5 = (char)result;
  if ((a3 & 1) != 0)
  {
    v6 = *((_QWORD *)a4 + 3);
    v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v7 = a2;
    }
  }
  else
  {
    v7 = a2;
  }
  v8 = *((_QWORD *)a4 + 2);
  if (v7 <= v8)
    v9 = *((_QWORD *)a4 + 2);
  else
    v9 = v7;
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage_Buffer>);
    v10 = (char *)swift_allocObject();
    v11 = _swift_stdlib_malloc_size(v10);
    v12 = v11 - 32;
    if (v11 < 32)
      v12 = v11 - 1;
    *((_QWORD *)v10 + 2) = v8;
    *((_QWORD *)v10 + 3) = 2 * (v12 >> 5);
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v13 = v10 + 32;
  v14 = a4 + 32;
  if ((v5 & 1) != 0)
  {
    if (v10 != a4 || v13 >= &v14[32 * v8])
      memmove(v13, v14, 32 * v8);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 32 * v8);
  }
  swift_release();
  return v10;
}

char *specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(char *result, int64_t a2, char a3, char *a4, uint64_t *a5)
{
  char v6;
  unint64_t v7;
  int64_t v8;
  uint64_t v9;
  uint64_t v10;
  char *v11;
  int64_t v12;
  uint64_t v13;
  char *v14;
  char *v15;

  v6 = (char)result;
  if ((a3 & 1) != 0)
  {
    v7 = *((_QWORD *)a4 + 3);
    v8 = v7 >> 1;
    if ((uint64_t)(v7 >> 1) < a2)
    {
      if (v8 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v8 = v7 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v7 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v8 = a2;
    }
  }
  else
  {
    v8 = a2;
  }
  v9 = *((_QWORD *)a4 + 2);
  if (v8 <= v9)
    v10 = *((_QWORD *)a4 + 2);
  else
    v10 = v8;
  if (v10)
  {
    __swift_instantiateConcreteTypeFromMangledName(a5);
    v11 = (char *)swift_allocObject();
    v12 = _swift_stdlib_malloc_size(v11);
    v13 = v12 - 32;
    if (v12 < 32)
      v13 = v12 - 25;
    *((_QWORD *)v11 + 2) = v9;
    *((_QWORD *)v11 + 3) = 2 * (v13 >> 3);
  }
  else
  {
    v11 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v14 = v11 + 32;
  v15 = a4 + 32;
  if ((v6 & 1) != 0)
  {
    if (v11 != a4 || v14 >= &v15[8 * v9])
      memmove(v14, v15, 8 * v9);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v14, v15, 8 * v9);
  }
  swift_release();
  return v11;
}

{
  char v6;
  unint64_t v7;
  int64_t v8;
  uint64_t v9;
  uint64_t v10;
  char *v11;
  int64_t v12;
  uint64_t v13;
  char *v14;
  char *v15;

  v6 = (char)result;
  if ((a3 & 1) != 0)
  {
    v7 = *((_QWORD *)a4 + 3);
    v8 = v7 >> 1;
    if ((uint64_t)(v7 >> 1) < a2)
    {
      if (v8 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v8 = v7 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v7 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v8 = a2;
    }
  }
  else
  {
    v8 = a2;
  }
  v9 = *((_QWORD *)a4 + 2);
  if (v8 <= v9)
    v10 = *((_QWORD *)a4 + 2);
  else
    v10 = v8;
  if (v10)
  {
    __swift_instantiateConcreteTypeFromMangledName(a5);
    v11 = (char *)swift_allocObject();
    v12 = _swift_stdlib_malloc_size(v11);
    v13 = v12 - 32;
    if (v12 < 32)
      v13 = v12 - 29;
    *((_QWORD *)v11 + 2) = v9;
    *((_QWORD *)v11 + 3) = 2 * (v13 >> 2);
  }
  else
  {
    v11 = (char *)MEMORY[0x1E0DEE9D8];
  }
  v14 = v11 + 32;
  v15 = a4 + 32;
  if ((v6 & 1) != 0)
  {
    if (v11 != a4 || v14 >= &v15[4 * v9])
      memmove(v14, v15, 4 * v9);
    *((_QWORD *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v14, v15, 4 * v9);
  }
  swift_release();
  return v11;
}

_QWORD *specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(_QWORD *result, int64_t a2, char a3, _QWORD *a4, uint64_t *a5, uint64_t *a6)
{
  char v8;
  unint64_t v9;
  int64_t v10;
  uint64_t v11;
  uint64_t v12;
  _QWORD *v13;
  int64_t v14;
  uint64_t v15;

  v8 = (char)result;
  if ((a3 & 1) != 0)
  {
    v9 = a4[3];
    v10 = v9 >> 1;
    if ((uint64_t)(v9 >> 1) < a2)
    {
      if (v10 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      v10 = v9 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v9 & 0xFFFFFFFFFFFFFFFELL) <= a2)
        v10 = a2;
    }
  }
  else
  {
    v10 = a2;
  }
  v11 = a4[2];
  if (v10 <= v11)
    v12 = a4[2];
  else
    v12 = v10;
  if (v12)
  {
    __swift_instantiateConcreteTypeFromMangledName(a5);
    v13 = (_QWORD *)swift_allocObject();
    v14 = _swift_stdlib_malloc_size(v13);
    v15 = v14 - 32;
    if (v14 < 32)
      v15 = v14 - 25;
    v13[2] = v11;
    v13[3] = 2 * (v15 >> 3);
  }
  else
  {
    v13 = (_QWORD *)MEMORY[0x1E0DEE9D8];
  }
  if ((v8 & 1) != 0)
  {
    if (v13 != a4 || v13 + 4 >= &a4[v11 + 4])
      memmove(v13 + 4, a4 + 4, 8 * v11);
    a4[2] = 0;
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(a6);
    swift_arrayInitWithCopy();
  }
  swift_release();
  return v13;
}

uint64_t partial apply for closure #1 in vImage.PixelBuffer<>.withUnsafePointerToVImageBuffer<A>(_:)(uint64_t a1, _QWORD *a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t result;

  result = (*(uint64_t (**)(void))(v2 + 40))();
  if (v3)
    *a2 = v3;
  return result;
}

uint64_t outlined init with take of vImage.BufferReference?(uint64_t a1, uint64_t a2)
{
  uint64_t v4;

  v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for vImage.BufferReference?);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(_QWORD *)(v4 - 8) + 32))(a2, a1, v4);
  return a2;
}

uint64_t outlined retain of vImage.BufferReference?(uint64_t a1)
{
  swift_retain();
  return a1;
}

uint64_t partial apply for closure #1 in vImage.PixelBuffer<>.pixelBuffers.getter@<X0>(__int128 *a1@<X0>, uint64_t *a2@<X8>)
{
  return closure #1 in vImage.PixelBuffer<>.pixelBuffers.getter(a1, a2);
}

uint64_t type metadata accessor for vImage.PixelBuffer(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return __swift_instantiateGenericMetadata(a1, a2, a3, a4, (uint64_t)&nominal type descriptor for vImage.PixelBuffer);
}

unint64_t lazy protocol witness table accessor for type [vImage.BufferWrapper] and conformance [A]()
{
  unint64_t result;
  uint64_t v1;

  result = lazy protocol witness table cache variable for type [vImage.BufferWrapper] and conformance [A];
  if (!lazy protocol witness table cache variable for type [vImage.BufferWrapper] and conformance [A])
  {
    v1 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for [vImage.BufferWrapper]);
    result = MEMORY[0x1D1794D08](MEMORY[0x1E0DEAF50], v1);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type [vImage.BufferWrapper] and conformance [A]);
  }
  return result;
}

uint64_t partial apply for closure #1 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)@<X0>(uint64_t a1@<X0>, _QWORD *a2@<X8>)
{
  return partial apply for closure #1 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)(a1, a2);
}

uint64_t partial apply for closure #1 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)@<X0>(uint64_t result@<X0>, _QWORD *a2@<X8>)
{
  uint64_t v2;

  if (*(_QWORD *)(*(_QWORD *)result + 16))
  {
    v2 = *(_QWORD *)(*(_QWORD *)result + 48);
    if ((v2 & 0x8000000000000000) == 0)
    {
      *a2 = v2;
      return result;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

_QWORD *partial apply for closure #3 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)@<X0>(uint64_t a1@<X8>)
{
  uint64_t v1;

  return closure #3 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)(*(_QWORD *)(v1 + 16), *(_QWORD *)(v1 + 24), a1);
}

uint64_t specialized vImage.PixelBuffer<>.byteCountPerPixel.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))() / 8;
}

vImage_Error partial apply for closure #1 in vImage.PixelBuffer<>.init<A>(pixelValues:size:pixelFormat:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in vImage.PixelBuffer<>.init<A>(pixelValues:size:pixelFormat:)(a1, a2, *(_QWORD *)(v2 + 48), *(_QWORD *)(v2 + 56), *(vImage_Buffer **)(v2 + 64), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32));
}

vImage_Error partial apply for closure #1 in vImage.PixelBuffer<>.array.getter(uint64_t a1, _QWORD *a2)
{
  uint64_t v2;

  return closure #1 in vImage.PixelBuffer<>.array.getter(a1, a2, *(_QWORD **)(v2 + 32), *(_QWORD *)(v2 + 40), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24));
}

ValueMetadata *type metadata accessor for vImage.Size()
{
  return &type metadata for vImage.Size;
}

uint64_t protocol witness for AccelerateBuffer.withUnsafeBufferPointer<A>(_:) in conformance <> vImage.PixelBuffer<A>(uint64_t (*a1)(uint64_t), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return protocol witness for AccelerateMatrixBuffer.withUnsafeBufferPointer<A>(_:) in conformance <> vImage.PixelBuffer<A>(a1, a2, a3, a4, a5);
}

uint64_t protocol witness for AccelerateMutableBuffer.withUnsafeMutableBufferPointer<A>(_:) in conformance <> vImage.PixelBuffer<A>(uint64_t (*a1)(_QWORD *), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return protocol witness for AccelerateMutableMatrixBuffer.withUnsafeMutableBufferPointer<A>(_:) in conformance <> vImage.PixelBuffer<A>(a1, a2, a3, a4, a5);
}

uint64_t vImage.MultidimensionalLookupTable.LookupTableReference.lookupTable.getter()
{
  uint64_t v0;

  return *(_QWORD *)(v0 + 16);
}

uint64_t vImage.MultidimensionalLookupTable.LookupTableReference.deinit()
{
  uint64_t v0;

  vImageMultidimensionalTable_Release(*(vImage_MultidimensionalTable *)(v0 + 16));
  return v0;
}

uint64_t vImage.MultidimensionalLookupTable.LookupTableReference.__deallocating_deinit()
{
  uint64_t v0;

  vImageMultidimensionalTable_Release(*(vImage_MultidimensionalTable *)(v0 + 16));
  return swift_deallocClassInstance();
}

uint64_t vImage.MultidimensionalLookupTable.lookupTableReference.getter()
{
  return swift_retain();
}

uint64_t vImage.MultidimensionalLookupTable.lookupTableReference.setter(uint64_t a1)
{
  _QWORD *v1;
  uint64_t result;

  result = swift_release();
  *v1 = a1;
  return result;
}

uint64_t (*vImage.MultidimensionalLookupTable.lookupTableReference.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t vImage.MultidimensionalLookupTable.entryCountPerSourceChannel.getter()
{
  return swift_bridgeObjectRetain();
}

uint64_t vImage.MultidimensionalLookupTable.sourceChannelCount.getter()
{
  uint64_t v0;

  return *(_QWORD *)(v0 + 16);
}

uint64_t vImage.MultidimensionalLookupTable.destinationChannelCount.getter()
{
  uint64_t v0;

  return *(_QWORD *)(v0 + 24);
}

uint64_t vImage.MultidimensionalLookupTable.init<A>(entryCountPerSourceChannel:destinationChannelCount:data:)@<X0>(uint64_t a1@<X0>, uint64_t *a2@<X1>, char *a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t *a6@<X8>)
{
  uint64_t v12;
  uint64_t v13;
  char *v14;
  uint64_t v15;
  int64_t v16;
  __int16 v17;
  uint64_t v18;
  uint64_t v19;
  unint64_t v20;
  uint64_t v21;
  unint64_t v22;
  unint64_t v23;
  uint64_t v24;
  uint64_t v25;
  char *v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t (*v33)(char *, uint64_t);
  uint64_t v34;
  void (*v35)(uint64_t *__return_ptr, const uint16_t *(*)@<X0>(const uint16_t *@<X0>, const uint16_t **@<X8>), uint64_t **, uint64_t, uint64_t, uint64_t);
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t result;
  uint64_t v40;
  uint64_t *v41;
  uint64_t v42;
  int v43;
  uint64_t *v44;
  uint64_t v45;
  uint64_t v46;
  char *v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  __int128 v51;

  v12 = *(_QWORD *)(a4 - 8);
  MEMORY[0x1E0C80A78](a1);
  v14 = (char *)&v44 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  v49 = 0;
  if (v15 < 1)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  v16 = *(_QWORD *)(a1 + 16);
  *(_QWORD *)&v51 = v16;
  *((_QWORD *)&v51 + 1) = a2;
  if (!v16)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v17 = specialized Sequence<>.min()(a1);
  if ((v17 & 0x100) != 0)
    goto LABEL_22;
  if ((_BYTE)v17)
  {
    v45 = v12;
    v46 = a5;
    v47 = a3;
    v44 = a6;
    v50 = a1;
    v48 = MEMORY[0x1E0DEE9D8];
    swift_bridgeObjectRetain();
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v16, 0);
    v18 = 0;
    v19 = v48;
    v20 = *(_QWORD *)(v48 + 16);
    do
    {
      v21 = *(unsigned __int8 *)(a1 + v18 + 32);
      v48 = v19;
      v22 = *(_QWORD *)(v19 + 24);
      v23 = v20 + 1;
      if (v20 >= v22 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v22 > 1), v20 + 1, 1);
        v19 = v48;
      }
      ++v18;
      *(_QWORD *)(v19 + 16) = v23;
      *(_QWORD *)(v19 + 8 * v20++ + 32) = v21;
    }
    while (v16 != v18);
    v24 = 0;
    v25 = 1;
    v27 = v46;
    v26 = v47;
    do
    {
      v28 = *(_QWORD *)(v19 + 8 * v24 + 32);
      v29 = v25 * v28;
      if ((unsigned __int128)(v25 * (__int128)v28) >> 64 != (v25 * v28) >> 63)
      {
        __break(1u);
        goto LABEL_17;
      }
      ++v24;
      v25 *= v28;
    }
    while (v23 != v24);
    swift_bridgeObjectRelease();
    v30 = v29 * (_QWORD)a2;
    if ((unsigned __int128)(v29 * (__int128)(uint64_t)a2) >> 64 != (v29 * (uint64_t)a2) >> 63)
      goto LABEL_20;
    v31 = v45;
    (*(void (**)(char *, char *, uint64_t))(v45 + 16))(v14, v26, a4);
    v32 = (*(uint64_t (**)(uint64_t, uint64_t))(v27 + 16))(a4, v27);
    v33 = *(uint64_t (**)(char *, uint64_t))(v31 + 8);
    v34 = v33(v14, a4);
    if (v30 == v32)
    {
      MEMORY[0x1E0C80A78](v34);
      *(&v44 - 4) = &v49;
      *(&v44 - 3) = a2;
      v42 = a1;
      v35 = *(void (**)(uint64_t *__return_ptr, const uint16_t *(*)@<X0>(const uint16_t *@<X0>, const uint16_t **@<X8>), uint64_t **, uint64_t, uint64_t, uint64_t))(v27 + 24);
      v36 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for OpaquePointer?);
      v35(&v48, partial apply for closure #2 in vImage.MultidimensionalLookupTable.init<A>(entryCountPerSourceChannel:destinationChannelCount:data:), &v44 - 6, v36, a4, v27);
      swift_bridgeObjectRelease();
      v37 = v48;
      if (!v48)
        goto LABEL_23;
      v33(v26, a4);
      type metadata accessor for vImage.MultidimensionalLookupTable.LookupTableReference();
      v38 = swift_allocObject();
      *(_QWORD *)(v38 + 16) = v37;
      result = swift_release();
      v40 = v50;
      v41 = v44;
      *v44 = v38;
      v41[1] = v40;
      *((_OWORD *)v41 + 1) = v51;
      return result;
    }
    goto LABEL_21;
  }
LABEL_19:
  __break(1u);
LABEL_20:
  __break(1u);
LABEL_21:
  __break(1u);
LABEL_22:
  __break(1u);
LABEL_23:
  v43 = 0;
  v42 = 158;
  result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t specialized Sequence<>.min()(uint64_t a1)
{
  unint64_t v1;
  unsigned int v2;
  unint64_t v3;
  unint64_t v4;
  unint64_t v5;
  uint8x16_t v6;
  uint8x16_t *v7;
  unint64_t v8;
  uint8x16_t v9;
  uint8x16_t v10;
  uint8x8_t v11;
  uint8x8_t *v12;
  unint64_t v13;
  uint8x8_t v14;
  unint64_t v15;
  unsigned __int8 *v16;
  char v17;
  unsigned int v18;

  v1 = *(_QWORD *)(a1 + 16);
  if (!v1)
  {
    LOBYTE(v2) = 0;
    return v2 | ((v1 == 0) << 8);
  }
  v2 = *(unsigned __int8 *)(a1 + 32);
  v3 = v1 - 1;
  if (v1 != 1)
  {
    if (v1 < 9)
    {
      v4 = 1;
      goto LABEL_17;
    }
    if (v1 >= 0x21)
    {
      v5 = v3 & 0xFFFFFFFFFFFFFFE0;
      v6 = (uint8x16_t)vdupq_n_s8(v2);
      v7 = (uint8x16_t *)(a1 + 49);
      v8 = v3 & 0xFFFFFFFFFFFFFFE0;
      v9 = v6;
      do
      {
        v6 = vminq_u8(v7[-1], v6);
        v9 = vminq_u8(*v7, v9);
        v7 += 2;
        v8 -= 32;
      }
      while (v8);
      v10 = vminq_u8(v6, v9);
      v10.i8[0] = vminvq_u8(v10);
      v2 = v10.i32[0];
      if (v3 == v5)
        return v2 | ((v1 == 0) << 8);
      if ((v3 & 0x18) == 0)
      {
        v4 = v5 | 1;
LABEL_17:
        v15 = v1 - v4;
        v16 = (unsigned __int8 *)(v4 + a1 + 32);
        do
        {
          v18 = *v16++;
          v17 = v18;
          if (v18 < v2)
            LOBYTE(v2) = v17;
          --v15;
        }
        while (v15);
        return v2 | ((v1 == 0) << 8);
      }
    }
    else
    {
      v5 = 0;
    }
    v4 = v3 & 0xFFFFFFFFFFFFFFF8 | 1;
    v11 = (uint8x8_t)vdup_n_s8(v2);
    v12 = (uint8x8_t *)(v5 + a1 + 33);
    v13 = v5 - (v3 & 0xFFFFFFFFFFFFFFF8);
    do
    {
      v14 = *v12++;
      v11 = vmin_u8(v14, v11);
      v13 += 8;
    }
    while (v13);
    LOBYTE(v2) = vminv_u8(v11);
    if (v3 == (v3 & 0xFFFFFFFFFFFFFFF8))
      return v2 | ((v1 == 0) << 8);
    goto LABEL_17;
  }
  return v2 | ((v1 == 0) << 8);
}

const uint16_t *closure #2 in vImage.MultidimensionalLookupTable.init<A>(entryCountPerSourceChannel:destinationChannelCount:data:)@<X0>(uint64_t numDestChannels@<X3>, const uint16_t *result@<X0>, uint64_t a3@<X2>, uint64_t a4@<X4>, const uint16_t **a5@<X8>)
{
  uint64_t v6;

  if (!result)
    goto LABEL_9;
  v6 = *(_QWORD *)(a3 + 16);
  if (v6 > 0xFFFFFFFFLL)
  {
    __break(1u);
  }
  else if (((v6 | numDestChannels) & 0x8000000000000000) == 0)
  {
    if (numDestChannels <= 0xFFFFFFFFLL)
    {
      result = (const uint16_t *)vImageMultidimensionalTable_Create(result, v6, numDestChannels, (const uint8_t *)(a4 + 32), kvImageMDTableHint_Float, 0, 0);
      *a5 = result;
      return result;
    }
    goto LABEL_8;
  }
  __break(1u);
LABEL_8:
  __break(1u);
LABEL_9:
  __break(1u);
  return result;
}

uint64_t vImage.MultidimensionalLookupTable.InterpolationMethod.vImageInterpolationMethod.getter()
{
  unsigned __int8 *v0;

  return *v0;
}

BOOL static vImage.MultidimensionalLookupTable.InterpolationMethod.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void vImage.MultidimensionalLookupTable.InterpolationMethod.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

Swift::Int vImage.MultidimensionalLookupTable.InterpolationMethod.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t vImage.MultidimensionalLookupTable.apply(sources:destinations:interpolation:)(uint64_t result, uint64_t a2, unsigned __int8 *a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v7;
  const vImage_Buffer *v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  __int128 v13;
  __int128 v14;
  unint64_t rowBytes;
  unint64_t width;
  const vImage_Buffer *v17;
  const vImage_Buffer *v18;
  uint64_t *v19;
  uint64_t v20;
  __int128 v21;
  __int128 v22;
  unint64_t v23;
  unint64_t v24;
  const vImage_Buffer *v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  const vImage_Buffer *v30;
  const vImage_Buffer *v31;

  v4 = *(_QWORD *)(v3 + 16);
  if (*(_QWORD *)(result + 16) != v4)
    goto LABEL_19;
  v5 = v3;
  v7 = *(_QWORD *)(v3 + 24);
  if (*(_QWORD *)(a2 + 16) != v7)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  v9 = (const vImage_Buffer *)MEMORY[0x1E0DEE9D8];
  if (v4)
  {
    v10 = result;
    v30 = (const vImage_Buffer *)MEMORY[0x1E0DEE9D8];
    result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v4, 0);
    v9 = v30;
    v11 = v10 + 32;
    while (1)
    {
      v12 = *(_QWORD *)v11;
      if (!*(_QWORD *)(*(_QWORD *)v11 + 16))
        break;
      v13 = *(_OWORD *)(v12 + 32);
      v14 = *(_OWORD *)(v12 + 48);
      width = v30->width;
      rowBytes = v30->rowBytes;
      if (width >= rowBytes >> 1)
      {
        v26 = v14;
        v28 = v13;
        result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(rowBytes > 1, width + 1, 1);
        v14 = v26;
        v13 = v28;
      }
      v30->width = width + 1;
      v17 = &v30[width];
      *(_OWORD *)&v17[1].data = v13;
      *(_OWORD *)&v17[1].width = v14;
      v11 += 8;
      if (!--v4)
        goto LABEL_9;
    }
    __break(1u);
    goto LABEL_18;
  }
LABEL_9:
  v18 = (const vImage_Buffer *)MEMORY[0x1E0DEE9D8];
  if (v7)
  {
    v31 = (const vImage_Buffer *)MEMORY[0x1E0DEE9D8];
    result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v7, 0);
    v18 = v31;
    v19 = (uint64_t *)(a2 + 32);
    while (1)
    {
      v20 = *v19;
      if (!*(_QWORD *)(*v19 + 16))
        break;
      v21 = *(_OWORD *)(v20 + 32);
      v22 = *(_OWORD *)(v20 + 48);
      v24 = v31->width;
      v23 = v31->rowBytes;
      if (v24 >= v23 >> 1)
      {
        v27 = v22;
        v29 = v21;
        result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v23 > 1, v24 + 1, 1);
        v22 = v27;
        v21 = v29;
      }
      v31->width = v24 + 1;
      v25 = &v31[v24];
      *(_OWORD *)&v25[1].data = v21;
      *(_OWORD *)&v25[1].width = v22;
      ++v19;
      if (!--v7)
        goto LABEL_15;
    }
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
LABEL_15:
  if (*(_QWORD *)v5)
  {
    vImageMultiDimensionalInterpolatedLookupTable_PlanarF(v9 + 1, v18 + 1, 0, *(vImage_MultidimensionalTable *)(*(_QWORD *)v5 + 16), (vImage_InterpolationMethod)*a3, 0);
    swift_bridgeObjectRelease();
    return swift_bridgeObjectRelease();
  }
LABEL_21:
  __break(1u);
  return result;
}

const vImage_Buffer *vImage.MultidimensionalLookupTable.apply<A, B>(source:destination:interpolation:)(uint64_t a1, uint64_t a2, unsigned __int8 *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  _QWORD *v7;
  const vImage_Buffer *v13;
  const vImage_Buffer *result;
  const vImage_Buffer *v15;
  uint64_t v16;
  const vImage_Buffer *v17;
  uint64_t v18;

  v13 = (const vImage_Buffer *)v7[2];
  result = (const vImage_Buffer *)(*(uint64_t (**)(uint64_t, uint64_t))(a6 + 32))(a4, a6);
  if (result != v13)
  {
    __break(1u);
    goto LABEL_6;
  }
  v15 = (const vImage_Buffer *)v7[3];
  result = (const vImage_Buffer *)(*(uint64_t (**)(uint64_t, uint64_t))(a7 + 32))(a5, a7);
  if (result != v15)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  type metadata accessor for vImage.PixelBuffer(0, a4, *(_QWORD *)(a6 + 8), v16);
  v17 = (const vImage_Buffer *)vImage.PixelBuffer<>.vImageBuffers.getter();
  type metadata accessor for vImage.PixelBuffer(0, a5, *(_QWORD *)(a7 + 8), v18);
  result = (const vImage_Buffer *)vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*v7)
  {
    vImageMultiDimensionalInterpolatedLookupTable_PlanarF(v17 + 1, result + 1, 0, *(vImage_MultidimensionalTable *)(*v7 + 16), (vImage_InterpolationMethod)*a3, 0);
    swift_bridgeObjectRelease();
    return (const vImage_Buffer *)swift_bridgeObjectRelease();
  }
LABEL_7:
  __break(1u);
  return result;
}

const uint16_t *partial apply for closure #2 in vImage.MultidimensionalLookupTable.init<A>(entryCountPerSourceChannel:destinationChannelCount:data:)@<X0>(const uint16_t *a1@<X0>, const uint16_t **a2@<X8>)
{
  uint64_t *v2;

  return closure #2 in vImage.MultidimensionalLookupTable.init<A>(entryCountPerSourceChannel:destinationChannelCount:data:)(v2[3], a1, v2[2], v2[4], a2);
}

uint64_t type metadata accessor for vImage.MultidimensionalLookupTable.LookupTableReference()
{
  return objc_opt_self();
}

unint64_t lazy protocol witness table accessor for type vImage.MultidimensionalLookupTable.InterpolationMethod and conformance vImage.MultidimensionalLookupTable.InterpolationMethod()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImage.MultidimensionalLookupTable.InterpolationMethod and conformance vImage.MultidimensionalLookupTable.InterpolationMethod;
  if (!lazy protocol witness table cache variable for type vImage.MultidimensionalLookupTable.InterpolationMethod and conformance vImage.MultidimensionalLookupTable.InterpolationMethod)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImage.MultidimensionalLookupTable.InterpolationMethod, &type metadata for vImage.MultidimensionalLookupTable.InterpolationMethod);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.MultidimensionalLookupTable.InterpolationMethod and conformance vImage.MultidimensionalLookupTable.InterpolationMethod);
  }
  return result;
}

uint64_t destroy for vImage.MultidimensionalLookupTable()
{
  swift_release();
  return swift_bridgeObjectRelease();
}

uint64_t initializeWithCopy for vImage.MultidimensionalLookupTable(uint64_t a1, uint64_t a2)
{
  uint64_t v3;

  v3 = *(_QWORD *)(a2 + 8);
  *(_QWORD *)a1 = *(_QWORD *)a2;
  *(_QWORD *)(a1 + 8) = v3;
  *(_OWORD *)(a1 + 16) = *(_OWORD *)(a2 + 16);
  swift_retain();
  swift_bridgeObjectRetain();
  return a1;
}

_QWORD *assignWithCopy for vImage.MultidimensionalLookupTable(_QWORD *a1, _QWORD *a2)
{
  *a1 = *a2;
  swift_retain();
  swift_release();
  a1[1] = a2[1];
  swift_bridgeObjectRetain();
  swift_bridgeObjectRelease();
  a1[2] = a2[2];
  a1[3] = a2[3];
  return a1;
}

_OWORD *assignWithTake for vImage.MultidimensionalLookupTable(_OWORD *a1, _OWORD *a2)
{
  swift_release();
  *a1 = *a2;
  swift_bridgeObjectRelease();
  a1[1] = a2[1];
  return a1;
}

uint64_t getEnumTagSinglePayload for vImage.MultidimensionalLookupTable(uint64_t a1, int a2)
{
  unint64_t v2;

  if (!a2)
    return 0;
  if (a2 < 0 && *(_BYTE *)(a1 + 32))
    return *(_DWORD *)a1 + 0x80000000;
  v2 = *(_QWORD *)(a1 + 8);
  if (v2 >= 0xFFFFFFFF)
    LODWORD(v2) = -1;
  return (v2 + 1);
}

uint64_t storeEnumTagSinglePayload for vImage.MultidimensionalLookupTable(uint64_t result, int a2, int a3)
{
  if (a2 < 0)
  {
    *(_QWORD *)(result + 16) = 0;
    *(_QWORD *)(result + 24) = 0;
    *(_QWORD *)result = a2 ^ 0x80000000;
    *(_QWORD *)(result + 8) = 0;
    if (a3 < 0)
      *(_BYTE *)(result + 32) = 1;
  }
  else
  {
    if ((a3 & 0x80000000) == 0)
    {
      if (!a2)
        return result;
LABEL_8:
      *(_QWORD *)(result + 8) = (a2 - 1);
      return result;
    }
    *(_BYTE *)(result + 32) = 0;
    if (a2)
      goto LABEL_8;
  }
  return result;
}

ValueMetadata *type metadata accessor for vImage.MultidimensionalLookupTable()
{
  return &type metadata for vImage.MultidimensionalLookupTable;
}

uint64_t method lookup function for vImage.MultidimensionalLookupTable.LookupTableReference()
{
  return swift_lookUpClassMethod();
}

uint64_t storeEnumTagSinglePayload for vImage.MultidimensionalLookupTable.InterpolationMethod(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 2 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 2) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFE)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFD)
    return ((uint64_t (*)(void))((char *)&loc_1CAB4D894 + 4 * byte_1CAB62E95[v4]))();
  *a1 = a2 + 2;
  return ((uint64_t (*)(void))((char *)sub_1CAB4D8C8 + 4 * byte_1CAB62E90[v4]))();
}

uint64_t sub_1CAB4D8C8(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB4D8D0(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB4D8D8);
  return result;
}

uint64_t sub_1CAB4D8E4(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB4D8ECLL);
  *(_BYTE *)result = a2 + 2;
  return result;
}

uint64_t sub_1CAB4D8F0(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB4D8F8(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vImage.MultidimensionalLookupTable.InterpolationMethod()
{
  return &type metadata for vImage.MultidimensionalLookupTable.InterpolationMethod;
}

uint64_t BNNS.FusedParametersLayer.__allocating_init(input:output:fusedLayerParameters:filterParameters:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint32_t a4, size_t a5, int (__cdecl *a6)(void **, size_t, size_t), void (__cdecl *a7)(void *))
{
  uint64_t v14;
  uint64_t v15;
  int v16;
  int v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  int v24;
  uint64_t v25;
  uint64_t v26;
  int v27;
  int v28;
  unsigned int v29;
  void *v30;
  void *v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  BOOL v36;
  __int128 __dst[53];
  _QWORD __src[105];
  _BYTE v39[40];
  _QWORD v40[5];
  _QWORD v41[5];
  __int128 v42[2];
  uint64_t v43;
  char v44[40];
  _QWORD v45[3];
  uint64_t v46;
  uint64_t v47;
  _QWORD v48[3];
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  uint64_t v60;
  int v61;
  uint64_t v62;
  int v63;
  uint64_t v64;
  uint64_t v65;

  v65 = *MEMORY[0x1E0C80C00];
  if (*(_QWORD *)(a3 + 16) != 2)
  {
    __break(1u);
    goto LABEL_40;
  }
  outlined init with copy of BNNSOptimizer(a3 + 32, (uint64_t)__src);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParameters);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParametersWrapper);
  if ((swift_dynamicCast() & 1) == 0)
  {
    memset(__dst, 0, 40);
    swift_bridgeObjectRelease();
    outlined destroy of FusableTernaryInputLayerParametersWrapper?((uint64_t)__dst, &demangling cache variable for type metadata for FusableLayerParametersWrapper?);
    return 0;
  }
  outlined init with take of FusableLayerParametersWrapper(__dst, (uint64_t)v48);
  if (*(_QWORD *)(a3 + 16) < 2uLL)
LABEL_40:
    __break(1u);
  outlined init with copy of BNNSOptimizer(a3 + 72, (uint64_t)v44);
  swift_bridgeObjectRelease();
  if ((swift_dynamicCast() & 1) == 0)
  {
    v43 = 0;
    memset(v42, 0, sizeof(v42));
    outlined destroy of FusableTernaryInputLayerParametersWrapper?((uint64_t)v42, &demangling cache variable for type metadata for FusableLayerParametersWrapper?);
LABEL_20:
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v48);
    return 0;
  }
  outlined init with take of FusableLayerParametersWrapper(v42, (uint64_t)v45);
  v14 = a2[17];
  v15 = a2[19];
  v16 = *((_DWORD *)a2 + 40);
  v17 = *(_DWORD *)(a1 + 144);
  v51 = *a2;
  v52 = *(_OWORD *)(a2 + 1);
  v53 = *(_OWORD *)(a2 + 3);
  v54 = *(_OWORD *)(a2 + 5);
  v55 = *(_OWORD *)(a2 + 7);
  v56 = *(_OWORD *)(a2 + 9);
  v57 = *(_OWORD *)(a2 + 11);
  v58 = *(_OWORD *)(a2 + 13);
  v59 = *(_OWORD *)(a2 + 15);
  v60 = v14;
  v61 = v17;
  v62 = v15;
  v63 = v16;
  v64 = *(uint64_t *)((char *)a2 + 164);
  v18 = v49;
  v19 = v50;
  __swift_mutable_project_boxed_opaque_existential_1((uint64_t)v48, v49);
  (*(void (**)(_QWORD *__return_ptr, uint64_t, uint64_t *, uint64_t, uint64_t))(v19 + 8))(v41, a1, &v51, v18, v19);
  v20 = v46;
  v21 = v47;
  __swift_mutable_project_boxed_opaque_existential_1((uint64_t)v45, v46);
  (*(void (**)(_QWORD *__return_ptr, uint64_t *, uint64_t *, uint64_t, uint64_t))(v21 + 8))(v40, &v51, a2, v20, v21);
  v22 = v49;
  v23 = v50;
  __swift_project_boxed_opaque_existential_1(v48, v49);
  v24 = (*(uint64_t (**)(uint64_t, uint64_t))(v23 + 16))(v22, v23);
  v25 = v46;
  v26 = v47;
  __swift_project_boxed_opaque_existential_1(v45, v46);
  v27 = (*(uint64_t (**)(uint64_t, uint64_t))(v26 + 16))(v25, v26);
  v28 = v27;
  v29 = v27 - 2;
  if ((v24 || v29 >= 4) && (v24 != 6 || v29 > 3))
  {
    if (v24 == 1 && v29 <= 3)
    {
      outlined init with copy of BNNSOptimizer((uint64_t)v41, (uint64_t)v39);
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
      type metadata accessor for BNNSLayerParametersFullyConnected(0);
      swift_dynamicCast();
      memcpy(__dst, __src, 0x2F0uLL);
      v30 = specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)((uint64_t)__dst, (uint64_t)v40, a4, a5, a6, a7, 1, v28);
    }
    else
    {
      v36 = v24 == 6 || v24 == 0;
      if (v36 && v27 == 7)
      {
        outlined init with copy of BNNSOptimizer((uint64_t)v41, (uint64_t)v39);
        __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
        type metadata accessor for BNNSLayerParametersConvolution(0);
        swift_dynamicCast();
        memcpy(__dst, __src, 0x348uLL);
        v30 = specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)((uint64_t)__dst, (uint64_t)v40, a4, a5, a6, a7, v24, 7);
      }
      else if (v24 == 1 && v27 == 7)
      {
        outlined init with copy of BNNSOptimizer((uint64_t)v41, (uint64_t)v39);
        __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
        type metadata accessor for BNNSLayerParametersFullyConnected(0);
        swift_dynamicCast();
        memcpy(__dst, __src, 0x2F0uLL);
        v30 = specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)((uint64_t)__dst, (uint64_t)v40, a4, a5, a6, a7, 1, 7);
      }
      else
      {
        if (v24 != 8 || v29 > 3)
          goto LABEL_19;
        outlined init with copy of BNNSOptimizer((uint64_t)v41, (uint64_t)v39);
        __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
        type metadata accessor for BNNSLayerParametersArithmetic(0);
        swift_dynamicCast();
        LODWORD(__dst[0]) = __src[0];
        *((_QWORD *)&__dst[0] + 1) = __src[1];
        LODWORD(__dst[1]) = __src[2];
        *(__int128 *)((char *)&__dst[1] + 4) = *(_OWORD *)((char *)&__src[2] + 4);
        DWORD1(__dst[2]) = HIDWORD(__src[4]);
        *(__int128 *)((char *)&__dst[2] + 8) = *(_OWORD *)&__src[5];
        *((_QWORD *)&__dst[3] + 1) = __src[7];
        v30 = specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)((uint64_t)__dst, (uint64_t)v40, a4, a5, a6, a7, 8, v28);
      }
    }
  }
  else
  {
    outlined init with copy of BNNSOptimizer((uint64_t)v41, (uint64_t)v39);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
    type metadata accessor for BNNSLayerParametersConvolution(0);
    swift_dynamicCast();
    memcpy(__dst, __src, 0x348uLL);
    v30 = specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)((uint64_t)__dst, (uint64_t)v40, a4, a5, a6, a7, v24, v28);
  }
  v31 = v30;
  type metadata accessor for BNNS.FusedParametersLayer();
  v32 = swift_allocObject();
  v33 = v32;
  *(_QWORD *)(v32 + 24) = MEMORY[0x1E0DEE9D8];
  if (!v31)
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
LABEL_19:
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v40);
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v41);
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v45);
    goto LABEL_20;
  }
  *(_QWORD *)(v32 + 16) = v31;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<FusableLayerParametersWrapperDeallocatable?>);
  v34 = swift_allocObject();
  *(_OWORD *)(v34 + 16) = xmmword_1CAB5E440;
  outlined init with copy of BNNSOptimizer((uint64_t)v48, (uint64_t)__src);
  swift_retain();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParametersWrapperDeallocatable);
  if ((swift_dynamicCast() & 1) == 0)
  {
    *(_QWORD *)(v34 + 64) = 0;
    *(_OWORD *)(v34 + 32) = 0u;
    *(_OWORD *)(v34 + 48) = 0u;
  }
  outlined init with copy of BNNSOptimizer((uint64_t)v45, (uint64_t)__dst);
  if ((swift_dynamicCast() & 1) == 0)
  {
    *(_QWORD *)(v34 + 104) = 0;
    *(_OWORD *)(v34 + 72) = 0u;
    *(_OWORD *)(v34 + 88) = 0u;
  }
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v40);
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v41);
  *(_QWORD *)(v33 + 24) = v34;
  swift_release();
  swift_bridgeObjectRelease();
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v45);
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v48);
  return v33;
}

uint64_t BNNS.FusedParametersLayer.__ivar_destroyer()
{
  return swift_bridgeObjectRelease();
}

uint64_t BNNS.FusedParametersLayer.deinit()
{
  uint64_t v0;
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  _QWORD v7[3];
  uint64_t v8;
  uint64_t v9;
  _BYTE v10[40];

  v1 = *(_QWORD *)(v0 + 24);
  v2 = *(_QWORD *)(v1 + 16);
  if (v2)
  {
    v3 = v1 + 32;
    swift_bridgeObjectRetain();
    do
    {
      outlined init with copy of FusableLayerParametersWrapperDeallocatable?(v3, (uint64_t)v10);
      outlined init with copy of FusableLayerParametersWrapperDeallocatable?((uint64_t)v10, (uint64_t)v7);
      v4 = v8;
      if (v8)
      {
        v5 = v9;
        __swift_project_boxed_opaque_existential_1(v7, v8);
        (*(void (**)(uint64_t, uint64_t))(v5 + 8))(v4, v5);
        __swift_destroy_boxed_opaque_existential_1((uint64_t)v7);
      }
      else
      {
        outlined destroy of FusableTernaryInputLayerParametersWrapper?((uint64_t)v7, &demangling cache variable for type metadata for FusableLayerParametersWrapperDeallocatable?);
      }
      outlined destroy of FusableTernaryInputLayerParametersWrapper?((uint64_t)v10, &demangling cache variable for type metadata for FusableLayerParametersWrapperDeallocatable?);
      v3 += 40;
      --v2;
    }
    while (v2);
    swift_bridgeObjectRelease();
  }
  BNNSFilterDestroy(*(void **)(v0 + 16));
  swift_bridgeObjectRelease();
  return v0;
}

uint64_t BNNS.FusedParametersLayer.__deallocating_deinit()
{
  BNNS.FusedParametersLayer.deinit();
  return swift_deallocClassInstance();
}

uint64_t BNNS.FusedParametersLayer.__allocating_init(bnnsFilter:)(uint64_t a1)
{
  uint64_t v2;
  uint64_t v3;

  v2 = swift_allocObject();
  v3 = v2;
  *(_QWORD *)(v2 + 24) = MEMORY[0x1E0DEE9D8];
  if (a1)
  {
    *(_QWORD *)(v2 + 16) = a1;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v3;
}

uint64_t type metadata accessor for BNNS.FusedParametersLayer()
{
  return objc_opt_self();
}

uint64_t outlined init with copy of FusableLayerParametersWrapperDeallocatable?(uint64_t a1, uint64_t a2)
{
  uint64_t v4;

  v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParametersWrapperDeallocatable?);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(_QWORD *)(v4 - 8) + 16))(a2, a1, v4);
  return a2;
}

uint64_t dispatch thunk of FusableLayerParametersWrapper.layerParameters(input:output:)(uint64_t *a1, uint64_t *a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4;
  int v5;
  uint64_t v6;
  int v7;
  uint64_t v8;
  int v9;
  uint64_t v10;
  int v11;
  uint64_t (*v12)(uint64_t *, uint64_t *);
  uint64_t v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  uint64_t v23;
  int v24;
  uint64_t v25;
  int v26;
  uint64_t v27;
  uint64_t v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  uint64_t v37;
  int v38;
  uint64_t v39;
  int v40;
  uint64_t v41;

  v4 = a1[17];
  v5 = *((_DWORD *)a1 + 36);
  v6 = a1[19];
  v7 = *((_DWORD *)a1 + 40);
  v8 = a2[17];
  v9 = *((_DWORD *)a2 + 36);
  v10 = a2[19];
  v11 = *((_DWORD *)a2 + 40);
  v12 = *(uint64_t (**)(uint64_t *, uint64_t *))(a4 + 8);
  v28 = *a1;
  v29 = *(_OWORD *)(a1 + 1);
  v30 = *(_OWORD *)(a1 + 3);
  v31 = *(_OWORD *)(a1 + 5);
  v32 = *(_OWORD *)(a1 + 7);
  v33 = *(_OWORD *)(a1 + 9);
  v34 = *(_OWORD *)(a1 + 11);
  v35 = *(_OWORD *)(a1 + 13);
  v36 = *(_OWORD *)(a1 + 15);
  v37 = v4;
  v38 = v5;
  v39 = v6;
  v40 = v7;
  v41 = *(uint64_t *)((char *)a1 + 164);
  v14 = *a2;
  v15 = *(_OWORD *)(a2 + 1);
  v16 = *(_OWORD *)(a2 + 3);
  v17 = *(_OWORD *)(a2 + 5);
  v18 = *(_OWORD *)(a2 + 7);
  v19 = *(_OWORD *)(a2 + 9);
  v20 = *(_OWORD *)(a2 + 11);
  v21 = *(_OWORD *)(a2 + 13);
  v22 = *(_OWORD *)(a2 + 15);
  v23 = v8;
  v24 = v9;
  v25 = v10;
  v26 = v11;
  v27 = *(uint64_t *)((char *)a2 + 164);
  return v12(&v28, &v14);
}

uint64_t dispatch thunk of FusableLayerParametersWrapperDeallocatable.deallocate()(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 8))();
}

uint64_t method lookup function for BNNS.FusedParametersLayer()
{
  return swift_lookUpClassMethod();
}

uint64_t BNNS.PaddingMode.bnnsPaddingMode.getter()
{
  uint64_t v0;

  if (*(unsigned __int8 *)(v0 + 4) < 2u)
    return 0;
  if (*(_DWORD *)v0)
    return 2;
  return 1;
}

uint64_t BNNS.PaddingMode.paddingBitPattern.getter()
{
  unsigned __int8 *v0;

  if (v0[4] >= 2u)
    return 0;
  else
    return *(unsigned int *)v0;
}

uint64_t BNNS.PaddingLayer.__allocating_init(input:output:mode:size:filterParameters:)(_OWORD *a1, __int128 *a2, int *a3, char *a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  int v25;
  unsigned int v26;
  unint64_t v27;
  uint64_t v28;
  uint64_t v29;
  int v30;
  int v31;
  int v32;
  int *v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  int v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  _OWORD __src[22];
  _BYTE __dst[352];
  unint64_t v51;
  uint64_t v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  int v60;
  int v61;
  uint64_t v62;

  v62 = *MEMORY[0x1E0C80C00];
  v12 = a2[8];
  v13 = a2[9];
  v14 = a2[6];
  __src[18] = a2[7];
  __src[19] = v12;
  v15 = a2[10];
  __src[20] = v13;
  __src[21] = v15;
  v16 = a2[4];
  v17 = a2[5];
  v18 = a2[2];
  __src[14] = a2[3];
  __src[15] = v16;
  __src[16] = v17;
  __src[17] = v14;
  v19 = *a2;
  __src[12] = a2[1];
  __src[13] = v18;
  v20 = a1[9];
  __src[8] = a1[8];
  __src[9] = v20;
  __src[10] = a1[10];
  __src[11] = v19;
  v21 = a1[5];
  __src[4] = a1[4];
  __src[5] = v21;
  v22 = a1[7];
  __src[6] = a1[6];
  __src[7] = v22;
  v23 = a1[1];
  __src[0] = *a1;
  __src[1] = v23;
  v24 = a1[3];
  __src[2] = a1[2];
  __src[3] = v24;
  v25 = *a3;
  v26 = *((unsigned __int8 *)a3 + 4);
  v27 = specialized static BNNS.arrayToTuple<A>(_:fillValue:)(&v44, &v43, &v42, &v41, &v40, &v39, &v38, a4, 0, 0);
  v29 = v28;
  swift_bridgeObjectRelease();
  if (v25)
    v30 = 2;
  else
    v30 = 1;
  if (v26 >= 2)
    v31 = 0;
  else
    v31 = v25;
  if (v26 >= 2)
    v32 = v30;
  else
    v32 = 0;
  memcpy(__dst, __src, sizeof(__dst));
  v51 = v27;
  v52 = v29;
  v53 = v44;
  v54 = v43;
  v55 = v42;
  v56 = v41;
  v57 = v40;
  v58 = v39;
  v59 = v38;
  v60 = v32;
  v61 = v31;
  if (a7 == 1)
  {
    v33 = 0;
  }
  else
  {
    v45 = a5;
    v46 = a6;
    v47 = a7;
    v48 = a8;
    v33 = &v45;
  }
  v34 = MEMORY[0x1D1794600](__dst, v33);
  type metadata accessor for BNNS.PaddingLayer();
  v35 = swift_allocObject();
  v36 = v35;
  if (v34)
  {
    *(_QWORD *)(v35 + 16) = v34;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v36;
}

uint64_t type metadata accessor for BNNS.PaddingLayer()
{
  return objc_opt_self();
}

uint64_t BNNS.PaddingLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.PaddingLayer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

ValueMetadata *type metadata accessor for BNNS.PaddingMode()
{
  return &type metadata for BNNS.PaddingMode;
}

uint64_t static vDSP.slidingWindowSum<A>(_:usingWindowLength:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return static vDSP.slidingWindowSum<A>(_:usingWindowLength:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.slidingWindowSum<A>(_:usingWindowLength:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.slidingWindowSum<A>(_:usingWindowLength:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.slidingWindowSum<A>(_:usingWindowLength:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vDSP.slidingWindowSum<A>(_:usingWindowLength:)(uint64_t a1, _QWORD *a2)
{
  return partial apply for closure #1 in static vDSP.slidingWindowSum<A>(_:usingWindowLength:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:));
}

{
  return partial apply for closure #1 in static vDSP.slidingWindowSum<A>(_:usingWindowLength:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:));
}

uint64_t static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  return static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:));
}

{
  return static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:));
}

uint64_t static vDSP.slidingWindowSum<A>(_:usingWindowLength:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t))
{
  uint64_t result;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;

  result = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  v13 = result - a2;
  if (__OFSUB__(result, a2))
  {
    __break(1u);
  }
  else
  {
    result = v13 + 1;
    if (!__OFADD__(v13, 1))
    {
      v14 = a3;
      v15 = a4;
      v16 = a1;
      v17 = a2;
      v18 = MEMORY[0x1E0C80A78](result);
      return a6(v18, a5);
    }
  }
  __break(1u);
  return result;
}

uint64_t static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t (*v18)(uint64_t);
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t result;
  uint64_t v24;
  BOOL v25;
  uint64_t v26;
  _QWORD v27[2];
  uint64_t v28;

  v28 = a8;
  v14 = *(_QWORD *)(a4 - 8);
  MEMORY[0x1E0C80A78](a1);
  v16 = (char *)v27 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  v18 = *(uint64_t (**)(uint64_t))(*(_QWORD *)(v17 + 8) + 16);
  v27[1] = v19;
  v21 = v18(v20);
  (*(void (**)(char *, uint64_t, uint64_t))(v14 + 16))(v16, a1, a4);
  v22 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  result = (*(uint64_t (**)(char *, uint64_t))(v14 + 8))(v16, a4);
  v24 = v21 + a2;
  if (__OFADD__(v21, a2))
  {
    __break(1u);
    goto LABEL_6;
  }
  v25 = __OFSUB__(v24, 1);
  v26 = v24 - 1;
  if (v25)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (v22 == v26)
  {
    MEMORY[0x1E0C80A78](result);
    v27[-8] = a4;
    v27[-7] = a5;
    v27[-6] = a6;
    v27[-5] = a7;
    v27[-4] = a1;
    v27[-3] = v21;
    v27[-2] = a2;
    return (*(uint64_t (**)(uint64_t))(a7 + 16))(v28);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(uint64_t result, uint64_t a2, _QWORD *a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(void))
{
  if (!result)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*a3)
  {
    if (((a5 | a4) & 0x8000000000000000) == 0)
      return a6();
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:));
}

{
  return partial apply for closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:));
}

uint64_t partial apply for closure #1 in static vDSP.slidingWindowSum<A>(_:usingWindowLength:)(uint64_t a1, _QWORD *a2, uint64_t *a3, unint64_t *a4, uint64_t (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  _QWORD *v5;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t result;

  v9 = v5[2];
  v10 = v5[3];
  v11 = v5[4];
  v12 = v5[5];
  v13 = v5[6];
  v14 = __swift_instantiateConcreteTypeFromMangledName(a3);
  v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a4, a3);
  result = a5(v11, v12, a1, v9, v14, v10, v15);
  *a2 = v13;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  _QWORD v6[3];
  __int128 v7;

  v3 = *(_QWORD *)(v2 + 16);
  v4 = *(_QWORD *)(v2 + 32);
  v6[2] = a1;
  v7 = *(_OWORD *)(v2 + 56);
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v4 + 24))(a2, v6, MEMORY[0x1E0DEE9C0] + 8, v3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(a1, a2, *(_QWORD **)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C988]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(a1, a2, *(_QWORD **)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), MEMORY[0x1E0C8C980]);
}

uint64_t static vForce.log<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.log<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.log<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vForce.log<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.log<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.log<A, B>(_:result:));
}

uint64_t static vForce.log<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.log<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.log<A, B>(_:result:));
}

uint64_t static vForce.log1p<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.log1p<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.log1p<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.log1p<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.log1p<A, B>(_:result:));
}

{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.log1p<A, B>(_:result:));
}

uint64_t static vForce.atan2<A, B, C>(x:y:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vForce.atan2<A, B, C>(x:y:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vForce.atan2<A, B, C>(x:y:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  char *v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  void (*v23)(char *);
  uint64_t v24;
  void (*v25)(char *, uint64_t, uint64_t);
  uint64_t (*v26)(uint64_t, uint64_t);
  uint64_t v27;
  uint64_t v28;
  uint64_t (*v29)(void);
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  void (*v35)(char *, uint64_t);
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  void (*v43)(char *, uint64_t);
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  int v55;
  uint64_t v56;

  v53 = a8;
  v54 = a3;
  v49 = a6;
  v56 = *MEMORY[0x1E0C80C00];
  v13 = *(_QWORD *)(a5 - 8);
  v14 = MEMORY[0x1E0C80A78](a1);
  v16 = (char *)&v44 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  v17 = MEMORY[0x1E0C80A78](v14);
  v19 = (char *)&v44 - v18;
  MEMORY[0x1E0C80A78](v17);
  v21 = (char *)&v44 - ((v20 + 15) & 0xFFFFFFFFFFFFFFF0);
  v51 = v22;
  v23 = *(void (**)(char *))(v22 + 16);
  v45 = v24;
  v23(v21);
  v50 = v13;
  v25 = *(void (**)(char *, uint64_t, uint64_t))(v13 + 16);
  v46 = a2;
  v25(v19, a2, a5);
  v26 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  v52 = a4;
  v47 = a7;
  v27 = v26(a4, a7);
  v48 = a9;
  v28 = *(_QWORD *)(a9 + 8);
  v29 = *(uint64_t (**)(void))(v28 + 16);
  v30 = v49;
  v31 = v29();
  v25(v16, (uint64_t)v19, a5);
  if (v27 != v31)
  {
    v43 = *(void (**)(char *, uint64_t))(v50 + 8);
    v43(v16, a5);
    v43(v19, a5);
    (*(void (**)(char *, uint64_t))(v51 + 8))(v21, v52);
    goto LABEL_7;
  }
  v32 = (*(uint64_t (**)(uint64_t))(v53 + 16))(a5);
  v33 = v30;
  v34 = ((uint64_t (*)(uint64_t, uint64_t))v29)(v30, v28);
  v35 = *(void (**)(char *, uint64_t))(v50 + 8);
  v35(v16, a5);
  v35(v19, a5);
  v36 = v52;
  (*(void (**)(char *, uint64_t))(v51 + 8))(v21, v52);
  if (v32 != v34)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  v37 = ((uint64_t (*)(uint64_t, uint64_t))v29)(v33, v28);
  if (v37 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (v37 > 0x7FFFFFFF)
    goto LABEL_9;
  v55 = v37;
  MEMORY[0x1E0C80A78](v37);
  *(&v44 - 10) = v36;
  *(&v44 - 9) = a5;
  v39 = v47;
  v38 = v48;
  *(&v44 - 8) = v33;
  *(&v44 - 7) = v39;
  *(&v44 - 6) = v53;
  *(&v44 - 5) = v38;
  v40 = v46;
  *(&v44 - 4) = v45;
  *(&v44 - 3) = v40;
  *(&v44 - 2) = (uint64_t)&v55;
  return (*(uint64_t (**)(uint64_t))(v38 + 16))(v41);
}

uint64_t static vForce.atan2<A, B>(x:y:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.atan2<A, B>(x:y:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.atan2<A, B>(x:y:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vForce.log<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.log<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.log<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.log1p<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.log1p<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.log1p<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.log1p<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.log1p<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.log1p<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.atan2<A, B, C>(x:y:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.atan2<A, B, C>(x:y:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.atan2<A, B, C>(x:y:result:));
}

uint64_t partial apply for closure #1 in static vForce.atan2<A, B>(x:y:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vForce.atan2<A, B>(x:y:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.atan2<A, B, C>(x:y:result:));
}

{
  return partial apply for closure #1 in static vForce.atan2<A, B>(x:y:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.atan2<A, B, C>(x:y:result:));
}

uint64_t partial apply for closure #1 in static vForce.atan2<A, B>(x:y:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t *v5;

  return closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, v5[6], v5[7], v5[2], v5[3], v5[4], v5[5], a3, a4, a5);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.atan2<A, B, C>(x:y:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.atan2<A, B, C>(x:y:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.atan2<A, B, C>(x:y:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.atan2<A, B, C>(x:y:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8D8F8]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8D900]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.log1p<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DA60]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DA68]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.log<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DA48]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E0C8DA98]);
}

uint64_t vDSP.FourierTransformDirection.dftDirection.getter()
{
  _BYTE *v0;

  if (*v0)
    return 0xFFFFFFFFLL;
  else
    return 1;
}

BOOL static vDSP.FourierTransformDirection.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void vDSP.FourierTransformDirection.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

Swift::Int vDSP.FourierTransformDirection.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

unint64_t lazy protocol witness table accessor for type vDSP.FourierTransformDirection and conformance vDSP.FourierTransformDirection()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vDSP.FourierTransformDirection and conformance vDSP.FourierTransformDirection;
  if (!lazy protocol witness table cache variable for type vDSP.FourierTransformDirection and conformance vDSP.FourierTransformDirection)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vDSP.FourierTransformDirection, &type metadata for vDSP.FourierTransformDirection);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.FourierTransformDirection and conformance vDSP.FourierTransformDirection);
  }
  return result;
}

uint64_t storeEnumTagSinglePayload for vDSP.FourierTransformDirection(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 1 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 1) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFF)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFE)
    return ((uint64_t (*)(void))((char *)&loc_1CAB4F3B8 + 4 * byte_1CAB63105[v4]))();
  *a1 = a2 + 1;
  return ((uint64_t (*)(void))((char *)sub_1CAB4F3EC + 4 * byte_1CAB63100[v4]))();
}

uint64_t sub_1CAB4F3EC(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB4F3F4(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB4F3FCLL);
  return result;
}

uint64_t sub_1CAB4F408(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB4F410);
  *(_BYTE *)result = a2 + 1;
  return result;
}

uint64_t sub_1CAB4F414(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB4F41C(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vDSP.FourierTransformDirection()
{
  return &type metadata for vDSP.FourierTransformDirection;
}

uint64_t static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;

  v3 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:));
}

{
  uint64_t v3;

  v3 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v3, partial apply for closure #1 in static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:));
}

uint64_t closure #1 in static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:)(_QWORD **a1, uint64_t *a2, uint64_t a3, float *a4, uint64_t a5, uint64_t a6)
{
  float v11;
  float v12;
  float v13;
  float v14;
  float v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t result;

  v11 = *a4;
  v12 = a4[1];
  v13 = a4[2];
  v14 = a4[3];
  v15 = a4[4];
  **a1 = 0;
  v16 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(a3, (uint64_t)a1, a5, v16, a6, v17, v11, v12, v13, v14, v15);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:)(_QWORD **a1, uint64_t *a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  int v6;
  __int128 v8;
  int v9;

  v3 = *(_QWORD *)(v2 + 16);
  v4 = *(_QWORD *)(v2 + 24);
  v5 = *(_QWORD *)(v2 + 32);
  v6 = *(_DWORD *)(v2 + 56);
  v8 = *(_OWORD *)(v2 + 40);
  v9 = v6;
  return closure #1 in static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:)(a1, a2, v5, (float *)&v8, v3, v4);
}

uint64_t static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7, float a8, float a9, float a10, float a11)
{
  uint64_t v21;
  uint64_t v22;
  char *v23;
  uint64_t (*v24)(uint64_t, uint64_t);
  uint64_t v25;
  uint64_t (*v26)(uint64_t);
  uint64_t v27;
  uint64_t v28;
  uint64_t result;
  uint64_t v30;
  _QWORD v31[2];
  uint64_t v32;

  v32 = a4;
  v21 = *(_QWORD *)(a3 - 8);
  MEMORY[0x1E0C80A78](a1);
  v23 = (char *)v31 - ((v22 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(char *, uint64_t))(v21 + 16))(v23, a1);
  v24 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  v25 = v24(a3, a5);
  v26 = *(uint64_t (**)(uint64_t))(*(_QWORD *)(a6 + 8) + 16);
  v31[1] = a2;
  v27 = v32;
  v28 = v26(v32);
  result = (*(uint64_t (**)(char *, uint64_t))(v21 + 8))(v23, a3);
  if (v25 != v28)
  {
    __break(1u);
    goto LABEL_6;
  }
  result = v24(a3, a5);
  if (__OFSUB__(result, 2))
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (((result - 2) & 0x8000000000000000) == 0)
  {
    MEMORY[0x1E0C80A78](result);
    v31[-10] = a3;
    v31[-9] = v27;
    v31[-8] = a5;
    v31[-7] = a6;
    v31[-6] = a1;
    *(float *)&v31[-5] = a7;
    *((float *)&v31[-5] + 1) = a8;
    *(float *)&v31[-4] = a9;
    *((float *)&v31[-4] + 1) = a10;
    *(float *)&v31[-3] = a11;
    v31[-2] = v30;
    return (*(uint64_t (**)(_QWORD))(a6 + 16))(partial apply for closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:));
  }
LABEL_7:
  __break(1u);
  return result;
}

void closure #1 in closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(const float *a1, int a2, __int128 *a3, float **a4, vDSP_Length __N)
{
  int v5;
  float *v6;
  __int128 v7;
  int v8;

  if (a1)
  {
    v5 = *((_DWORD *)a3 + 4);
    v7 = *a3;
    v8 = v5;
    v6 = *a4;
    if (v6)
    {
      vDSP_deq22(a1, 1, (const float *)&v7, v6, 1, __N);
      return;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
}

uint64_t closure #1 in static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:)(_QWORD *a1, uint64_t *a2, uint64_t a3, double *a4, uint64_t a5, uint64_t a6)
{
  double v11;
  double v12;
  double v13;
  double v14;
  double v15;
  _QWORD *v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t result;

  v11 = *a4;
  v12 = a4[1];
  v13 = a4[2];
  v14 = a4[3];
  v15 = a4[4];
  v16 = (_QWORD *)*a1;
  *v16 = 0;
  v16[1] = 0;
  v17 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v18 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(a3, (uint64_t)a1, a5, v17, a6, v18, v11, v12, v13, v14, v15);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7, double a8, double a9, double a10, double a11)
{
  uint64_t v21;
  uint64_t v22;
  char *v23;
  uint64_t (*v24)(uint64_t, uint64_t);
  uint64_t v25;
  uint64_t (*v26)(uint64_t);
  uint64_t v27;
  uint64_t v28;
  uint64_t result;
  uint64_t v30;
  _QWORD v31[2];
  uint64_t v32;

  v32 = a4;
  v21 = *(_QWORD *)(a3 - 8);
  MEMORY[0x1E0C80A78](a1);
  v23 = (char *)v31 - ((v22 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(char *, uint64_t))(v21 + 16))(v23, a1);
  v24 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  v25 = v24(a3, a5);
  v26 = *(uint64_t (**)(uint64_t))(*(_QWORD *)(a6 + 8) + 16);
  v31[1] = a2;
  v27 = v32;
  v28 = v26(v32);
  result = (*(uint64_t (**)(char *, uint64_t))(v21 + 8))(v23, a3);
  if (v25 != v28)
  {
    __break(1u);
    goto LABEL_6;
  }
  result = v24(a3, a5);
  if (__OFSUB__(result, 2))
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (((result - 2) & 0x8000000000000000) == 0)
  {
    MEMORY[0x1E0C80A78](result);
    v31[-12] = a3;
    v31[-11] = v27;
    v31[-10] = a5;
    v31[-9] = a6;
    v31[-8] = a1;
    *(double *)&v31[-7] = a7;
    *(double *)&v31[-6] = a8;
    *(double *)&v31[-5] = a9;
    *(double *)&v31[-4] = a10;
    *(double *)&v31[-3] = a11;
    v31[-2] = v30;
    return (*(uint64_t (**)(_QWORD))(a6 + 16))(partial apply for closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:));
  }
LABEL_7:
  __break(1u);
  return result;
}

void closure #1 in closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(const double *a1, int a2, uint64_t a3, double **a4, vDSP_Length __N)
{
  uint64_t v5;
  __int128 v6;
  double *v7;
  _OWORD v8[2];
  uint64_t v9;

  if (a1)
  {
    v5 = *(_QWORD *)(a3 + 32);
    v6 = *(_OWORD *)(a3 + 16);
    v8[0] = *(_OWORD *)a3;
    v8[1] = v6;
    v9 = v5;
    v7 = *a4;
    if (v7)
    {
      vDSP_deq22D(a1, 1, (const double *)v8, v7, 1, __N);
      return;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
}

uint64_t partial apply for closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(uint64_t a1)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  int v4;
  uint64_t v5;
  _OWORD v7[2];
  int v8;
  uint64_t v9;
  uint64_t v10;

  v2 = *(_QWORD *)(v1 + 16);
  v3 = *(_QWORD *)(v1 + 32);
  v4 = *(_DWORD *)(v1 + 72);
  v5 = *(_QWORD *)(v1 + 80);
  v7[1] = *(_OWORD *)(v1 + 56);
  v8 = v4;
  v9 = a1;
  v10 = v5;
  return (*(uint64_t (**)(float (*)(const float *, int), _OWORD *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:), v7, MEMORY[0x1E0DEE9C0] + 8, v2);
}

{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  __int128 v6;
  _OWORD v8[3];
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;

  v2 = *(_QWORD *)(v1 + 16);
  v3 = *(_QWORD *)(v1 + 32);
  v4 = *(_QWORD *)(v1 + 88);
  v5 = *(_QWORD *)(v1 + 96);
  v6 = *(_OWORD *)(v1 + 72);
  v8[1] = *(_OWORD *)(v1 + 56);
  v8[2] = v6;
  v9 = v4;
  v10 = a1;
  v11 = v5;
  return (*(uint64_t (**)(double (*)(const double *, int), _OWORD *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:), v8, MEMORY[0x1E0DEE9C0] + 8, v2);
}

uint64_t partial apply for closure #1 in static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:)(_QWORD *a1, uint64_t *a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  __int128 v7;
  _OWORD v9[2];
  uint64_t v10;

  v3 = *(_QWORD *)(v2 + 16);
  v4 = *(_QWORD *)(v2 + 24);
  v5 = *(_QWORD *)(v2 + 32);
  v6 = *(_QWORD *)(v2 + 72);
  v7 = *(_OWORD *)(v2 + 56);
  v9[0] = *(_OWORD *)(v2 + 40);
  v9[1] = v7;
  v10 = v6;
  return closure #1 in static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:)(a1, a2, v5, (double *)v9, v3, v4);
}

double partial apply for closure #1 in closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(const double *a1, int a2)
{
  uint64_t v2;
  uint64_t v3;
  __int128 v4;
  double result;
  _OWORD v6[2];
  uint64_t v7;

  v3 = *(_QWORD *)(v2 + 48);
  v4 = *(_OWORD *)(v2 + 32);
  v6[0] = *(_OWORD *)(v2 + 16);
  v6[1] = v4;
  v7 = v3;
  closure #1 in closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(a1, a2, (uint64_t)v6, *(double ***)(v2 + 56), *(_QWORD *)(v2 + 64));
  return result;
}

float partial apply for closure #1 in closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(const float *a1, int a2)
{
  uint64_t v2;
  int v3;
  float result;
  __int128 v5;
  int v6;

  v3 = *(_DWORD *)(v2 + 32);
  v5 = *(_OWORD *)(v2 + 16);
  v6 = v3;
  closure #1 in closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(a1, a2, &v5, *(float ***)(v2 + 40), *(_QWORD *)(v2 + 48));
  return result;
}

uint64_t vImage.PixelBuffer<>.premultiply(alpha:)(uint64_t *a1)
{
  return vImage.PixelBuffer<>.premultiply(alpha:)(a1, MEMORY[0x1E0C8D610]);
}

{
  return vImage.PixelBuffer<>.premultiply(alpha:)(a1, MEMORY[0x1E0C8D618]);
}

uint64_t vImage.PixelBuffer<>.unpremultiply(alpha:)(uint64_t *a1)
{
  return vImage.PixelBuffer<>.premultiply(alpha:)(a1, MEMORY[0x1E0C8D7B8]);
}

{
  return vImage.PixelBuffer<>.premultiply(alpha:)(a1, MEMORY[0x1E0C8D7C0]);
}

uint64_t vImage.PixelBuffer<>.premultiply(channelOrdering:)(_BYTE *a1)
{
  return vImage.PixelBuffer<>.premultiply(channelOrdering:)(a1, MEMORY[0x1E0C8D600], MEMORY[0x1E0C8D630]);
}

{
  return vImage.PixelBuffer<>.premultiply(channelOrdering:)(a1, MEMORY[0x1E0C8D5F8], MEMORY[0x1E0C8D628]);
}

{
  return vImage.PixelBuffer<>.premultiply(channelOrdering:)(a1, MEMORY[0x1E0C8D608], MEMORY[0x1E0C8D638]);
}

uint64_t vImage.PixelBuffer<>.unpremultiply(channelOrdering:)(_BYTE *a1)
{
  return vImage.PixelBuffer<>.premultiply(channelOrdering:)(a1, MEMORY[0x1E0C8D7A8], MEMORY[0x1E0C8D7D8]);
}

{
  return vImage.PixelBuffer<>.premultiply(channelOrdering:)(a1, MEMORY[0x1E0C8D7A0], MEMORY[0x1E0C8D7D0]);
}

{
  return vImage.PixelBuffer<>.premultiply(channelOrdering:)(a1, MEMORY[0x1E0C8D7B0], MEMORY[0x1E0C8D7E0]);
}

uint64_t vImage.PixelBuffer<>.premultiply()()
{
  return vImage.PixelBuffer<>.premultiply()(MEMORY[0x1E0C8D620]);
}

uint64_t vImage.PixelBuffer<>.unpremultiply()()
{
  return vImage.PixelBuffer<>.premultiply()(MEMORY[0x1E0C8D7C8]);
}

uint64_t vImage.PixelBuffer<>.premultiply()(uint64_t (*a1)(_OWORD *, _OWORD *, _QWORD))
{
  uint64_t *v1;
  uint64_t v2;
  __int128 v3;
  _OWORD v5[2];
  uint64_t v6;

  v6 = *MEMORY[0x1E0C80C00];
  v2 = *v1;
  if (!*(_QWORD *)(*v1 + 16))
    __break(1u);
  v3 = *(_OWORD *)(v2 + 48);
  v5[0] = *(_OWORD *)(v2 + 32);
  v5[1] = v3;
  return a1(v5, v5, 0);
}

uint64_t vImage.PixelBuffer<>.premultiply(alpha:)(uint64_t *a1, uint64_t (*a2)(_OWORD *, _OWORD *, _OWORD *, _QWORD))
{
  uint64_t *v2;
  uint64_t v3;
  __int128 v4;
  uint64_t v5;
  __int128 v6;
  _OWORD v8[2];
  _OWORD v9[2];
  uint64_t v10;

  v10 = *MEMORY[0x1E0C80C00];
  v3 = *v2;
  if (!*(_QWORD *)(*v2 + 16))
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  v4 = *(_OWORD *)(v3 + 48);
  v9[0] = *(_OWORD *)(v3 + 32);
  v9[1] = v4;
  v5 = *a1;
  if (!*(_QWORD *)(*a1 + 16))
    goto LABEL_5;
  v6 = *(_OWORD *)(v5 + 48);
  v8[0] = *(_OWORD *)(v5 + 32);
  v8[1] = v6;
  return a2(v9, v8, v9, 0);
}

uint64_t vImage.PixelBuffer<>.premultiply(channelOrdering:)(_BYTE *a1, uint64_t (*a2)(_OWORD *, _OWORD *, _QWORD), uint64_t (*a3)(_OWORD *, _OWORD *, _QWORD))
{
  uint64_t *v3;
  uint64_t v4;
  __int128 v5;
  _OWORD v7[2];
  uint64_t v8;

  v8 = *MEMORY[0x1E0C80C00];
  v4 = *v3;
  if (!*(_QWORD *)(*v3 + 16))
    __break(1u);
  v5 = *(_OWORD *)(v4 + 48);
  v7[0] = *(_OWORD *)(v4 + 32);
  v7[1] = v5;
  if (*a1)
    return a3(v7, v7, 0);
  else
    return a2(v7, v7, 0);
}

vImage_Error vImage.PixelBuffer<>.premultipliedAlphaBlend(_:topLayer:destination:)(_BYTE *a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  _QWORD *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  _QWORD *v10;
  uint64_t v11;
  uint64_t v12;
  void *v13;
  size_t v14;
  void *v15;
  size_t v16;
  void *v17;
  size_t v18;
  vImage_Error result;
  vImage_Buffer dest;
  vImage_Buffer srcTop;
  vImage_Buffer srcBottom;
  uint64_t v23;

  v23 = *MEMORY[0x1E0C80C00];
  v4 = *(_QWORD **)a2;
  if (!*(_QWORD *)(*(_QWORD *)a2 + 16))
  {
    __break(1u);
    goto LABEL_27;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (!v5)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v6)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v7 = *(_QWORD **)a3;
  if (!*(_QWORD *)(*(_QWORD *)a3 + 16))
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v8)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (!v9)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (v5 != v8)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (v6 != v9)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v10 = *(_QWORD **)v3;
  if (!*(_QWORD *)(*(_QWORD *)v3 + 16))
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v11)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v12)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (v11 != v5)
  {
LABEL_43:
    __break(1u);
LABEL_44:
    __break(1u);
  }
  if (v12 != v6)
    goto LABEL_44;
  v13 = (void *)v10[4];
  v14 = v10[7];
  srcBottom.data = v13;
  srcBottom.height = v6;
  srcBottom.width = v5;
  srcBottom.rowBytes = v14;
  v15 = (void *)v4[4];
  v16 = v4[7];
  srcTop.data = v15;
  srcTop.height = v6;
  srcTop.width = v5;
  srcTop.rowBytes = v16;
  v17 = (void *)v7[4];
  v18 = v7[7];
  dest.data = v17;
  dest.height = v6;
  dest.width = v5;
  dest.rowBytes = v18;
  switch(*a1)
  {
    case 0:
      result = vImagePremultipliedAlphaBlendMultiply_RGBA8888(&srcTop, &srcBottom, &dest, 0);
      break;
    case 1:
      result = vImagePremultipliedAlphaBlendScreen_RGBA8888(&srcTop, &srcBottom, &dest, 0);
      break;
    case 2:
      result = vImagePremultipliedAlphaBlendDarken_RGBA8888(&srcTop, &srcBottom, &dest, 0);
      break;
    case 3:
      result = vImagePremultipliedAlphaBlendLighten_RGBA8888(&srcTop, &srcBottom, &dest, 0);
      break;
    default:
      _assertionFailure(_:_:file:line:flags:)();
      __break(1u);
      JUMPOUT(0x1CAB516FCLL);
  }
  return result;
}

vImage_Error vImage.PixelBuffer<>.alphaComposite(_:topLayer:destination:)(unsigned __int8 *a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  _QWORD *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  _QWORD *v10;
  uint64_t v11;
  uint64_t v12;
  void *v13;
  size_t v14;
  void *v15;
  size_t v16;
  void *v17;
  size_t v18;
  int v19;
  vImage_Buffer dest;
  vImage_Buffer srcTop;
  vImage_Buffer srcBottom;
  uint64_t v24;

  v24 = *MEMORY[0x1E0C80C00];
  v4 = *(_QWORD **)a2;
  if (!*(_QWORD *)(*(_QWORD *)a2 + 16))
  {
    __break(1u);
    goto LABEL_29;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (!v5)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v6)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v7 = *(_QWORD **)a3;
  if (!*(_QWORD *)(*(_QWORD *)a3 + 16))
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (!v8)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v9)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (v5 != v8)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (v6 != v9)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v10 = *(_QWORD **)v3;
  if (!*(_QWORD *)(*(_QWORD *)v3 + 16))
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v11)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!v12)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (v11 != v5)
  {
LABEL_45:
    __break(1u);
LABEL_46:
    __break(1u);
  }
  if (v12 != v6)
    goto LABEL_46;
  v13 = (void *)v10[4];
  v14 = v10[7];
  srcBottom.data = v13;
  srcBottom.height = v6;
  srcBottom.width = v5;
  srcBottom.rowBytes = v14;
  v15 = (void *)v4[4];
  v16 = v4[7];
  srcTop.data = v15;
  srcTop.height = v6;
  srcTop.width = v5;
  srcTop.rowBytes = v16;
  v17 = (void *)v7[4];
  v18 = v7[7];
  dest.data = v17;
  dest.height = v6;
  dest.width = v5;
  dest.rowBytes = v18;
  v19 = *a1;
  if (a1[1] != 1)
    return vImagePremultipliedConstAlphaBlend_ARGB8888(&srcTop, v19, &srcBottom, &dest, 0);
  if (!*a1)
    return vImageAlphaBlend_ARGB8888(&srcTop, &srcBottom, &dest, 0);
  if (v19 == 1)
    return vImagePremultipliedAlphaBlend_ARGB8888(&srcTop, &srcBottom, &dest, 0);
  return vImageAlphaBlend_NonpremultipliedToPremultiplied_ARGB8888(&srcTop, &srcBottom, &dest, 0);
}

vImage_Error vImage.PixelBuffer<>.alphaComposite(_:topLayer:destination:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  _QWORD *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  _QWORD *v7;
  uint64_t v8;
  uint64_t v9;
  _QWORD *v10;
  uint64_t v11;
  uint64_t v12;
  void *v13;
  size_t v14;
  void *v15;
  size_t v16;
  void *v17;
  size_t v18;
  Pixel_F v19;
  vImage_Buffer dest;
  vImage_Buffer srcTop;
  vImage_Buffer srcBottom;
  uint64_t v24;

  v24 = *MEMORY[0x1E0C80C00];
  v4 = *(_QWORD **)a2;
  if (!*(_QWORD *)(*(_QWORD *)a2 + 16))
  {
    __break(1u);
    goto LABEL_29;
  }
  v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (!v5)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v6)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v7 = *(_QWORD **)a3;
  if (!*(_QWORD *)(*(_QWORD *)a3 + 16))
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (!v8)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v9)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (v5 != v8)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (v6 != v9)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v10 = *(_QWORD **)v3;
  if (!*(_QWORD *)(*(_QWORD *)v3 + 16))
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v11)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!v12)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (v11 != v5)
  {
LABEL_45:
    __break(1u);
LABEL_46:
    __break(1u);
  }
  if (v12 != v6)
    goto LABEL_46;
  v13 = (void *)v10[4];
  v14 = v10[7];
  srcBottom.data = v13;
  srcBottom.height = v6;
  srcBottom.width = v5;
  srcBottom.rowBytes = v14;
  v15 = (void *)v4[4];
  v16 = v4[7];
  srcTop.data = v15;
  srcTop.height = v6;
  srcTop.width = v5;
  srcTop.rowBytes = v16;
  v17 = (void *)v7[4];
  v18 = v7[7];
  dest.data = v17;
  dest.height = v6;
  dest.width = v5;
  dest.rowBytes = v18;
  v19 = *(float *)a1;
  if (*(_BYTE *)(a1 + 4) != 1)
    return vImagePremultipliedConstAlphaBlend_ARGBFFFF(&srcTop, *(Pixel_F *)a1, &srcBottom, &dest, 0);
  if (v19 == 0.0)
    return vImageAlphaBlend_ARGBFFFF(&srcTop, &srcBottom, &dest, 0);
  if (LODWORD(v19) == 1)
    return vImagePremultipliedAlphaBlend_ARGBFFFF(&srcTop, &srcBottom, &dest, 0);
  return vImageAlphaBlend_NonpremultipliedToPremultiplied_ARGBFFFF(&srcTop, &srcBottom, &dest, 0);
}

BOOL static vImage.BlendMode.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void vImage.BlendMode.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

Swift::Int vImage.BlendMode.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

unint64_t lazy protocol witness table accessor for type vImage.BlendMode and conformance vImage.BlendMode()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImage.BlendMode and conformance vImage.BlendMode;
  if (!lazy protocol witness table cache variable for type vImage.BlendMode and conformance vImage.BlendMode)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImage.BlendMode, &type metadata for vImage.BlendMode);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.BlendMode and conformance vImage.BlendMode);
  }
  return result;
}

uint64_t storeEnumTagSinglePayload for vImage.BlendMode(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 3 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 3) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFD)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFC)
    return ((uint64_t (*)(void))((char *)&loc_1CAB51B88 + 4 * byte_1CAB631AD[v4]))();
  *a1 = a2 + 3;
  return ((uint64_t (*)(void))((char *)sub_1CAB51BBC + 4 * byte_1CAB631A8[v4]))();
}

uint64_t sub_1CAB51BBC(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB51BC4(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB51BCCLL);
  return result;
}

uint64_t sub_1CAB51BD8(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB51BE0);
  *(_BYTE *)result = a2 + 3;
  return result;
}

uint64_t sub_1CAB51BE4(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB51BEC(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vImage.BlendMode()
{
  return &type metadata for vImage.BlendMode;
}

uint64_t *initializeBufferWithCopyOfBuffer for vImage.CompositeMode(uint64_t *a1, uint64_t *a2, uint64_t a3)
{
  uint64_t v5;
  uint64_t v6;
  unsigned int v7;
  size_t v8;
  unint64_t v9;
  uint64_t v10;
  unsigned int v11;
  _BOOL8 v12;
  BOOL v13;
  uint64_t v14;
  uint64_t v17;
  uint64_t v18;
  unsigned int v19;
  _BOOL8 v20;

  v5 = *(_QWORD *)(a3 + 16);
  v6 = *(_QWORD *)(v5 - 8);
  v7 = *(_DWORD *)(v6 + 84);
  v8 = *(_QWORD *)(v6 + 64);
  v9 = v8;
  if (v7 <= 2)
  {
    if (v8 <= 3)
    {
      v11 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v11 > 0xFFFE)
      {
        v10 = 4;
      }
      else
      {
        v12 = v11 != 0;
        v13 = v11 >= 0xFF;
        v10 = 2;
        if (!v13)
          v10 = v12;
      }
    }
    else
    {
      v10 = 1;
    }
    v9 = v10 + v8;
  }
  v14 = *(_DWORD *)(v6 + 80);
  if (v14 <= 7 && v9 <= 0x18 && (*(_DWORD *)(v6 + 80) & 0x100000) == 0)
  {
    if ((*(unsigned int (**)(uint64_t *, uint64_t, _QWORD))(v6 + 48))(a2, 3, *(_QWORD *)(a3 + 16)))
    {
      if (v7 <= 2)
      {
        if (v8 <= 3)
        {
          v19 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
          if (v19 > 0xFFFE)
          {
            v18 = 4;
          }
          else
          {
            v20 = v19 != 0;
            v13 = v19 >= 0xFF;
            v18 = 2;
            if (!v13)
              v18 = v20;
          }
        }
        else
        {
          v18 = 1;
        }
        v8 += v18;
      }
      memcpy(a1, a2, v8);
    }
    else
    {
      (*(void (**)(uint64_t *, uint64_t *, uint64_t))(v6 + 16))(a1, a2, v5);
      (*(void (**)(uint64_t *, _QWORD, uint64_t, uint64_t))(v6 + 56))(a1, 0, 3, v5);
    }
  }
  else
  {
    v17 = *a2;
    *a1 = *a2;
    a1 = (uint64_t *)(v17 + ((v14 + 16) & ~v14));
    swift_retain();
  }
  return a1;
}

uint64_t destroy for vImage.CompositeMode(uint64_t a1, uint64_t a2)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t result;

  v3 = *(_QWORD *)(a2 + 16);
  v4 = *(_QWORD *)(v3 - 8);
  result = (*(uint64_t (**)(uint64_t, uint64_t, uint64_t))(v4 + 48))(a1, 3, v3);
  if (!(_DWORD)result)
    return (*(uint64_t (**)(uint64_t, uint64_t))(v4 + 8))(a1, v3);
  return result;
}

void *initializeWithCopy for vImage.CompositeMode(void *a1, const void *a2, uint64_t a3)
{
  uint64_t v5;
  uint64_t v6;
  unsigned int v7;
  size_t v8;
  uint64_t v9;
  unsigned int v10;
  _BOOL8 v11;
  BOOL v12;

  v5 = *(_QWORD *)(a3 + 16);
  v6 = *(_QWORD *)(v5 - 8);
  if ((*(unsigned int (**)(const void *, uint64_t, uint64_t))(v6 + 48))(a2, 3, v5))
  {
    v7 = *(_DWORD *)(v6 + 84);
    v8 = *(_QWORD *)(v6 + 64);
    if (v7 <= 2)
    {
      if (v8 <= 3)
      {
        v10 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
        if (v10 > 0xFFFE)
        {
          v9 = 4;
        }
        else
        {
          v11 = v10 != 0;
          v12 = v10 >= 0xFF;
          v9 = 2;
          if (!v12)
            v9 = v11;
        }
      }
      else
      {
        v9 = 1;
      }
      v8 += v9;
    }
    memcpy(a1, a2, v8);
  }
  else
  {
    (*(void (**)(void *, const void *, uint64_t))(v6 + 16))(a1, a2, v5);
    (*(void (**)(void *, _QWORD, uint64_t, uint64_t))(v6 + 56))(a1, 0, 3, v5);
  }
  return a1;
}

void *assignWithCopy for vImage.CompositeMode(void *a1, void *a2, uint64_t a3)
{
  uint64_t v5;
  uint64_t v6;
  uint64_t (*v7)(void *, uint64_t, uint64_t);
  int v8;
  int v9;
  unsigned int v10;
  size_t v11;
  uint64_t v12;
  unsigned int v13;
  _BOOL8 v14;
  BOOL v15;

  v5 = *(_QWORD *)(a3 + 16);
  v6 = *(_QWORD *)(v5 - 8);
  v7 = *(uint64_t (**)(void *, uint64_t, uint64_t))(v6 + 48);
  v8 = v7(a1, 3, v5);
  v9 = v7(a2, 3, v5);
  if (v8)
  {
    if (v9)
    {
      v10 = *(_DWORD *)(v6 + 84);
      v11 = *(_QWORD *)(v6 + 64);
      if (v10 <= 2)
      {
        if (v11 > 3)
        {
LABEL_5:
          v12 = 1;
LABEL_16:
          v11 += v12;
          goto LABEL_17;
        }
LABEL_9:
        v13 = (~(-1 << (8 * v11)) - v10 + 3) >> (8 * v11);
        if (v13 > 0xFFFE)
        {
          v12 = 4;
        }
        else
        {
          v14 = v13 != 0;
          v15 = v13 >= 0xFF;
          v12 = 2;
          if (!v15)
            v12 = v14;
        }
        goto LABEL_16;
      }
      goto LABEL_17;
    }
    (*(void (**)(void *, void *, uint64_t))(v6 + 16))(a1, a2, v5);
    (*(void (**)(void *, _QWORD, uint64_t, uint64_t))(v6 + 56))(a1, 0, 3, v5);
  }
  else
  {
    if (v9)
    {
      (*(void (**)(void *, uint64_t))(v6 + 8))(a1, v5);
      v10 = *(_DWORD *)(v6 + 84);
      v11 = *(_QWORD *)(v6 + 64);
      if (v10 <= 2)
      {
        if (v11 > 3)
          goto LABEL_5;
        goto LABEL_9;
      }
LABEL_17:
      memcpy(a1, a2, v11);
      return a1;
    }
    (*(void (**)(void *, void *, uint64_t))(v6 + 24))(a1, a2, v5);
  }
  return a1;
}

void *initializeWithTake for vImage.CompositeMode(void *a1, const void *a2, uint64_t a3)
{
  uint64_t v5;
  uint64_t v6;
  unsigned int v7;
  size_t v8;
  uint64_t v9;
  unsigned int v10;
  _BOOL8 v11;
  BOOL v12;

  v5 = *(_QWORD *)(a3 + 16);
  v6 = *(_QWORD *)(v5 - 8);
  if ((*(unsigned int (**)(const void *, uint64_t, uint64_t))(v6 + 48))(a2, 3, v5))
  {
    v7 = *(_DWORD *)(v6 + 84);
    v8 = *(_QWORD *)(v6 + 64);
    if (v7 <= 2)
    {
      if (v8 <= 3)
      {
        v10 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
        if (v10 > 0xFFFE)
        {
          v9 = 4;
        }
        else
        {
          v11 = v10 != 0;
          v12 = v10 >= 0xFF;
          v9 = 2;
          if (!v12)
            v9 = v11;
        }
      }
      else
      {
        v9 = 1;
      }
      v8 += v9;
    }
    memcpy(a1, a2, v8);
  }
  else
  {
    (*(void (**)(void *, const void *, uint64_t))(v6 + 32))(a1, a2, v5);
    (*(void (**)(void *, _QWORD, uint64_t, uint64_t))(v6 + 56))(a1, 0, 3, v5);
  }
  return a1;
}

void *assignWithTake for vImage.CompositeMode(void *a1, void *a2, uint64_t a3)
{
  uint64_t v5;
  uint64_t v6;
  uint64_t (*v7)(void *, uint64_t, uint64_t);
  int v8;
  int v9;
  unsigned int v10;
  size_t v11;
  uint64_t v12;
  unsigned int v13;
  _BOOL8 v14;
  BOOL v15;

  v5 = *(_QWORD *)(a3 + 16);
  v6 = *(_QWORD *)(v5 - 8);
  v7 = *(uint64_t (**)(void *, uint64_t, uint64_t))(v6 + 48);
  v8 = v7(a1, 3, v5);
  v9 = v7(a2, 3, v5);
  if (v8)
  {
    if (v9)
    {
      v10 = *(_DWORD *)(v6 + 84);
      v11 = *(_QWORD *)(v6 + 64);
      if (v10 <= 2)
      {
        if (v11 > 3)
        {
LABEL_5:
          v12 = 1;
LABEL_16:
          v11 += v12;
          goto LABEL_17;
        }
LABEL_9:
        v13 = (~(-1 << (8 * v11)) - v10 + 3) >> (8 * v11);
        if (v13 > 0xFFFE)
        {
          v12 = 4;
        }
        else
        {
          v14 = v13 != 0;
          v15 = v13 >= 0xFF;
          v12 = 2;
          if (!v15)
            v12 = v14;
        }
        goto LABEL_16;
      }
      goto LABEL_17;
    }
    (*(void (**)(void *, void *, uint64_t))(v6 + 32))(a1, a2, v5);
    (*(void (**)(void *, _QWORD, uint64_t, uint64_t))(v6 + 56))(a1, 0, 3, v5);
  }
  else
  {
    if (v9)
    {
      (*(void (**)(void *, uint64_t))(v6 + 8))(a1, v5);
      v10 = *(_DWORD *)(v6 + 84);
      v11 = *(_QWORD *)(v6 + 64);
      if (v10 <= 2)
      {
        if (v11 > 3)
          goto LABEL_5;
        goto LABEL_9;
      }
LABEL_17:
      memcpy(a1, a2, v11);
      return a1;
    }
    (*(void (**)(void *, void *, uint64_t))(v6 + 40))(a1, a2, v5);
  }
  return a1;
}

uint64_t getEnumTagSinglePayload for vImage.CompositeMode(uint64_t a1, unsigned int a2, uint64_t a3)
{
  uint64_t v4;
  unsigned int v5;
  unsigned int v6;
  uint64_t v7;
  uint64_t v8;
  unsigned int v9;
  _BOOL8 v10;
  BOOL v11;
  int v12;
  char v13;
  int v14;
  unsigned int v15;
  int v16;
  int v17;
  unsigned int v18;

  v4 = *(_QWORD *)(*(_QWORD *)(a3 + 16) - 8);
  v5 = *(_DWORD *)(v4 + 84);
  v6 = v5 - 3;
  v7 = *(_QWORD *)(v4 + 64);
  if (v5 <= 2)
  {
    v6 = 0;
    if (v7 <= 3)
    {
      v9 = (~(-1 << (8 * v7)) - v5 + 3) >> (8 * v7);
      if (v9 > 0xFFFE)
      {
        v8 = 4;
      }
      else
      {
        v10 = v9 != 0;
        v11 = v9 >= 0xFF;
        v8 = 2;
        if (!v11)
          v8 = v10;
      }
    }
    else
    {
      v8 = 1;
    }
    v7 += v8;
  }
  if (!a2)
    return 0;
  v12 = a2 - v6;
  if (a2 <= v6)
    goto LABEL_29;
  v13 = 8 * v7;
  if (v7 <= 3)
  {
    v15 = ((v12 + ~(-1 << v13)) >> v13) + 1;
    if (HIWORD(v15))
    {
      v14 = *(_DWORD *)(a1 + v7);
      if (!v14)
        goto LABEL_29;
      goto LABEL_20;
    }
    if (v15 > 0xFF)
    {
      v14 = *(unsigned __int16 *)(a1 + v7);
      if (!*(_WORD *)(a1 + v7))
        goto LABEL_29;
      goto LABEL_20;
    }
    if (v15 < 2)
    {
LABEL_29:
      if (v6)
      {
        v18 = (*(uint64_t (**)(void))(v4 + 48))();
        if (v18 >= 4)
          return v18 - 3;
        else
          return 0;
      }
      return 0;
    }
  }
  v14 = *(unsigned __int8 *)(a1 + v7);
  if (!*(_BYTE *)(a1 + v7))
    goto LABEL_29;
LABEL_20:
  v16 = (v14 - 1) << v13;
  if (v7 > 3)
    v16 = 0;
  if ((_DWORD)v7)
  {
    if (v7 <= 3)
      v17 = v7;
    else
      v17 = 4;
    __asm { BR              X12 }
  }
  return v6 + v16 + 1;
}

void storeEnumTagSinglePayload for vImage.CompositeMode(_WORD *a1, unsigned int a2, unsigned int a3, uint64_t a4)
{
  uint64_t v6;
  unsigned int v7;
  unsigned int v8;
  size_t v9;
  uint64_t v10;
  unsigned int v11;
  _BOOL8 v12;
  BOOL v13;
  unsigned int v14;
  unsigned int v15;
  int v16;
  unsigned int v17;
  int v18;

  v6 = *(_QWORD *)(*(_QWORD *)(a4 + 16) - 8);
  v7 = *(_DWORD *)(v6 + 84);
  v8 = v7 - 3;
  v9 = *(_QWORD *)(v6 + 64);
  if (v7 <= 2)
  {
    v8 = 0;
    if (v9 <= 3)
    {
      v11 = (~(-1 << (8 * v9)) - v7 + 3) >> (8 * v9);
      if (v11 > 0xFFFE)
      {
        v10 = 4;
      }
      else
      {
        v12 = v11 != 0;
        v13 = v11 >= 0xFF;
        v10 = 2;
        if (!v13)
          v10 = v12;
      }
    }
    else
    {
      v10 = 1;
    }
    v9 += v10;
  }
  v13 = a3 >= v8;
  v14 = a3 - v8;
  if (v14 != 0 && v13)
  {
    if (v9 <= 3)
    {
      v17 = ((v14 + ~(-1 << (8 * v9))) >> (8 * v9)) + 1;
      if (HIWORD(v17))
      {
        v15 = 4u;
      }
      else if (v17 >= 0x100)
      {
        v15 = 2;
      }
      else
      {
        v15 = v17 > 1;
      }
    }
    else
    {
      v15 = 1u;
    }
  }
  else
  {
    v15 = 0u;
  }
  if (v8 < a2)
  {
    v16 = ~v8 + a2;
    if (v9 < 4)
    {
      if ((_DWORD)v9)
      {
        v18 = v16 & ~(-1 << (8 * v9));
        bzero(a1, v9);
        if ((_DWORD)v9 == 3)
        {
          *a1 = v18;
          *((_BYTE *)a1 + 2) = BYTE2(v18);
        }
        else if ((_DWORD)v9 == 2)
        {
          *a1 = v18;
        }
        else
        {
          *(_BYTE *)a1 = v18;
        }
      }
    }
    else
    {
      bzero(a1, v9);
      *(_DWORD *)a1 = v16;
    }
    __asm { BR              X10 }
  }
  __asm { BR              X11 }
}

uint64_t getEnumTag for vImage.CompositeMode(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(uint64_t, uint64_t))(*(_QWORD *)(*(_QWORD *)(a2 + 16) - 8) + 48))(a1, 3);
}

uint64_t destructiveInjectEnumTag for vImage.CompositeMode(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(uint64_t, uint64_t, uint64_t))(*(_QWORD *)(*(_QWORD *)(a3 + 16) - 8) + 56))(a1, a2, 3);
}

uint64_t type metadata accessor for vImage.CompositeMode(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return __swift_instantiateGenericMetadata(a1, a2, a3, a4, (uint64_t)&nominal type descriptor for vImage.CompositeMode);
}

uint64_t static vDSP.minimum<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return static vDSP.minimum<A>(_:_:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.minimum<A>(_:_:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.minimum<A>(_:_:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.minimum<A>(_:_:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vDSP.minimum<A>(_:_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.minimum<A>(_:_:)(a1, a2, v2[4], v2[5], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.minimum<A, B>(_:_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.minimum<A>(_:_:)(a1, a2, v2[4], v2[5], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.minimum<A, B>(_:_:result:));
}

uint64_t static vDSP.minimum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  return static vDSP.minimum<A, B>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.minimum<A, B>(_:_:result:));
}

{
  return static vDSP.minimum<A, B>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.minimum<A, B>(_:_:result:));
}

uint64_t closure #1 in static vDSP.minimum<A>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t *a7, unint64_t *a8, void (*a9)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16;
  uint64_t v17;
  uint64_t result;

  v16 = __swift_instantiateConcreteTypeFromMangledName(a7);
  v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a8, a7);
  a9(a3, a4, a1, a5, v16, a6, v17);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t static vDSP.maximum<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return static vDSP.minimum<A>(_:_:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.maximum<A>(_:_:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.minimum<A>(_:_:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.maximum<A>(_:_:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.maximum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  return static vDSP.minimum<A, B>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.maximum<A, B>(_:_:result:));
}

{
  return static vDSP.minimum<A, B>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.maximum<A, B>(_:_:result:));
}

uint64_t static vDSP.minimum<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t))
{
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  char *v13;
  uint64_t v14;
  char *v15;
  void (*v16)(char *, uint64_t);
  uint64_t (*v17)(uint64_t, uint64_t);
  uint64_t v18;
  uint64_t v19;
  void (*v20)(char *, uint64_t);
  uint64_t result;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t (*v26)(uint64_t, uint64_t);

  v25 = a5;
  v26 = a6;
  v10 = *(_QWORD *)(a3 - 8);
  v11 = MEMORY[0x1E0C80A78](a1);
  v13 = (char *)&v24 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1E0C80A78](v11);
  v15 = (char *)&v24 - v14;
  v16 = *(void (**)(char *, uint64_t))(v10 + 16);
  v16((char *)&v24 - v14, a1);
  ((void (*)(char *, uint64_t, uint64_t))v16)(v13, a2, a3);
  v17 = *(uint64_t (**)(uint64_t, uint64_t))(a4 + 16);
  v18 = v17(a3, a4);
  v19 = v17(a3, a4);
  v20 = *(void (**)(char *, uint64_t))(v10 + 8);
  v20(v13, a3);
  result = ((uint64_t (*)(char *, uint64_t))v20)(v15, a3);
  if (v18 == v19)
  {
    v22 = v17(a3, a4);
    v23 = MEMORY[0x1E0C80A78](v22);
    *(&v24 - 4) = a3;
    *(&v24 - 3) = a4;
    *(&v24 - 2) = a1;
    *(&v24 - 1) = a2;
    return v26(v23, v25);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t static vDSP.minimum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t (*v12)(uint64_t, uint64_t);
  uint64_t v13;
  uint64_t v14;
  uint64_t result;
  uint64_t v16;

  v12 = *(uint64_t (**)(uint64_t, uint64_t))(a6 + 16);
  v13 = v12(a4, a6);
  v14 = v12(a4, a6);
  result = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a7 + 8) + 16))(a5);
  if (v14 >= v13)
    v16 = v13;
  else
    v16 = v14;
  if (result < v16)
    v16 = result;
  if (v16 < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1E0C80A78](result);
    return (*(uint64_t (**)(uint64_t))(a7 + 16))(a8);
  }
  return result;
}

uint64_t static vDSP.absolute<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.absolute<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.absolute<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.absolute<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.absolute<A, B>(_:result:));
}

{
  return static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.absolute<A, B>(_:result:));
}

uint64_t static vDSP.negativeAbsolute<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.negativeAbsolute<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.negativeAbsolute<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.negativeAbsolute<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.negativeAbsolute<A, B>(_:result:));
}

{
  return static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.negativeAbsolute<A, B>(_:result:));
}

uint64_t static vDSP.negative<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.negative<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.negative<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.negative<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.negative<A, B>(_:result:));
}

{
  return static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.negative<A, B>(_:result:));
}

uint64_t closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:)(uint64_t result, uint64_t a2, _QWORD *a3, uint64_t a4, uint64_t (*a5)(void))
{
  if (!result)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*a3)
  {
    if ((a4 & 0x8000000000000000) == 0)
      return a5();
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vDSP.reverse<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.reverse<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.reverse<A>(_:));
}

{
  return static vDSP.reverse<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.reverse<A>(_:));
}

uint64_t static vDSP.reverse<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t result;

  result = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a3 + 8) + 16))(a2);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1E0C80A78](result);
    return (*(uint64_t (**)(uint64_t))(a3 + 16))(a4);
  }
  return result;
}

uint64_t vDSP.SortOrder.init(rawValue:)@<X0>(uint64_t result@<X0>, char *a2@<X8>)
{
  char v2;

  if ((_DWORD)result == -1)
    v2 = 1;
  else
    v2 = 2;
  if ((_DWORD)result == 1)
    v2 = 0;
  *a2 = v2;
  return result;
}

uint64_t vDSP.SortOrder.rawValue.getter()
{
  _BYTE *v0;

  if (*v0)
    return 0xFFFFFFFFLL;
  else
    return 1;
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance vDSP.SortOrder()
{
  unsigned __int8 *v0;
  int v1;
  Swift::UInt32 v2;

  v1 = *v0;
  Hasher.init(_seed:)();
  if (v1)
    v2 = -1;
  else
    v2 = 1;
  Hasher._combine(_:)(v2);
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance vDSP.SortOrder()
{
  _BYTE *v0;
  Swift::UInt32 v1;

  if (*v0)
    v1 = -1;
  else
    v1 = 1;
  Hasher._combine(_:)(v1);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance vDSP.SortOrder()
{
  unsigned __int8 *v0;
  int v1;
  Swift::UInt32 v2;

  v1 = *v0;
  Hasher.init(_seed:)();
  if (v1)
    v2 = -1;
  else
    v2 = 1;
  Hasher._combine(_:)(v2);
  return Hasher._finalize()();
}

_DWORD *protocol witness for RawRepresentable.init(rawValue:) in conformance vDSP.SortOrder@<X0>(_DWORD *result@<X0>, char *a2@<X8>)
{
  char v2;
  char v3;

  if (*result == -1)
    v2 = 1;
  else
    v2 = 2;
  if (*result == 1)
    v3 = 0;
  else
    v3 = v2;
  *a2 = v3;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance vDSP.SortOrder(int *a1@<X8>)
{
  _BYTE *v1;
  int v2;

  if (*v1)
    v2 = -1;
  else
    v2 = 1;
  *a1 = v2;
}

uint64_t static vDSP.sort<A>(_:sortOrder:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return static vDSP.sort<A>(_:sortOrder:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.sort<A>(_:sortOrder:));
}

{
  return static vDSP.sort<A>(_:sortOrder:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.sort<A>(_:sortOrder:));
}

uint64_t static vDSP.sort<A>(_:sortOrder:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t result;

  result = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a4 + 8) + 16))(a3);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1E0C80A78](result);
    return (*(uint64_t (**)(uint64_t))(a4 + 16))(a5);
  }
  return result;
}

uint64_t static vDSP.square<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.square<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.square<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.square<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.square<A, B>(_:result:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.square<A, B>(_:result:));
}

uint64_t static vDSP.signedSquare<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.signedSquare<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.signedSquare<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.signedSquare<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.signedSquare<A, B>(_:result:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.signedSquare<A, B>(_:result:));
}

uint64_t static vDSP.trunc<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.trunc<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.trunc<A>(_:), (uint64_t (*)(uint64_t, uint64_t, _QWORD *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.trunc<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.trunc<A, B>(_:result:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.trunc<A, B>(_:result:));
}

uint64_t static vDSP.countZeroCrossings<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.countZeroCrossings<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.countZeroCrossings<A>(_:));
}

{
  return static vDSP.countZeroCrossings<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.countZeroCrossings<A>(_:));
}

uint64_t static vDSP.countZeroCrossings<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v6;

  v6 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  if (v6 < 0)
    __break(1u);
  MEMORY[0x1E0C80A78](v6);
  (*(void (**)(uint64_t))(a3 + 24))(a4);
  return 0;
}

uint64_t partial apply for closure #1 in static vDSP.minimum<A, B>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.minimum<A, B>(_:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.minimum<A, B>(_:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.minimum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  __int128 v5;
  uint64_t v7;
  __int128 v8;
  __int128 v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;

  v3 = *(_QWORD *)(v2 + 56);
  v4 = *(_QWORD *)(v2 + 64);
  v5 = *(_OWORD *)(v2 + 32);
  v8 = *(_OWORD *)(v2 + 16);
  v9 = v5;
  v10 = v3;
  v11 = a1;
  v12 = v4;
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t, _QWORD))(v5 + 24))(a2, &v7, MEMORY[0x1E0DEE9C0] + 8, v8);
}

uint64_t partial apply for closure #1 in static vDSP.maximum<A>(_:_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.minimum<A>(_:_:)(a1, a2, v2[4], v2[5], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.maximum<A, B>(_:_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.minimum<A>(_:_:)(a1, a2, v2[4], v2[5], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.maximum<A, B>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.maximum<A, B>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.minimum<A, B>(_:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.minimum<A, B>(_:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.absolute<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.absolute<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.absolute<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.absolute<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.negativeAbsolute<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.negativeAbsolute<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.negativeAbsolute<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.negativeAbsolute<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.negativeAbsolute<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.negativeAbsolute<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.negative<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.negative<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.negative<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.negative<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.negative<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.negative<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.reverse<A>(_:)(uint64_t *a1)
{
  return partial apply for closure #1 in static vDSP.reverse<A>(_:)(a1, MEMORY[0x1E0C8C7F0]);
}

{
  return partial apply for closure #1 in static vDSP.reverse<A>(_:)(a1, MEMORY[0x1E0C8C7F8]);
}

uint64_t partial apply for closure #1 in static vDSP.reverse<A>(_:)(uint64_t *a1, uint64_t (*a2)(uint64_t, uint64_t, _QWORD))
{
  uint64_t v2;
  uint64_t result;

  result = *a1;
  if (result)
    return a2(result, 1, *(_QWORD *)(v2 + 16));
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.sort<A>(_:sortOrder:)(uint64_t *a1)
{
  return partial apply for closure #1 in static vDSP.sort<A>(_:sortOrder:)(a1, MEMORY[0x1E0C8C8F0]);
}

{
  return partial apply for closure #1 in static vDSP.sort<A>(_:sortOrder:)(a1, MEMORY[0x1E0C8C8F8]);
}

uint64_t partial apply for closure #1 in static vDSP.sort<A>(_:sortOrder:)(uint64_t *a1, uint64_t (*a2)(void))
{
  uint64_t result;

  result = *a1;
  if (result)
    return a2();
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.square<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.square<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.square<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.square<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.square<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.square<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.signedSquare<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.signedSquare<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.signedSquare<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.signedSquare<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.signedSquare<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.signedSquare<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.trunc<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.trunc<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.doubleToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.trunc<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.trunc<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.trunc<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.trunc<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.countZeroCrossings<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.countZeroCrossings<A>(_:)(a1, a2, MEMORY[0x1E0C8C290]);
}

{
  return partial apply for closure #1 in static vDSP.countZeroCrossings<A>(_:)(a1, a2, MEMORY[0x1E0C8C298]);
}

uint64_t partial apply for closure #1 in static vDSP.countZeroCrossings<A>(_:)(uint64_t result, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, _QWORD, _QWORD, _QWORD, _QWORD))
{
  _QWORD *v3;

  if (result)
    return a3(result, 1, v3[2], v3[3], v3[4], v3[2]);
  __break(1u);
  return result;
}

unint64_t lazy protocol witness table accessor for type vDSP.SortOrder and conformance vDSP.SortOrder()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vDSP.SortOrder and conformance vDSP.SortOrder;
  if (!lazy protocol witness table cache variable for type vDSP.SortOrder and conformance vDSP.SortOrder)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vDSP.SortOrder, &type metadata for vDSP.SortOrder);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.SortOrder and conformance vDSP.SortOrder);
  }
  return result;
}

uint64_t storeEnumTagSinglePayload for vDSP.SortOrder(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 1 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 1) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFF)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFE)
    return ((uint64_t (*)(void))((char *)&loc_1CAB53770 + 4 * byte_1CAB6327D[v4]))();
  *a1 = a2 + 1;
  return ((uint64_t (*)(void))((char *)sub_1CAB537A4 + 4 * byte_1CAB63278[v4]))();
}

uint64_t sub_1CAB537A4(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB537AC(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB537B4);
  return result;
}

uint64_t sub_1CAB537C0(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB537C8);
  *(_BYTE *)result = a2 + 1;
  return result;
}

uint64_t sub_1CAB537CC(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB537D4(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vDSP.SortOrder()
{
  return &type metadata for vDSP.SortOrder;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.trunc<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C628]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C620]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.signedSquare<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C938]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C930]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.square<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C928]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E0C8C920]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.negative<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:)(a1, a2, *(_QWORD **)(v2 + 16), *(_QWORD *)(v2 + 24), MEMORY[0x1E0C8C770]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:)(a1, a2, *(_QWORD **)(v2 + 16), *(_QWORD *)(v2 + 24), MEMORY[0x1E0C8C768]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.negativeAbsolute<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:)(a1, a2, *(_QWORD **)(v2 + 16), *(_QWORD *)(v2 + 24), MEMORY[0x1E0C8C758]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:)(a1, a2, *(_QWORD **)(v2 + 16), *(_QWORD *)(v2 + 24), MEMORY[0x1E0C8C750]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:)(a1, a2, *(_QWORD **)(v2 + 16), *(_QWORD *)(v2 + 24), MEMORY[0x1E0C8C360]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:)(a1, a2, *(_QWORD **)(v2 + 16), *(_QWORD *)(v2 + 24), MEMORY[0x1E0C8C358]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(uint64_t **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C6C8]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(uint64_t **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C6C0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(uint64_t **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C6E8]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(uint64_t **)(v2 + 32), *(_QWORD *)(v2 + 40), MEMORY[0x1E0C8C6D8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  _QWORD *v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD v8[5];

  v4 = v3[2];
  v5 = v3[4];
  v6 = v3[7];
  v8[2] = a1;
  v8[3] = a2;
  v8[4] = v6;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E0DEE9C0] + 8, v4);
}

void __swiftcall vImage_CGImageFormat.init(cgImage:)(vImage_CGImageFormat_optional *__return_ptr retstr, CGImageRef cgImage)
{
  size_t v3[5];

  specialized vImage_CGImageFormat.init(cgImage:)(cgImage, v3);
  outlined init with take of vImage_CGImageFormat?((uint64_t)v3, (uint64_t)retstr);
}

void vImage_CGImageFormat.init(bitsPerComponent:bitsPerPixel:colorSpace:bitmapInfo:renderingIntent:)(uint64_t a1@<X0>, unint64_t a2@<X1>, uint64_t a3@<X2>, unsigned int a4@<W3>, unsigned int a5@<W4>, uint64_t *a6@<X8>)
{
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;

  if (a1 <= 0 || (a2 & 0x8000000000000000) != 0)
  {

    v7 = 0;
    v8 = 0;
    v9 = 0;
    a3 = 1;
    goto LABEL_7;
  }
  if (HIDWORD(a1))
  {
    __break(1u);
  }
  else if (!HIDWORD(a2))
  {
    v7 = a1 | (a2 << 32);
    v8 = a4;
    v9 = a5;
LABEL_7:
    *a6 = v7;
    a6[1] = a3;
    a6[2] = v8;
    a6[3] = 0;
    a6[4] = v9;
    return;
  }
  __break(1u);
}

uint64_t vImage_CGImageFormat.componentCount.getter()
{
  uint64_t v0;
  __int128 v1;
  vImage_CGImageFormat v3;
  uint64_t v4;

  v4 = *MEMORY[0x1E0C80C00];
  v1 = *(_OWORD *)(v0 + 16);
  *(_OWORD *)&v3.bitsPerComponent = *(_OWORD *)v0;
  *(_OWORD *)&v3.bitmapInfo = v1;
  *(_QWORD *)&v3.renderingIntent = *(_QWORD *)(v0 + 32);
  return vImageCGImageFormat_GetComponentCount(&v3);
}

void specialized vImage_CGImageFormat.init(cgImage:)(CGImage *a1@<X0>, size_t *a2@<X8>)
{
  CGColorSpace *v4;
  uint64_t v5;
  size_t BitsPerComponent;
  size_t v7;
  size_t BitsPerPixel;
  size_t v9;
  id v10;
  CGBitmapInfo BitmapInfo;
  CGColorRenderingIntent RenderingIntent;
  size_t v13;
  size_t v14;
  size_t v15;

  v4 = CGImageGetColorSpace(a1);
  if (!v4)
  {

    v13 = 0;
    v14 = 0;
    v15 = 0;
    v5 = 1;
    goto LABEL_8;
  }
  v5 = (uint64_t)v4;
  BitsPerComponent = CGImageGetBitsPerComponent(a1);
  if ((BitsPerComponent & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_10;
  }
  v7 = BitsPerComponent;
  if (HIDWORD(BitsPerComponent))
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  BitsPerPixel = CGImageGetBitsPerPixel(a1);
  if ((BitsPerPixel & 0x8000000000000000) != 0)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  v9 = BitsPerPixel;
  if (!HIDWORD(BitsPerPixel))
  {
    v10 = (id)v5;
    BitmapInfo = CGImageGetBitmapInfo(a1);
    RenderingIntent = CGImageGetRenderingIntent(a1);

    v13 = v7 | (v9 << 32);
    v14 = BitmapInfo;
    v15 = RenderingIntent;
LABEL_8:
    *a2 = v13;
    a2[1] = v5;
    a2[2] = v14;
    a2[3] = 0;
    a2[4] = v15;
    return;
  }
LABEL_12:
  __break(1u);
}

uint64_t static vDSP.phase<A>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return static vDSP.phase<A>(_:result:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in static vDSP.phase<A>(_:result:));
}

{
  return static vDSP.phase<A>(_:result:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in static vDSP.phase<A>(_:result:));
}

_QWORD *partial apply for closure #1 in static vDSP.phase<A>(_:result:)(_QWORD *a1)
{
  return partial apply for closure #1 in static vDSP.phase<A>(_:result:)(a1, MEMORY[0x1E0C8CB00]);
}

{
  return partial apply for closure #1 in static vDSP.phase<A>(_:result:)(a1, MEMORY[0x1E0C8CB08]);
}

uint64_t static vDSP.phase<A>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t result;

  result = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a5 + 8) + 16))(a4);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1E0C80A78](result);
    return (*(uint64_t (**)(uint64_t))(a5 + 16))(a6);
  }
  return result;
}

uint64_t static vDSP.copy(_:to:count:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return static vDSP.copy(_:to:count:)(a1, a2, a3, a4, MEMORY[0x1E0C8CAD8]);
}

{
  return static vDSP.copy(_:to:count:)(a1, a2, a3, a4, MEMORY[0x1E0C8CAE0]);
}

uint64_t static vDSP.copy(_:to:count:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(_QWORD *, uint64_t, uint64_t, uint64_t, uint64_t))
{
  _QWORD v6[3];

  v6[2] = *MEMORY[0x1E0C80C00];
  if (a4 < 0)
    __break(1u);
  v6[0] = a1;
  v6[1] = a2;
  return a5(v6, 1, a3, 1, a4);
}

uint64_t static vDSP.conjugate(_:count:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return static vDSP.conjugate(_:count:result:)(a1, a2, a3, a4, MEMORY[0x1E0C8CA90]);
}

{
  return static vDSP.conjugate(_:count:result:)(a1, a2, a3, a4, MEMORY[0x1E0C8CA98]);
}

uint64_t static vDSP.conjugate(_:count:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(_QWORD *, uint64_t, uint64_t, uint64_t, uint64_t))
{
  _QWORD v6[3];

  v6[2] = *MEMORY[0x1E0C80C00];
  v6[0] = a1;
  v6[1] = a2;
  if (a3 < 0)
    __break(1u);
  return a5(v6, 1, a4, 1, a3);
}

uint64_t static vDSP.divide<A>(_:by:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.divide<A>(_:by:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.divide<A>(_:by:result:));
}

{
  return static vDSP.divide<A>(_:by:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.divide<A>(_:by:result:));
}

uint64_t static vDSP.multiply<A>(_:by:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.divide<A>(_:by:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A>(_:by:result:));
}

{
  return static vDSP.divide<A>(_:by:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A>(_:by:result:));
}

uint64_t static vDSP.divide<A>(_:by:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  _QWORD v8[2];
  _BYTE v9[16];
  uint64_t v10;
  uint64_t v11;
  _QWORD *v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;

  v15 = *MEMORY[0x1E0C80C00];
  v8[0] = a1;
  v8[1] = a2;
  v10 = a5;
  v11 = a6;
  v12 = v8;
  v13 = a4;
  v14 = a3;
  return (*(uint64_t (**)(uint64_t, _BYTE *, uint64_t, uint64_t, uint64_t))(a6 + 24))(a7, v9, MEMORY[0x1E0DEE9C0] + 8, a5, a6);
}

uint64_t closure #1 in closure #1 in static vDSP.divide<A>(_:by:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v11;

  if (result)
  {
    v11 = result;
    result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a6, a7);
    if ((result & 0x8000000000000000) == 0)
      return a8(a3, 1, v11, 1, a4, 1, result);
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t static vDSP.multiply(_:by:count:useConjugate:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, char a6, uint64_t a7)
{
  return static vDSP.multiply(_:by:count:useConjugate:result:)(a1, a2, a3, a4, a5, a6, a7, MEMORY[0x1E0C8CAE8]);
}

{
  return static vDSP.multiply(_:by:count:useConjugate:result:)(a1, a2, a3, a4, a5, a6, a7, MEMORY[0x1E0C8CAF0]);
}

uint64_t static vDSP.multiply(_:by:count:useConjugate:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, char a6, uint64_t a7, uint64_t (*a8)(_QWORD *, uint64_t, _QWORD *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v9;
  _QWORD v11[2];
  _QWORD v12[3];

  v12[2] = *MEMORY[0x1E0C80C00];
  v12[0] = a1;
  v12[1] = a2;
  v11[0] = a3;
  v11[1] = a4;
  if (a5 < 0)
    __break(1u);
  if ((a6 & 1) != 0)
    v9 = 0xFFFFFFFFLL;
  else
    v9 = 1;
  return a8(v12, 1, v11, 1, a7, 1, a5, v9);
}

uint64_t static vDSP.add(_:to:count:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.add(_:to:count:result:)(a1, a2, a3, a4, a5, a6, MEMORY[0x1E0C8CA70]);
}

{
  return static vDSP.add(_:to:count:result:)(a1, a2, a3, a4, a5, a6, MEMORY[0x1E0C8CA78]);
}

uint64_t static vDSP.add(_:to:count:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(_QWORD *, uint64_t, _QWORD *, uint64_t, uint64_t, uint64_t, uint64_t))
{
  _QWORD v8[2];
  _QWORD v9[3];

  v9[2] = *MEMORY[0x1E0C80C00];
  v9[0] = a1;
  v9[1] = a2;
  v8[0] = a3;
  v8[1] = a4;
  if (a5 < 0)
    __break(1u);
  return a7(v9, 1, v8, 1, a6, 1, a5);
}

uint64_t static vDSP.divide(_:by:count:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.divide(_:by:count:result:)(a1, a2, a3, a4, a5, a6, MEMORY[0x1E0C8CAA0]);
}

{
  return static vDSP.divide(_:by:count:result:)(a1, a2, a3, a4, a5, a6, MEMORY[0x1E0C8CAA8]);
}

uint64_t static vDSP.divide(_:by:count:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(_QWORD *, uint64_t, _QWORD *, uint64_t, uint64_t, uint64_t, uint64_t))
{
  _QWORD v8[2];
  _QWORD v9[3];

  v9[2] = *MEMORY[0x1E0C80C00];
  v9[0] = a1;
  v9[1] = a2;
  v8[0] = a3;
  v8[1] = a4;
  if (a5 < 0)
    __break(1u);
  return a7(v8, 1, v9, 1, a6, 1, a5);
}

uint64_t static vDSP.subtract(_:from:count:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.subtract(_:from:count:result:)(a1, a2, a3, a4, a5, a6, MEMORY[0x1E0C8CB10]);
}

{
  return static vDSP.subtract(_:from:count:result:)(a1, a2, a3, a4, a5, a6, MEMORY[0x1E0C8CB18]);
}

uint64_t static vDSP.subtract(_:from:count:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(_QWORD *, uint64_t, _QWORD *, uint64_t, uint64_t, uint64_t, uint64_t))
{
  _QWORD v8[2];
  _QWORD v9[3];

  v9[2] = *MEMORY[0x1E0C80C00];
  v9[0] = a3;
  v9[1] = a4;
  v8[0] = a1;
  v8[1] = a2;
  if (a5 < 0)
    __break(1u);
  return a7(v8, 1, v9, 1, a6, 1, a5);
}

uint64_t static vDSP.absolute<A>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return static vDSP.absolute<A>(_:result:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in static vDSP.absolute<A>(_:result:));
}

{
  return static vDSP.absolute<A>(_:result:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in static vDSP.absolute<A>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.absolute<A>(_:result:)(_QWORD *a1)
{
  uint64_t *v1;

  return closure #1 in static vDSP.absolute<A>(_:result:)(a1, v1[2], v1[3], v1[4], MEMORY[0x1E0C8CA60]);
}

{
  uint64_t *v1;

  return closure #1 in static vDSP.absolute<A>(_:result:)(a1, v1[2], v1[3], v1[4], MEMORY[0x1E0C8CA68]);
}

uint64_t static vDSP.absolute<A>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v11;
  _QWORD v13[6];

  v11 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a5 + 8) + 16))(a4);
  v13[2] = a1;
  v13[3] = a2;
  v13[4] = v11;
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t, uint64_t))(a5 + 16))(a6, v13, MEMORY[0x1E0DEE9C0] + 8, a4, a5);
}

uint64_t closure #1 in static vDSP.absolute<A>(_:result:)(_QWORD *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(_QWORD *, uint64_t))
{
  _QWORD v6[3];

  v6[2] = *MEMORY[0x1E0C80C00];
  v6[0] = a2;
  v6[1] = a3;
  if (!*a1)
LABEL_5:
    __break(1u);
  if (a4 < 0)
  {
    __break(1u);
    goto LABEL_5;
  }
  return a5(v6, 1);
}

uint64_t static vDSP.squareMagnitudes<A>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return static vDSP.phase<A>(_:result:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in static vDSP.squareMagnitudes<A>(_:result:));
}

{
  return static vDSP.phase<A>(_:result:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in static vDSP.squareMagnitudes<A>(_:result:));
}

_QWORD *partial apply for closure #1 in static vDSP.squareMagnitudes<A>(_:result:)(_QWORD *a1)
{
  return partial apply for closure #1 in static vDSP.phase<A>(_:result:)(a1, MEMORY[0x1E0C8CAB8]);
}

{
  return partial apply for closure #1 in static vDSP.phase<A>(_:result:)(a1, MEMORY[0x1E0C8CAC0]);
}

_QWORD *partial apply for closure #1 in static vDSP.phase<A>(_:result:)(_QWORD *result, uint64_t (*a2)(__int128 *, uint64_t, _QWORD, uint64_t, uint64_t))
{
  uint64_t v2;
  uint64_t v3;
  __int128 v4;

  v3 = *(_QWORD *)(v2 + 32);
  v4 = *(_OWORD *)(v2 + 16);
  if (*result)
    return (_QWORD *)a2(&v4, 1, *result, 1, v3);
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A>(_:by:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A>(_:by:result:)(a1, a2, MEMORY[0x1E0C8CA40]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A>(_:by:result:)(a1, a2, MEMORY[0x1E0C8CA38]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A>(_:by:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t *v3;

  return closure #1 in closure #1 in static vDSP.divide<A>(_:by:result:)(a1, a2, v3[4], v3[5], v3[6], v3[2], v3[3], a3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.divide<A>(_:by:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A>(_:by:result:)(a1, a2, MEMORY[0x1E0C8CA30]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A>(_:by:result:)(a1, a2, MEMORY[0x1E0C8CA28]);
}

uint64_t BNNS.NearestNeighborsRef.init(capacity:dimensionCount:neighborCount:dataType:)(uint64_t result, uint64_t a2, uint64_t a3)
{
  uint64_t v3;

  if (result > 0xFFFFFFFFLL)
  {
    __break(1u);
    goto LABEL_8;
  }
  if (a2 > 0xFFFFFFFFLL)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if ((a2 | result | a3) < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (a3 > 0xFFFFFFFFLL)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  result = MEMORY[0x1D17944D4]();
  if (result)
  {
    *(_QWORD *)(v3 + 16) = result;
    return v3;
  }
LABEL_11:
  __break(1u);
  return result;
}

uint64_t BNNS.NearestNeighborsRef.__deallocating_deinit()
{
  uint64_t v0;

  MEMORY[0x1D1794510](*(_QWORD *)(v0 + 16));
  return swift_deallocClassInstance();
}

uint64_t BNNS.NearestNeighbors.init(capacity:dimensionCount:neighborCount:dataType:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, int a4@<W3>, uint64_t a5@<X8>)
{
  uint64_t result;

  type metadata accessor for BNNS.NearestNeighborsRef();
  swift_allocObject();
  result = BNNS.NearestNeighborsRef.init(capacity:dimensionCount:neighborCount:dataType:)(a1, a2, a3);
  *(_QWORD *)a5 = result;
  *(_QWORD *)(a5 + 8) = a2;
  *(_QWORD *)(a5 + 16) = a1;
  *(_QWORD *)(a5 + 24) = a3;
  *(_DWORD *)(a5 + 32) = a4;
  return result;
}

uint64_t type metadata accessor for BNNS.NearestNeighborsRef()
{
  return objc_opt_self();
}

void BNNS.NearestNeighbors.append(samples:)(uint64_t a1)
{
  uint64_t v1;
  uint64_t *v2;
  uint64_t v4;
  unint64_t v5;
  _BYTE v6[136];
  _BYTE v7[136];
  _BYTE v8[136];
  _BYTE v9[8];
  uint64_t v10;
  _BYTE v11[136];

  v2 = (uint64_t *)v1;
  if (*(_DWORD *)(a1 + 144) != *(_DWORD *)(v1 + 32))
  {
    __break(1u);
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  v4 = *v2;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v8);
  outlined init with take of BNNS.Shape((uint64_t)v8, (uint64_t)v11);
  if (_s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v11) - 1 >= 4)
  {
LABEL_12:
    __break(1u);
    return;
  }
  outlined init with take of UnsafeMutableRawPointer?(a1 + 136, (uint64_t)v9);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v9, (uint64_t)&v10);
  if (!v10)
    return;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v7);
  outlined init with take of BNNS.Shape((uint64_t)v7, (uint64_t)v8);
  outlined init with take of BNNS.Shape((uint64_t)v8, (uint64_t)v6);
  BNNS.Shape.size.getter();
  if ((v5 & 0x8000000000000000) != 0)
    goto LABEL_9;
  if (HIDWORD(v5))
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (MEMORY[0x1D17947F8](*(_QWORD *)(v4 + 16)))
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
}

uint64_t BNNS.NearestNeighbors.apply(index:outputIndices:outputDistances:)(uint64_t result, char a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4;
  uint64_t v5;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;

  if ((a2 & 1) != 0)
    v5 = -1;
  else
    v5 = result;
  if (v5 < (uint64_t)0xFFFFFFFF80000000)
  {
    __break(1u);
    goto LABEL_10;
  }
  if (v5 > 0x7FFFFFFF)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  v7 = *(_QWORD *)(*(_QWORD *)v4 + 16);
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)&v8);
  result = outlined init with take of UnsafeMutableRawPointer?((uint64_t)&v8, (uint64_t)&v9);
  if (!v9)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  if (*(_QWORD *)(a4 + 136))
    return MEMORY[0x1D17947EC](v7, v5);
LABEL_12:
  __break(1u);
  return result;
}

uint64_t destroy for BNNS.NearestNeighbors()
{
  return swift_release();
}

uint64_t initializeWithCopy for BNNS.NearestNeighbors(uint64_t a1, uint64_t a2)
{
  *(_QWORD *)a1 = *(_QWORD *)a2;
  *(_OWORD *)(a1 + 8) = *(_OWORD *)(a2 + 8);
  *(_QWORD *)(a1 + 24) = *(_QWORD *)(a2 + 24);
  *(_DWORD *)(a1 + 32) = *(_DWORD *)(a2 + 32);
  swift_retain();
  return a1;
}

uint64_t assignWithCopy for BNNS.NearestNeighbors(uint64_t a1, uint64_t a2)
{
  *(_QWORD *)a1 = *(_QWORD *)a2;
  swift_retain();
  swift_release();
  *(_QWORD *)(a1 + 8) = *(_QWORD *)(a2 + 8);
  *(_QWORD *)(a1 + 16) = *(_QWORD *)(a2 + 16);
  *(_QWORD *)(a1 + 24) = *(_QWORD *)(a2 + 24);
  *(_DWORD *)(a1 + 32) = *(_DWORD *)(a2 + 32);
  return a1;
}

__n128 __swift_memcpy36_8(uint64_t a1, uint64_t a2)
{
  __n128 result;
  __int128 v3;

  result = *(__n128 *)a2;
  v3 = *(_OWORD *)(a2 + 16);
  *(_DWORD *)(a1 + 32) = *(_DWORD *)(a2 + 32);
  *(__n128 *)a1 = result;
  *(_OWORD *)(a1 + 16) = v3;
  return result;
}

uint64_t assignWithTake for BNNS.NearestNeighbors(uint64_t a1, uint64_t a2)
{
  *(_QWORD *)a1 = *(_QWORD *)a2;
  swift_release();
  *(_OWORD *)(a1 + 8) = *(_OWORD *)(a2 + 8);
  *(_QWORD *)(a1 + 24) = *(_QWORD *)(a2 + 24);
  *(_DWORD *)(a1 + 32) = *(_DWORD *)(a2 + 32);
  return a1;
}

uint64_t getEnumTagSinglePayload for BNNS.NearestNeighbors(uint64_t *a1, int a2)
{
  uint64_t v2;

  if (!a2)
    return 0;
  if (a2 < 0 && *((_BYTE *)a1 + 36))
    return *(_DWORD *)a1 + 0x80000000;
  v2 = *a1;
  if ((unint64_t)*a1 >= 0xFFFFFFFF)
    LODWORD(v2) = -1;
  return (v2 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.NearestNeighbors(uint64_t result, int a2, int a3)
{
  if (a2 < 0)
  {
    *(_QWORD *)(result + 16) = 0;
    *(_QWORD *)(result + 24) = 0;
    *(_DWORD *)(result + 32) = 0;
    *(_QWORD *)result = a2 ^ 0x80000000;
    *(_QWORD *)(result + 8) = 0;
    if (a3 < 0)
      *(_BYTE *)(result + 36) = 1;
  }
  else
  {
    if ((a3 & 0x80000000) == 0)
    {
      if (!a2)
        return result;
LABEL_8:
      *(_QWORD *)result = (a2 - 1);
      return result;
    }
    *(_BYTE *)(result + 36) = 0;
    if (a2)
      goto LABEL_8;
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.NearestNeighbors()
{
  return &type metadata for BNNS.NearestNeighbors;
}

uint64_t vDSP.DCTTransformType.dctType.getter()
{
  unsigned __int8 *v0;

  return *v0 + 2;
}

BOOL static vDSP.DCTTransformType.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void vDSP.DCTTransformType.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

void *static vDSP.DCTTransformType.allCases.getter()
{
  return &outlined read-only object #0 of static vDSP.DCTTransformType.allCases.getter;
}

Swift::Int vDSP.DCTTransformType.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

void protocol witness for static CaseIterable.allCases.getter in conformance vDSP.DCTTransformType(_QWORD *a1@<X8>)
{
  *a1 = &outlined read-only object #0 of protocol witness for static CaseIterable.allCases.getter in conformance vDSP.DCTTransformType;
}

vDSP_DFT_SetupStruct *vDSP.DCT.__allocating_init(previous:count:transformType:)(vDSP_DFT_SetupStruct *a1, vDSP_Length a2, unsigned __int8 *a3)
{
  swift_allocObject();
  return vDSP.DCT.init(previous:count:transformType:)(a1, a2, a3);
}

vDSP_DFT_SetupStruct *vDSP.DCT.init(previous:count:transformType:)(vDSP_DFT_SetupStruct *result, vDSP_Length a2, unsigned __int8 *a3)
{
  uint64_t v3;
  vDSP_DFT_Setup Setup;

  if (result)
    result = (vDSP_DFT_SetupStruct *)*((_QWORD *)result + 2);
  if ((a2 & 0x8000000000000000) != 0)
  {
    __break(1u);
  }
  else
  {
    Setup = vDSP_DCT_CreateSetup(result, a2, (vDSP_DCT_Type)(*a3 + 2));
    swift_release();
    if (Setup)
    {
      *(_QWORD *)(v3 + 16) = Setup;
    }
    else
    {
      type metadata accessor for vDSP.DCT();
      swift_deallocPartialClassInstance();
      return 0;
    }
    return (vDSP_DFT_SetupStruct *)v3;
  }
  return result;
}

uint64_t type metadata accessor for vDSP.DCT()
{
  return objc_opt_self();
}

uint64_t vDSP.DCT.transform<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;

  v3 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v3, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in vDSP.DCT.transform<A>(_:));
}

uint64_t closure #1 in vDSP.DCT.transform<A>(_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t result;
  _QWORD v15[8];

  v11 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v12 = lazy protocol witness table accessor for type [vDSP.DCTTransformType] and conformance [A](&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&protocol conformance descriptor for UnsafeMutableBufferPointer<A>);
  v13 = *(_QWORD *)(a3 + 16);
  v15[2] = a5;
  v15[3] = v11;
  v15[4] = a6;
  v15[5] = v12;
  v15[6] = a4;
  v15[7] = v13;
  (*(void (**)(uint64_t (*)(uint64_t), _QWORD *, uint64_t, uint64_t, uint64_t))(v12 + 16))(partial apply for closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dctSetup:source:destination:), v15, MEMORY[0x1E0DEE9C0] + 8, v11, v12);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t partial apply for closure #1 in vDSP.DCT.transform<A>(_:)(uint64_t a1, uint64_t *a2)
{
  uint64_t *v2;

  return closure #1 in vDSP.DCT.transform<A>(_:)(a1, a2, v2[4], v2[5], v2[2], v2[3]);
}

uint64_t vDSP.DCT.transform<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v6;
  uint64_t v7;
  _QWORD v9[10];

  v7 = *(_QWORD *)(v6 + 16);
  v9[2] = a3;
  v9[3] = a4;
  v9[4] = a5;
  v9[5] = a6;
  v9[6] = a1;
  v9[7] = v7;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t), _QWORD *, uint64_t, uint64_t, uint64_t))(a6 + 16))(partial apply for closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dctSetup:source:destination:), v9, MEMORY[0x1E0DEE9C0] + 8, a4, a6);
}

uint64_t vDSP.DCT.deinit()
{
  uint64_t v0;

  vDSP_DFT_DestroySetup(*(vDSP_DFT_Setup *)(v0 + 16));
  return v0;
}

uint64_t vDSP.DCT.__deallocating_deinit()
{
  uint64_t v0;

  vDSP_DFT_DestroySetup(*(vDSP_DFT_Setup *)(v0 + 16));
  return swift_deallocClassInstance();
}

unint64_t lazy protocol witness table accessor for type vDSP.DCTTransformType and conformance vDSP.DCTTransformType()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vDSP.DCTTransformType and conformance vDSP.DCTTransformType;
  if (!lazy protocol witness table cache variable for type vDSP.DCTTransformType and conformance vDSP.DCTTransformType)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vDSP.DCTTransformType, &type metadata for vDSP.DCTTransformType);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.DCTTransformType and conformance vDSP.DCTTransformType);
  }
  return result;
}

uint64_t associated type witness table accessor for CaseIterable.AllCases : Collection in vDSP.DCTTransformType()
{
  return lazy protocol witness table accessor for type [vDSP.DCTTransformType] and conformance [A](&lazy protocol witness table cache variable for type [vDSP.DCTTransformType] and conformance [A], &demangling cache variable for type metadata for [vDSP.DCTTransformType], MEMORY[0x1E0DEAF50]);
}

_UNKNOWN **associated type witness table accessor for vDSP_FloatingPointDiscreteCosineTransformable.DCTFunctions : vDSP_DCTFunctions in Float()
{
  return &protocol witness table for vDSP.VectorizableFloat;
}

uint64_t storeEnumTagSinglePayload for vDSP.DCTTransformType(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 2 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 2) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFE)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFD)
    return ((uint64_t (*)(void))((char *)&loc_1CAB55190 + 4 * byte_1CAB63395[v4]))();
  *a1 = a2 + 2;
  return ((uint64_t (*)(void))((char *)sub_1CAB551C4 + 4 * asc_1CAB63390[v4]))();
}

uint64_t sub_1CAB551C4(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB551CC(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB551D4);
  return result;
}

uint64_t sub_1CAB551E0(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB551E8);
  *(_BYTE *)result = a2 + 2;
  return result;
}

uint64_t sub_1CAB551EC(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB551F4(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vDSP.DCTTransformType()
{
  return &type metadata for vDSP.DCTTransformType;
}

uint64_t method lookup function for vDSP.DCT()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of vDSP.DCT.__allocating_init(previous:count:transformType:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(v0 + 88))();
}

uint64_t dispatch thunk of vDSP.DCT.transform<A>(_:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 96))();
}

uint64_t dispatch thunk of vDSP.DCT.transform<A, B>(_:result:)()
{
  uint64_t v0;

  return (*(uint64_t (**)(void))(*(_QWORD *)v0 + 104))();
}

uint64_t partial apply for closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dctSetup:source:destination:)(uint64_t a1)
{
  _QWORD *v1;
  uint64_t v2;
  uint64_t v3;
  _QWORD v5[4];

  v5[3] = a1;
  v2 = v1[2];
  v3 = v1[4];
  v5[2] = v1[7];
  return (*(uint64_t (**)(void (*)(const float *), _QWORD *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dctSetup:source:destination:), v5, MEMORY[0x1E0DEE9C0] + 8, v2);
}

void partial apply for closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dctSetup:source:destination:)(const float *__Input)
{
  uint64_t v1;
  float *v2;

  if (__Input)
  {
    v2 = **(float ***)(v1 + 24);
    if (v2)
    {
      vDSP_DCT_Execute(*(const vDSP_DFT_SetupStruct **)(v1 + 16), __Input, v2);
      return;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
}

uint64_t lazy protocol witness table accessor for type [vDSP.DCTTransformType] and conformance [A](unint64_t *a1, uint64_t *a2, uint64_t a3)
{
  uint64_t result;
  uint64_t v6;

  result = *a1;
  if (!result)
  {
    v6 = __swift_instantiateConcreteTypeFromMangledNameAbstract(a2);
    result = MEMORY[0x1D1794D08](a3, v6);
    atomic_store(result, a1);
  }
  return result;
}

uint64_t static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  void (*v22)(char *);
  uint64_t v23;
  void (*v24)(char *, uint64_t, uint64_t);
  uint64_t (*v25)(uint64_t, uint64_t);
  uint64_t v26;
  uint64_t (*v27)(uint64_t, uint64_t);
  uint64_t v28;
  uint64_t result;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;

  v42 = a3;
  v43 = a6;
  v14 = *(_QWORD *)(a4 - 8);
  v15 = MEMORY[0x1E0C80A78](a1);
  v17 = (char *)&v37 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = *(_QWORD *)(v18 - 8);
  MEMORY[0x1E0C80A78](v15);
  v21 = (char *)&v37 - ((v20 + 15) & 0xFFFFFFFFFFFFFFF0);
  v22 = *(void (**)(char *))(v19 + 16);
  v40 = v23;
  v22(v21);
  v24 = *(void (**)(char *, uint64_t, uint64_t))(v14 + 16);
  v38 = a1;
  v24(v17, a1, a4);
  v25 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  v41 = a8;
  v26 = v25(a5, a8);
  v27 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  v39 = a7;
  v28 = v27(a4, a7);
  (*(void (**)(char *, uint64_t))(v14 + 8))(v17, a4);
  result = (*(uint64_t (**)(char *, uint64_t))(v19 + 8))(v21, a5);
  if (v26 != v28)
  {
    __break(1u);
    goto LABEL_6;
  }
  v30 = v43;
  result = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a9 + 8) + 16))(v43);
  if (result < 0)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  v31 = result;
  v32 = v40;
  v33 = v41;
  result = v25(a5, v41);
  if ((result & 0x8000000000000000) == 0)
  {
    v34 = MEMORY[0x1E0C80A78](result);
    *(&v37 - 10) = a4;
    *(&v37 - 9) = a5;
    v35 = v39;
    *(&v37 - 8) = v30;
    *(&v37 - 7) = v35;
    *(&v37 - 6) = v33;
    *(&v37 - 5) = a9;
    *(&v37 - 4) = v38;
    *(&v37 - 3) = v32;
    *(&v37 - 2) = v31;
    *(&v37 - 1) = v34;
    return (*(uint64_t (**)(uint64_t))(a9 + 16))(v36);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:));
}

{
  return partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:));
}

void static vDSP.linearInterpolate<A, B>(values:atIndices:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  float v6;
  uint64_t v7;

  v6 = static vDSP.maximum<A>(_:)(a2, a4, a6);
  if ((~LODWORD(v6) & 0x7F800000) == 0)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (v6 <= -9.2234e18)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (v6 >= 9.2234e18)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (!__OFADD__((uint64_t)v6, 1))
  {
    v7 = MEMORY[0x1E0C80A78]((uint64_t)v6 + 1);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v7, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(values:atIndices:));
    return;
  }
LABEL_9:
  __break(1u);
}

{
  double v6;
  uint64_t v7;

  v6 = static vDSP.maximum<A>(_:)(a2, a4, a6);
  if ((~*(_QWORD *)&v6 & 0x7FF0000000000000) == 0)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (v6 <= -9.22337204e18)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (v6 >= 9.22337204e18)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (!__OFADD__((uint64_t)v6, 1))
  {
    v7 = MEMORY[0x1E0C80A78]((uint64_t)v6 + 1);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v7, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(values:atIndices:));
    return;
  }
LABEL_9:
  __break(1u);
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(values:atIndices:)(uint64_t a1, _QWORD *a2)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:));
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t *a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v8;

  if (!a3)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!result)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  v8 = *a5;
  if (v8)
    return a8(a3, 1, result, 1, v8, 1, a6, a7);
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(uint64_t a1, float a2, float a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  uint64_t v16;
  uint64_t v17;
  char *v18;
  void (*v19)(char *);
  uint64_t v20;
  uint64_t (*v21)(uint64_t, uint64_t);
  uint64_t v22;
  uint64_t v23;
  uint64_t (*v24)(uint64_t, uint64_t);
  uint64_t v25;
  uint64_t v26;
  uint64_t result;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;

  v39 = a6;
  v40 = a8;
  v38 = a1;
  v16 = *(_QWORD *)(a7 - 8);
  MEMORY[0x1E0C80A78](a1);
  v18 = (char *)&v34 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = *(void (**)(char *))(v16 + 16);
  v35 = v20;
  v19(v18);
  v21 = *(uint64_t (**)(uint64_t, uint64_t))(a10 + 16);
  v37 = a10;
  v22 = v21(a7, a10);
  v36 = a11;
  v23 = *(_QWORD *)(a11 + 8);
  v24 = *(uint64_t (**)(uint64_t, uint64_t))(v23 + 16);
  v25 = v40;
  v26 = v24(v40, v23);
  result = (*(uint64_t (**)(char *, uint64_t))(v16 + 8))(v18, a7);
  if (v22 != v26)
  {
    __break(1u);
    goto LABEL_6;
  }
  v28 = v38;
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a9 + 16))(v39, a9);
  if (result < 0)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  v29 = result;
  result = v24(v25, v23);
  if ((result & 0x8000000000000000) == 0)
  {
    v30 = MEMORY[0x1E0C80A78](result);
    *(&v34 - 12) = v31;
    *(&v34 - 11) = a7;
    *(&v34 - 10) = v25;
    *(&v34 - 9) = a9;
    v32 = v36;
    *(&v34 - 8) = v37;
    *(&v34 - 7) = v32;
    v33 = v35;
    *(&v34 - 6) = v28;
    *(&v34 - 5) = v33;
    *((float *)&v34 - 8) = a2;
    *((float *)&v34 - 7) = a3;
    *(&v34 - 3) = v29;
    *(&v34 - 2) = v30;
    return (*(uint64_t (**)(_QWORD))(v32 + 16))(partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:));
  }
LABEL_7:
  __break(1u);
  return result;
}

void closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(const float *a1, int a2, const float *__C, int a4, vDSP_Length __M, float **a6, vDSP_Length __N, float a8, float a9)
{
  float v9;
  float v10;

  if (!a1)
  {
    __break(1u);
    goto LABEL_6;
  }
  v10 = a8;
  v9 = a9;
  if (!__C)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*a6)
  {
    vDSP_vtabi(a1, 1, &v10, &v9, __C, __M, *a6, 1, __N);
    return;
  }
LABEL_7:
  __break(1u);
}

uint64_t static vDSP.linearInterpolate<A, B>(lookupTable:withOffsets:scale:baseOffset:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v6;

  v6 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(lookupTable:withOffsets:scale:baseOffset:));
}

{
  uint64_t v6;

  v6 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(lookupTable:withOffsets:scale:baseOffset:));
}

uint64_t closure #1 in static vDSP.linearInterpolate<A, B>(lookupTable:withOffsets:scale:baseOffset:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, float a10)
{
  uint64_t v19;
  uint64_t v20;
  uint64_t result;

  v19 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v20 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(a3, a9, a10, a4, a1, a5, a6, v19, a7, a8, v20);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
  *a2 = result;
  return result;
}

uint64_t static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(uint64_t a1, double a2, double a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  uint64_t v16;
  uint64_t v17;
  char *v18;
  void (*v19)(char *);
  uint64_t v20;
  uint64_t (*v21)(uint64_t, uint64_t);
  uint64_t v22;
  uint64_t v23;
  uint64_t (*v24)(uint64_t, uint64_t);
  uint64_t v25;
  uint64_t v26;
  uint64_t result;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;

  v39 = a6;
  v40 = a8;
  v38 = a1;
  v16 = *(_QWORD *)(a7 - 8);
  MEMORY[0x1E0C80A78](a1);
  v18 = (char *)&v34 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v19 = *(void (**)(char *))(v16 + 16);
  v35 = v20;
  v19(v18);
  v21 = *(uint64_t (**)(uint64_t, uint64_t))(a10 + 16);
  v37 = a10;
  v22 = v21(a7, a10);
  v36 = a11;
  v23 = *(_QWORD *)(a11 + 8);
  v24 = *(uint64_t (**)(uint64_t, uint64_t))(v23 + 16);
  v25 = v40;
  v26 = v24(v40, v23);
  result = (*(uint64_t (**)(char *, uint64_t))(v16 + 8))(v18, a7);
  if (v22 != v26)
  {
    __break(1u);
    goto LABEL_6;
  }
  v28 = v38;
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a9 + 16))(v39, a9);
  if (result < 0)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  v29 = result;
  result = v24(v25, v23);
  if ((result & 0x8000000000000000) == 0)
  {
    v30 = MEMORY[0x1E0C80A78](result);
    *(&v34 - 12) = v31;
    *(&v34 - 11) = a7;
    *(&v34 - 10) = v25;
    *(&v34 - 9) = a9;
    v32 = v36;
    *(&v34 - 8) = v37;
    *(&v34 - 7) = v32;
    v33 = v35;
    *(&v34 - 6) = v28;
    *(&v34 - 5) = v33;
    *((double *)&v34 - 4) = a2;
    *((double *)&v34 - 3) = a3;
    *(&v34 - 2) = v29;
    *(&v34 - 1) = v30;
    return (*(uint64_t (**)(_QWORD))(v32 + 16))(partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:));
  }
LABEL_7:
  __break(1u);
  return result;
}

void closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(const double *a1, int a2, const double *__C, int a4, vDSP_Length __M, double **a6, vDSP_Length __N, double a8, double a9)
{
  double v9;
  double v10;

  if (!a1)
  {
    __break(1u);
    goto LABEL_6;
  }
  v10 = a8;
  v9 = a9;
  if (!__C)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*a6)
  {
    vDSP_vtabiD(a1, 1, &v10, &v9, __C, __M, *a6, 1, __N);
    return;
  }
LABEL_7:
  __break(1u);
}

uint64_t closure #1 in static vDSP.linearInterpolate<A, B>(lookupTable:withOffsets:scale:baseOffset:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, double a10)
{
  uint64_t v19;
  uint64_t v20;
  uint64_t result;

  v19 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v20 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(a3, a9, a10, a4, a1, a5, a6, v19, a7, a8, v20);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
  *a2 = result;
  return result;
}

uint64_t static vDSP.fill<A>(_:with:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t result;

  result = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a3 + 8) + 16))(a2);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1E0C80A78](result);
    return (*(uint64_t (**)(_QWORD))(a3 + 16))(partial apply for closure #1 in static vDSP.fill<A>(_:with:));
  }
  return result;
}

{
  uint64_t result;

  result = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a3 + 8) + 16))(a2);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1E0C80A78](result);
    return (*(uint64_t (**)(_QWORD))(a3 + 16))(partial apply for closure #1 in static vDSP.fill<A>(_:with:));
  }
  return result;
}

uint64_t static vDSP.clear<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.reverse<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.clear<A>(_:));
}

{
  return static vDSP.reverse<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.clear<A>(_:));
}

BOOL static vDSP.WindowSequence.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void vDSP.WindowSequence.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

Swift::Int vDSP.WindowSequence.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t static vDSP.window<A>(ofType:usingSequence:count:isHalfWindow:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t result;

  if (a3 < 1)
  {
    __break(1u);
  }
  else
  {
    if (a5 == MEMORY[0x1E0DEB188])
    {
      MEMORY[0x1E0C80A78](a1);
      specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.window<A>(ofType:usingSequence:count:isHalfWindow:));
      goto LABEL_6;
    }
    if (a5 == MEMORY[0x1E0DEB070])
    {
      MEMORY[0x1E0C80A78](a1);
      specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #2 in static vDSP.window<A>(ofType:usingSequence:count:isHalfWindow:));
LABEL_6:
      v7 = _arrayForceCast<A, B>(_:)();
      swift_bridgeObjectRelease();
      return v7;
    }
  }
  result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:));
}

{
  return static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:));
}

uint64_t static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t result;

  result = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a5 + 8) + 16))(a4);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1E0C80A78](result);
    return (*(uint64_t (**)(uint64_t))(a5 + 16))(a6);
  }
  return result;
}

void closure #1 in static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(uint64_t a1, char a2)
{
  __asm { BR              X10 }
}

uint64_t sub_1CAB56348(_QWORD *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(void))
{
  if (!*a1)
  {
    __break(1u);
    JUMPOUT(0x1CAB563C8);
  }
  return a5();
}

uint64_t static vDSP.ramp(withInitialValue:increment:count:)(int64_t a1, float a2, float a3)
{
  uint64_t v6;
  float v8;
  float __A;
  uint64_t v10;

  v10 = *MEMORY[0x1E0C80C00];
  if (a1 < 1)
    __break(1u);
  v6 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(_QWORD *)(v6 + 16) = a1;
  v8 = a3;
  __A = a2;
  vDSP_vramp(&__A, &v8, (float *)(v6 + 32), 1, a1);
  *(_QWORD *)(v6 + 16) = a1;
  return v6;
}

uint64_t static vDSP.formRamp<A>(withInitialValue:increment:result:)(float a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v9;
  float v11;
  float v12;
  _BYTE v13[16];
  float *v14;
  float *v15;
  uint64_t v16;
  uint64_t v17;

  v17 = *MEMORY[0x1E0C80C00];
  v9 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a5 + 8) + 16))(a4);
  if (v9 < 0)
    __break(1u);
  v11 = a1;
  v12 = a2;
  v14 = &v11;
  v15 = &v12;
  v16 = v9;
  return (*(uint64_t (**)(_QWORD *(*)(_QWORD *), _BYTE *, uint64_t, uint64_t, uint64_t))(a5 + 16))(partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:), v13, MEMORY[0x1E0DEE9C0] + 8, a4, a5);
}

uint64_t static vDSP.ramp(withInitialValue:increment:count:)(int64_t a1, double a2, double a3)
{
  uint64_t v6;
  double __B;
  double __A[2];

  __A[1] = *(double *)MEMORY[0x1E0C80C00];
  if (a1 < 1)
    __break(1u);
  v6 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(_QWORD *)(v6 + 16) = a1;
  __B = a3;
  __A[0] = a2;
  vDSP_vrampD(__A, &__B, (double *)(v6 + 32), 1, a1);
  *(_QWORD *)(v6 + 16) = a1;
  return v6;
}

uint64_t static vDSP.formRamp<A>(withInitialValue:increment:result:)(double a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v9;
  double v11;
  double v12;
  _BYTE v13[16];
  double *v14;
  double *v15;
  uint64_t v16;
  uint64_t v17;

  v17 = *MEMORY[0x1E0C80C00];
  v9 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a5 + 8) + 16))(a4);
  if (v9 < 0)
    __break(1u);
  v11 = a1;
  v12 = a2;
  v14 = &v11;
  v15 = &v12;
  v16 = v9;
  return (*(uint64_t (**)(_QWORD *(*)(_QWORD *), _BYTE *, uint64_t, uint64_t, uint64_t))(a5 + 16))(partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:), v13, MEMORY[0x1E0DEE9C0] + 8, a4, a5);
}

uint64_t static vDSP.ramp(in:count:)(int64_t a1, float a2, float a3)
{
  return static vDSP.ramp(in:count:)(a1, a2, a3);
}

{
  uint64_t v6;
  float v8;
  float __B;
  uint64_t v10;

  v10 = *MEMORY[0x1E0C80C00];
  if (a1 < 1)
    __break(1u);
  v6 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(_QWORD *)(v6 + 16) = a1;
  v8 = a2;
  __B = (float)(a3 - a2) / (float)((float)(unint64_t)a1 + -1.0);
  vDSP_vramp(&v8, &__B, (float *)(v6 + 32), 1, a1);
  *(_QWORD *)(v6 + 16) = a1;
  return v6;
}

uint64_t static vDSP.formRamp<A>(in:result:)(uint64_t a1, uint64_t a2, uint64_t a3, float a4, float a5)
{
  return static vDSP.formRamp<A>(in:result:)(a4, a5, a1, a2, a3, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:));
}

uint64_t static vDSP.ramp(in:count:)(int64_t a1, double a2, double a3)
{
  return static vDSP.ramp(in:count:)(a1, a2, a3);
}

{
  uint64_t v6;
  double __A;
  double __B[2];

  __B[1] = *(double *)MEMORY[0x1E0C80C00];
  if (a1 < 1)
    __break(1u);
  v6 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(_QWORD *)(v6 + 16) = a1;
  __A = a2;
  __B[0] = (a3 - a2) / ((double)(unint64_t)a1 + -1.0);
  vDSP_vrampD(&__A, __B, (double *)(v6 + 32), 1, a1);
  *(_QWORD *)(v6 + 16) = a1;
  return v6;
}

uint64_t static vDSP.formRamp<A>(in:result:)(uint64_t a1, uint64_t a2, uint64_t a3, double a4, double a5)
{
  return static vDSP.formRamp<A>(in:result:)(a4, a5, a1, a2, a3, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:));
}

uint64_t static vDSP.ramp(from:through:count:)(int64_t a1, float a2, float a3)
{
  return static vDSP.ramp(in:count:)(a1, a2, a3);
}

uint64_t static vDSP.formRamp<A>(from:through:result:)(uint64_t a1, uint64_t a2, uint64_t a3, float a4, float a5)
{
  return static vDSP.formRamp<A>(in:result:)(a4, a5, a1, a2, a3, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:));
}

uint64_t static vDSP.formRamp<A>(in:result:)(float a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  unint64_t v11;
  float v13;
  float v14;
  _BYTE v15[16];
  float *v16;
  float *v17;
  unint64_t v18;
  uint64_t v19;

  v19 = *MEMORY[0x1E0C80C00];
  v11 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a5 + 8) + 16))(a4);
  if ((v11 & 0x8000000000000000) != 0)
    __break(1u);
  v13 = a1;
  v14 = (float)(a2 - a1) / (float)((float)v11 + -1.0);
  v16 = &v13;
  v17 = &v14;
  v18 = v11;
  return (*(uint64_t (**)(uint64_t, _BYTE *, uint64_t, uint64_t, uint64_t))(a5 + 16))(a6, v15, MEMORY[0x1E0DEE9C0] + 8, a4, a5);
}

uint64_t static vDSP.ramp(from:through:count:)(int64_t a1, double a2, double a3)
{
  return static vDSP.ramp(in:count:)(a1, a2, a3);
}

uint64_t static vDSP.formRamp<A>(from:through:result:)(uint64_t a1, uint64_t a2, uint64_t a3, double a4, double a5)
{
  return static vDSP.formRamp<A>(in:result:)(a4, a5, a1, a2, a3, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:));
}

uint64_t static vDSP.formRamp<A>(in:result:)(double a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  unint64_t v11;
  double v13;
  double v14;
  _BYTE v15[16];
  double *v16;
  double *v17;
  unint64_t v18;
  uint64_t v19;

  v19 = *MEMORY[0x1E0C80C00];
  v11 = (*(uint64_t (**)(uint64_t))(*(_QWORD *)(a5 + 8) + 16))(a4);
  if ((v11 & 0x8000000000000000) != 0)
    __break(1u);
  v13 = a1;
  v14 = (a2 - a1) / ((double)v11 + -1.0);
  v16 = &v13;
  v17 = &v14;
  v18 = v11;
  return (*(uint64_t (**)(uint64_t, _BYTE *, uint64_t, uint64_t, uint64_t))(a5 + 16))(a6, v15, MEMORY[0x1E0DEE9C0] + 8, a4, a5);
}

uint64_t static vDSP.ramp<A>(withInitialValue:multiplyingBy:increment:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4;

  v4 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v4, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.ramp<A>(withInitialValue:multiplyingBy:increment:));
}

{
  uint64_t v4;

  v4 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v4, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.ramp<A>(withInitialValue:multiplyingBy:increment:));
}

uint64_t closure #1 in static vDSP.ramp<A>(withInitialValue:multiplyingBy:increment:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  uint64_t v14;
  uint64_t v15;
  uint64_t result;

  v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(a3, a7, a4, a1, a5, v14, a6, v15);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(uint64_t a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v13;
  uint64_t v14;
  char *v15;
  void (*v16)(char *);
  uint64_t v17;
  uint64_t (*v18)(uint64_t, uint64_t);
  uint64_t v19;
  uint64_t v20;
  uint64_t (*v21)(uint64_t, uint64_t);
  uint64_t v22;
  uint64_t result;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;

  v28 = a1;
  v13 = *(_QWORD *)(a5 - 8);
  MEMORY[0x1E0C80A78](a1);
  v15 = (char *)&v26 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0);
  v16 = *(void (**)(char *))(v13 + 16);
  v27 = v17;
  v16(v15);
  v18 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  v29 = a7;
  v19 = v18(a5, a7);
  v30 = a8;
  v20 = *(_QWORD *)(a8 + 8);
  v21 = *(uint64_t (**)(uint64_t, uint64_t))(v20 + 16);
  v22 = v21(a6, v20);
  result = (*(uint64_t (**)(char *, uint64_t))(v13 + 8))(v15, a5);
  if (v19 == v22)
  {
    result = v21(a6, v20);
    if ((result & 0x8000000000000000) == 0)
    {
      v24 = MEMORY[0x1E0C80A78](result);
      *(&v26 - 8) = a5;
      *(&v26 - 7) = a6;
      v25 = v30;
      *(&v26 - 6) = v29;
      *(&v26 - 5) = v25;
      *(&v26 - 4) = v27;
      *((float *)&v26 - 6) = a2;
      *(&v26 - 2) = v28;
      *(&v26 - 1) = v24;
      return (*(uint64_t (**)(_QWORD))(v25 + 16))(partial apply for closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:));
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

void closure #1 in closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(const float *a1, int a2, float *a3, float **a4, vDSP_Length __N, float a6)
{
  float __Step;
  uint64_t v7;

  v7 = *MEMORY[0x1E0C80C00];
  __Step = a6;
  if (!a1)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  if (!*a4)
    goto LABEL_5;
  vDSP_vrampmul(a1, 1, a3, &__Step, *a4, 1, __N);
}

uint64_t closure #1 in static vDSP.ramp<A>(withInitialValue:multiplyingBy:increment:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  uint64_t v14;
  uint64_t v15;
  uint64_t result;

  v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(a3, a7, a4, a1, a5, v14, a6, v15);
  result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(uint64_t a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v13;
  uint64_t v14;
  char *v15;
  void (*v16)(char *);
  uint64_t v17;
  uint64_t (*v18)(uint64_t, uint64_t);
  uint64_t v19;
  uint64_t v20;
  uint64_t (*v21)(uint64_t, uint64_t);
  uint64_t v22;
  uint64_t result;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;

  v28 = a1;
  v13 = *(_QWORD *)(a5 - 8);
  MEMORY[0x1E0C80A78](a1);
  v15 = (char *)&v26 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0);
  v16 = *(void (**)(char *))(v13 + 16);
  v27 = v17;
  v16(v15);
  v18 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  v29 = a7;
  v19 = v18(a5, a7);
  v30 = a8;
  v20 = *(_QWORD *)(a8 + 8);
  v21 = *(uint64_t (**)(uint64_t, uint64_t))(v20 + 16);
  v22 = v21(a6, v20);
  result = (*(uint64_t (**)(char *, uint64_t))(v13 + 8))(v15, a5);
  if (v19 == v22)
  {
    result = v21(a6, v20);
    if ((result & 0x8000000000000000) == 0)
    {
      v24 = MEMORY[0x1E0C80A78](result);
      *(&v26 - 8) = a5;
      *(&v26 - 7) = a6;
      v25 = v30;
      *(&v26 - 6) = v29;
      *(&v26 - 5) = v25;
      *(&v26 - 4) = v27;
      *((double *)&v26 - 3) = a2;
      *(&v26 - 2) = v28;
      *(&v26 - 1) = v24;
      return (*(uint64_t (**)(_QWORD))(v25 + 16))(partial apply for closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:));
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

void closure #1 in closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(const double *a1, int a2, double *a3, double **a4, vDSP_Length __N, double a6)
{
  double v6[2];

  v6[1] = *(double *)MEMORY[0x1E0C80C00];
  v6[0] = a6;
  if (!a1)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  if (!*a4)
    goto LABEL_5;
  vDSP_vrampmulD(a1, 1, a3, v6, *a4, 1, __N);
}

uint64_t static vDSP.stereoRamp<A>(withInitialValue:multiplyingBy:_:increment:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v5;
  uint64_t result;

  v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.stereoRamp<A>(withInitialValue:multiplyingBy:_:increment:));
  result = 0;
  __break(1u);
  return result;
}

{
  uint64_t v5;
  uint64_t result;

  v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in static vDSP.stereoRamp<A>(withInitialValue:multiplyingBy:_:increment:));
  result = 0;
  __break(1u);
  return result;
}

uint64_t static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(uint64_t a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  char *v22;
  void (*v23)(char *);
  uint64_t v24;
  uint64_t (*v25)(uint64_t, uint64_t);
  uint64_t v26;
  uint64_t v27;
  void (*v28)(char *, uint64_t);
  uint64_t result;
  uint64_t v30;
  uint64_t (*v31)(uint64_t, uint64_t);
  uint64_t v32;
  uint64_t v33;
  char *v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  char *v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;

  v50 = a6;
  v51 = a8;
  v45 = a1;
  v15 = *(_QWORD *)(a7 - 8);
  v16 = MEMORY[0x1E0C80A78](a1);
  v47 = (char *)&v43 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v18 = MEMORY[0x1E0C80A78](v16);
  v20 = (char *)&v43 - v19;
  MEMORY[0x1E0C80A78](v18);
  v22 = (char *)&v43 - v21;
  v23 = *(void (**)(char *))(v15 + 16);
  v49 = v24;
  v23((char *)&v43 - v21);
  v44 = a4;
  ((void (*)(char *, uint64_t, uint64_t))v23)(v20, a4, a7);
  v25 = *(uint64_t (**)(uint64_t, uint64_t))(a9 + 16);
  v26 = v25(a7, a9);
  v48 = a9;
  v27 = v25(a7, a9);
  v28 = *(void (**)(char *, uint64_t))(v15 + 8);
  v28(v20, a7);
  result = ((uint64_t (*)(char *, uint64_t))v28)(v22, a7);
  if (v26 != v27)
  {
    __break(1u);
    goto LABEL_7;
  }
  v30 = *(_QWORD *)(a10 + 8);
  v31 = *(uint64_t (**)(uint64_t, uint64_t))(v30 + 16);
  v46 = a5;
  v32 = v51;
  v33 = v31(v51, v30);
  result = v31(v32, v30);
  if (v33 != result)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  v43 = a10;
  v34 = v47;
  ((void (*)(char *, uint64_t, uint64_t))v23)(v47, v49, a7);
  v35 = v48;
  v36 = v25(a7, v48);
  v37 = v51;
  v38 = v31(v51, v30);
  result = ((uint64_t (*)(char *, uint64_t))v28)(v34, a7);
  if (v36 != v38)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v39 = v50;
  result = v31(v37, v30);
  if ((result & 0x8000000000000000) == 0)
  {
    v40 = MEMORY[0x1E0C80A78](result);
    *(&v43 - 10) = a7;
    *(&v43 - 9) = v37;
    v41 = v43;
    *(&v43 - 8) = v35;
    *(&v43 - 7) = v41;
    v42 = v49;
    *(&v43 - 6) = v39;
    *(&v43 - 5) = v42;
    *(&v43 - 4) = v44;
    *((float *)&v43 - 6) = a2;
    *(&v43 - 2) = v45;
    *(&v43 - 1) = v40;
    return (*(uint64_t (**)(_QWORD))(v41 + 16))(partial apply for closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:));
  }
LABEL_9:
  __break(1u);
  return result;
}

void closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(const float *__I1, int a2, const float *__I0, int a4, float *__Start, float **a6, float **a7, vDSP_Length __N, float a9)
{
  float *v9;
  float *v10;
  float __Step;
  uint64_t v12;

  v12 = *MEMORY[0x1E0C80C00];
  __Step = a9;
  if (!__I0)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (!__I1)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  v9 = *a6;
  if (!v9)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  v10 = *a7;
  if (!v10)
    goto LABEL_9;
  vDSP_vrampmul2(__I0, __I1, 1, __Start, &__Step, v9, v10, 1, __N);
}

uint64_t static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(uint64_t a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  char *v22;
  void (*v23)(char *);
  uint64_t v24;
  uint64_t (*v25)(uint64_t, uint64_t);
  uint64_t v26;
  uint64_t v27;
  void (*v28)(char *, uint64_t);
  uint64_t result;
  uint64_t v30;
  uint64_t (*v31)(uint64_t, uint64_t);
  uint64_t v32;
  uint64_t v33;
  char *v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  char *v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;

  v50 = a6;
  v51 = a8;
  v45 = a1;
  v15 = *(_QWORD *)(a7 - 8);
  v16 = MEMORY[0x1E0C80A78](a1);
  v47 = (char *)&v43 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v18 = MEMORY[0x1E0C80A78](v16);
  v20 = (char *)&v43 - v19;
  MEMORY[0x1E0C80A78](v18);
  v22 = (char *)&v43 - v21;
  v23 = *(void (**)(char *))(v15 + 16);
  v49 = v24;
  v23((char *)&v43 - v21);
  v44 = a4;
  ((void (*)(char *, uint64_t, uint64_t))v23)(v20, a4, a7);
  v25 = *(uint64_t (**)(uint64_t, uint64_t))(a9 + 16);
  v26 = v25(a7, a9);
  v48 = a9;
  v27 = v25(a7, a9);
  v28 = *(void (**)(char *, uint64_t))(v15 + 8);
  v28(v20, a7);
  result = ((uint64_t (*)(char *, uint64_t))v28)(v22, a7);
  if (v26 != v27)
  {
    __break(1u);
    goto LABEL_7;
  }
  v30 = *(_QWORD *)(a10 + 8);
  v31 = *(uint64_t (**)(uint64_t, uint64_t))(v30 + 16);
  v46 = a5;
  v32 = v51;
  v33 = v31(v51, v30);
  result = v31(v32, v30);
  if (v33 != result)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  v43 = a10;
  v34 = v47;
  ((void (*)(char *, uint64_t, uint64_t))v23)(v47, v49, a7);
  v35 = v48;
  v36 = v25(a7, v48);
  v37 = v51;
  v38 = v31(v51, v30);
  result = ((uint64_t (*)(char *, uint64_t))v28)(v34, a7);
  if (v36 != v38)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  v39 = v50;
  result = v31(v37, v30);
  if ((result & 0x8000000000000000) == 0)
  {
    v40 = MEMORY[0x1E0C80A78](result);
    *(&v43 - 10) = a7;
    *(&v43 - 9) = v37;
    v41 = v43;
    *(&v43 - 8) = v35;
    *(&v43 - 7) = v41;
    v42 = v49;
    *(&v43 - 6) = v39;
    *(&v43 - 5) = v42;
    *(&v43 - 4) = v44;
    *((double *)&v43 - 3) = a2;
    *(&v43 - 2) = v45;
    *(&v43 - 1) = v40;
    return (*(uint64_t (**)(_QWORD))(v41 + 16))(partial apply for closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:));
  }
LABEL_9:
  __break(1u);
  return result;
}

void closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(const double *__I1, int a2, const double *__I0, int a4, double *__Start, double **a6, double **a7, vDSP_Length __N, double a9)
{
  double *v9;
  double *v10;
  double __Step[2];

  __Step[1] = *(double *)MEMORY[0x1E0C80C00];
  __Step[0] = a9;
  if (!__I0)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (!__I1)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  v9 = *a6;
  if (!v9)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  v10 = *a7;
  if (!v10)
    goto LABEL_9;
  vDSP_vrampmul2D(__I0, __I1, 1, __Start, __Step, v9, v10, 1, __N);
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v7;
  __int128 v8;
  uint64_t v9;
  __int128 v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  __int128 v14;

  v3 = *(_QWORD *)(v2 + 32);
  v4 = *(_QWORD *)(v2 + 56);
  v5 = *(_QWORD *)(v2 + 72);
  v8 = *(_OWORD *)(v2 + 16);
  v9 = v3;
  v10 = *(_OWORD *)(v2 + 40);
  v11 = v4;
  v12 = v5;
  v13 = a1;
  v14 = *(_OWORD *)(v2 + 80);
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t, _QWORD))(v10 + 24))(a2, &v7, MEMORY[0x1E0DEE9C0] + 8, v8);
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(uint64_t a1)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v8;
  __int128 v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;

  v2 = *(_QWORD *)(v1 + 32);
  v3 = *(_QWORD *)(v1 + 56);
  v4 = *(_QWORD *)(v1 + 72);
  v5 = *(_QWORD *)(v1 + 88);
  v6 = *(_QWORD *)(v1 + 96);
  v9 = *(_OWORD *)(v1 + 16);
  v10 = v2;
  v11 = *(_OWORD *)(v1 + 40);
  v12 = v3;
  v13 = v4;
  v14 = *(_QWORD *)(v1 + 80);
  v15 = v5;
  v16 = a1;
  v17 = v6;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t), uint64_t *, uint64_t, _QWORD))(v11 + 24))(partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:), &v8, MEMORY[0x1E0DEE9C0] + 8, v9);
}

{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v7;
  __int128 v8;
  uint64_t v9;
  __int128 v10;
  uint64_t v11;
  uint64_t v12;
  __int128 v13;
  uint64_t v14;
  uint64_t v15;

  v2 = *(_QWORD *)(v1 + 32);
  v3 = *(_QWORD *)(v1 + 56);
  v4 = *(_QWORD *)(v1 + 72);
  v5 = *(_QWORD *)(v1 + 96);
  v8 = *(_OWORD *)(v1 + 16);
  v9 = v2;
  v10 = *(_OWORD *)(v1 + 40);
  v11 = v3;
  v12 = v4;
  v13 = *(_OWORD *)(v1 + 80);
  v14 = v5;
  v15 = a1;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t), uint64_t *, uint64_t, _QWORD))(v10 + 24))(partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:), &v7, MEMORY[0x1E0DEE9C0] + 8, v8);
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(lookupTable:withOffsets:scale:baseOffset:)(uint64_t a1, uint64_t *a2)
{
  uint64_t v2;

  return closure #1 in static vDSP.linearInterpolate<A, B>(lookupTable:withOffsets:scale:baseOffset:)(a1, a2, *(_QWORD *)(v2 + 48), *(_QWORD *)(v2 + 56), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), *(float *)(v2 + 64), *(float *)(v2 + 68));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.linearInterpolate<A, B>(lookupTable:withOffsets:scale:baseOffset:)(a1, a2, *(_QWORD *)(v2 + 48), *(_QWORD *)(v2 + 56), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), *(double *)(v2 + 64), *(double *)(v2 + 72));
}

void partial apply for closure #1 in static vDSP.fill<A>(_:with:)(float **a1)
{
  uint64_t v1;
  vDSP_Length v2;
  float __A;

  v2 = *(_QWORD *)(v1 + 24);
  __A = *(float *)(v1 + 16);
  if (*a1)
    vDSP_vfill(&__A, *a1, 1, v2);
  else
    __break(1u);
}

void partial apply for closure #1 in static vDSP.fill<A>(_:with:)(double **a1)
{
  uint64_t v1;
  vDSP_Length v2;
  double __A;

  v2 = *(_QWORD *)(v1 + 24);
  __A = *(double *)(v1 + 16);
  if (*a1)
    vDSP_vfillD(&__A, *a1, 1, v2);
  else
    __break(1u);
}

uint64_t partial apply for closure #1 in static vDSP.clear<A>(_:)(uint64_t *a1)
{
  return partial apply for closure #1 in static vDSP.reverse<A>(_:)(a1, MEMORY[0x1E0C8C400]);
}

{
  return partial apply for closure #1 in static vDSP.reverse<A>(_:)(a1, MEMORY[0x1E0C8C408]);
}

void partial apply for closure #2 in static vDSP.window<A>(ofType:usingSequence:count:isHalfWindow:)(uint64_t a1, _QWORD *a2)
{
  partial apply for closure #2 in static vDSP.window<A>(ofType:usingSequence:count:isHalfWindow:)(a1, a2);
}

{
  uint64_t v2;
  char *v4;
  uint64_t v5;

  if ((*(_QWORD *)(a1 + 8) & 0x8000000000000000) != 0)
  {
    __break(1u);
  }
  else
  {
    v4 = *(char **)(v2 + 16);
    v5 = *(_QWORD *)(v2 + 32);
    closure #1 in static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(a1, *v4);
    *a2 = v5;
  }
}

void partial apply for closure #1 in static vDSP.window<A>(ofType:usingSequence:count:isHalfWindow:)(uint64_t a1, _QWORD *a2)
{
  partial apply for closure #2 in static vDSP.window<A>(ofType:usingSequence:count:isHalfWindow:)(a1, a2);
}

void partial apply for closure #1 in static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(uint64_t a1)
{
  uint64_t v1;

  closure #1 in static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(a1, *(_BYTE *)(v1 + 16));
}

{
  uint64_t v1;

  closure #1 in static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(a1, *(_BYTE *)(v1 + 16));
}

uint64_t partial apply for closure #1 in static vDSP.ramp<A>(withInitialValue:multiplyingBy:increment:)(uint64_t a1, uint64_t *a2)
{
  uint64_t v2;

  return closure #1 in static vDSP.ramp<A>(withInitialValue:multiplyingBy:increment:)(a1, a2, *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(float *)(v2 + 48));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.ramp<A>(withInitialValue:multiplyingBy:increment:)(a1, a2, *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), *(_QWORD *)(v2 + 16), *(_QWORD *)(v2 + 24), *(double *)(v2 + 48));
}

uint64_t partial apply for closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(uint64_t a1)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  _DWORD v6[6];
  uint64_t v7;
  uint64_t v8;

  v2 = *(_QWORD *)(v1 + 16);
  v3 = *(_QWORD *)(v1 + 32);
  v4 = *(_QWORD *)(v1 + 64);
  v6[4] = *(_DWORD *)(v1 + 56);
  v7 = v4;
  v8 = a1;
  return (*(uint64_t (**)(void (*)(const float *, int), _DWORD *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:), v6, MEMORY[0x1E0DEE9C0] + 8, v2);
}

{
  _QWORD *v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  _QWORD v6[5];

  v2 = v1[2];
  v3 = v1[4];
  v4 = v1[8];
  v6[2] = v1[7];
  v6[3] = v4;
  v6[4] = a1;
  return (*(uint64_t (**)(void (*)(const double *, int), _QWORD *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:), v6, MEMORY[0x1E0DEE9C0] + 8, v2);
}

uint64_t partial apply for closure #1 in static vDSP.stereoRamp<A>(withInitialValue:multiplyingBy:_:increment:)(uint64_t a1, uint64_t *a2)
{
  uint64_t v2;
  uint64_t *v4;
  uint64_t result;
  uint64_t v6;

  v4 = *(uint64_t **)(v2 + 32);
  v6 = *(_QWORD *)(v2 + 40);
  *v4 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in closure #1 in static vDSP.stereoRamp<A>(withInitialValue:multiplyingBy:_:increment:));
  result = swift_bridgeObjectRelease();
  *a2 = v6;
  return result;
}

{
  uint64_t v2;
  uint64_t *v4;
  uint64_t result;
  uint64_t v6;

  v4 = *(uint64_t **)(v2 + 32);
  v6 = *(_QWORD *)(v2 + 40);
  *v4 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v6, (uint64_t (*)(_QWORD *, uint64_t *))partial apply for closure #1 in closure #1 in static vDSP.stereoRamp<A>(withInitialValue:multiplyingBy:_:increment:));
  result = swift_bridgeObjectRelease();
  *a2 = v6;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(uint64_t a1)
{
  uint64_t v1;
  uint64_t v2;
  int v3;
  uint64_t v4;
  _QWORD v6[3];
  __int128 v7;
  uint64_t v8;
  __int128 v9;
  int v10;
  uint64_t v11;
  uint64_t v12;

  v2 = *(_QWORD *)(v1 + 40);
  v3 = *(_DWORD *)(v1 + 72);
  v4 = *(_QWORD *)(v1 + 80);
  v6[2] = *(_QWORD *)(v1 + 16);
  v7 = *(_OWORD *)(v1 + 24);
  v8 = v2;
  v9 = *(_OWORD *)(v1 + 56);
  v10 = v3;
  v11 = v4;
  v12 = a1;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t), _QWORD *, uint64_t, _QWORD))(v2 + 16))(partial apply for closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:), v6, MEMORY[0x1E0DEE9C0] + 8, v7);
}

{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  _QWORD v6[3];
  __int128 v7;
  uint64_t v8;
  __int128 v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;

  v2 = *(_QWORD *)(v1 + 40);
  v3 = *(_QWORD *)(v1 + 72);
  v4 = *(_QWORD *)(v1 + 80);
  v6[2] = *(_QWORD *)(v1 + 16);
  v7 = *(_OWORD *)(v1 + 24);
  v8 = v2;
  v9 = *(_OWORD *)(v1 + 56);
  v10 = v3;
  v11 = v4;
  v12 = a1;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t), _QWORD *, uint64_t, _QWORD))(v2 + 16))(partial apply for closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:), v6, MEMORY[0x1E0DEE9C0] + 8, v7);
}

unint64_t lazy protocol witness table accessor for type vDSP.WindowSequence and conformance vDSP.WindowSequence()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vDSP.WindowSequence and conformance vDSP.WindowSequence;
  if (!lazy protocol witness table cache variable for type vDSP.WindowSequence and conformance vDSP.WindowSequence)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vDSP.WindowSequence, &type metadata for vDSP.WindowSequence);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.WindowSequence and conformance vDSP.WindowSequence);
  }
  return result;
}

uint64_t storeEnumTagSinglePayload for vDSP.WindowSequence(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 3 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 3) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFD)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFC)
    return ((uint64_t (*)(void))((char *)&loc_1CAB58178 + 4 * byte_1CAB63525[v4]))();
  *a1 = a2 + 3;
  return ((uint64_t (*)(void))((char *)sub_1CAB581AC + 4 * asc_1CAB63520[v4]))();
}

uint64_t sub_1CAB581AC(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB581B4(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB581BCLL);
  return result;
}

uint64_t sub_1CAB581C8(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB581D0);
  *(_BYTE *)result = a2 + 3;
  return result;
}

uint64_t sub_1CAB581D4(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB581DC(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vDSP.WindowSequence()
{
  return &type metadata for vDSP.WindowSequence;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(uint64_t a1)
{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  __int128 v4;
  uint64_t v6;
  __int128 v7;
  __int128 v8;
  uint64_t v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v12;

  v2 = *(_QWORD *)(v1 + 56);
  v3 = *(_QWORD *)(v1 + 64);
  v4 = *(_OWORD *)(v1 + 32);
  v7 = *(_OWORD *)(v1 + 16);
  v8 = v4;
  v9 = v2;
  v10 = v3;
  v11 = *(_OWORD *)(v1 + 72);
  v12 = a1;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t), uint64_t *, uint64_t, _QWORD))(v4 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:), &v6, MEMORY[0x1E0DEE9C0] + 8, v7);
}

{
  uint64_t v1;
  uint64_t v2;
  int v3;
  __int128 v4;
  uint64_t v6;
  __int128 v7;
  __int128 v8;
  uint64_t v9;
  int v10;
  __int128 v11;
  uint64_t v12;

  v2 = *(_QWORD *)(v1 + 56);
  v3 = *(_DWORD *)(v1 + 64);
  v4 = *(_OWORD *)(v1 + 32);
  v7 = *(_OWORD *)(v1 + 16);
  v8 = v4;
  v9 = v2;
  v10 = v3;
  v11 = *(_OWORD *)(v1 + 72);
  v12 = a1;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t), uint64_t *, uint64_t, _QWORD))(v4 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:), &v6, MEMORY[0x1E0DEE9C0] + 8, v7);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD v8[5];
  __int128 v9;
  uint64_t v10;
  uint64_t v11;

  v3 = *(_QWORD *)(v2 + 16);
  v4 = *(_QWORD *)(v2 + 32);
  v5 = *(_QWORD *)(v2 + 80);
  v6 = *(_QWORD *)(v2 + 88);
  v8[2] = *(_QWORD *)(v2 + 56);
  v8[3] = a1;
  v8[4] = a2;
  v9 = *(_OWORD *)(v2 + 64);
  v10 = v5;
  v11 = v6;
  return (*(uint64_t (**)(void (*)(const double *, int), _QWORD *, uint64_t, uint64_t))(v4 + 24))(partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:), v8, MEMORY[0x1E0DEE9C0] + 8, v3);
}

{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _DWORD v8[6];
  uint64_t v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v12;
  uint64_t v13;

  v3 = *(_QWORD *)(v2 + 16);
  v4 = *(_QWORD *)(v2 + 32);
  v5 = *(_QWORD *)(v2 + 80);
  v6 = *(_QWORD *)(v2 + 88);
  v8[4] = *(_DWORD *)(v2 + 56);
  v9 = a1;
  v10 = a2;
  v11 = *(_OWORD *)(v2 + 64);
  v12 = v5;
  v13 = v6;
  return (*(uint64_t (**)(void (*)(const float *, int), _DWORD *, uint64_t, uint64_t))(v4 + 24))(partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:), v8, MEMORY[0x1E0DEE9C0] + 8, v3);
}

void partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(const double *a1, int a2)
{
  uint64_t v2;

  closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(a1, a2, *(const double **)(v2 + 24), *(_QWORD *)(v2 + 32), *(double **)(v2 + 40), *(double ***)(v2 + 48), *(double ***)(v2 + 56), *(_QWORD *)(v2 + 64), *(double *)(v2 + 16));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.stereoRamp<A>(withInitialValue:multiplyingBy:_:increment:)(uint64_t a1, _QWORD *a2)
{
  double *v2;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  double v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t result;
  uint64_t v14;

  v4 = *((_QWORD *)v2 + 2);
  v14 = *((_QWORD *)v2 + 3);
  v5 = *((_QWORD *)v2 + 4);
  v6 = *((_QWORD *)v2 + 5);
  v7 = *((_QWORD *)v2 + 6);
  v8 = v2[7];
  v9 = *((_QWORD *)v2 + 8);
  v10 = *((_QWORD *)v2 + 9);
  v11 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  v12 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  result = static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(v5, v8, v6, v7, a1, v9, v4, v11, v14, v12);
  *a2 = v10;
  return result;
}

{
  uint64_t v2;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  float v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t result;
  uint64_t v14;

  v4 = *(_QWORD *)(v2 + 16);
  v14 = *(_QWORD *)(v2 + 24);
  v5 = *(_QWORD *)(v2 + 32);
  v6 = *(_QWORD *)(v2 + 40);
  v7 = *(_QWORD *)(v2 + 48);
  v8 = *(float *)(v2 + 56);
  v9 = *(_QWORD *)(v2 + 64);
  v10 = *(_QWORD *)(v2 + 72);
  v11 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  v12 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  result = static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(v5, v8, v6, v7, a1, v9, v4, v11, v14, v12);
  *a2 = v10;
  return result;
}

void partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(const float *a1, int a2)
{
  uint64_t v2;

  closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(a1, a2, *(const float **)(v2 + 24), *(_QWORD *)(v2 + 32), *(float **)(v2 + 40), *(float ***)(v2 + 48), *(float ***)(v2 + 56), *(_QWORD *)(v2 + 64), *(float *)(v2 + 16));
}

void partial apply for closure #1 in closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(const double *a1, int a2)
{
  uint64_t v2;

  closure #1 in closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(a1, a2, *(double **)(v2 + 24), *(double ***)(v2 + 32), *(_QWORD *)(v2 + 40), *(double *)(v2 + 16));
}

void partial apply for closure #1 in closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(const float *a1, int a2)
{
  uint64_t v2;

  closure #1 in closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(a1, a2, *(float **)(v2 + 24), *(float ***)(v2 + 32), *(_QWORD *)(v2 + 40), *(float *)(v2 + 16));
}

_QWORD *partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:)(_QWORD *a1)
{
  return partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:)(a1, MEMORY[0x1E0C8C7A8]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:)(a1, MEMORY[0x1E0C8C7A0]);
}

_QWORD *partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:)(_QWORD *result, uint64_t (*a2)(_QWORD, _QWORD, _QWORD, uint64_t, _QWORD))
{
  _QWORD *v2;

  if (*result)
    return (_QWORD *)a2(v2[2], v2[3], *result, 1, v2[4]);
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  __int128 v5;
  uint64_t v6;
  _OWORD v8[2];
  uint64_t v9;
  uint64_t v10;
  __int128 v11;
  uint64_t v12;

  v3 = *(_QWORD *)(v2 + 24);
  v4 = *(_QWORD *)(v2 + 48);
  v5 = *(_OWORD *)(v2 + 88);
  v6 = *(_QWORD *)(v2 + 104);
  v8[1] = *(_OWORD *)(v2 + 72);
  v9 = a1;
  v10 = a2;
  v11 = v5;
  v12 = v6;
  return (*(uint64_t (**)(void (*)(const double *, int), _OWORD *, uint64_t, uint64_t))(v4 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:), v8, MEMORY[0x1E0DEE9C0] + 8, v3);
}

{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  __int128 v5;
  _QWORD v7[5];
  __int128 v8;

  v3 = *(_QWORD *)(v2 + 24);
  v4 = *(_QWORD *)(v2 + 48);
  v5 = *(_OWORD *)(v2 + 80);
  v7[2] = *(_QWORD *)(v2 + 72);
  v7[3] = a1;
  v7[4] = a2;
  v8 = v5;
  return (*(uint64_t (**)(void (*)(const float *, int), _QWORD *, uint64_t, uint64_t))(v4 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:), v7, MEMORY[0x1E0DEE9C0] + 8, v3);
}

void partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(const double *a1, int a2)
{
  uint64_t v2;

  closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(a1, a2, *(const double **)(v2 + 32), *(_QWORD *)(v2 + 40), *(_QWORD *)(v2 + 48), *(double ***)(v2 + 56), *(_QWORD *)(v2 + 64), *(double *)(v2 + 16), *(double *)(v2 + 24));
}

void partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(const float *a1, int a2)
{
  uint64_t v2;

  closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(a1, a2, *(const float **)(v2 + 24), *(_QWORD *)(v2 + 32), *(_QWORD *)(v2 + 40), *(float ***)(v2 + 48), *(_QWORD *)(v2 + 56), *(float *)(v2 + 16), *(float *)(v2 + 20));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, a2, MEMORY[0x1E0C8C650]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, a2, MEMORY[0x1E0C8C648]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _QWORD v8[5];
  __int128 v9;

  v4 = *(_QWORD *)(v3 + 24);
  v5 = *(_QWORD *)(v3 + 48);
  v6 = *(_QWORD *)(v3 + 72);
  v8[2] = a1;
  v8[3] = a2;
  v8[4] = v6;
  v9 = *(_OWORD *)(v3 + 80);
  return (*(uint64_t (**)(uint64_t, _QWORD *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E0DEE9C0] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v3;

  return closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, a2, *(_QWORD *)(v3 + 16), *(_QWORD *)(v3 + 24), *(uint64_t **)(v3 + 32), *(_QWORD *)(v3 + 40), *(_QWORD *)(v3 + 48), a3);
}

uint64_t vImage.BufferType.init(bufferTypeCode:model:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X1>, char *a3@<X8>)
{
  char v3;
  BOOL v4;
  char v5;
  char v6;
  unint64_t v7;
  unint64_t v8;

  v3 = 0;
  switch(result)
  {
    case 1:
      v4 = (a2 & 0x1FFFFFFF8) == 0;
      v5 = 17;
      v6 = 8 * a2;
      v7 = 0x141111110E031110;
      goto LABEL_5;
    case 2:
      v4 = (a2 & 0x1FFFFFFF8) == 0;
      v5 = 18;
      v6 = 8 * a2;
      v7 = 0x151212120C041212;
      goto LABEL_5;
    case 3:
      v4 = (a2 & 0x1FFFFFFF8) == 0;
      v5 = 19;
      v6 = 8 * a2;
      v7 = 0x161313130D051313;
LABEL_5:
      v8 = v7 >> v6;
      if (v4)
        v5 = v8;
      *a3 = v5;
      return result;
    case 4:
      *a3 = 2;
      return result;
    case 17:
      goto LABEL_10;
    case 18:
      *a3 = 11;
      return result;
    case 19:
      *a3 = 6;
      return result;
    case 20:
      *a3 = 15;
      return result;
    case 21:
      *a3 = 9;
      return result;
    case 22:
      *a3 = 7;
      return result;
    case 23:
      *a3 = 8;
      return result;
    case 24:
      *a3 = 1;
      return result;
    case 25:
      *a3 = 10;
      return result;
    default:
      v3 = 23;
LABEL_10:
      *a3 = v3;
      return result;
  }
}

double static vImage_ARGBToYpCbCrMatrix.itu_R_601_4.getter@<D0>(_OWORD *a1@<X8>)
{
  double result;

  result = 0.0000849609638;
  *a1 = xmmword_1CAB63AE0;
  a1[1] = xmmword_1CAB63AF0;
  return result;
}

double static vImage_ARGBToYpCbCrMatrix.itu_R_709_2.getter@<D0>(_OWORD *a1@<X8>)
{
  double result;

  result = 0.000352343834;
  *a1 = xmmword_1CAB63B00;
  a1[1] = xmmword_1CAB63B10;
  return result;
}

vImageCVImageFormatRef static vImageCVImageFormatRef.make(format:matrix:chromaSiting:colorSpace:alphaIsOpaqueHint:)(char *a1, _OWORD *a2, char *a3, CGColorSpace *a4, char a5)
{
  uint64_t v5;
  uint64_t v6;
  __int128 v7;
  vImage_ARGBToYpCbCrMatrix v9;
  uint64_t v10;

  v10 = *MEMORY[0x1E0C80C00];
  v5 = *a1;
  v6 = *a3;
  v7 = a2[1];
  *(_OWORD *)&v9.R_Yp = *a2;
  *(_OWORD *)&v9.G_Cb = v7;
  return vImageCVImageFormat_Create(dword_1CAB6397C[v5], &v9, **((CFStringRef **)&unk_1E84F4530 + v6), a4, a5 & 1);
}

vImageCVImageFormatRef static vImageCVImageFormatRef.make(buffer:)(__CVBuffer *a1)
{
  vImageCVImageFormatRef result;

  result = vImageCVImageFormat_CreateWithCVPixelBuffer(a1);
  if (!result)
    __break(1u);
  return result;
}

BOOL vImageCVImageFormatRef.alphaIsOpaqueHint.getter()
{
  const vImageCVImageFormat *v0;

  return vImageCVImageFormat_GetAlphaHint(v0) != 0;
}

vImage_Error key path setter for vImageCVImageFormatRef.alphaIsOpaqueHint : vImageCVImageFormatRef(unsigned __int8 *a1, vImageCVImageFormat **a2)
{
  int v2;
  vImageCVImageFormat *v3;
  int v4;
  vImage_Error result;

  v2 = *a1;
  v3 = *a2;
  if (v2)
    v4 = 2;
  else
    v4 = 0;
  result = vImageCVImageFormat_SetAlphaHint(v3, v4);
  if (result)
  {
    result = _assertionFailure(_:_:file:line:flags:)();
    __break(1u);
  }
  return result;
}

vImage_Error vImageCVImageFormatRef.alphaIsOpaqueHint.setter(char a1)
{
  vImageCVImageFormat *v1;
  int v2;
  vImage_Error result;

  if ((a1 & 1) != 0)
    v2 = 2;
  else
    v2 = 0;
  result = vImageCVImageFormat_SetAlphaHint(v1, v2);
  if (result)
  {
    result = _assertionFailure(_:_:file:line:flags:)();
    __break(1u);
  }
  return result;
}

vImage_Error (*vImageCVImageFormatRef.alphaIsOpaqueHint.modify(uint64_t a1))(uint64_t a1)
{
  const vImageCVImageFormat *v1;

  *(_QWORD *)a1 = v1;
  *(_BYTE *)(a1 + 8) = vImageCVImageFormat_GetAlphaHint(v1) != 0;
  return vImageCVImageFormatRef.alphaIsOpaqueHint.modify;
}

vImage_Error vImageCVImageFormatRef.alphaIsOpaqueHint.modify(uint64_t a1)
{
  int v1;
  vImage_Error result;

  if (*(_BYTE *)(a1 + 8))
    v1 = 2;
  else
    v1 = 0;
  result = vImageCVImageFormat_SetAlphaHint(*(vImageCVImageFormatRef *)a1, v1);
  if (result)
  {
    result = _assertionFailure(_:_:file:line:flags:)();
    __break(1u);
  }
  return result;
}

uint64_t vImageCVImageFormatRef.channelCount.getter()
{
  const vImageCVImageFormat *v0;

  return vImageCVImageFormat_GetChannelCount(v0);
}

char *vImageCVImageFormatRef.channels.getter()
{
  const vImageCVImageFormat *v0;
  const vImageBufferTypeCode *ChannelNames;
  uint32_t ChannelCount;
  _QWORD *v3;
  const vImageCVImageFormat *v4;
  char *v5;

  ChannelNames = vImageCVImageFormat_GetChannelNames(v0);
  ChannelCount = vImageCVImageFormat_GetChannelCount(v0);
  v3 = specialized _copyCollectionToContiguousArray<A>(_:)(ChannelNames, ChannelCount);
  v4 = v0;
  v5 = specialized Sequence.compactMap<A>(_:)((uint64_t)v3, v4);
  swift_release();

  return v5;
}

CGColorSpaceRef vImageCVImageFormatRef.colorSpace.getter()
{
  const vImageCVImageFormat *v0;

  return vImageCVImageFormat_GetColorSpace(v0);
}

const vImageChannelDescription *vImageCVImageFormatRef.channelDescription(bufferType:)@<X0>(uint64_t a1@<X8>)
{
  const vImageCVImageFormat *v1;
  vImageBufferTypeCode v3;
  const vImageChannelDescription *result;
  __int128 v5;
  __int128 v6;

  v3 = vImage.BufferType.bufferTypeCode.getter();
  result = vImageCVImageFormat_GetChannelDescription(v1, v3);
  if (result)
  {
    v5 = *(_OWORD *)&result->min;
    v6 = *(_OWORD *)&result->full;
  }
  else
  {
    v5 = 0uLL;
    v6 = 0uLL;
  }
  *(_OWORD *)a1 = v5;
  *(_OWORD *)(a1 + 16) = v6;
  *(_BYTE *)(a1 + 32) = result == 0;
  return result;
}

uint64_t vImage.BufferType.bufferTypeCode.getter()
{
  char *v0;

  return dword_1CAB63A7C[*v0];
}

void vImageCVImageFormatRef.chromaSiting.getter(char *a1@<X8>)
{
  const vImageCVImageFormat *v1;
  __CFString *ChromaSiting;

  ChromaSiting = (__CFString *)vImageCVImageFormat_GetChromaSiting(v1);
  vImageCVImageFormatRef.ChromaSiting.init(location:)(ChromaSiting, a1);
}

void vImageCVImageFormatRef.chromaSiting.setter(uint64_t a1)
{
  __asm { BR              X11 }
}

void sub_1CAB58F74()
{
  vImageCVImageFormat *v0;
  __CFString *v1;
  vImage_Error v2;

  v1 = (__CFString *)(id)*MEMORY[0x1E0CA8D00];
  v2 = vImageCVImageFormat_SetChromaSiting(v0, v1);

  if (v2)
  {
    _assertionFailure(_:_:file:line:flags:)();
    __break(1u);
  }
}

void vImageCVImageFormatRef.ChromaSiting.init(location:)(void *a1@<X0>, char *a2@<X8>)
{
  void *v4;
  id v5;
  id v6;
  char v7;
  char v8;
  void *v9;
  id v10;
  id v11;
  char v12;
  void *v13;
  id v14;
  id v15;
  char v16;
  void *v17;
  id v18;
  id v19;
  char v20;
  void *v21;
  id v22;
  id v23;
  char v24;
  void *v25;
  id v26;
  id v27;
  char v28;
  id v29;
  id v30;
  id v31;
  char v32;

  if (!a1)
    goto LABEL_16;
  v4 = (void *)*MEMORY[0x1E0CA8D10];
  type metadata accessor for CFStringRef(0);
  lazy protocol witness table accessor for type CFStringRef and conformance CFStringRef();
  v5 = v4;
  v6 = a1;
  v7 = static _CFObject.== infix(_:_:)();

  if ((v7 & 1) == 0)
  {
    v9 = (void *)*MEMORY[0x1E0CA8D00];
    v10 = v6;
    v11 = v9;
    v12 = static _CFObject.== infix(_:_:)();

    if ((v12 & 1) != 0)
    {

      v8 = 1;
      goto LABEL_17;
    }
    v13 = (void *)*MEMORY[0x1E0CA8D20];
    v14 = v10;
    v15 = v13;
    v16 = static _CFObject.== infix(_:_:)();

    if ((v16 & 1) != 0)
    {

      v8 = 2;
      goto LABEL_17;
    }
    v17 = (void *)*MEMORY[0x1E0CA8D18];
    v18 = v14;
    v19 = v17;
    v20 = static _CFObject.== infix(_:_:)();

    if ((v20 & 1) != 0)
    {

      v8 = 3;
      goto LABEL_17;
    }
    v21 = (void *)*MEMORY[0x1E0CA8CF8];
    v22 = v18;
    v23 = v21;
    v24 = static _CFObject.== infix(_:_:)();

    if ((v24 & 1) != 0)
    {

      v8 = 4;
      goto LABEL_17;
    }
    v25 = (void *)*MEMORY[0x1E0CA8CF0];
    v26 = v22;
    v27 = v25;
    v28 = static _CFObject.== infix(_:_:)();

    if ((v28 & 1) != 0)
    {

      v8 = 5;
      goto LABEL_17;
    }
    v29 = (id)*MEMORY[0x1E0CA8D08];
    v30 = v26;
    v31 = v29;
    v32 = static _CFObject.== infix(_:_:)();

    if ((v32 & 1) != 0)
    {
      v8 = 6;
      goto LABEL_17;
    }
LABEL_16:
    v8 = 7;
    goto LABEL_17;
  }

  v8 = 0;
LABEL_17:
  *a2 = v8;
}

void (*vImageCVImageFormatRef.chromaSiting.modify(const vImageCVImageFormat **a1))(uint64_t a1, char a2)
{
  const vImageCVImageFormat *v1;
  char *v2;
  __CFString *ChromaSiting;

  *a1 = v1;
  v2 = (char *)(a1 + 1);
  ChromaSiting = (__CFString *)vImageCVImageFormat_GetChromaSiting(v1);
  vImageCVImageFormatRef.ChromaSiting.init(location:)(ChromaSiting, v2);
  return vImageCVImageFormatRef.chromaSiting.modify;
}

void vImageCVImageFormatRef.chromaSiting.modify(uint64_t a1, char a2)
{
  char *v2;
  char v3;

  v2 = (char *)(a1 + 8);
  if ((a2 & 1) != 0)
  {
    v3 = *v2;
    v2 = &v3;
  }
  vImageCVImageFormatRef.chromaSiting.setter((uint64_t)v2);
}

vImage_Error key path setter for vImageCVImageFormatRef.colorSpace : vImageCVImageFormatRef(CGColorSpaceRef *a1, vImageCVImageFormatRef *a2)
{
  vImage_Error result;

  result = vImageCVImageFormat_SetColorSpace(*a2, *a1);
  if (result)
  {
    result = _assertionFailure(_:_:file:line:flags:)();
    __break(1u);
  }
  return result;
}

void vImageCVImageFormatRef.colorSpace.setter(CGColorSpaceRef colorspace)
{
  vImageCVImageFormat *v1;

  if (vImageCVImageFormat_SetColorSpace(v1, colorspace))
  {
    _assertionFailure(_:_:file:line:flags:)();
    __break(1u);
  }
  else
  {

  }
}

void (*vImageCVImageFormatRef.colorSpace.modify(CGColorSpaceRef *a1))(CGColorSpaceRef *a1)
{
  const vImageCVImageFormat *v1;

  a1[1] = v1;
  *a1 = vImageCVImageFormat_GetColorSpace(v1);
  return vImageCVImageFormatRef.colorSpace.modify;
}

void vImageCVImageFormatRef.colorSpace.modify(CGColorSpaceRef *a1)
{
  CGColorSpaceRef v1;

  v1 = *a1;
  if (vImageCVImageFormat_SetColorSpace(a1[1], *a1))
  {
    _assertionFailure(_:_:file:line:flags:)();
    __break(1u);
  }
  else
  {

  }
}

uint64_t vImageCVImageFormatRef.formatCode.getter()
{
  const vImageCVImageFormat *v0;

  return vImageCVImageFormat_GetFormatCode(v0);
}

void vImage.BufferType.init(rawValue:)()
{
  _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
}

uint64_t vImage.BufferType.rawValue.getter()
{
  unsigned __int8 *v0;

  return *v0;
}

void protocol witness for RawRepresentable.init(rawValue:) in conformance vImage.BufferType()
{
  _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
}

void protocol witness for RawRepresentable.rawValue.getter in conformance vImage.BufferType(_QWORD *a1@<X8>)
{
  unsigned __int8 *v1;

  *a1 = *v1;
}

BOOL static vImageCVImageFormatRef.Format.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void vImageCVImageFormatRef.Format.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

char *specialized Sequence.compactMap<A>(_:)(uint64_t a1, const vImageCVImageFormat *a2)
{
  uint64_t v2;
  uint64_t v5;
  char *v6;
  uint64_t v7;
  CGColorSpace *ColorSpace;
  CGColorSpace *v9;
  CGColorSpaceModel Model;
  char v11;
  unint64_t v12;
  unint64_t v13;
  char v15;

  v2 = *(_QWORD *)(a1 + 16);
  if (!v2)
    return (char *)MEMORY[0x1E0DEE9D8];
  swift_bridgeObjectRetain();
  v5 = 0;
  v6 = (char *)MEMORY[0x1E0DEE9D8];
  do
  {
    v7 = *(unsigned int *)(a1 + 4 * v5 + 32);
    ColorSpace = vImageCVImageFormat_GetColorSpace(a2);
    v9 = ColorSpace;
    if (ColorSpace)
    {
      Model = CGColorSpaceGetModel(ColorSpace);

    }
    else
    {
      Model = kCGColorSpaceModelMonochrome;
    }
    vImage.BufferType.init(bufferTypeCode:model:)(v7, Model | ((unint64_t)(v9 == 0) << 32), &v15);
    v11 = v15;
    if (v15 != 23)
    {
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
        v6 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v6 + 2) + 1, 1, v6);
      v13 = *((_QWORD *)v6 + 2);
      v12 = *((_QWORD *)v6 + 3);
      if (v13 >= v12 >> 1)
        v6 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v12 > 1), v13 + 1, 1, v6);
      *((_QWORD *)v6 + 2) = v13 + 1;
      v6[v13 + 32] = v11;
    }
    ++v5;
  }
  while (v2 != v5);
  swift_bridgeObjectRelease();
  return v6;
}

unint64_t lazy protocol witness table accessor for type vImage.BufferType and conformance vImage.BufferType()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImage.BufferType and conformance vImage.BufferType;
  if (!lazy protocol witness table cache variable for type vImage.BufferType and conformance vImage.BufferType)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImage.BufferType, &type metadata for vImage.BufferType);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.BufferType and conformance vImage.BufferType);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type vImageCVImageFormatRef.Format and conformance vImageCVImageFormatRef.Format()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImageCVImageFormatRef.Format and conformance vImageCVImageFormatRef.Format;
  if (!lazy protocol witness table cache variable for type vImageCVImageFormatRef.Format and conformance vImageCVImageFormatRef.Format)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImageCVImageFormatRef.Format, &type metadata for vImageCVImageFormatRef.Format);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImageCVImageFormatRef.Format and conformance vImageCVImageFormatRef.Format);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type vImageCVImageFormatRef.ChromaSiting and conformance vImageCVImageFormatRef.ChromaSiting()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type vImageCVImageFormatRef.ChromaSiting and conformance vImageCVImageFormatRef.ChromaSiting;
  if (!lazy protocol witness table cache variable for type vImageCVImageFormatRef.ChromaSiting and conformance vImageCVImageFormatRef.ChromaSiting)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for vImageCVImageFormatRef.ChromaSiting, &type metadata for vImageCVImageFormatRef.ChromaSiting);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImageCVImageFormatRef.ChromaSiting and conformance vImageCVImageFormatRef.ChromaSiting);
  }
  return result;
}

uint64_t sub_1CAB597F0@<X0>(vImageConstCVImageFormatRef *a1@<X0>, BOOL *a2@<X8>)
{
  uint64_t result;

  result = vImageCVImageFormat_GetAlphaHint(*a1);
  *a2 = (_DWORD)result != 0;
  return result;
}

void sub_1CAB59824(vImageConstCVImageFormatRef *a1@<X0>, _BYTE *a2@<X8>)
{
  __CFString *ChromaSiting;
  char v4;

  ChromaSiting = (__CFString *)vImageCVImageFormat_GetChromaSiting(*a1);
  vImageCVImageFormatRef.ChromaSiting.init(location:)(ChromaSiting, &v4);
  *a2 = v4;
}

void sub_1CAB59860(char *a1)
{
  char v1;

  v1 = *a1;
  vImageCVImageFormatRef.chromaSiting.setter((uint64_t)&v1);
}

CGColorSpaceRef sub_1CAB59894@<X0>(vImageConstCVImageFormatRef *a1@<X0>, CGColorSpaceRef *a2@<X8>)
{
  CGColorSpaceRef result;

  result = vImageCVImageFormat_GetColorSpace(*a1);
  *a2 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for vImage.BufferType(unsigned __int8 *a1, unsigned int a2)
{
  int v2;
  int v3;
  int v4;
  unsigned int v6;
  BOOL v7;
  int v8;

  if (!a2)
    return 0;
  if (a2 < 0xEA)
    goto LABEL_17;
  if (a2 + 22 >= 0xFFFF00)
    v2 = 4;
  else
    v2 = 2;
  if ((a2 + 22) >> 8 < 0xFF)
    v3 = 1;
  else
    v3 = v2;
  if (v3 == 4)
  {
    v4 = *(_DWORD *)(a1 + 1);
    if (v4)
      return (*a1 | (v4 << 8)) - 22;
  }
  else
  {
    if (v3 == 2)
    {
      v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1))
        goto LABEL_17;
      return (*a1 | (v4 << 8)) - 22;
    }
    v4 = a1[1];
    if (a1[1])
      return (*a1 | (v4 << 8)) - 22;
  }
LABEL_17:
  v6 = *a1;
  v7 = v6 >= 0x17;
  v8 = v6 - 23;
  if (!v7)
    v8 = -1;
  return (v8 + 1);
}

uint64_t storeEnumTagSinglePayload for vImage.BufferType(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 22 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 22) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xEA)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xE9)
    return ((uint64_t (*)(void))((char *)&loc_1CAB5999C + 4 * byte_1CAB63646[v4]))();
  *a1 = a2 + 22;
  return ((uint64_t (*)(void))((char *)sub_1CAB599D0 + 4 * byte_1CAB63641[v4]))();
}

uint64_t sub_1CAB599D0(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB599D8(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB599E0);
  return result;
}

uint64_t sub_1CAB599EC(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB599F4);
  *(_BYTE *)result = a2 + 22;
  return result;
}

uint64_t sub_1CAB599F8(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB59A00(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vImage.BufferType()
{
  return &type metadata for vImage.BufferType;
}

uint64_t getEnumTagSinglePayload for vImageCVImageFormatRef.Format(unsigned __int8 *a1, unsigned int a2)
{
  int v2;
  int v3;
  int v4;
  unsigned int v6;
  BOOL v7;
  int v8;

  if (!a2)
    return 0;
  if (a2 < 0xC1)
    goto LABEL_17;
  if (a2 + 63 >= 0xFFFF00)
    v2 = 4;
  else
    v2 = 2;
  if ((a2 + 63) >> 8 < 0xFF)
    v3 = 1;
  else
    v3 = v2;
  if (v3 == 4)
  {
    v4 = *(_DWORD *)(a1 + 1);
    if (v4)
      return (*a1 | (v4 << 8)) - 63;
  }
  else
  {
    if (v3 == 2)
    {
      v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1))
        goto LABEL_17;
      return (*a1 | (v4 << 8)) - 63;
    }
    v4 = a1[1];
    if (a1[1])
      return (*a1 | (v4 << 8)) - 63;
  }
LABEL_17:
  v6 = *a1;
  v7 = v6 >= 0x40;
  v8 = v6 - 64;
  if (!v7)
    v8 = -1;
  return (v8 + 1);
}

uint64_t storeEnumTagSinglePayload for vImageCVImageFormatRef.Format(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 63 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 63) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xC1)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xC0)
    return ((uint64_t (*)(void))((char *)&loc_1CAB59AF8 + 4 * byte_1CAB63650[v4]))();
  *a1 = a2 + 63;
  return ((uint64_t (*)(void))((char *)sub_1CAB59B2C + 4 * byte_1CAB6364B[v4]))();
}

uint64_t sub_1CAB59B2C(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB59B34(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB59B3CLL);
  return result;
}

uint64_t sub_1CAB59B48(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB59B50);
  *(_BYTE *)result = a2 + 63;
  return result;
}

uint64_t sub_1CAB59B54(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB59B5C(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vImageCVImageFormatRef.Format()
{
  return &type metadata for vImageCVImageFormatRef.Format;
}

uint64_t getEnumTagSinglePayload for vImageCVImageFormatRef.ChromaSiting(unsigned __int8 *a1, unsigned int a2)
{
  int v2;
  int v3;
  int v4;
  unsigned int v6;
  BOOL v7;
  int v8;

  if (!a2)
    return 0;
  if (a2 < 0xFA)
    goto LABEL_17;
  if (a2 + 6 >= 0xFFFF00)
    v2 = 4;
  else
    v2 = 2;
  if ((a2 + 6) >> 8 < 0xFF)
    v3 = 1;
  else
    v3 = v2;
  if (v3 == 4)
  {
    v4 = *(_DWORD *)(a1 + 1);
    if (v4)
      return (*a1 | (v4 << 8)) - 6;
  }
  else
  {
    if (v3 == 2)
    {
      v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1))
        goto LABEL_17;
      return (*a1 | (v4 << 8)) - 6;
    }
    v4 = a1[1];
    if (a1[1])
      return (*a1 | (v4 << 8)) - 6;
  }
LABEL_17:
  v6 = *a1;
  v7 = v6 >= 7;
  v8 = v6 - 7;
  if (!v7)
    v8 = -1;
  return (v8 + 1);
}

uint64_t storeEnumTagSinglePayload for vImageCVImageFormatRef.ChromaSiting(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 6 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 6) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFA)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xF9)
    return ((uint64_t (*)(void))((char *)&loc_1CAB59C54 + 4 * byte_1CAB6365A[v4]))();
  *a1 = a2 + 6;
  return ((uint64_t (*)(void))((char *)sub_1CAB59C88 + 4 * byte_1CAB63655[v4]))();
}

uint64_t sub_1CAB59C88(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB59C90(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB59C98);
  return result;
}

uint64_t sub_1CAB59CA4(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB59CACLL);
  *(_BYTE *)result = a2 + 6;
  return result;
}

uint64_t sub_1CAB59CB0(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB59CB8(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for vImageCVImageFormatRef.ChromaSiting()
{
  return &type metadata for vImageCVImageFormatRef.ChromaSiting;
}

unint64_t lazy protocol witness table accessor for type CFStringRef and conformance CFStringRef()
{
  unint64_t result;
  uint64_t v1;

  result = lazy protocol witness table cache variable for type CFStringRef and conformance CFStringRef;
  if (!lazy protocol witness table cache variable for type CFStringRef and conformance CFStringRef)
  {
    type metadata accessor for CFStringRef(255);
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for CFStringRef, v1);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type CFStringRef and conformance CFStringRef);
  }
  return result;
}

uint64_t BNNS.LossLayer.__allocating_init(input:output:lossFunction:lossReduction:filterParameters:)(_OWORD *a1, __int128 *a2, __int128 *a3, unsigned __int8 *a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  unsigned __int8 v16;
  int *v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  int v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  _OWORD v31[5];
  uint64_t v32;

  v32 = *MEMORY[0x1E0C80C00];
  outlined init with take of BNNS.LossFunction(a3, v31);
  v25 = a3[1];
  v26 = *a3;
  v23 = a3[3];
  v24 = a3[2];
  v22 = a3[4];
  v16 = *a4;
  if (a7 == 1)
  {
    v17 = 0;
  }
  else
  {
    v27 = a5;
    v28 = a6;
    v29 = a7;
    v30 = a8;
    v17 = &v27;
  }
  v18 = closure #1 in BNNS.LossLayer.init(input:output:lossFunction:lossReduction:filterParameters:)((uint64_t)v17, v31, a1, a2, v16);
  type metadata accessor for BNNS.LossLayer();
  v19 = swift_allocObject();
  v20 = v19;
  *(_QWORD *)(v19 + 24) = 0xFFFFFFFF00000000;
  *(_OWORD *)(v19 + 32) = 0u;
  *(_OWORD *)(v19 + 48) = 0u;
  *(_OWORD *)(v19 + 64) = 0u;
  *(_OWORD *)(v19 + 80) = 0u;
  *(_QWORD *)(v19 + 96) = 0;
  if (v18)
  {
    *(_QWORD *)(v19 + 16) = v18;
    *(_OWORD *)(v19 + 24) = v26;
    *(_OWORD *)(v19 + 40) = v25;
    *(_OWORD *)(v19 + 56) = v24;
    *(_OWORD *)(v19 + 72) = v23;
    *(_OWORD *)(v19 + 88) = v22;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v20;
}

uint64_t closure #1 in BNNS.LossLayer.init(input:output:lossFunction:lossReduction:filterParameters:)(uint64_t a1, _OWORD *a2, _OWORD *a3, __int128 *a4, unsigned __int8 a5)
{
  uint64_t v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  __int128 v15;
  __int128 v16;
  __int128 v17;
  __int128 v18;
  __int128 v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  int v24;
  int v25;
  int v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  __int128 v52;
  __int128 v53;
  __int128 v54;
  __int128 v55;
  __int128 v56;
  __int128 v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  __int128 v61;
  __int128 v62;
  uint64_t result;
  int v64;
  __int128 v65;
  __int128 v66;
  __int128 v67;
  __int128 v68;
  __int128 v69;
  __int128 v70;
  __int128 v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  int v76;
  __int128 v77;
  __int128 v78;
  __int128 v79;
  __int128 v80;
  __int128 v81;
  __int128 v82;
  __int128 v83;
  __int128 v84;
  __int128 v85;
  __int128 v86;
  __int128 v87;
  int v88;
  int v89;
  __int128 v90;
  __int128 v91;
  char v92;
  int v93;
  __int128 v94;
  uint64_t v95;
  uint64_t v96;
  _BYTE v97[180];
  __int128 v98;
  int v100;
  __int128 v101;
  __int128 v102;
  uint64_t v103;
  __int128 v104;
  uint64_t v105;
  uint64_t v106;
  uint64_t v107;

  v107 = *MEMORY[0x1E0C80C00];
  outlined init with take of BNNS.LossFunction(a2, &v100);
  v10 = v103;
  switch(v103 >> 29)
  {
    case 0u:
      v11 = a3[6];
      *(_OWORD *)&v97[116] = a3[7];
      v12 = a3[9];
      *(_OWORD *)&v97[132] = a3[8];
      *(_OWORD *)&v97[148] = v12;
      *(_OWORD *)&v97[164] = a3[10];
      v13 = a3[2];
      *(_OWORD *)&v97[52] = a3[3];
      v14 = a3[5];
      *(_OWORD *)&v97[68] = a3[4];
      *(_OWORD *)&v97[84] = v14;
      *(_OWORD *)&v97[100] = v11;
      v15 = a3[1];
      *(_OWORD *)&v97[4] = *a3;
      *(_OWORD *)&v97[20] = v15;
      *(_OWORD *)&v97[36] = v13;
      v16 = a4[8];
      v17 = a4[9];
      v18 = a4[6];
      v84 = a4[7];
      v85 = v16;
      v19 = a4[10];
      v86 = v17;
      v87 = v19;
      v20 = a4[4];
      v82 = a4[5];
      v83 = v18;
      v21 = a4[2];
      v80 = a4[3];
      v81 = v20;
      v22 = a4[1];
      v77 = *a4;
      v103 &= 0xFFFFFFFF1FFFFFFFLL;
      v78 = v22;
      v79 = v21;
      v73 = *(_OWORD *)&v97[128];
      v74 = *(_OWORD *)&v97[144];
      v75 = *(_OWORD *)&v97[160];
      v69 = *(_OWORD *)&v97[64];
      v70 = *(_OWORD *)&v97[80];
      v71 = *(_OWORD *)&v97[96];
      v72 = *(_OWORD *)&v97[112];
      v65 = *(_OWORD *)v97;
      v66 = *(_OWORD *)&v97[16];
      v23 = *(_OWORD *)&v97[48];
      v67 = *(_OWORD *)&v97[32];
      v24 = v100;
      v25 = a5;
      v26 = 1;
      goto LABEL_6;
    case 1u:
      v27 = a3[6];
      *(_OWORD *)&v97[116] = a3[7];
      v28 = a3[9];
      *(_OWORD *)&v97[132] = a3[8];
      *(_OWORD *)&v97[148] = v28;
      *(_OWORD *)&v97[164] = a3[10];
      v29 = a3[2];
      *(_OWORD *)&v97[52] = a3[3];
      v30 = a3[5];
      *(_OWORD *)&v97[68] = a3[4];
      *(_OWORD *)&v97[84] = v30;
      *(_OWORD *)&v97[100] = v27;
      v31 = a3[1];
      *(_OWORD *)&v97[4] = *a3;
      *(_OWORD *)&v97[20] = v31;
      *(_OWORD *)&v97[36] = v29;
      v32 = a4[8];
      v33 = a4[9];
      v34 = a4[6];
      v84 = a4[7];
      v85 = v32;
      v35 = a4[10];
      v86 = v33;
      v87 = v35;
      v36 = a4[4];
      v82 = a4[5];
      v83 = v34;
      v37 = a4[2];
      v80 = a4[3];
      v81 = v36;
      v38 = a4[1];
      v77 = *a4;
      v103 &= 0xFFFFFFFF1FFFFFFFLL;
      v78 = v38;
      v79 = v37;
      v73 = *(_OWORD *)&v97[128];
      v74 = *(_OWORD *)&v97[144];
      v75 = *(_OWORD *)&v97[160];
      v69 = *(_OWORD *)&v97[64];
      v70 = *(_OWORD *)&v97[80];
      v71 = *(_OWORD *)&v97[96];
      v72 = *(_OWORD *)&v97[112];
      v65 = *(_OWORD *)v97;
      v66 = *(_OWORD *)&v97[16];
      v23 = *(_OWORD *)&v97[48];
      v67 = *(_OWORD *)&v97[32];
      v24 = v100;
      v25 = a5;
      v26 = 2;
      goto LABEL_6;
    case 2u:
      v39 = a3[6];
      *(_OWORD *)&v97[116] = a3[7];
      v40 = a3[9];
      *(_OWORD *)&v97[132] = a3[8];
      *(_OWORD *)&v97[148] = v40;
      *(_OWORD *)&v97[164] = a3[10];
      v41 = a3[2];
      *(_OWORD *)&v97[52] = a3[3];
      v42 = a3[5];
      *(_OWORD *)&v97[68] = a3[4];
      *(_OWORD *)&v97[84] = v42;
      *(_OWORD *)&v97[100] = v39;
      v43 = a3[1];
      *(_OWORD *)&v97[4] = *a3;
      *(_OWORD *)&v97[20] = v43;
      *(_OWORD *)&v97[36] = v41;
      v44 = a4[8];
      v45 = a4[9];
      v46 = a4[6];
      v84 = a4[7];
      v85 = v44;
      v47 = a4[10];
      v86 = v45;
      v87 = v47;
      v48 = a4[4];
      v82 = a4[5];
      v83 = v46;
      v49 = a4[2];
      v80 = a4[3];
      v81 = v48;
      v50 = a4[1];
      v77 = *a4;
      v103 &= 0xFFFFFFFF1FFFFFFFLL;
      v78 = v50;
      v79 = v49;
      v73 = *(_OWORD *)&v97[128];
      v74 = *(_OWORD *)&v97[144];
      v75 = *(_OWORD *)&v97[160];
      v69 = *(_OWORD *)&v97[64];
      v70 = *(_OWORD *)&v97[80];
      v71 = *(_OWORD *)&v97[96];
      v72 = *(_OWORD *)&v97[112];
      v65 = *(_OWORD *)v97;
      v66 = *(_OWORD *)&v97[16];
      v23 = *(_OWORD *)&v97[48];
      v67 = *(_OWORD *)&v97[32];
      v24 = v100;
      v25 = a5;
      v26 = 4;
LABEL_6:
      v64 = v26;
      v68 = v23;
      v76 = *(_DWORD *)&v97[176];
      v88 = v25;
      v89 = v24;
      break;
    case 3u:
      v51 = a3[6];
      *(_OWORD *)&v97[116] = a3[7];
      v52 = a3[9];
      *(_OWORD *)&v97[132] = a3[8];
      *(_OWORD *)&v97[148] = v52;
      *(_OWORD *)&v97[164] = a3[10];
      v53 = a3[2];
      *(_OWORD *)&v97[52] = a3[3];
      v54 = a3[5];
      *(_OWORD *)&v97[68] = a3[4];
      *(_OWORD *)&v97[84] = v54;
      *(_OWORD *)&v97[100] = v51;
      v55 = a3[1];
      *(_OWORD *)&v97[4] = *a3;
      *(_OWORD *)&v97[20] = v55;
      *(_OWORD *)&v97[36] = v53;
      v56 = a4[8];
      v57 = a4[9];
      v58 = a4[6];
      v84 = a4[7];
      v85 = v56;
      v59 = a4[10];
      v86 = v57;
      v87 = v59;
      v60 = a4[4];
      v82 = a4[5];
      v83 = v58;
      v61 = a4[2];
      v80 = a4[3];
      v81 = v60;
      v62 = a4[1];
      v77 = *a4;
      v103 &= 0xFFFFFFFF1FFFFFFFLL;
      v78 = v62;
      v79 = v61;
      v73 = *(_OWORD *)&v97[128];
      v74 = *(_OWORD *)&v97[144];
      v75 = *(_OWORD *)&v97[160];
      v69 = *(_OWORD *)&v97[64];
      v70 = *(_OWORD *)&v97[80];
      v71 = *(_OWORD *)&v97[96];
      v72 = *(_OWORD *)&v97[112];
      v65 = *(_OWORD *)v97;
      v66 = *(_OWORD *)&v97[16];
      v67 = *(_OWORD *)&v97[32];
      v64 = 5;
      v68 = *(_OWORD *)&v97[48];
      v76 = *(_DWORD *)&v97[176];
      v88 = a5;
      v89 = v100;
      v90 = v101;
      v91 = v102;
      v92 = v10 & 1;
      v93 = HIDWORD(v10);
      v94 = v104;
      v95 = v105;
      v96 = v106;
      break;
    default:
      outlined init with take of BNNS.LossFunction(a2, &v98);
      __asm { BR              X14 }
      return result;
  }
  return MEMORY[0x1D17945E8](&v64, a1);
}

uint64_t BNNS.LossFunction.bnnsLossFunction.getter()
{
  uint64_t v0;

  return ((uint64_t (*)(uint64_t))((char *)sub_1CAB5A584
                                          + 4 * byte_1CAB63B29[*(_QWORD *)(v0 + 40) >> 29]))(1);
}

uint64_t sub_1CAB5A584()
{
  return 2;
}

uint64_t sub_1CAB5A58C()
{
  return 4;
}

uint64_t sub_1CAB5A594()
{
  return 5;
}

uint64_t sub_1CAB5A59C@<X0>(uint64_t a1@<X8>)
{
  uint64_t v1;
  _QWORD *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  int8x16_t v6;
  int8x16_t v7;
  int8x16_t v8;
  uint64_t v10;
  BOOL v11;

  v4 = v2[1];
  v3 = v2[2];
  v5 = v2[8] | v2[9];
  if (a1 == 0x80000000)
  {
    v6.i64[0] = v2[4];
    v6.i64[1] = v2[3];
    v7.i64[0] = v2[7];
    v7.i64[1] = v2[6];
    v8 = vorrq_s8(v7, v6);
    if (!(v5 | *(_QWORD *)&vorr_s8(*(int8x8_t *)v8.i8, (int8x8_t)*(_OWORD *)&vextq_s8(v8, v8, 8uLL)) | v4 | v3 | v1))
      return 3;
  }
  v10 = v5 | v2[7] | v2[6] | v2[4] | v2[3] | v3 | v4;
  if (a1 == 0x80000000 && v1 == 1 && !v10)
    return 6;
  if (a1 == 0x80000000 && v1 == 2 && !v10)
    return 7;
  if (a1 == 0x80000000 && v1 == 3 && !v10)
    return 8;
  if (v10)
    v11 = 0;
  else
    v11 = v1 == 4;
  if (v11 && a1 == 0x80000000)
    return 9;
  else
    return 10;
}

uint64_t BNNS.LossReduction.bnnsLossReductionFunction.getter()
{
  unsigned __int8 *v0;

  return *v0;
}

uint64_t BNNS.LossLayer.apply(batchSize:input:labels:output:generatingInputGradient:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _OWORD *a5)
{
  uint64_t v5;
  void *v6;
  void *v7;
  const void *v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  size_t in_delta_stride;
  uint64_t result;
  _BYTE *v16;
  _BYTE *v17;
  size_t v18;
  size_t v19;
  void *out;
  void *in;
  const void *v23;
  void *v24;
  unint64_t v25;
  unint64_t v26;
  unint64_t v27;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  unint64_t v36;
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  unint64_t v45;
  unint64_t v46;
  unint64_t v47;
  unint64_t v48;
  _BYTE v49[136];
  _BYTE v50[136];
  BNNSNDArrayDescriptor in_delta;
  _BYTE v52[136];
  _BYTE v53[136];
  _BYTE v54[136];
  uint64_t v55;

  v55 = *MEMORY[0x1E0C80C00];
  v6 = *(void **)(a2 + 136);
  if (v6 && (v7 = *(void **)(a4 + 136)) != 0 && (v8 = *(const void **)(a3 + 136)) != 0)
  {
    v9 = a5[9];
    *(_OWORD *)&in_delta.stride[7] = a5[8];
    *(_OWORD *)&in_delta.data_type = v9;
    *(_OWORD *)&in_delta.table_data_type = a5[10];
    v10 = a5[5];
    *(_OWORD *)&in_delta.size[7] = a5[4];
    *(_OWORD *)&in_delta.stride[1] = v10;
    v11 = a5[7];
    *(_OWORD *)&in_delta.stride[3] = a5[6];
    *(_OWORD *)&in_delta.stride[5] = v11;
    v12 = a5[1];
    *(_OWORD *)&in_delta.flags = *a5;
    *(_OWORD *)&in_delta.size[1] = v12;
    v13 = a5[3];
    *(_OWORD *)&in_delta.size[3] = a5[2];
    *(_OWORD *)&in_delta.size[5] = v13;
    v23 = v8;
    v24 = *(void **)(v5 + 16);
    out = v7;
    in = v6;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v50);
    outlined init with take of BNNS.Shape((uint64_t)v50, (uint64_t)v52);
    outlined init with take of BNNS.Shape((uint64_t)v52, (uint64_t)v49);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v52, (uint64_t)v49);
    BNNS.Shape.stride.getter();
    v19 = specialized static BNNS.calculateBatchStride(size:stride:)(v41, v42, v43, v44, v45, v46, v47, v48, v41, v42, v43, v44, v45, v46, v47, v48);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v49);
    outlined init with take of BNNS.Shape((uint64_t)v49, (uint64_t)v53);
    outlined init with take of BNNS.Shape((uint64_t)v53, (uint64_t)&v41);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v53, (uint64_t)&v41);
    BNNS.Shape.stride.getter();
    v18 = specialized static BNNS.calculateBatchStride(size:stride:)(v33, v34, v35, v36, v37, v38, v39, v40, v33, v34, v35, v36, v37, v38, v39, v40);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v41);
    outlined init with take of BNNS.Shape((uint64_t)&v41, (uint64_t)v54);
    outlined init with take of BNNS.Shape((uint64_t)v54, (uint64_t)&v33);
    BNNS.Shape.size.getter();
    outlined init with take of BNNS.Shape((uint64_t)v54, (uint64_t)&v33);
    BNNS.Shape.stride.getter();
    in_delta_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v25, v26, v27, v28, v29, v30, v31, v32, v25, v26, v27, v28, v29, v30, v31, v32);
    result = BNNSLossFilterApplyBatch(v24, a1, in, v19, v23, v18, 0, 0, out, &in_delta, in_delta_stride);
    if (!(_DWORD)result)
      return result;
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v16 = 0;
  }
  else
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v17 = 2;
  }
  return swift_willThrow();
}

uint64_t BNNS.LossLayer.__allocating_init(bnnsFilter:)(uint64_t a1)
{
  uint64_t v2;
  uint64_t v3;

  v2 = swift_allocObject();
  v3 = v2;
  *(_QWORD *)(v2 + 24) = 0xFFFFFFFF00000000;
  *(_OWORD *)(v2 + 32) = 0u;
  *(_OWORD *)(v2 + 48) = 0u;
  *(_OWORD *)(v2 + 64) = 0u;
  *(_OWORD *)(v2 + 80) = 0u;
  *(_QWORD *)(v2 + 96) = 0;
  if (a1)
  {
    *(_QWORD *)(v2 + 16) = a1;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v3;
}

_OWORD *outlined init with take of BNNS.LossFunction(_OWORD *a1, _OWORD *a2)
{
  __int128 v2;
  __int128 v3;
  __int128 v4;

  *a2 = *a1;
  v2 = a1[1];
  v3 = a1[2];
  v4 = a1[4];
  a2[3] = a1[3];
  a2[4] = v4;
  a2[1] = v2;
  a2[2] = v3;
  return a2;
}

uint64_t type metadata accessor for BNNS.LossLayer()
{
  return objc_opt_self();
}

uint64_t BNNS.LossLayer.deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.LossLayer.__deallocating_deinit()
{
  uint64_t v0;

  BNNSFilterDestroy(*(void **)(v0 + 16));
  return swift_deallocClassInstance();
}

uint64_t BNNS.LossLayer.apply(batchSize:input:labels:output:weights:broadcastsWeights:generatingInputGradient:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, char a6, _OWORD *a7)
{
  uint64_t v7;
  uint64_t v8;
  uint64_t result;
  const void *v16;
  const void *v17;
  _BYTE *v18;
  uint64_t v19;
  __int128 v20;
  __int128 v21;
  __int128 v22;
  __int128 v23;
  __int128 v24;
  _BYTE *v25;
  void *v26;
  BNNSNDArrayDescriptor v27;
  _DWORD v28[34];
  _QWORD v29[39];
  _OWORD v30[3];
  _BYTE v31[8];
  _BYTE v32[8];
  _BYTE v33[8];
  _BYTE v34[8];
  const void *v35;
  void *v36;
  const void *v37;
  _QWORD v38[4];

  v8 = v7;
  v38[1] = *MEMORY[0x1E0C80C00];
  outlined init with take of BNNSNDArrayDescriptor?(a5, (uint64_t)v29, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v29) == 1)
    return BNNS.LossLayer.apply(batchSize:input:labels:output:generatingInputGradient:)(a1, a2, a3, a4, a7);
  v30[0] = *(_OWORD *)&v29[16];
  v30[1] = *(_OWORD *)&v29[18];
  v30[2] = *(_OWORD *)&v29[20];
  *(_OWORD *)&v29[31] = *(_OWORD *)&v29[8];
  *(_OWORD *)&v29[33] = *(_OWORD *)&v29[10];
  *(_OWORD *)&v29[35] = *(_OWORD *)&v29[12];
  *(_OWORD *)&v29[37] = *(_OWORD *)&v29[14];
  *(_OWORD *)&v29[23] = *(_OWORD *)v29;
  *(_OWORD *)&v29[25] = *(_OWORD *)&v29[2];
  *(_OWORD *)&v29[27] = *(_OWORD *)&v29[4];
  *(_OWORD *)&v29[29] = *(_OWORD *)&v29[6];
  outlined init with take of BNNSNDArrayDescriptor?(a2 + 136, (uint64_t)v34, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v34, (uint64_t)&v35, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v16 = v35;
  if (!v35)
    goto LABEL_10;
  outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v33, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v33, (uint64_t)&v36, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  v26 = v36;
  if (!v36
    || (outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v30 + 8, (uint64_t)v31, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v31, (uint64_t)v38, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), (v17 = (const void *)v38[0]) == 0)|| (outlined init with take of BNNSNDArrayDescriptor?(a3 + 136, (uint64_t)v32, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v32, (uint64_t)&v37, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), !v37))
  {
LABEL_10:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v18 = 2;
    return swift_willThrow();
  }
  if ((a6 & 1) == 0)
  {
    v19 = *(_QWORD *)(v8 + 64);
    if (*(_QWORD *)(v8 + 24) < 0xFFFFFFFF00000000 || (v19 & 0xFFFFFFFE) != 0)
      __asm { BR              X1 }
    goto LABEL_16;
  }
  v20 = a7[9];
  *(_OWORD *)&v27.stride[7] = a7[8];
  *(_OWORD *)&v27.data_type = v20;
  *(_OWORD *)&v27.table_data_type = a7[10];
  v21 = a7[5];
  *(_OWORD *)&v27.size[7] = a7[4];
  *(_OWORD *)&v27.stride[1] = v21;
  v22 = a7[7];
  *(_OWORD *)&v27.stride[3] = a7[6];
  *(_OWORD *)&v27.stride[5] = v22;
  v23 = a7[1];
  *(_OWORD *)&v27.flags = *a7;
  *(_OWORD *)&v27.size[1] = v23;
  v24 = a7[3];
  *(_OWORD *)&v27.size[3] = a7[2];
  *(_OWORD *)&v27.size[5] = v24;
  result = closure #1 in BNNS.LossLayer.apply(batchSize:input:labels:output:weights:broadcastsWeights:generatingInputGradient:)(&v27, v8, a1, v16, v37, v17, v28, 1uLL, v26);
  if (v28[0])
  {
LABEL_16:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v25 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t closure #1 in BNNS.LossLayer.apply(batchSize:input:labels:output:weights:broadcastsWeights:generatingInputGradient:)@<X0>(BNNSNDArrayDescriptor *a1@<X0>, uint64_t a2@<X1>, size_t a3@<X2>, const void *a4@<X3>, const void *a5@<X5>, const void *a6@<X7>, _DWORD *a7@<X8>, size_t a8, void *out)
{
  unint64_t v9;
  unint64_t v10;
  unint64_t v11;
  unint64_t v12;
  unint64_t v13;
  unint64_t v14;
  unint64_t v15;
  unint64_t v16;
  size_t in_delta_stride;
  uint64_t result;
  unint64_t labels_stride;
  unint64_t in_stride;
  void *v21;
  unint64_t v28;
  unint64_t v29;
  unint64_t v30;
  unint64_t v31;
  unint64_t v32;
  unint64_t v33;
  unint64_t v34;
  unint64_t v35;
  _BYTE v36[136];
  unint64_t v37;
  unint64_t v38;
  unint64_t v39;
  unint64_t v40;
  unint64_t v41;
  unint64_t v42;
  unint64_t v43;
  unint64_t v44;
  _BYTE v45[136];
  _BYTE v46[136];
  _BYTE v47[136];
  unint64_t v48;
  unint64_t v49;
  unint64_t v50;
  unint64_t v51;
  unint64_t v52;
  unint64_t v53;
  unint64_t v54;
  unint64_t v55;
  _BYTE v56[144];

  v21 = *(void **)(a2 + 16);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v46);
  outlined init with take of BNNS.Shape((uint64_t)v46, (uint64_t)v47);
  outlined init with take of BNNS.Shape((uint64_t)v47, (uint64_t)v56);
  BNNS.Shape.size.getter();
  v9 = v48;
  v10 = v49;
  v11 = v50;
  v12 = v51;
  v13 = v52;
  v14 = v53;
  v15 = v54;
  v16 = v55;
  outlined init with take of BNNS.Shape((uint64_t)v47, (uint64_t)v56);
  BNNS.Shape.stride.getter();
  in_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v9, v10, v11, v12, v13, v14, v15, v16, v48, v49, v50, v51, v52, v53, v54, v55);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v45);
  outlined init with take of BNNS.Shape((uint64_t)v45, (uint64_t)&v48);
  outlined init with take of BNNS.Shape((uint64_t)&v48, (uint64_t)v56);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)&v48, (uint64_t)v56);
  BNNS.Shape.stride.getter();
  labels_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v37, v38, v39, v40, v41, v42, v43, v44, v37, v38, v39, v40, v41, v42, v43, v44);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v37);
  outlined init with take of BNNS.Shape((uint64_t)&v37, (uint64_t)v56);
  outlined init with take of BNNS.Shape((uint64_t)v56, (uint64_t)v36);
  BNNS.Shape.size.getter();
  outlined init with take of BNNS.Shape((uint64_t)v56, (uint64_t)v36);
  BNNS.Shape.stride.getter();
  in_delta_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v28, v29, v30, v31, v32, v33, v34, v35, v28, v29, v30, v31, v32, v33, v34, v35);
  result = BNNSLossFilterApplyBatch(v21, a3, a4, in_stride, a5, labels_stride, a6, a8, out, a1, in_delta_stride);
  *a7 = result;
  return result;
}

uint64_t BNNS.LossFunction.YoloParameters.init(huberDelta:gridColumnCount:gridRowsCount:anchorBoxCount:anchorBoxSize:rescore:xyScale:whScale:objectScale:noObjectScale:classificationScale:objectMinimumIoU:noObjectMaximumIoU:anchorsData:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, char a5@<W4>, uint64_t a6@<X5>, uint64_t a7@<X8>, float a8@<S0>, float a9@<S1>, float a10@<S2>, float a11@<S3>, float a12@<S4>, float a13@<S5>, float a14@<S6>, float a15@<S7>)
{
  *(float *)a7 = a8;
  *(_QWORD *)(a7 + 8) = result;
  *(_QWORD *)(a7 + 16) = a2;
  *(_QWORD *)(a7 + 24) = a3;
  *(_QWORD *)(a7 + 32) = a4;
  *(_BYTE *)(a7 + 40) = a5;
  *(float *)(a7 + 44) = a9;
  *(float *)(a7 + 48) = a10;
  *(float *)(a7 + 52) = a11;
  *(float *)(a7 + 56) = a12;
  *(float *)(a7 + 60) = a13;
  *(float *)(a7 + 64) = a14;
  *(float *)(a7 + 68) = a15;
  *(_QWORD *)(a7 + 72) = a6;
  return result;
}

float BNNS.LossFunction.YoloParameters.huberDelta.getter()
{
  uint64_t v0;

  return *(float *)v0;
}

uint64_t BNNS.LossFunction.YoloParameters.gridColumnCount.getter()
{
  uint64_t v0;

  return *(_QWORD *)(v0 + 8);
}

uint64_t BNNS.LossFunction.YoloParameters.gridRowsCount.getter()
{
  uint64_t v0;

  return *(_QWORD *)(v0 + 16);
}

uint64_t BNNS.LossFunction.YoloParameters.anchorBoxCount.getter()
{
  uint64_t v0;

  return *(_QWORD *)(v0 + 24);
}

uint64_t BNNS.LossFunction.YoloParameters.anchorBoxSize.getter()
{
  uint64_t v0;

  return *(_QWORD *)(v0 + 32);
}

uint64_t BNNS.LossFunction.YoloParameters.rescore.getter()
{
  uint64_t v0;

  return *(unsigned __int8 *)(v0 + 40);
}

float BNNS.LossFunction.YoloParameters.xyScale.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 44);
}

float BNNS.LossFunction.YoloParameters.whScale.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 48);
}

float BNNS.LossFunction.YoloParameters.objectScale.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 52);
}

float BNNS.LossFunction.YoloParameters.noObjectScale.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 56);
}

float BNNS.LossFunction.YoloParameters.classificationScale.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 60);
}

float BNNS.LossFunction.YoloParameters.objectMinimumIoU.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 64);
}

float BNNS.LossFunction.YoloParameters.noObjectMaximumIoU.getter()
{
  uint64_t v0;

  return *(float *)(v0 + 68);
}

uint64_t BNNS.LossFunction.YoloParameters.anchorsData.getter()
{
  uint64_t v0;

  return *(_QWORD *)(v0 + 72);
}

BOOL static BNNS.LossReduction.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void BNNS.LossReduction.hash(into:)()
{
  unsigned __int8 *v0;

  Hasher._combine(_:)(*v0);
}

Swift::Int BNNS.LossReduction.hashValue.getter()
{
  unsigned __int8 *v0;
  Swift::UInt v1;

  v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

unint64_t lazy protocol witness table accessor for type BNNS.LossReduction and conformance BNNS.LossReduction()
{
  unint64_t result;

  result = lazy protocol witness table cache variable for type BNNS.LossReduction and conformance BNNS.LossReduction;
  if (!lazy protocol witness table cache variable for type BNNS.LossReduction and conformance BNNS.LossReduction)
  {
    result = MEMORY[0x1D1794D08](&protocol conformance descriptor for BNNS.LossReduction, &type metadata for BNNS.LossReduction);
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.LossReduction and conformance BNNS.LossReduction);
  }
  return result;
}

uint64_t method lookup function for BNNS.LossLayer()
{
  return swift_lookUpClassMethod();
}

uint64_t dispatch thunk of BNNS.LossLayer.apply(batchSize:input:labels:output:generatingInputGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5)
{
  uint64_t v5;
  uint64_t v6;
  int v7;
  uint64_t v8;
  int v9;
  uint64_t v10;
  int v11;
  uint64_t v12;
  int v13;
  uint64_t v14;
  int v15;
  uint64_t v16;
  int v17;
  uint64_t v18;
  int v19;
  uint64_t v20;
  int v21;
  uint64_t (*v22)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *);
  __int128 v23;
  __int128 v24;
  __int128 v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  uint64_t v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  uint64_t v40;
  __int128 v41;
  uint64_t v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;
  __int128 v48;
  __int128 v49;
  __int128 v50;
  __int128 v51;
  uint64_t v52;
  int v53;
  uint64_t v54;
  int v55;
  uint64_t v56;
  uint64_t v57;
  __int128 v58;
  __int128 v59;
  __int128 v60;
  __int128 v61;
  __int128 v62;
  __int128 v63;
  __int128 v64;
  __int128 v65;
  uint64_t v66;
  int v67;
  uint64_t v68;
  int v69;
  uint64_t v70;
  uint64_t v71;
  __int128 v72;
  __int128 v73;
  __int128 v74;
  __int128 v75;
  __int128 v76;
  __int128 v77;
  __int128 v78;
  __int128 v79;
  uint64_t v80;
  int v81;
  uint64_t v82;
  int v83;
  uint64_t v84;
  uint64_t v85;
  __int128 v86;
  __int128 v87;
  __int128 v88;
  __int128 v89;
  __int128 v90;
  __int128 v91;
  __int128 v92;
  __int128 v93;
  uint64_t v94;
  int v95;
  uint64_t v96;
  int v97;
  uint64_t v98;

  v6 = a2[17];
  v7 = *((_DWORD *)a2 + 36);
  v8 = a2[19];
  v9 = *((_DWORD *)a2 + 40);
  v10 = a3[17];
  v11 = *((_DWORD *)a3 + 36);
  v12 = a3[19];
  v13 = *((_DWORD *)a3 + 40);
  v14 = a4[17];
  v15 = *((_DWORD *)a4 + 36);
  v16 = a4[19];
  v17 = *((_DWORD *)a4 + 40);
  v18 = a5[17];
  v19 = *((_DWORD *)a5 + 36);
  v20 = a5[19];
  v21 = *((_DWORD *)a5 + 40);
  v22 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(_QWORD *)v5 + 128);
  v85 = *a2;
  v86 = *(_OWORD *)(a2 + 1);
  v87 = *(_OWORD *)(a2 + 3);
  v88 = *(_OWORD *)(a2 + 5);
  v89 = *(_OWORD *)(a2 + 7);
  v90 = *(_OWORD *)(a2 + 9);
  v91 = *(_OWORD *)(a2 + 11);
  v92 = *(_OWORD *)(a2 + 13);
  v93 = *(_OWORD *)(a2 + 15);
  v94 = v6;
  v95 = v7;
  v96 = v8;
  v97 = v9;
  v98 = *(uint64_t *)((char *)a2 + 164);
  v71 = *a3;
  v72 = *(_OWORD *)(a3 + 1);
  v73 = *(_OWORD *)(a3 + 3);
  v74 = *(_OWORD *)(a3 + 5);
  v75 = *(_OWORD *)(a3 + 7);
  v76 = *(_OWORD *)(a3 + 9);
  v77 = *(_OWORD *)(a3 + 11);
  v23 = *(_OWORD *)(a3 + 15);
  v78 = *(_OWORD *)(a3 + 13);
  v24 = *(_OWORD *)(a4 + 1);
  v25 = *(_OWORD *)(a4 + 3);
  v26 = *(_OWORD *)(a4 + 5);
  v27 = *(_OWORD *)(a4 + 7);
  v28 = *(_OWORD *)(a4 + 9);
  v29 = *(_OWORD *)(a4 + 11);
  v30 = *(_OWORD *)(a4 + 13);
  v31 = *a4;
  v32 = *(_OWORD *)(a4 + 15);
  v33 = *(_OWORD *)(a5 + 1);
  v34 = *(_OWORD *)(a5 + 3);
  v35 = *(_OWORD *)(a5 + 5);
  v36 = *(_OWORD *)(a5 + 7);
  v37 = *(_OWORD *)(a5 + 9);
  v38 = *(_OWORD *)(a5 + 11);
  v39 = *(_OWORD *)(a5 + 13);
  v40 = *a5;
  v41 = *(_OWORD *)(a5 + 15);
  v79 = v23;
  v80 = v10;
  v81 = v11;
  v82 = v12;
  v83 = v13;
  v84 = *(uint64_t *)((char *)a3 + 164);
  *(_QWORD *)&v23 = *(uint64_t *)((char *)a5 + 164);
  v57 = v31;
  v58 = v24;
  *(_QWORD *)&v24 = *(uint64_t *)((char *)a4 + 164);
  v59 = v25;
  v60 = v26;
  v61 = v27;
  v62 = v28;
  v63 = v29;
  v64 = v30;
  v65 = v32;
  v66 = v14;
  v67 = v15;
  v68 = v16;
  v69 = v17;
  v70 = v24;
  v43 = v40;
  v44 = v33;
  v45 = v34;
  v46 = v35;
  v47 = v36;
  v48 = v37;
  v49 = v38;
  v50 = v39;
  v51 = v41;
  v52 = v18;
  v53 = v19;
  v54 = v20;
  v55 = v21;
  v56 = v23;
  return v22(a1, &v85, &v71, &v57, &v43);
}

__n128 __swift_memcpy80_8(uint64_t a1, uint64_t a2)
{
  __n128 result;
  __int128 v3;
  __int128 v4;

  *(_OWORD *)a1 = *(_OWORD *)a2;
  result = *(__n128 *)(a2 + 16);
  v3 = *(_OWORD *)(a2 + 32);
  v4 = *(_OWORD *)(a2 + 64);
  *(_OWORD *)(a1 + 48) = *(_OWORD *)(a2 + 48);
  *(_OWORD *)(a1 + 64) = v4;
  *(__n128 *)(a1 + 16) = result;
  *(_OWORD *)(a1 + 32) = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.LossFunction(uint64_t a1, int a2)
{
  unsigned int v2;
  int v3;

  if (!a2)
    return 0;
  if (a2 < 0 && *(_BYTE *)(a1 + 80))
    return *(_DWORD *)a1 + 0x80000000;
  v2 = *(_DWORD *)(a1 + 4);
  if (v2 > 0x80000000)
    v3 = ~v2;
  else
    v3 = -1;
  return (v3 + 1);
}

double storeEnumTagSinglePayload for BNNS.LossFunction(uint64_t a1, int a2, int a3)
{
  double result;

  if (a2 < 0)
  {
    *(_QWORD *)(a1 + 72) = 0;
    result = 0.0;
    *(_OWORD *)(a1 + 56) = 0u;
    *(_OWORD *)(a1 + 40) = 0u;
    *(_OWORD *)(a1 + 24) = 0u;
    *(_OWORD *)(a1 + 8) = 0u;
    *(_QWORD *)a1 = a2 ^ 0x80000000;
    if (a3 < 0)
      *(_BYTE *)(a1 + 80) = 1;
  }
  else
  {
    if ((a3 & 0x80000000) == 0)
    {
      if (!a2)
        return result;
LABEL_8:
      *(_QWORD *)a1 = (unint64_t)-a2 << 32;
      result = 0.0;
      *(_OWORD *)(a1 + 8) = 0u;
      *(_OWORD *)(a1 + 24) = 0u;
      *(_OWORD *)(a1 + 40) = 0u;
      *(_OWORD *)(a1 + 56) = 0u;
      *(_QWORD *)(a1 + 72) = 0;
      return result;
    }
    *(_BYTE *)(a1 + 80) = 0;
    if (a2)
      goto LABEL_8;
  }
  return result;
}

uint64_t getEnumTag for BNNS.LossFunction(_DWORD *a1)
{
  int v1;

  v1 = a1[10];
  if (v1 >= 0)
    return v1 >> 29;
  else
    return (*a1 + 4);
}

uint64_t destructiveProjectEnumData for BNNS.LossFunction(uint64_t result)
{
  *(_QWORD *)(result + 40) &= 0xFFFFFFFF1FFFFFFFLL;
  return result;
}

unsigned int *destructiveInjectEnumTag for BNNS.LossFunction(unsigned int *result, unsigned int a2)
{
  unint64_t v2;

  if (a2 < 4)
  {
    v2 = *((_QWORD *)result + 5) & 0xFFFFFFFF00000001 | (a2 << 29);
    *(_QWORD *)result = *result;
    *((_QWORD *)result + 5) = v2;
  }
  else
  {
    *(_QWORD *)result = a2 - 4;
    *(_OWORD *)(result + 2) = 0u;
    *(_OWORD *)(result + 6) = 0u;
    *((_QWORD *)result + 5) = 0x80000000;
    *((_OWORD *)result + 3) = 0u;
    *((_OWORD *)result + 4) = 0u;
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.LossFunction()
{
  return &type metadata for BNNS.LossFunction;
}

uint64_t getEnumTagSinglePayload for BNNS.LossFunction.YoloParameters(uint64_t a1, unsigned int a2)
{
  unsigned int v3;
  BOOL v4;
  int v5;

  if (!a2)
    return 0;
  if (a2 >= 0xFF && *(_BYTE *)(a1 + 80))
    return (*(_DWORD *)a1 + 255);
  v3 = *(unsigned __int8 *)(a1 + 40);
  v4 = v3 >= 2;
  v5 = (v3 + 2147483646) & 0x7FFFFFFF;
  if (!v4)
    v5 = -1;
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.LossFunction.YoloParameters(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(_QWORD *)(result + 72) = 0;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_QWORD *)result = a2 - 255;
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 80) = 1;
  }
  else
  {
    if (a3 >= 0xFF)
      *(_BYTE *)(result + 80) = 0;
    if (a2)
      *(_BYTE *)(result + 40) = a2 + 1;
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.LossFunction.YoloParameters()
{
  return &type metadata for BNNS.LossFunction.YoloParameters;
}

uint64_t storeEnumTagSinglePayload for BNNS.LossReduction(_BYTE *a1, unsigned int a2, unsigned int a3)
{
  int v3;
  uint64_t v4;

  if (a3 + 4 >= 0xFFFF00)
    v3 = 4;
  else
    v3 = 2;
  if ((a3 + 4) >> 8 < 0xFF)
    LODWORD(v4) = 1;
  else
    LODWORD(v4) = v3;
  if (a3 >= 0xFC)
    v4 = v4;
  else
    v4 = 0;
  if (a2 <= 0xFB)
    return ((uint64_t (*)(void))((char *)&loc_1CAB5B6E8 + 4 * byte_1CAB63B33[v4]))();
  *a1 = a2 + 4;
  return ((uint64_t (*)(void))((char *)sub_1CAB5B71C + 4 * byte_1CAB63B2E[v4]))();
}

uint64_t sub_1CAB5B71C(uint64_t result)
{
  char v1;

  *(_BYTE *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB5B724(uint64_t result, int a2)
{
  *(_WORD *)(result + 1) = 0;
  if (a2)
    JUMPOUT(0x1CAB5B72CLL);
  return result;
}

uint64_t sub_1CAB5B738(uint64_t result, int a2)
{
  *(_DWORD *)(result + 1) = 0;
  if (!a2)
    JUMPOUT(0x1CAB5B740);
  *(_BYTE *)result = a2 + 4;
  return result;
}

uint64_t sub_1CAB5B744(uint64_t result)
{
  int v1;

  *(_DWORD *)(result + 1) = v1;
  return result;
}

uint64_t sub_1CAB5B74C(uint64_t result)
{
  __int16 v1;

  *(_WORD *)(result + 1) = v1;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.LossReduction()
{
  return &type metadata for BNNS.LossReduction;
}

uint64_t static BNNS.clipByGlobalNorm(threshold:inputs:outputs:globalNorm:)(uint64_t a1, uint64_t a2, float a3, float a4)
{
  size_t v4;
  char *v7;
  __int128 *v9;
  char *v10;
  _OWORD *v11;
  unint64_t v12;
  unint64_t v13;
  int64_t v14;
  __int128 *v15;
  _OWORD *v16;
  unint64_t v17;
  unint64_t v18;
  int v19;
  uint64_t result;
  _BYTE *v21;
  int64_t v23;
  char *v24;
  char *v25;
  __int128 v26;
  __int128 v27;
  __int128 v28;
  __int128 v29;
  __int128 v30;
  __int128 v31;
  __int128 v32;
  __int128 v33;
  __int128 v34;
  __int128 v35;
  __int128 v36;
  __int128 v37;
  __int128 v38;
  __int128 v39;
  __int128 v40;
  __int128 v41;
  __int128 v42;
  __int128 v43;
  __int128 v44;
  __int128 v45;
  __int128 v46;
  __int128 v47;

  v4 = *(_QWORD *)(a1 + 16);
  if (v4 != *(_QWORD *)(a2 + 16))
    __break(1u);
  v7 = (char *)MEMORY[0x1E0DEE9D8];
  if (v4)
  {
    v24 = (char *)MEMORY[0x1E0DEE9D8];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v4, 0);
    v9 = (__int128 *)(a1 + 32);
    v10 = v24;
    v23 = v4;
    do
    {
      v42 = v9[8];
      v44 = v9[9];
      v46 = v9[10];
      v34 = v9[4];
      v36 = v9[5];
      v38 = v9[6];
      v40 = v9[7];
      v26 = *v9;
      v28 = v9[1];
      v30 = v9[2];
      v32 = v9[3];
      v11 = (_OWORD *)swift_slowAlloc();
      v11[8] = v42;
      v11[9] = v44;
      v11[10] = v46;
      v11[4] = v34;
      v11[5] = v36;
      v11[6] = v38;
      v11[7] = v40;
      *v11 = v26;
      v11[1] = v28;
      v11[2] = v30;
      v11[3] = v32;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v10 + 2) + 1, 1);
        v10 = v24;
      }
      v13 = *((_QWORD *)v10 + 2);
      v12 = *((_QWORD *)v10 + 3);
      if (v13 >= v12 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v12 > 1), v13 + 1, 1);
        v10 = v24;
      }
      *((_QWORD *)v10 + 2) = v13 + 1;
      *(_QWORD *)&v10[8 * v13 + 32] = v11;
      v9 += 11;
      --v4;
    }
    while (v4);
    v25 = v7;
    v14 = v23;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v23, 0);
    v15 = (__int128 *)(a2 + 32);
    do
    {
      v43 = v15[8];
      v45 = v15[9];
      v47 = v15[10];
      v35 = v15[4];
      v37 = v15[5];
      v39 = v15[6];
      v41 = v15[7];
      v27 = *v15;
      v29 = v15[1];
      v31 = v15[2];
      v33 = v15[3];
      v16 = (_OWORD *)swift_slowAlloc();
      v16[8] = v43;
      v16[9] = v45;
      v16[10] = v47;
      v16[4] = v35;
      v16[5] = v37;
      v16[6] = v39;
      v16[7] = v41;
      *v16 = v27;
      v16[1] = v29;
      v16[2] = v31;
      v16[3] = v33;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v7 + 2) + 1, 1);
        v7 = v25;
      }
      v18 = *((_QWORD *)v7 + 2);
      v17 = *((_QWORD *)v7 + 3);
      if (v18 >= v17 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v17 > 1), v18 + 1, 1);
        v7 = v25;
      }
      *((_QWORD *)v7 + 2) = v18 + 1;
      *(_QWORD *)&v7[8 * v18 + 32] = v16;
      v15 += 11;
      --v14;
    }
    while (v14);
    v4 = v23;
  }
  else
  {
    v10 = (char *)MEMORY[0x1E0DEE9D8];
  }
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    v7 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v7 + 2), 0, v7);
  swift_bridgeObjectRetain();
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    v10 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((_QWORD *)v10 + 2), 0, v10);
  swift_bridgeObjectRetain();
  v19 = BNNSClipByGlobalNorm((BNNSNDArrayDescriptor **)v7 + 4, (const BNNSNDArrayDescriptor **)v10 + 4, v4, a3, a4);
  swift_bridgeObjectRelease_n();
  result = swift_bridgeObjectRelease_n();
  if (v19)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v21 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.clip(to:input:output:)(_OWORD *a1, _OWORD *a2, float a3, float a4)
{
  __int128 v4;
  __int128 v5;
  __int128 v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  uint64_t result;
  _BYTE *v15;
  BNNSNDArrayDescriptor v16;
  BNNSNDArrayDescriptor src;
  uint64_t v18;

  v18 = *MEMORY[0x1E0C80C00];
  v4 = a1[9];
  *(_OWORD *)&src.stride[7] = a1[8];
  *(_OWORD *)&src.data_type = v4;
  *(_OWORD *)&src.table_data_type = a1[10];
  v5 = a1[5];
  *(_OWORD *)&src.size[7] = a1[4];
  *(_OWORD *)&src.stride[1] = v5;
  v6 = a1[7];
  *(_OWORD *)&src.stride[3] = a1[6];
  *(_OWORD *)&src.stride[5] = v6;
  v7 = a1[1];
  *(_OWORD *)&src.flags = *a1;
  *(_OWORD *)&src.size[1] = v7;
  v8 = a1[3];
  *(_OWORD *)&src.size[3] = a1[2];
  *(_OWORD *)&src.size[5] = v8;
  v9 = a2[9];
  *(_OWORD *)&v16.stride[7] = a2[8];
  *(_OWORD *)&v16.data_type = v9;
  *(_OWORD *)&v16.table_data_type = a2[10];
  v10 = a2[5];
  *(_OWORD *)&v16.size[7] = a2[4];
  *(_OWORD *)&v16.stride[1] = v10;
  v11 = a2[7];
  *(_OWORD *)&v16.stride[3] = a2[6];
  *(_OWORD *)&v16.stride[5] = v11;
  v12 = a2[1];
  *(_OWORD *)&v16.flags = *a2;
  *(_OWORD *)&v16.size[1] = v12;
  v13 = a2[3];
  *(_OWORD *)&v16.size[3] = a2[2];
  *(_OWORD *)&v16.size[5] = v13;
  result = BNNSClipByValue(&v16, &src, a3, a4);
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v15 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.clipByNorm(threshold:input:output:axes:)(_OWORD *a1, _OWORD *a2, uint64_t a3, float a4)
{
  __int128 v5;
  __int128 v6;
  __int128 v7;
  __int128 v8;
  __int128 v9;
  __int128 v10;
  __int128 v11;
  __int128 v12;
  __int128 v13;
  __int128 v14;
  uint32_t v15;
  uint64_t result;
  _BYTE *v17;
  BNNSNDArrayDescriptor v18;
  BNNSNDArrayDescriptor src;
  uint64_t v20;

  v20 = *MEMORY[0x1E0C80C00];
  v5 = a1[9];
  *(_OWORD *)&src.stride[7] = a1[8];
  *(_OWORD *)&src.data_type = v5;
  *(_OWORD *)&src.table_data_type = a1[10];
  v6 = a1[5];
  *(_OWORD *)&src.size[7] = a1[4];
  *(_OWORD *)&src.stride[1] = v6;
  v7 = a1[7];
  *(_OWORD *)&src.stride[3] = a1[6];
  *(_OWORD *)&src.stride[5] = v7;
  v8 = a1[1];
  *(_OWORD *)&src.flags = *a1;
  *(_OWORD *)&src.size[1] = v8;
  v9 = a1[3];
  *(_OWORD *)&src.size[3] = a1[2];
  *(_OWORD *)&src.size[5] = v9;
  v10 = a2[9];
  *(_OWORD *)&v18.stride[7] = a2[8];
  *(_OWORD *)&v18.data_type = v10;
  *(_OWORD *)&v18.table_data_type = a2[10];
  v11 = a2[5];
  *(_OWORD *)&v18.size[7] = a2[4];
  *(_OWORD *)&v18.stride[1] = v11;
  v12 = a2[7];
  *(_OWORD *)&v18.stride[3] = a2[6];
  *(_OWORD *)&v18.stride[5] = v12;
  v13 = a2[1];
  *(_OWORD *)&v18.flags = *a2;
  *(_OWORD *)&v18.size[1] = v13;
  v14 = a2[3];
  *(_OWORD *)&v18.size[3] = a2[2];
  *(_OWORD *)&v18.size[5] = v14;
  v15 = specialized static BNNS.computeAxisFlags(_:)(a3);
  result = BNNSClipByNorm(&v18, &src, a4, v15);
  if ((_DWORD)result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v17 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t specialized static BNNS.computeAxisFlags(_:)(uint64_t a1)
{
  uint64_t v1;
  int64_t v2;
  uint64_t v3;
  int8x16_t *v4;
  unint64_t v5;
  uint64_t v6;
  unint64_t v7;
  unint64_t v8;
  uint64_t v9;
  uint64_t v10;
  int8x16_t *v11;
  int8x16_t v12;
  uint64_t v13;
  int8x16_t v14;
  int8x16_t v15;
  int8x8_t v16;
  unint64_t v17;
  __int32 *v18;
  int v19;
  int8x16_t *v21;

  if (a1)
    v1 = a1;
  else
    v1 = MEMORY[0x1E0DEE9D8];
  v2 = *(_QWORD *)(v1 + 16);
  if (!v2)
  {
    swift_bridgeObjectRetain();
    swift_bridgeObjectRelease();
    v4 = (int8x16_t *)MEMORY[0x1E0DEE9D8];
    v8 = *(_QWORD *)(MEMORY[0x1E0DEE9D8] + 16);
    if (v8)
      goto LABEL_10;
LABEL_13:
    v10 = 0;
    goto LABEL_19;
  }
  v21 = (int8x16_t *)MEMORY[0x1E0DEE9D8];
  swift_bridgeObjectRetain();
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v2, 0);
  v3 = 0;
  v4 = v21;
  v5 = v21[1].u64[0];
  do
  {
    v6 = *(_QWORD *)(v1 + 8 * v3 + 32);
    v7 = v21[1].u64[1];
    if (v5 >= v7 >> 1)
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v7 > 1), v5 + 1, 1);
    ++v3;
    v21[1].i64[0] = v5 + 1;
    v21[2].i32[v5++] = 1 << v6;
  }
  while (v2 != v3);
  swift_bridgeObjectRelease();
  v8 = v21[1].u64[0];
  if (!v8)
    goto LABEL_13;
LABEL_10:
  if (v8 < 8)
  {
    v9 = 0;
    LODWORD(v10) = 0;
LABEL_17:
    v17 = v8 - v9;
    v18 = &v4[2].i32[v9];
    do
    {
      v19 = *v18++;
      v10 = v19 | v10;
      --v17;
    }
    while (v17);
    goto LABEL_19;
  }
  v9 = v8 & 0x7FFFFFFFFFFFFFF8;
  v11 = v4 + 3;
  v12 = 0uLL;
  v13 = v8 & 0x7FFFFFFFFFFFFFF8;
  v14 = 0uLL;
  do
  {
    v12 = vorrq_s8(v11[-1], v12);
    v14 = vorrq_s8(*v11, v14);
    v11 += 2;
    v13 -= 8;
  }
  while (v13);
  v15 = vorrq_s8(v14, v12);
  v16 = vorr_s8(*(int8x8_t *)v15.i8, (int8x8_t)*(_OWORD *)&vextq_s8(v15, v15, 8uLL));
  v10 = (v16.i32[0] | v16.i32[1]);
  if (v8 != v9)
    goto LABEL_17;
LABEL_19:
  swift_bridgeObjectRelease();
  return v10;
}

BOOL __isOSVersionAtLeast(int a1, int a2, int a3)
{
  BOOL v6;

  if (qword_1EF97D820 == -1)
  {
    v6 = _MergedGlobals < a1;
    if (_MergedGlobals > a1)
      return 1;
  }
  else
  {
    dispatch_once_f(&qword_1EF97D820, 0, (dispatch_function_t)compatibilityInitializeAvailabilityCheck);
    v6 = _MergedGlobals < a1;
    if (_MergedGlobals > a1)
      return 1;
  }
  if (v6)
    return 0;
  if (dword_1EF97D814 > a2)
    return 1;
  return dword_1EF97D814 >= a2 && dword_1EF97D818 >= a3;
}

uint64_t compatibilityInitializeAvailabilityCheck()
{
  return _initializeAvailabilityCheck(1);
}

uint64_t __isPlatformVersionAtLeast(uint64_t a1, int a2, int a3, int a4)
{
  BOOL v8;

  if (qword_1EF97D828 == -1)
  {
    if (qword_1EF97D830)
      return _availability_version_check();
  }
  else
  {
    dispatch_once_f(&qword_1EF97D828, 0, (dispatch_function_t)initializeAvailabilityCheck);
    if (qword_1EF97D830)
      return _availability_version_check();
  }
  if (qword_1EF97D820 == -1)
  {
    v8 = _MergedGlobals < a2;
    if (_MergedGlobals > a2)
      return 1;
  }
  else
  {
    dispatch_once_f(&qword_1EF97D820, 0, (dispatch_function_t)compatibilityInitializeAvailabilityCheck);
    v8 = _MergedGlobals < a2;
    if (_MergedGlobals > a2)
      return 1;
  }
  if (v8)
    return 0;
  if (dword_1EF97D814 > a3)
    return 1;
  return dword_1EF97D814 >= a3 && dword_1EF97D818 >= a4;
}

uint64_t initializeAvailabilityCheck()
{
  return _initializeAvailabilityCheck(0);
}

uint64_t _initializeAvailabilityCheck(uint64_t result)
{
  uint64_t (*v1)(void);
  BOOL v2;
  unsigned __int8 *v4;
  uint64_t (*v5)(_QWORD);
  unint64_t v6;
  uint64_t (*v7)(_QWORD, uint64_t, _QWORD, _QWORD);
  uint64_t (*v8)(_QWORD, const char *, uint64_t, unsigned __int8 *);
  uint64_t (*v9)(uint64_t, uint64_t);
  uint64_t (*v10)(uint64_t);
  uint64_t (*v11)(void);
  FILE *v12;
  FILE *v13;
  uint64_t v14;
  size_t v15;
  void *v16;
  uint64_t v17;
  uint64_t v18;
  FILE *v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  void *v26;
  FILE *v27;
  unsigned int (*v28)(uint64_t, char *, uint64_t, uint64_t);
  char v29[32];
  uint64_t v30;

  v30 = *MEMORY[0x1E0C80C00];
  v1 = (uint64_t (*)(void))qword_1EF97D830;
  if (qword_1EF97D830)
    v2 = (_DWORD)result == 0;
  else
    v2 = 0;
  if (!v2)
  {
    if (MEMORY[0x1E0C80CC0])
    {
      qword_1EF97D830 = (uint64_t)MEMORY[0x1E0C80CC0];
      v1 = MEMORY[0x1E0C80CC0];
    }
    if (!v1 || (_DWORD)result != 0)
    {
      result = (uint64_t)dlsym((void *)0xFFFFFFFFFFFFFFFELL, "kCFAllocatorNull");
      if (result)
      {
        v4 = *(unsigned __int8 **)result;
        result = (uint64_t)dlsym((void *)0xFFFFFFFFFFFFFFFELL, "CFDataCreateWithBytesNoCopy");
        if (result)
        {
          v5 = (uint64_t (*)(_QWORD))result;
          v6 = (unint64_t)dlsym((void *)0xFFFFFFFFFFFFFFFELL, "CFPropertyListCreateWithData");
          result = (uint64_t)dlsym((void *)0xFFFFFFFFFFFFFFFELL, "CFPropertyListCreateFromXMLData");
          if (v6 | result)
          {
            v7 = (uint64_t (*)(_QWORD, uint64_t, _QWORD, _QWORD))result;
            result = (uint64_t)dlsym((void *)0xFFFFFFFFFFFFFFFELL, "CFStringCreateWithCStringNoCopy");
            if (result)
            {
              v8 = (uint64_t (*)(_QWORD, const char *, uint64_t, unsigned __int8 *))result;
              result = (uint64_t)dlsym((void *)0xFFFFFFFFFFFFFFFELL, "CFDictionaryGetValue");
              if (result)
              {
                v9 = (uint64_t (*)(uint64_t, uint64_t))result;
                result = (uint64_t)dlsym((void *)0xFFFFFFFFFFFFFFFELL, "CFGetTypeID");
                if (result)
                {
                  v10 = (uint64_t (*)(uint64_t))result;
                  result = (uint64_t)dlsym((void *)0xFFFFFFFFFFFFFFFELL, "CFStringGetTypeID");
                  if (result)
                  {
                    v11 = (uint64_t (*)(void))result;
                    result = (uint64_t)dlsym((void *)0xFFFFFFFFFFFFFFFELL, "CFStringGetCString");
                    v28 = (unsigned int (*)(uint64_t, char *, uint64_t, uint64_t))result;
                    if (result)
                    {
                      result = (uint64_t)dlsym((void *)0xFFFFFFFFFFFFFFFELL, "CFRelease");
                      if (result)
                      {
                        v12 = (FILE *)result;
                        result = (uint64_t)fopen("/System/Library/CoreServices/SystemVersion.plist", "r");
                        if (result)
                        {
                          v13 = (FILE *)result;
                          v27 = v12;
                          fseek((FILE *)result, 0, 2);
                          v14 = MEMORY[0x1D1794A74](v13);
                          if (v14 < 0)
                          {
                            v16 = 0;
                          }
                          else
                          {
                            v15 = v14;
                            rewind(v13);
                            v16 = malloc(v15);
                            if (v16)
                            {
                              v26 = v16;
                              if (fread(v16, 1uLL, v15, v13) == v15)
                              {
                                v17 = v5(0);
                                if (v17)
                                {
                                  v18 = v17;
                                  v19 = v13;
                                  if (v6)
                                    v20 = ((uint64_t (*)(_QWORD, uint64_t, _QWORD, _QWORD, _QWORD))v6)(0, v17, 0, 0, 0);
                                  else
                                    v20 = v7(0, v17, 0, 0);
                                  v21 = v20;
                                  if (v20)
                                  {
                                    v22 = v8(0, "ProductVersion", 1536, v4);
                                    if (v22)
                                    {
                                      v23 = v22;
                                      v24 = v9(v21, v22);
                                      ((void (*)(uint64_t))v27)(v23);
                                      if (v24)
                                      {
                                        v25 = v10(v24);
                                        if (v25 == v11())
                                        {
                                          if (v28(v24, v29, 32, 134217984))
                                            sscanf(v29, "%d.%d.%d", &_MergedGlobals, &dword_1EF97D814, &dword_1EF97D818);
                                        }
                                      }
                                    }
                                    ((void (*)(uint64_t))v27)(v21);
                                    v19 = v13;
                                  }
                                  ((void (*)(uint64_t))v27)(v18);
                                }
                                else
                                {
                                  v19 = v13;
                                }
                                v16 = v26;
                              }
                              else
                              {
                                v16 = v26;
                                v19 = v13;
                              }
                              goto LABEL_41;
                            }
                          }
                          v19 = v13;
LABEL_41:
                          free(v16);
                          return fclose(v19);
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
  return result;
}

uint64_t static _CFObject.== infix(_:_:)()
{
  return MEMORY[0x1E0DEF0A0]();
}

uint64_t _CFObject.hash(into:)()
{
  return MEMORY[0x1E0DEF0A8]();
}

uint64_t _CFObject.hashValue.getter()
{
  return MEMORY[0x1E0DEF0B0]();
}

uint64_t BinaryFloatingPoint.init<A>(_:)()
{
  return MEMORY[0x1E0DE9DF0]();
}

uint64_t static BinaryFloatingPoint<>.random<A>(in:using:)()
{
  return MEMORY[0x1E0DE9DF8]();
}

uint64_t static BinaryFloatingPoint<>.random(in:)()
{
  return MEMORY[0x1E0DE9E00]();
}

uint64_t dispatch thunk of static Comparable.< infix(_:_:)()
{
  return MEMORY[0x1E0DEA3C0]();
}

uint64_t dispatch thunk of MutableCollection.withContiguousMutableStorageIfAvailable<A>(_:)()
{
  return MEMORY[0x1E0DEA400]();
}

uint64_t type metadata accessor for ClosedRange()
{
  return MEMORY[0x1E0DEA4B0]();
}

uint64_t type metadata accessor for UnsafePointer()
{
  return MEMORY[0x1E0DEA548]();
}

uint64_t UnsafeBufferPointer.baseAddress.getter()
{
  return MEMORY[0x1E0DEA578]();
}

uint64_t UnsafeBufferPointer.init(start:count:)()
{
  return MEMORY[0x1E0DEA588]();
}

uint64_t type metadata accessor for UnsafeBufferPointer()
{
  return MEMORY[0x1E0DEA5B8]();
}

uint64_t String.utf8CString.getter()
{
  return MEMORY[0x1E0DEA630]();
}

Swift::Void __swiftcall String.append(_:)(Swift::String a1)
{
  MEMORY[0x1E0DEA800](a1._countAndFlagsBits, a1._object);
}

uint64_t String.init(cString:)()
{
  return MEMORY[0x1E0DEA828]();
}

uint64_t dispatch thunk of Sequence.withContiguousStorageIfAvailable<A>(_:)()
{
  return MEMORY[0x1E0DEAA80]();
}

Swift::Void __swiftcall Array.reserveCapacity(_:)(Swift::Int a1)
{
  MEMORY[0x1E0DEADD8](a1);
}

Swift::Void __swiftcall Array._makeMutableAndUnique()()
{
  MEMORY[0x1E0DEAE08]();
}

uint64_t static Array._allocateUninitialized(_:)()
{
  return MEMORY[0x1E0DEAE10]();
}

uint64_t Array.withUnsafeBufferPointer<A>(_:)()
{
  return MEMORY[0x1E0DEAE20]();
}

uint64_t static Array._allocateBufferUninitialized(minimumCapacity:)()
{
  return MEMORY[0x1E0DEAE28]();
}

uint64_t Array.init(_unsafeUninitializedCapacity:initializingWith:)()
{
  return MEMORY[0x1E0DEAE30]();
}

uint64_t Array.count.getter()
{
  return MEMORY[0x1E0DEAE60]();
}

uint64_t Array.endIndex.getter()
{
  return MEMORY[0x1E0DEAE98]();
}

uint64_t type metadata accessor for Array()
{
  return MEMORY[0x1E0DEAEC8]();
}

uint64_t Array.init<A>(_:)()
{
  return MEMORY[0x1E0DEAF00]();
}

uint64_t Array.subscript.getter()
{
  return MEMORY[0x1E0DEAF78]();
}

uint64_t Double.significandWidth.getter()
{
  return MEMORY[0x1E0DEB008]();
}

uint64_t Double.exponent.getter()
{
  return MEMORY[0x1E0DEB048]();
}

uint64_t Set.init(minimumCapacity:)()
{
  return MEMORY[0x1E0DEB258]();
}

uint64_t dispatch thunk of Collection.startIndex.getter()
{
  return MEMORY[0x1E0DEB528]();
}

uint64_t dispatch thunk of Collection.count.getter()
{
  return MEMORY[0x1E0DEB560]();
}

uint64_t dispatch thunk of Collection.formIndex(after:)()
{
  return MEMORY[0x1E0DEB5B0]();
}

uint64_t Collection.count.getter()
{
  return MEMORY[0x1E0DEB618]();
}

uint64_t dispatch thunk of Collection.subscript.read()
{
  return MEMORY[0x1E0DEB6D0]();
}

uint64_t static UnsafeMutablePointer.allocate(capacity:)()
{
  return MEMORY[0x1E0DEB908]();
}

uint64_t type metadata accessor for UnsafeMutablePointer()
{
  return MEMORY[0x1E0DEB910]();
}

uint64_t type metadata accessor for Optional()
{
  return MEMORY[0x1E0DEB940]();
}

uint64_t UnsafeMutableBufferPointer.initialize<A>(from:)()
{
  return MEMORY[0x1E0DEB978]();
}

uint64_t UnsafeMutableBufferPointer.baseAddress.getter()
{
  return MEMORY[0x1E0DEB980]();
}

uint64_t UnsafeMutableBufferPointer.init(start:count:)()
{
  return MEMORY[0x1E0DEB988]();
}

uint64_t UnsafeMutableBufferPointer.endIndex.getter()
{
  return MEMORY[0x1E0DEB9A8]();
}

uint64_t type metadata accessor for UnsafeMutableBufferPointer()
{
  return MEMORY[0x1E0DEB9C0]();
}

uint64_t dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)()
{
  return MEMORY[0x1E0DEBD70]();
}

uint64_t dispatch thunk of BinaryInteger._lowWord.getter()
{
  return MEMORY[0x1E0DEBDF8]();
}

uint64_t dispatch thunk of BinaryInteger.bitWidth.getter()
{
  return MEMORY[0x1E0DEBE00]();
}

uint64_t dispatch thunk of static BinaryInteger.isSigned.getter()
{
  return MEMORY[0x1E0DEBE10]();
}

Swift::Void __swiftcall ArraySlice._makeMutableAndUnique()()
{
  MEMORY[0x1E0DEBED0]();
}

uint64_t ArraySlice.withUnsafeBufferPointer<A>(_:)()
{
  return MEMORY[0x1E0DEBED8]();
}

uint64_t ArraySlice.count.getter()
{
  return MEMORY[0x1E0DEBEE8]();
}

Swift::Int __swiftcall ArraySlice._getCount()()
{
  return MEMORY[0x1E0DEBF10]();
}

uint64_t type metadata accessor for ArraySlice()
{
  return MEMORY[0x1E0DEBF30]();
}

uint64_t SetAlgebra.init<A>(_:)()
{
  return MEMORY[0x1E0DEC050]();
}

uint64_t static _SetStorage.copy(original:)()
{
  return MEMORY[0x1E0DEC248]();
}

uint64_t static _SetStorage.resize(original:capacity:move:)()
{
  return MEMORY[0x1E0DEC250]();
}

Swift::Void __swiftcall _StringGuts.grow(_:)(Swift::Int a1)
{
  MEMORY[0x1E0DEC2A8](a1);
}

uint64_t type metadata accessor for _ArrayBuffer()
{
  return MEMORY[0x1E0DEC3F8]();
}

Swift::Void __swiftcall ContiguousArray.reserveCapacity(_:)(Swift::Int a1)
{
  MEMORY[0x1E0DEC680](a1);
}

Swift::Void __swiftcall ContiguousArray._makeMutableAndUnique()()
{
  MEMORY[0x1E0DEC6A0]();
}

uint64_t ContiguousArray.withUnsafeBufferPointer<A>(_:)()
{
  return MEMORY[0x1E0DEC6B0]();
}

uint64_t ContiguousArray.count.getter()
{
  return MEMORY[0x1E0DEC6D0]();
}

uint64_t ContiguousArray.append(_:)()
{
  return MEMORY[0x1E0DEC6E0]();
}

uint64_t ContiguousArray.init()()
{
  return MEMORY[0x1E0DEC700]();
}

uint64_t type metadata accessor for ContiguousArray()
{
  return MEMORY[0x1E0DEC708]();
}

uint64_t ContiguousArray.init<A>(_:)()
{
  return MEMORY[0x1E0DEC718]();
}

uint64_t _arrayForceCast<A, B>(_:)()
{
  return MEMORY[0x1E0DEC8D8]();
}

uint64_t _print_unlocked<A, B>(_:_:)()
{
  return MEMORY[0x1E0DEC8E0]();
}

uint64_t static FixedWidthInteger.random<A>(in:using:)()
{
  return MEMORY[0x1E0DECA58]();
}

uint64_t static FixedWidthInteger.random(in:)()
{
  return MEMORY[0x1E0DECA68]();
}

uint64_t FixedWidthInteger.init<A>(exactly:)()
{
  return MEMORY[0x1E0DECA78]();
}

uint64_t _assertionFailure(_:_:file:line:flags:)()
{
  return MEMORY[0x1E0DECBE8]();
}

uint64_t _ContiguousArrayBuffer.firstElementAddress.getter()
{
  return MEMORY[0x1E0DED198]();
}

uint64_t dispatch thunk of CustomStringConvertible.description.getter()
{
  return MEMORY[0x1E0DED1D0]();
}

uint64_t dispatch thunk of ExpressibleByFloatLiteral.init(floatLiteral:)()
{
  return MEMORY[0x1E0DED4C0]();
}

uint64_t isKnownUniquelyReferenced<A>(_:)()
{
  return MEMORY[0x1E0DED540]();
}

uint64_t dispatch thunk of ExpressibleByIntegerLiteral.init(integerLiteral:)()
{
  return MEMORY[0x1E0DED5C0]();
}

uint64_t dispatch thunk of _ExpressibleByBuiltinFloatLiteral.init(_builtinFloatLiteral:)()
{
  return MEMORY[0x1E0DED9D0]();
}

uint64_t dispatch thunk of _ExpressibleByBuiltinIntegerLiteral.init(_builtinIntegerLiteral:)()
{
  return MEMORY[0x1E0DED9E8]();
}

uint64_t ELEMENT_TYPE_OF_SET_VIOLATES_HASHABLE_REQUIREMENTS(_:)()
{
  return MEMORY[0x1E0DEDAE0]();
}

uint64_t Error._getEmbeddedNSError()()
{
  return MEMORY[0x1E0DEDB10]();
}

uint64_t Error._code.getter()
{
  return MEMORY[0x1E0DEDB18]();
}

uint64_t Error._domain.getter()
{
  return MEMORY[0x1E0DEDB20]();
}

uint64_t Error._userInfo.getter()
{
  return MEMORY[0x1E0DEDB28]();
}

uint64_t Error<>._code.getter()
{
  return MEMORY[0x1E0DEDB30]();
}

uint64_t static Hasher._hash(seed:_:)()
{
  return MEMORY[0x1E0DEDEF0]();
}

uint64_t Hasher.init(_seed:)()
{
  return MEMORY[0x1E0DEDEF8]();
}

Swift::Void __swiftcall Hasher._combine(_:)(Swift::UInt a1)
{
  MEMORY[0x1E0DEDF10](a1);
}

Swift::Void __swiftcall Hasher._combine(_:)(Swift::UInt32 a1)
{
  MEMORY[0x1E0DEDF28](*(_QWORD *)&a1);
}

Swift::Int __swiftcall Hasher._finalize()()
{
  return MEMORY[0x1E0DEDF40]();
}

uint64_t _typeName(_:qualified:)()
{
  return MEMORY[0x1E0DEE978]();
}

uint64_t BLASGetThreading()
{
  return MEMORY[0x1E0C8B478]();
}

uint64_t BLASSetThreading()
{
  return MEMORY[0x1E0C8B480]();
}

int BNNSArithmeticFilterApplyBackwardBatch(void *filter, size_t batch_size, size_t number_of_inputs, const void **in, const size_t *in_stride, BNNSNDArrayDescriptor **in_delta, const size_t *in_delta_stride, const void *out, const size_t out_stride, BNNSNDArrayDescriptor *out_delta, const size_t out_delta_stride)
{
  return MEMORY[0x1E0C8B498](filter, batch_size, number_of_inputs, in, in_stride, in_delta, in_delta_stride, out);
}

int BNNSArithmeticFilterApplyBatch(void *filter, size_t batch_size, size_t number_of_inputs, const void **in, const size_t *in_stride, void *out, size_t out_stride)
{
  return MEMORY[0x1E0C8B4A8](filter, batch_size, number_of_inputs, in, in_stride, out, out_stride);
}

int BNNSBandPart(const int num_lower, const int num_upper, const BNNSNDArrayDescriptor *input, BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8B4B0](*(_QWORD *)&num_lower, *(_QWORD *)&num_upper, input, output, filter_params);
}

int BNNSClipByGlobalNorm(BNNSNDArrayDescriptor **dest, const BNNSNDArrayDescriptor **src, size_t count, float max_norm, float use_norm)
{
  return MEMORY[0x1E0C8B4C0](dest, src, count, max_norm, use_norm);
}

int BNNSClipByNorm(BNNSNDArrayDescriptor *dest, const BNNSNDArrayDescriptor *src, float max_norm, uint32_t axis_flags)
{
  return MEMORY[0x1E0C8B4C8](dest, src, *(_QWORD *)&axis_flags, max_norm);
}

int BNNSClipByValue(BNNSNDArrayDescriptor *dest, const BNNSNDArrayDescriptor *src, float min_val, float max_val)
{
  return MEMORY[0x1E0C8B4D0](dest, src, min_val, max_val);
}

int BNNSCompareTensor(const BNNSNDArrayDescriptor *in0, const BNNSNDArrayDescriptor *in1, BNNSRelationalOperator op, BNNSNDArrayDescriptor *out)
{
  return MEMORY[0x1E0C8B4D8](in0, in1, *(_QWORD *)&op, out);
}

int BNNSComputeNorm(BNNSNDArrayDescriptor *dest, const BNNSNDArrayDescriptor *src, BNNSNormType norm_type, uint32_t axis_flags)
{
  return MEMORY[0x1E0C8B4E8](dest, src, *(_QWORD *)&norm_type, *(_QWORD *)&axis_flags);
}

int BNNSComputeNormBackward(const void *in, BNNSNDArrayDescriptor *in_delta, const void *out, const BNNSNDArrayDescriptor *out_delta, BNNSNormType norm_type, uint32_t axis_flags)
{
  return MEMORY[0x1E0C8B4F0](in, in_delta, out, out_delta, *(_QWORD *)&norm_type, *(_QWORD *)&axis_flags);
}

int BNNSCopy(BNNSNDArrayDescriptor *dest, const BNNSNDArrayDescriptor *src, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8B500](dest, src, filter_params);
}

void *__cdecl BNNSCreateNearestNeighbors(const unsigned int max_n_samples, const unsigned int n_features, const unsigned int n_neighbors, const BNNSDataType data_type, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B508](*(_QWORD *)&max_n_samples, *(_QWORD *)&n_features, *(_QWORD *)&n_neighbors, *(_QWORD *)&data_type, filter_params);
}

void *__cdecl BNNSCreateRandomGenerator(BNNSRandomGeneratorMethod method, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B510](*(_QWORD *)&method, filter_params);
}

void *__cdecl BNNSCreateRandomGeneratorWithSeed(BNNSRandomGeneratorMethod method, uint64_t seed, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B518](*(_QWORD *)&method, seed, filter_params);
}

int BNNSCropResize(const BNNSLayerParametersCropResize *layer_params, const BNNSNDArrayDescriptor *input, const BNNSNDArrayDescriptor *roi, BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8B520](layer_params, input, roi, output, filter_params);
}

int BNNSCropResizeBackward(const BNNSLayerParametersCropResize *layer_params, BNNSNDArrayDescriptor *in_delta, const BNNSNDArrayDescriptor *roi, const BNNSNDArrayDescriptor *out_delta, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8B528](layer_params, in_delta, roi, out_delta, filter_params);
}

void BNNSDestroyNearestNeighbors(void *knn)
{
  MEMORY[0x1E0C8B540](knn);
}

void BNNSDestroyRandomGenerator(void *generator)
{
  MEMORY[0x1E0C8B548](generator);
}

int BNNSDirectApplyActivationBatch(const BNNSLayerParametersActivation *layer_params, const BNNSFilterParameters *filter_params, size_t batch_size, size_t in_stride, size_t out_stride)
{
  return MEMORY[0x1E0C8B550](layer_params, filter_params, batch_size, in_stride, out_stride);
}

int BNNSDirectApplyQuantizer(const BNNSLayerParametersQuantization *layer_params, const BNNSFilterParameters *filter_params, size_t batch_size, size_t input_stride, size_t output_stride)
{
  return MEMORY[0x1E0C8B588](layer_params, filter_params, batch_size, input_stride, output_stride);
}

int BNNSDirectApplyReduction(const BNNSLayerParametersReduction *layer_params, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8B598](layer_params, filter_params);
}

int BNNSFilterApplyBackwardBatch(void *filter, size_t batch_size, const void *in, size_t in_stride, BNNSNDArrayDescriptor *in_delta, size_t in_delta_stride, const void *out, size_t out_stride, const BNNSNDArrayDescriptor *out_delta, size_t out_delta_stride, BNNSNDArrayDescriptor *weights_delta, BNNSNDArrayDescriptor *bias_delta)
{
  return MEMORY[0x1E0C8B5C0](filter, batch_size, in, in_stride, in_delta, in_delta_stride, out, out_stride);
}

int BNNSFilterApplyBackwardTwoInputBatch(void *filter, size_t batch_size, const void *inA, size_t inA_stride, BNNSNDArrayDescriptor *inA_delta, size_t inA_delta_stride, const void *inB, size_t inB_stride, BNNSNDArrayDescriptor *inB_delta, size_t inB_delta_stride, const void *out, size_t out_stride, const BNNSNDArrayDescriptor *out_delta, size_t out_delta_stride, BNNSNDArrayDescriptor *weights_delta, BNNSNDArrayDescriptor *bias_delta)
{
  return MEMORY[0x1E0C8B5C8](filter, batch_size, inA, inA_stride, inA_delta, inA_delta_stride, inB, inB_stride);
}

int BNNSFilterApplyBatch(void *filter, size_t batch_size, const void *in, size_t in_stride, void *out, size_t out_stride)
{
  return MEMORY[0x1E0C8B5D0](filter, batch_size, in, in_stride, out, out_stride);
}

int BNNSFilterApplyTwoInputBatch(void *filter, size_t batch_size, const void *inA, size_t inA_stride, const void *inB, size_t inB_stride, void *out, size_t out_stride)
{
  return MEMORY[0x1E0C8B5E8](filter, batch_size, inA, inA_stride, inB, inB_stride, out, out_stride);
}

void *__cdecl BNNSFilterCreateFusedLayer(const size_t number_of_fused_filters, const BNNSFilterType *filter_type, const void **layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B610](number_of_fused_filters, filter_type, layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerActivation(const BNNSLayerParametersActivation *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B618](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerArithmetic(const BNNSLayerParametersArithmetic *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B628](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerBroadcastMatMul(const BNNSLayerParametersBroadcastMatMul *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B638](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerConvolution(const BNNSLayerParametersConvolution *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B648](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerDropout(const BNNSLayerParametersDropout *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B650](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerEmbedding(const BNNSLayerParametersEmbedding *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B658](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerFullyConnected(const BNNSLayerParametersFullyConnected *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B668](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerGram(const BNNSLayerParametersGram *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B670](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerLoss(const void *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B678](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerNormalization(BNNSFilterType normType, const BNNSLayerParametersNormalization *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B688](*(_QWORD *)&normType, layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerPadding(const BNNSLayerParametersPadding *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B690](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerPermute(const BNNSLayerParametersPermute *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B698](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerPooling(const BNNSLayerParametersPooling *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B6A0](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerReduction(const BNNSLayerParametersReduction *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B6B0](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerResize(const BNNSLayerParametersResize *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B6B8](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerTransposedConvolution(const BNNSLayerParametersConvolution *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1E0C8B6C0](layer_params, filter_params);
}

void BNNSFilterDestroy(void *filter)
{
  MEMORY[0x1E0C8B6E8](filter);
}

int BNNSFusedFilterApplyBackwardBatch(void *filter, size_t batch_size, const void *in, size_t in_stride, BNNSNDArrayDescriptor *in_delta, size_t in_delta_stride, const void *out, size_t out_stride, BNNSNDArrayDescriptor *out_delta, size_t out_delta_stride, BNNSNDArrayDescriptor **delta_parameters)
{
  return MEMORY[0x1E0C8B700](filter, batch_size, in, in_stride, in_delta, in_delta_stride, out, out_stride);
}

int BNNSFusedFilterApplyBackwardMultiInputBatch(void *filter, size_t batch_size, size_t number_of_inputs, const void **in, const size_t *in_stride, BNNSNDArrayDescriptor **in_delta, const size_t *in_delta_stride, const void *out, const size_t out_stride, BNNSNDArrayDescriptor *out_delta, size_t out_delta_stride, BNNSNDArrayDescriptor **delta_parameters)
{
  return MEMORY[0x1E0C8B708](filter, batch_size, number_of_inputs, in, in_stride, in_delta, in_delta_stride, out);
}

int BNNSFusedFilterApplyBatch(void *filter, size_t batch_size, const void *in, size_t in_stride, void *out, size_t out_stride, BOOL training)
{
  return MEMORY[0x1E0C8B710](filter, batch_size, in, in_stride, out, out_stride, training);
}

int BNNSFusedFilterApplyMultiInputBatch(void *filter, size_t batch_size, size_t number_of_inputs, const void **in, const size_t *in_stride, void *out, size_t out_stride, BOOL training)
{
  return MEMORY[0x1E0C8B718](filter, batch_size, number_of_inputs, in, in_stride, out, out_stride, training);
}

int BNNSGather(size_t axis, const BNNSNDArrayDescriptor *input, const BNNSNDArrayDescriptor *indices, BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8B720](axis, input, indices, output, filter_params);
}

int BNNSGatherND(const BNNSNDArrayDescriptor *input, const BNNSNDArrayDescriptor *indices, BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8B728](input, indices, output, filter_params);
}

uint64_t BNNSGraphCompileFromFile_v2()
{
  return MEMORY[0x1E0C8B730]();
}

uint64_t BNNSGraphCompileOptionsDestroy()
{
  return MEMORY[0x1E0C8B748]();
}

uint64_t BNNSGraphCompileOptionsGetGenerateDebugInfo()
{
  return MEMORY[0x1E0C8B750]();
}

uint64_t BNNSGraphCompileOptionsGetOptimizationPreference()
{
  return MEMORY[0x1E0C8B758]();
}

uint64_t BNNSGraphCompileOptionsGetTargetSingleThread()
{
  return MEMORY[0x1E0C8B760]();
}

uint64_t BNNSGraphCompileOptionsMakeDefault()
{
  return MEMORY[0x1E0C8B768]();
}

uint64_t BNNSGraphCompileOptionsSetGenerateDebugInfo()
{
  return MEMORY[0x1E0C8B790]();
}

uint64_t BNNSGraphCompileOptionsSetOptimizationPreference()
{
  return MEMORY[0x1E0C8B7C0]();
}

uint64_t BNNSGraphCompileOptionsSetTargetSingleThread()
{
  return MEMORY[0x1E0C8B7F8]();
}

uint64_t BNNSGraphContextDestroy_v2()
{
  return MEMORY[0x1E0C8B820]();
}

uint64_t BNNSGraphContextEnableNanAndInfChecks()
{
  return MEMORY[0x1E0C8B828]();
}

uint64_t BNNSGraphContextExecute_v2()
{
  return MEMORY[0x1E0C8B830]();
}

uint64_t BNNSGraphContextGetTensor()
{
  return MEMORY[0x1E0C8B848]();
}

uint64_t BNNSGraphContextMake()
{
  return MEMORY[0x1E0C8B868]();
}

uint64_t BNNSGraphContextSetArgumentType()
{
  return MEMORY[0x1E0C8B888]();
}

uint64_t BNNSGraphContextSetBatchSize_v2()
{
  return MEMORY[0x1E0C8B898]();
}

uint64_t BNNSGraphContextSetDynamicShapes_v2()
{
  return MEMORY[0x1E0C8B8A0]();
}

uint64_t BNNSGraphGetArgumentCount()
{
  return MEMORY[0x1E0C8B8D8]();
}

uint64_t BNNSGraphGetArgumentNames()
{
  return MEMORY[0x1E0C8B8E8]();
}

uint64_t BNNSGraphGetArgumentPosition()
{
  return MEMORY[0x1E0C8B8F0]();
}

uint64_t BNNSGraphGetFunctionCount()
{
  return MEMORY[0x1E0C8B900]();
}

uint64_t BNNSGraphGetFunctionNames()
{
  return MEMORY[0x1E0C8B910]();
}

int BNNSLossFilterApplyBatch(void *filter, size_t batch_size, const void *in, size_t in_stride, const void *labels, size_t labels_stride, const void *weights, size_t weights_size, void *out, BNNSNDArrayDescriptor *in_delta, size_t in_delta_stride)
{
  return MEMORY[0x1E0C8B978](filter, batch_size, in, in_stride, labels, labels_stride, weights, weights_size);
}

int BNNSMatMul(const BOOL transA, const BOOL transB, const float alpha, const BNNSNDArrayDescriptor *inputA, const BNNSNDArrayDescriptor *inputB, const BNNSNDArrayDescriptor *output, void *workspace, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8B980](transA, transB, inputA, inputB, output, workspace, filter_params, alpha);
}

ssize_t BNNSMatMulWorkspaceSize(const BOOL transA, const BOOL transB, const float alpha, const BNNSNDArrayDescriptor *inputA, const BNNSNDArrayDescriptor *inputB, const BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8B988](transA, transB, inputA, inputB, output, filter_params, alpha);
}

int BNNSNDArrayFullyConnectedSparsifySparseCOO(const BNNSNDArrayDescriptor *in_dense_shape, const BNNSNDArrayDescriptor *in_indices, const BNNSNDArrayDescriptor *in_values, BNNSNDArrayDescriptor *out, const BNNSSparsityParameters *sparse_params, const size_t batch_size, void *workspace, const size_t workspace_size, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8B9A0](in_dense_shape, in_indices, in_values, out, sparse_params, batch_size, workspace, workspace_size);
}

int BNNSNDArrayFullyConnectedSparsifySparseCSR(const BNNSNDArrayDescriptor *in_dense_shape, const BNNSNDArrayDescriptor *in_column_indices, const BNNSNDArrayDescriptor *in_row_starts, const BNNSNDArrayDescriptor *in_values, BNNSNDArrayDescriptor *out, const BNNSSparsityParameters *sparse_params, const size_t batch_size, void *workspace, const size_t workspace_size, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8B9A8](in_dense_shape, in_column_indices, in_row_starts, in_values, out, sparse_params, batch_size, workspace);
}

size_t BNNSNDArrayGetDataSize(const BNNSNDArrayDescriptor *array)
{
  return MEMORY[0x1E0C8B9B0](array);
}

int BNNSNearestNeighborsGetInfo(void *knn, const int sample_number, int *indices, void *distances)
{
  return MEMORY[0x1E0C8B9B8](knn, *(_QWORD *)&sample_number, indices, distances);
}

int BNNSNearestNeighborsLoad(void *knn, const unsigned int n_new_samples, const void *data_ptr)
{
  return MEMORY[0x1E0C8B9C0](knn, *(_QWORD *)&n_new_samples, data_ptr);
}

int BNNSNormalizationFilterApplyBackwardBatch(void *filter, size_t batch_size, BNNSNDArrayDescriptor *in_delta, size_t in_delta_stride, const void *out, size_t out_stride, BNNSNDArrayDescriptor *out_delta, size_t out_delta_stride, BNNSNDArrayDescriptor *beta_delta, BNNSNDArrayDescriptor *gamma_delta)
{
  return MEMORY[0x1E0C8B9C8](filter, batch_size, in_delta, in_delta_stride, out, out_stride, out_delta, out_delta_stride);
}

int BNNSNormalizationFilterApplyBatch(void *filter, size_t batch_size, const void *in, size_t in_stride, void *out, size_t out_stride, BOOL training)
{
  return MEMORY[0x1E0C8B9D0](filter, batch_size, in, in_stride, out, out_stride, training);
}

int BNNSOptimizerStep(BNNSOptimizerFunction function, const void *OptimizerAlgFields, size_t number_of_parameters, BNNSNDArrayDescriptor **parameters, const BNNSNDArrayDescriptor **gradients, BNNSNDArrayDescriptor **accumulators, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8B9D8](*(_QWORD *)&function, OptimizerAlgFields, number_of_parameters, parameters, gradients, accumulators, filter_params);
}

int BNNSPoolingFilterApplyBackwardBatch(void *filter, size_t batch_size, const void *in, size_t in_stride, BNNSNDArrayDescriptor *in_delta, size_t in_delta_stride, const void *out, size_t out_stride, BNNSNDArrayDescriptor *out_delta, size_t out_delta_stride, BNNSNDArrayDescriptor *bias_delta, const size_t *indices, size_t idx_stride)
{
  return MEMORY[0x1E0C8B9E0](filter, batch_size, in, in_stride, in_delta, in_delta_stride, out, out_stride);
}

int BNNSPoolingFilterApplyBackwardBatchEx(void *filter, size_t batch_size, const void *in, size_t in_stride, BNNSNDArrayDescriptor *in_delta, size_t in_delta_stride, const void *out, size_t out_stride, BNNSNDArrayDescriptor *out_delta, size_t out_delta_stride, BNNSNDArrayDescriptor *bias_delta, const BNNSDataType indices_data_type, const void *indices, size_t idx_stride)
{
  return MEMORY[0x1E0C8B9E8](filter, batch_size, in, in_stride, in_delta, in_delta_stride, out, out_stride);
}

int BNNSPoolingFilterApplyBatch(void *filter, size_t batch_size, const void *in, size_t in_stride, void *out, size_t out_stride, size_t *indices, size_t idx_stride)
{
  return MEMORY[0x1E0C8B9F0](filter, batch_size, in, in_stride, out, out_stride, indices, idx_stride);
}

int BNNSPoolingFilterApplyBatchEx(void *filter, size_t batch_size, const void *in, size_t in_stride, void *out, size_t out_stride, const BNNSDataType indices_data_type, void *indices, size_t idx_stride)
{
  return MEMORY[0x1E0C8B9F8](filter, batch_size, in, in_stride, out, out_stride, *(_QWORD *)&indices_data_type, indices);
}

int BNNSRandomFillNormalFloat(void *generator, BNNSNDArrayDescriptor *desc, float mean, float stddev)
{
  return MEMORY[0x1E0C8BA00](generator, desc, mean, stddev);
}

int BNNSRandomFillUniformFloat(void *generator, BNNSNDArrayDescriptor *desc, float a, float b)
{
  return MEMORY[0x1E0C8BA08](generator, desc, a, b);
}

int BNNSRandomFillUniformInt(void *generator, BNNSNDArrayDescriptor *desc, int64_t a, int64_t b)
{
  return MEMORY[0x1E0C8BA10](generator, desc, a, b);
}

int BNNSRandomGeneratorGetState(void *generator, size_t state_size, void *state)
{
  return MEMORY[0x1E0C8BA18](generator, state_size, state);
}

int BNNSRandomGeneratorSetState(void *generator, size_t state_size, void *state)
{
  return MEMORY[0x1E0C8BA20](generator, state_size, state);
}

size_t BNNSRandomGeneratorStateSize(void *generator)
{
  return MEMORY[0x1E0C8BA28](generator);
}

int BNNSScatter(size_t axis, BNNSReduceFunction op, const BNNSNDArrayDescriptor *input, const BNNSNDArrayDescriptor *indices, BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8BA30](axis, *(_QWORD *)&op, input, indices, output, filter_params);
}

int BNNSScatterND(BNNSReduceFunction op, const BNNSNDArrayDescriptor *input, const BNNSNDArrayDescriptor *indices, BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8BA38](*(_QWORD *)&op, input, indices, output, filter_params);
}

int BNNSShuffle(const BNNSShuffleType type, const BNNSNDArrayDescriptor *input, BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8BA48](*(_QWORD *)&type, input, output, filter_params);
}

int BNNSTile(const BNNSNDArrayDescriptor *input, BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8BA50](input, output, filter_params);
}

int BNNSTileBackward(BNNSNDArrayDescriptor *in_delta, const BNNSNDArrayDescriptor *out_delta, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8BA58](in_delta, out_delta, filter_params);
}

int BNNSTranspose(BNNSNDArrayDescriptor *dest, const BNNSNDArrayDescriptor *src, size_t axis0, size_t axis1, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1E0C8BA60](dest, src, axis0, axis1, filter_params);
}

CGColorSpaceModel CGColorSpaceGetModel(CGColorSpaceRef space)
{
  return MEMORY[0x1E0C9BEC8](space);
}

CGBitmapInfo CGImageGetBitmapInfo(CGImageRef image)
{
  return MEMORY[0x1E0C9CBF0](image);
}

size_t CGImageGetBitsPerComponent(CGImageRef image)
{
  return MEMORY[0x1E0C9CBF8](image);
}

size_t CGImageGetBitsPerPixel(CGImageRef image)
{
  return MEMORY[0x1E0C9CC00](image);
}

CGColorSpaceRef CGImageGetColorSpace(CGImageRef image)
{
  return (CGColorSpaceRef)MEMORY[0x1E0C9CC20](image);
}

CGColorRenderingIntent CGImageGetRenderingIntent(CGImageRef image)
{
  return MEMORY[0x1E0C9CCA0](image);
}

CGFloat CGRectGetHeight(CGRect rect)
{
  CGFloat result;

  MEMORY[0x1E0C9D580]((__n128)rect.origin, *(__n128 *)&rect.origin.y, (__n128)rect.size, *(__n128 *)&rect.size.height);
  return result;
}

CGFloat CGRectGetWidth(CGRect rect)
{
  CGFloat result;

  MEMORY[0x1E0C9D5D0]((__n128)rect.origin, *(__n128 *)&rect.origin.y, (__n128)rect.size, *(__n128 *)&rect.size.height);
  return result;
}

CGRect CGRectIntegral(CGRect rect)
{
  double v1;
  double v2;
  double v3;
  double v4;
  CGRect result;

  MEMORY[0x1E0C9D5F0]((__n128)rect.origin, *(__n128 *)&rect.origin.y, (__n128)rect.size, *(__n128 *)&rect.size.height);
  result.size.height = v4;
  result.size.width = v3;
  result.origin.y = v2;
  result.origin.x = v1;
  return result;
}

CGRect CGRectIntersection(CGRect r1, CGRect r2)
{
  double v2;
  double v3;
  double v4;
  double v5;
  CGRect result;

  MEMORY[0x1E0C9D5F8]((__n128)r1.origin, *(__n128 *)&r1.origin.y, (__n128)r1.size, *(__n128 *)&r1.size.height, (__n128)r2.origin, *(__n128 *)&r2.origin.y, (__n128)r2.size, *(__n128 *)&r2.size.height);
  result.size.height = v5;
  result.size.width = v4;
  result.origin.y = v3;
  result.origin.x = v2;
  return result;
}

BOOL CGRectIsEmpty(CGRect rect)
{
  return MEMORY[0x1E0C9D608]((__n128)rect.origin, *(__n128 *)&rect.origin.y, (__n128)rect.size, *(__n128 *)&rect.size.height);
}

CVReturn CVPixelBufferCreateWithBytes(CFAllocatorRef allocator, size_t width, size_t height, OSType pixelFormatType, void *baseAddress, size_t bytesPerRow, CVPixelBufferReleaseBytesCallback releaseCallback, void *releaseRefCon, CFDictionaryRef pixelBufferAttributes, CVPixelBufferRef *pixelBufferOut)
{
  return MEMORY[0x1E0CA8AB8](allocator, width, height, *(_QWORD *)&pixelFormatType, baseAddress, bytesPerRow, releaseCallback, releaseRefCon);
}

void *__cdecl CVPixelBufferGetBaseAddressOfPlane(CVPixelBufferRef pixelBuffer, size_t planeIndex)
{
  return (void *)MEMORY[0x1E0CA8AE8](pixelBuffer, planeIndex);
}

size_t CVPixelBufferGetBytesPerRowOfPlane(CVPixelBufferRef pixelBuffer, size_t planeIndex)
{
  return MEMORY[0x1E0CA8B00](pixelBuffer, planeIndex);
}

size_t CVPixelBufferGetHeight(CVPixelBufferRef pixelBuffer)
{
  return MEMORY[0x1E0CA8B20](pixelBuffer);
}

size_t CVPixelBufferGetHeightOfPlane(CVPixelBufferRef pixelBuffer, size_t planeIndex)
{
  return MEMORY[0x1E0CA8B28](pixelBuffer, planeIndex);
}

size_t CVPixelBufferGetPlaneCount(CVPixelBufferRef pixelBuffer)
{
  return MEMORY[0x1E0CA8B40](pixelBuffer);
}

size_t CVPixelBufferGetWidth(CVPixelBufferRef pixelBuffer)
{
  return MEMORY[0x1E0CA8B50](pixelBuffer);
}

size_t CVPixelBufferGetWidthOfPlane(CVPixelBufferRef pixelBuffer, size_t planeIndex)
{
  return MEMORY[0x1E0CA8B58](pixelBuffer, planeIndex);
}

CVReturn CVPixelBufferLockBaseAddress(CVPixelBufferRef pixelBuffer, CVPixelBufferLockFlags lockFlags)
{
  return MEMORY[0x1E0CA8B70](pixelBuffer, lockFlags);
}

CVReturn CVPixelBufferUnlockBaseAddress(CVPixelBufferRef pixelBuffer, CVPixelBufferLockFlags unlockFlags)
{
  return MEMORY[0x1E0CA8C00](pixelBuffer, unlockFlags);
}

uint64_t _availability_version_check()
{
  return MEMORY[0x1E0C80CC0]();
}

uint64_t _swift_isClassOrObjCExistentialType()
{
  return MEMORY[0x1E0DEE9F8]();
}

uint64_t _swift_stdlib_reportUnimplementedInitializer()
{
  return MEMORY[0x1E0DEEA40]();
}

void bzero(void *a1, size_t a2)
{
  MEMORY[0x1E0C81758](a1, a2);
}

void dispatch_once_f(dispatch_once_t *predicate, void *context, dispatch_function_t function)
{
  MEMORY[0x1E0C82E10](predicate, context, function);
}

void *__cdecl dlsym(void *__handle, const char *__symbol)
{
  return (void *)MEMORY[0x1E0C83050](__handle, __symbol);
}

int fclose(FILE *a1)
{
  return MEMORY[0x1E0C832F8](a1);
}

FILE *__cdecl fopen(const char *__filename, const char *__mode)
{
  return (FILE *)MEMORY[0x1E0C83460](__filename, __mode);
}

size_t fread(void *__ptr, size_t __size, size_t __nitems, FILE *__stream)
{
  return MEMORY[0x1E0C834A0](__ptr, __size, __nitems, __stream);
}

void free(void *a1)
{
  MEMORY[0x1E0C834A8](a1);
}

int fseek(FILE *a1, uint64_t a2, int a3)
{
  return MEMORY[0x1E0C83530](a1, a2, *(_QWORD *)&a3);
}

uint64_t ftell(FILE *a1)
{
  return MEMORY[0x1E0C83580](a1);
}

float log2f(float a1)
{
  float result;

  MEMORY[0x1E0C83BF0](a1);
  return result;
}

void *__cdecl malloc(size_t __size)
{
  return (void *)MEMORY[0x1E0C83E68](__size);
}

size_t malloc_size(const void *ptr)
{
  return MEMORY[0x1E0C83EE0](ptr);
}

void *__cdecl memcpy(void *__dst, const void *__src, size_t __n)
{
  return (void *)MEMORY[0x1E0C84088](__dst, __src, __n);
}

void *__cdecl memmove(void *__dst, const void *__src, size_t __len)
{
  return (void *)MEMORY[0x1E0C84098](__dst, __src, __len);
}

uint64_t objc_opt_self()
{
  return MEMORY[0x1E0DE7D58]();
}

void objc_release(id a1)
{
  MEMORY[0x1E0DE7D78](a1);
}

id objc_retain(id a1)
{
  return (id)MEMORY[0x1E0DE7E50](a1);
}

id objc_retainAutoreleasedReturnValue(id a1)
{
  return (id)MEMORY[0x1E0DE7E68](a1);
}

double quadrature_integrate(const quadrature_integrate_function *__f, double __a, double __b, const quadrature_integrate_options *options, quadrature_status *status, double *abs_error, size_t workspace_size, void *workspace)
{
  double result;

  MEMORY[0x1E0C8BDC0](__f, options, status, abs_error, workspace_size, workspace, __a, __b);
  return result;
}

void rewind(FILE *a1)
{
  MEMORY[0x1E0C85120](a1);
}

int sscanf(const char *a1, const char *a2, ...)
{
  return MEMORY[0x1E0C85498](a1, a2);
}

uint64_t swift_allocBox()
{
  return MEMORY[0x1E0DEEA98]();
}

uint64_t swift_allocError()
{
  return MEMORY[0x1E0DEEAA8]();
}

uint64_t swift_allocObject()
{
  return MEMORY[0x1E0DEEAB0]();
}

uint64_t swift_allocateGenericClassMetadata()
{
  return MEMORY[0x1E0DEEAB8]();
}

uint64_t swift_allocateGenericValueMetadata()
{
  return MEMORY[0x1E0DEEAC0]();
}

uint64_t swift_arrayDestroy()
{
  return MEMORY[0x1E0DEEAD8]();
}

uint64_t swift_arrayInitWithCopy()
{
  return MEMORY[0x1E0DEEAE0]();
}

uint64_t swift_beginAccess()
{
  return MEMORY[0x1E0DEEAF8]();
}

uint64_t swift_bridgeObjectRelease()
{
  return MEMORY[0x1E0DEEB08]();
}

uint64_t swift_bridgeObjectRelease_n()
{
  return MEMORY[0x1E0DEEB10]();
}

uint64_t swift_bridgeObjectRetain()
{
  return MEMORY[0x1E0DEEB18]();
}

uint64_t swift_bridgeObjectRetain_n()
{
  return MEMORY[0x1E0DEEB20]();
}

uint64_t swift_checkMetadataState()
{
  return MEMORY[0x1E0DEEB28]();
}

uint64_t swift_deallocClassInstance()
{
  return MEMORY[0x1E0DEEB50]();
}

uint64_t swift_deallocObject()
{
  return MEMORY[0x1E0DEEB60]();
}

uint64_t swift_deallocPartialClassInstance()
{
  return MEMORY[0x1E0DEEB68]();
}

uint64_t swift_dynamicCast()
{
  return MEMORY[0x1E0DEEB80]();
}

uint64_t swift_dynamicCastMetatype()
{
  return MEMORY[0x1E0DEEB98]();
}

uint64_t swift_endAccess()
{
  return MEMORY[0x1E0DEEBE0]();
}

uint64_t swift_errorRelease()
{
  return MEMORY[0x1E0DEEBF0]();
}

uint64_t swift_getAssociatedConformanceWitness()
{
  return MEMORY[0x1E0DEEC00]();
}

uint64_t swift_getAssociatedTypeWitness()
{
  return MEMORY[0x1E0DEEC08]();
}

uint64_t swift_getForeignTypeMetadata()
{
  return MEMORY[0x1E0DEEC78]();
}

uint64_t swift_getGenericMetadata()
{
  return MEMORY[0x1E0DEECB0]();
}

uint64_t swift_getTupleTypeMetadata2()
{
  return MEMORY[0x1E0DEED20]();
}

uint64_t swift_getTypeByMangledNameInContext2()
{
  return MEMORY[0x1E0DEED38]();
}

uint64_t swift_getTypeByMangledNameInContextInMetadataState2()
{
  return MEMORY[0x1E0DEED40]();
}

uint64_t swift_getWitnessTable()
{
  return MEMORY[0x1E0DEED50]();
}

uint64_t swift_initClassMetadata2()
{
  return MEMORY[0x1E0DEED58]();
}

uint64_t swift_initEnumMetadataSinglePayload()
{
  return MEMORY[0x1E0DEED70]();
}

uint64_t swift_initStackObject()
{
  return MEMORY[0x1E0DEED78]();
}

uint64_t swift_isEscapingClosureAtFileLocation()
{
  return MEMORY[0x1E0DEED98]();
}

uint64_t swift_isUniquelyReferenced_nonNull_native()
{
  return MEMORY[0x1E0DEEDD0]();
}

uint64_t swift_lookUpClassMethod()
{
  return MEMORY[0x1E0DEEDE0]();
}

uint64_t swift_makeBoxUnique()
{
  return MEMORY[0x1E0DEEDE8]();
}

uint64_t swift_once()
{
  return MEMORY[0x1E0DEEE00]();
}

uint64_t swift_release()
{
  return MEMORY[0x1E0DEEE30]();
}

uint64_t swift_retain()
{
  return MEMORY[0x1E0DEEE48]();
}

uint64_t swift_setDeallocating()
{
  return MEMORY[0x1E0DEEE70]();
}

uint64_t swift_slowAlloc()
{
  return MEMORY[0x1E0DEEE78]();
}

uint64_t swift_slowDealloc()
{
  return MEMORY[0x1E0DEEE80]();
}

uint64_t swift_task_alloc()
{
  return MEMORY[0x1E0DF1000]();
}

uint64_t swift_task_dealloc()
{
  return MEMORY[0x1E0DF1010]();
}

uint64_t swift_task_switch()
{
  return MEMORY[0x1E0DF1050]();
}

uint64_t swift_unexpectedError()
{
  return MEMORY[0x1E0DEEEB8]();
}

uint64_t swift_unknownObjectRelease()
{
  return MEMORY[0x1E0DEEEC0]();
}

uint64_t swift_unknownObjectRetain()
{
  return MEMORY[0x1E0DEEED0]();
}

uint64_t swift_willThrow()
{
  return MEMORY[0x1E0DEEFC0]();
}

vDSP_DFT_Setup vDSP_DCT_CreateSetup(vDSP_DFT_Setup __Previous, vDSP_Length __Length, vDSP_DCT_Type __Type)
{
  return (vDSP_DFT_Setup)MEMORY[0x1E0C8BF40](__Previous, __Length, *(_QWORD *)&__Type);
}

void vDSP_DCT_Execute(const vDSP_DFT_SetupStruct *__Setup, const float *__Input, float *__Output)
{
  MEMORY[0x1E0C8BF48](__Setup, __Input, __Output);
}

void vDSP_DFT_DestroySetup(vDSP_DFT_Setup __Setup)
{
  MEMORY[0x1E0C8BF50](__Setup);
}

void vDSP_DFT_DestroySetupD(vDSP_DFT_SetupD __Setup)
{
  MEMORY[0x1E0C8BF58](__Setup);
}

void vDSP_DFT_Interleaved_DestroySetup(vDSP_DFT_Interleaved_Setup Setup)
{
  MEMORY[0x1E0C8BF80](Setup);
}

void vDSP_DFT_Interleaved_DestroySetupD(vDSP_DFT_Interleaved_SetupD Setup)
{
  MEMORY[0x1E0C8BF88](Setup);
}

void vDSP_biquadm(vDSP_biquadm_Setup __Setup, const float **__X, vDSP_Stride __IX, float **__Y, vDSP_Stride __IY, vDSP_Length __N)
{
  MEMORY[0x1E0C8BFF0](__Setup, __X, __IX, __Y, __IY, __N);
}

void vDSP_biquadmD(vDSP_biquadm_SetupD __Setup, const double **__X, vDSP_Stride __IX, double **__Y, vDSP_Stride __IY, vDSP_Length __N)
{
  MEMORY[0x1E0C8BFF8](__Setup, __X, __IX, __Y, __IY, __N);
}

void vDSP_conv(const float *__A, vDSP_Stride __IA, const float *__F, vDSP_Stride __IF, float *__C, vDSP_Stride __IC, vDSP_Length __N, vDSP_Length __P)
{
  MEMORY[0x1E0C8C050](__A, __IA, __F, __IF, __C, __IC, __N, __P);
}

void vDSP_convD(const double *__A, vDSP_Stride __IA, const double *__F, vDSP_Stride __IF, double *__C, vDSP_Stride __IC, vDSP_Length __N, vDSP_Length __P)
{
  MEMORY[0x1E0C8C058](__A, __IA, __F, __IF, __C, __IC, __N, __P);
}

FFTSetup vDSP_create_fftsetup(vDSP_Length __Log2n, FFTRadix __Radix)
{
  return (FFTSetup)MEMORY[0x1E0C8C060](__Log2n, *(_QWORD *)&__Radix);
}

FFTSetupD vDSP_create_fftsetupD(vDSP_Length __Log2n, FFTRadix __Radix)
{
  return (FFTSetupD)MEMORY[0x1E0C8C068](__Log2n, *(_QWORD *)&__Radix);
}

void vDSP_ctoz(const DSPComplex *__C, vDSP_Stride __IC, const DSPSplitComplex *__Z, vDSP_Stride __IZ, vDSP_Length __N)
{
  MEMORY[0x1E0C8C070](__C, __IC, __Z, __IZ, __N);
}

void vDSP_ctozD(const DSPDoubleComplex *__C, vDSP_Stride __IC, const DSPDoubleSplitComplex *__Z, vDSP_Stride __IZ, vDSP_Length __N)
{
  MEMORY[0x1E0C8C078](__C, __IC, __Z, __IZ, __N);
}

void vDSP_deq22(const float *__A, vDSP_Stride __IA, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C080](__A, __IA, __B, __C, __IC, __N);
}

void vDSP_deq22D(const double *__A, vDSP_Stride __IA, const double *__B, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C088](__A, __IA, __B, __C, __IC, __N);
}

void vDSP_destroy_fftsetup(FFTSetup __setup)
{
  MEMORY[0x1E0C8C0A0](__setup);
}

void vDSP_destroy_fftsetupD(FFTSetupD __setup)
{
  MEMORY[0x1E0C8C0A8](__setup);
}

void vDSP_fft_zrop(FFTSetup __Setup, const DSPSplitComplex *__A, vDSP_Stride __IA, const DSPSplitComplex *__C, vDSP_Stride __IC, vDSP_Length __Log2N, FFTDirection __Direction)
{
  MEMORY[0x1E0C8C140](__Setup, __A, __IA, __C, __IC, __Log2N, *(_QWORD *)&__Direction);
}

void vDSP_fft_zropD(FFTSetupD __Setup, const DSPDoubleSplitComplex *__A, vDSP_Stride __IA, const DSPDoubleSplitComplex *__C, vDSP_Stride __IC, vDSP_Length __Log2N, FFTDirection __Direction)
{
  MEMORY[0x1E0C8C148](__Setup, __A, __IA, __C, __IC, __Log2N, *(_QWORD *)&__Direction);
}

void vDSP_normalize(const float *__A, vDSP_Stride __IA, float *__C, vDSP_Stride __IC, float *__Mean, float *__StandardDeviation, vDSP_Length __N)
{
  MEMORY[0x1E0C8C280](__A, __IA, __C, __IC, __Mean, __StandardDeviation, __N);
}

void vDSP_normalizeD(const double *__A, vDSP_Stride __IA, double *__C, vDSP_Stride __IC, double *__Mean, double *__StandardDeviation, vDSP_Length __N)
{
  MEMORY[0x1E0C8C288](__A, __IA, __C, __IC, __Mean, __StandardDeviation, __N);
}

void vDSP_svdiv(const float *__A, const float *__B, vDSP_Stride __IB, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C2D8](__A, __B, __IB, __C, __IC, __N);
}

void vDSP_svdivD(const double *__A, const double *__B, vDSP_Stride __IB, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C2E0](__A, __B, __IB, __C, __IC, __N);
}

void vDSP_vclip(const float *__A, vDSP_Stride __IA, const float *__B, const float *__C, float *__D, vDSP_Stride __ID, vDSP_Length __N)
{
  MEMORY[0x1E0C8C3D8](__A, __IA, __B, __C, __D, __ID, __N);
}

void vDSP_vclipc(const float *__A, vDSP_Stride __IA, const float *__B, const float *__C, float *__D, vDSP_Stride __ID, vDSP_Length __N, vDSP_Length *__NLow, vDSP_Length *__NHigh)
{
  MEMORY[0x1E0C8C3F0](__A, __IA, __B, __C, __D, __ID, __N, __NLow);
}

void vDSP_vclipcD(const double *__A, vDSP_Stride __IA, const double *__B, const double *__C, double *__D, vDSP_Stride __ID, vDSP_Length __N, vDSP_Length *__NLow, vDSP_Length *__NHigh)
{
  MEMORY[0x1E0C8C3F8](__A, __IA, __B, __C, __D, __ID, __N, __NLow);
}

void vDSP_vdbcon(const float *__A, vDSP_Stride __IA, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N, unsigned int __F)
{
  MEMORY[0x1E0C8C420](__A, __IA, __B, __C, __IC, __N, *(_QWORD *)&__F);
}

void vDSP_vdbconD(const double *__A, vDSP_Stride __IA, const double *__B, double *__C, vDSP_Stride __IC, vDSP_Length __N, unsigned int __F)
{
  MEMORY[0x1E0C8C428](__A, __IA, __B, __C, __IC, __N, *(_QWORD *)&__F);
}

void vDSP_vfill(const float *__A, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C460](__A, __C, __IC, __N);
}

void vDSP_vfillD(const double *__A, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C470](__A, __C, __IC, __N);
}

void vDSP_vintb(const float *__A, vDSP_Stride __IA, const float *__B, vDSP_Stride __IB, const float *__C, float *__D, vDSP_Stride __ID, vDSP_Length __N)
{
  MEMORY[0x1E0C8C670](__A, __IA, __B, __IB, __C, __D, __ID, __N);
}

void vDSP_vintbD(const double *__A, vDSP_Stride __IA, const double *__B, vDSP_Stride __IB, const double *__C, double *__D, vDSP_Stride __ID, vDSP_Length __N)
{
  MEMORY[0x1E0C8C678](__A, __IA, __B, __IB, __C, __D, __ID, __N);
}

void vDSP_vlim(const float *__A, vDSP_Stride __IA, const float *__B, const float *__C, float *__D, vDSP_Stride __ID, vDSP_Length __N)
{
  MEMORY[0x1E0C8C680](__A, __IA, __B, __C, __D, __ID, __N);
}

void vDSP_vramp(const float *__A, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C7A0](__A, __B, __C, __IC, __N);
}

void vDSP_vrampD(const double *__A, const double *__B, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C7A8](__A, __B, __C, __IC, __N);
}

void vDSP_vrampmul(const float *__I, vDSP_Stride __IS, float *__Start, const float *__Step, float *__O, vDSP_Stride __OS, vDSP_Length __N)
{
  MEMORY[0x1E0C8C7B0](__I, __IS, __Start, __Step, __O, __OS, __N);
}

void vDSP_vrampmul2(const float *__I0, const float *__I1, vDSP_Stride __IS, float *__Start, const float *__Step, float *__O0, float *__O1, vDSP_Stride __OS, vDSP_Length __N)
{
  MEMORY[0x1E0C8C7B8](__I0, __I1, __IS, __Start, __Step, __O0, __O1, __OS);
}

void vDSP_vrampmul2D(const double *__I0, const double *__I1, vDSP_Stride __IS, double *__Start, const double *__Step, double *__O0, double *__O1, vDSP_Stride __OS, vDSP_Length __N)
{
  MEMORY[0x1E0C8C7C0](__I0, __I1, __IS, __Start, __Step, __O0, __O1, __OS);
}

void vDSP_vrampmulD(const double *__I, vDSP_Stride __IS, double *__Start, const double *__Step, double *__O, vDSP_Stride __OS, vDSP_Length __N)
{
  MEMORY[0x1E0C8C7C8](__I, __IS, __Start, __Step, __O, __OS, __N);
}

void vDSP_vrsum(const float *__A, vDSP_Stride __IA, const float *__S, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C7E0](__A, __IA, __S, __C, __IC, __N);
}

void vDSP_vrsumD(const double *__A, vDSP_Stride __IA, const double *__S, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C7E8](__A, __IA, __S, __C, __IC, __N);
}

void vDSP_vsdiv(const float *__A, vDSP_Stride __IA, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C860](__A, __IA, __B, __C, __IC, __N);
}

void vDSP_vsimps(const float *__A, vDSP_Stride __IA, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C870](__A, __IA, __B, __C, __IC, __N);
}

void vDSP_vsimpsD(const double *__A, vDSP_Stride __IA, const double *__B, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C878](__A, __IA, __B, __C, __IC, __N);
}

void vDSP_vsma(const float *__A, vDSP_Stride __IA, const float *__B, const float *__C, vDSP_Stride __IC, float *__D, vDSP_Stride __ID, vDSP_Length __N)
{
  MEMORY[0x1E0C8C888](__A, __IA, __B, __C, __IC, __D, __ID, __N);
}

void vDSP_vsmaD(const double *__A, vDSP_Stride __IA, const double *__B, const double *__C, vDSP_Stride __IC, double *__D, vDSP_Stride __ID, vDSP_Length __N)
{
  MEMORY[0x1E0C8C890](__A, __IA, __B, __C, __IC, __D, __ID, __N);
}

void vDSP_vsmsa(const float *__A, vDSP_Stride __IA, const float *__B, const float *__C, float *__D, vDSP_Stride __ID, vDSP_Length __N)
{
  MEMORY[0x1E0C8C898](__A, __IA, __B, __C, __D, __ID, __N);
}

void vDSP_vsmsaD(const double *__A, vDSP_Stride __IA, const double *__B, const double *__C, double *__D, vDSP_Stride __ID, vDSP_Length __N)
{
  MEMORY[0x1E0C8C8A8](__A, __IA, __B, __C, __D, __ID, __N);
}

void vDSP_vsmsma(const float *__A, vDSP_Stride __IA, const float *__B, const float *__C, vDSP_Stride __IC, const float *__D, float *__E, vDSP_Stride __IE, vDSP_Length __N)
{
  MEMORY[0x1E0C8C8C0](__A, __IA, __B, __C, __IC, __D, __E, __IE);
}

void vDSP_vsmsmaD(const double *__A, vDSP_Stride __IA, const double *__B, const double *__C, vDSP_Stride __IC, const double *__D, double *__E, vDSP_Stride __IE, vDSP_Length __N)
{
  MEMORY[0x1E0C8C8D0](__A, __IA, __B, __C, __IC, __D, __E, __IE);
}

void vDSP_vsmul(const float *__A, vDSP_Stride __IA, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C8E0](__A, __IA, __B, __C, __IC, __N);
}

void vDSP_vsmulD(const double *__A, vDSP_Stride __IA, const double *__B, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C8E8](__A, __IA, __B, __C, __IC, __N);
}

void vDSP_vtabi(const float *__A, vDSP_Stride __IA, const float *__S1, const float *__S2, const float *__C, vDSP_Length __M, float *__D, vDSP_Stride __ID, vDSP_Length __N)
{
  MEMORY[0x1E0C8C990](__A, __IA, __S1, __S2, __C, __M, __D, __ID);
}

void vDSP_vtabiD(const double *__A, vDSP_Stride __IA, const double *__S1, const double *__S2, const double *__C, vDSP_Length __M, double *__D, vDSP_Stride __ID, vDSP_Length __N)
{
  MEMORY[0x1E0C8C998](__A, __IA, __S1, __S2, __C, __M, __D, __ID);
}

void vDSP_vthr(const float *__A, vDSP_Stride __IA, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C9A8](__A, __IA, __B, __C, __IC, __N);
}

void vDSP_vthrD(const double *__A, vDSP_Stride __IA, const double *__B, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C9B0](__A, __IA, __B, __C, __IC, __N);
}

void vDSP_vthres(const float *__A, vDSP_Stride __IA, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C9B8](__A, __IA, __B, __C, __IC, __N);
}

void vDSP_vthresD(const double *__A, vDSP_Stride __IA, const double *__B, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C9C8](__A, __IA, __B, __C, __IC, __N);
}

void vDSP_vthrsc(const float *__A, vDSP_Stride __IA, const float *__B, const float *__C, float *__D, vDSP_Stride __ID, vDSP_Length __N)
{
  MEMORY[0x1E0C8C9D0](__A, __IA, __B, __C, __D, __ID, __N);
}

void vDSP_vthrscD(const double *__A, vDSP_Stride __IA, const double *__B, const double *__C, double *__D, vDSP_Stride __ID, vDSP_Length __N)
{
  MEMORY[0x1E0C8C9D8](__A, __IA, __B, __C, __D, __ID, __N);
}

void vDSP_vtrapz(const float *__A, vDSP_Stride __IA, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C9F0](__A, __IA, __B, __C, __IC, __N);
}

void vDSP_vtrapzD(const double *__A, vDSP_Stride __IA, const double *__B, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8C9F8](__A, __IA, __B, __C, __IC, __N);
}

void vDSP_ztoc(const DSPSplitComplex *__Z, vDSP_Stride __IZ, DSPComplex *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8CA50](__Z, __IZ, __C, __IC, __N);
}

void vDSP_ztocD(const DSPDoubleSplitComplex *__Z, vDSP_Stride __IZ, DSPDoubleComplex *__C, vDSP_Stride __IC, vDSP_Length __N)
{
  MEMORY[0x1E0C8CA58](__Z, __IZ, __C, __IC, __N);
}

vImage_Error vImageAffineWarpCG_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_CGAffineTransform *transform, Pixel_8 backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CB40](src, dest, tempBuffer, transform, backColor, *(_QWORD *)&flags);
}

vImage_Error vImageAffineWarpCG_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_CGAffineTransform *transform, Pixel_F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CB48](src, dest, tempBuffer, transform, *(_QWORD *)&flags, backColor);
}

vImage_Error vImageAffineWarpD_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_AffineTransform_Double *transform, const Pixel_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CB60](src, dest, tempBuffer, transform, backColor, *(_QWORD *)&flags);
}

vImage_Error vImageAlphaBlend_ARGB8888(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CB98](srcTop, srcBottom, dest, *(_QWORD *)&flags);
}

vImage_Error vImageAlphaBlend_ARGBFFFF(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CBA0](srcTop, srcBottom, dest, *(_QWORD *)&flags);
}

vImage_Error vImageAlphaBlend_NonpremultipliedToPremultiplied_ARGB8888(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CBA8](srcTop, srcBottom, dest, *(_QWORD *)&flags);
}

vImage_Error vImageAlphaBlend_NonpremultipliedToPremultiplied_ARGBFFFF(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CBB0](srcTop, srcBottom, dest, *(_QWORD *)&flags);
}

vImage_Error vImageBoxConvolve_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, uint32_t kernel_height, uint32_t kernel_width, const Pixel_8888 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CBB8](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width, backgroundColor);
}

vImage_Error vImageBoxConvolve_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, uint32_t kernel_height, uint32_t kernel_width, Pixel_8 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CBC8](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width, backgroundColor);
}

vImage_Error vImageBuffer_CopyToCVPixelBuffer(const vImage_Buffer *buffer, const vImage_CGImageFormat *bufferFormat, CVPixelBufferRef cvPixelBuffer, vImageCVImageFormatRef cvImageFormat, const CGFloat *backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CBF0](buffer, bufferFormat, cvPixelBuffer, cvImageFormat, backgroundColor, *(_QWORD *)&flags);
}

CGSize vImageBuffer_GetSize(const vImage_Buffer *buf)
{
  double v1;
  double v2;
  CGSize result;

  MEMORY[0x1E0C8CBF8](buf);
  result.height = v2;
  result.width = v1;
  return result;
}

vImage_Error vImageBuffer_Init(vImage_Buffer *buf, vImagePixelCount height, vImagePixelCount width, uint32_t pixelBits, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CC00](buf, height, width, *(_QWORD *)&pixelBits, *(_QWORD *)&flags);
}

vImage_Error vImageBuffer_InitForCopyFromCVPixelBuffer(vImage_Buffer *buffers, const vImageConverterRef converter, const CVPixelBufferRef pixelBuffer, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CC10](buffers, converter, pixelBuffer, *(_QWORD *)&flags);
}

vImage_Error vImageBuffer_InitWithCGImage(vImage_Buffer *buf, vImage_CGImageFormat *format, const CGFloat *backgroundColor, CGImageRef image, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CC28](buf, format, backgroundColor, image, *(_QWORD *)&flags);
}

vImage_Error vImageBuffer_InitWithCVPixelBuffer(vImage_Buffer *buffer, vImage_CGImageFormat *desiredFormat, CVPixelBufferRef cvPixelBuffer, vImageCVImageFormatRef cvImageFormat, const CGFloat *backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CC30](buffer, desiredFormat, cvPixelBuffer, cvImageFormat, backgroundColor, *(_QWORD *)&flags);
}

uint32_t vImageCGImageFormat_GetComponentCount(const vImage_CGImageFormat *format)
{
  return MEMORY[0x1E0C8CC78](format);
}

Boolean vImageCGImageFormat_IsEqual(const vImage_CGImageFormat *f1, const vImage_CGImageFormat *f2)
{
  return MEMORY[0x1E0C8CC80](f1, f2);
}

vImageCVImageFormatRef vImageCVImageFormat_Create(uint32_t imageFormatType, const vImage_ARGBToYpCbCrMatrix *matrix, CFStringRef cvImageBufferChromaLocation, CGColorSpaceRef baseColorspace, int alphaIsOneHint)
{
  return (vImageCVImageFormatRef)MEMORY[0x1E0C8CC88](*(_QWORD *)&imageFormatType, matrix, cvImageBufferChromaLocation, baseColorspace, *(_QWORD *)&alphaIsOneHint);
}

vImageCVImageFormatRef vImageCVImageFormat_CreateWithCVPixelBuffer(CVPixelBufferRef buffer)
{
  return (vImageCVImageFormatRef)MEMORY[0x1E0C8CC90](buffer);
}

int vImageCVImageFormat_GetAlphaHint(vImageConstCVImageFormatRef format)
{
  return MEMORY[0x1E0C8CC98](format);
}

uint32_t vImageCVImageFormat_GetChannelCount(vImageConstCVImageFormatRef format)
{
  return MEMORY[0x1E0C8CCA0](format);
}

const vImageChannelDescription *__cdecl vImageCVImageFormat_GetChannelDescription(vImageConstCVImageFormatRef format, vImageBufferTypeCode type)
{
  return (const vImageChannelDescription *)MEMORY[0x1E0C8CCA8](format, *(_QWORD *)&type);
}

const vImageBufferTypeCode *__cdecl vImageCVImageFormat_GetChannelNames(vImageConstCVImageFormatRef format)
{
  return (const vImageBufferTypeCode *)MEMORY[0x1E0C8CCB0](format);
}

CFStringRef vImageCVImageFormat_GetChromaSiting(vImageConstCVImageFormatRef format)
{
  return (CFStringRef)MEMORY[0x1E0C8CCB8](format);
}

CGColorSpaceRef vImageCVImageFormat_GetColorSpace(vImageConstCVImageFormatRef format)
{
  return (CGColorSpaceRef)MEMORY[0x1E0C8CCC0](format);
}

uint32_t vImageCVImageFormat_GetFormatCode(vImageConstCVImageFormatRef format)
{
  return MEMORY[0x1E0C8CCC8](format);
}

vImage_Error vImageCVImageFormat_SetAlphaHint(vImageCVImageFormatRef format, int alphaIsOne)
{
  return MEMORY[0x1E0C8CCD8](format, *(_QWORD *)&alphaIsOne);
}

vImage_Error vImageCVImageFormat_SetChromaSiting(vImageCVImageFormatRef format, CFStringRef siting)
{
  return MEMORY[0x1E0C8CCE0](format, siting);
}

vImage_Error vImageCVImageFormat_SetColorSpace(vImageCVImageFormatRef format, CGColorSpaceRef colorspace)
{
  return MEMORY[0x1E0C8CCE8](format, colorspace);
}

vImage_Error vImageConvert_16UToF(const vImage_Buffer *src, const vImage_Buffer *dest, float offset, float scale, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CD38](src, dest, *(_QWORD *)&flags, offset, scale);
}

vImage_Error vImageConvert_16Uto16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CD48](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_420Yp8_CbCr8ToARGB8888(const vImage_Buffer *srcYp, const vImage_Buffer *srcCbCr, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], const uint8_t alpha, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CD70](srcYp, srcCbCr, dest, info, permuteMap, alpha, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_ARGB16UToARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const uint8_t permuteMap[4], uint8_t copyMask, const Pixel_8888 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CE08](src, dest, permuteMap, copyMask, backgroundColor, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_ARGB16UtoPlanar16U(const vImage_Buffer *argbSrc, const vImage_Buffer *aDest, const vImage_Buffer *rDest, const vImage_Buffer *gDest, const vImage_Buffer *bDest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CE20](argbSrc, aDest, rDest, gDest, bDest, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_ARGB8888ToARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, const uint8_t permuteMap[4], uint8_t copyMask, const Pixel_ARGB_16U backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CE88](src, dest, permuteMap, copyMask, backgroundColor, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_ARGB8888toPlanar8(const vImage_Buffer *srcARGB, const vImage_Buffer *destA, const vImage_Buffer *destR, const vImage_Buffer *destG, const vImage_Buffer *destB, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CEA0](srcARGB, destA, destR, destG, destB, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_ARGB8888toPlanarF(const vImage_Buffer *src, const vImage_Buffer *alpha, const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, const Pixel_FFFF maxFloat, const Pixel_FFFF minFloat, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CEB0](src, alpha, red, green, blue, maxFloat, minFloat, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_ARGB8888toRGB888(const vImage_Buffer *a1, const vImage_Buffer *a2, vImage_Flags a3)
{
  return MEMORY[0x1E0C8CEB8](a1, a2, *(_QWORD *)&a3);
}

vImage_Error vImageConvert_ARGBFFFFtoARGB8888_dithered(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_FFFF maxFloat, const Pixel_FFFF minFloat, int dither, const uint8_t permuteMap[4], vImage_Flags flags)
{
  return MEMORY[0x1E0C8CEC0](src, dest, maxFloat, minFloat, *(_QWORD *)&dither, permuteMap, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_ARGBFFFFtoPlanar8(const vImage_Buffer *src, const vImage_Buffer *alpha, const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, const Pixel_FFFF maxFloat, const Pixel_FFFF minFloat, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CEC8](src, alpha, red, green, blue, maxFloat, minFloat, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_ARGBFFFFtoPlanarF(const vImage_Buffer *srcARGB, const vImage_Buffer *destA, const vImage_Buffer *destR, const vImage_Buffer *destG, const vImage_Buffer *destB, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CED8](srcARGB, destA, destR, destG, destB, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_ARGBFFFFtoRGBFFF(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CEE0](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_AnyToAny(const vImageConverterRef converter, const vImage_Buffer *srcs, const vImage_Buffer *dests, void *tempBuffer, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CEF0](converter, srcs, dests, tempBuffer, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_ChunkyToPlanar8(const void *srcChannels[], const vImage_Buffer *destPlanarBuffers[], unsigned int channelCount, size_t srcStrideBytes, vImagePixelCount srcWidth, vImagePixelCount srcHeight, size_t srcRowBytes, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CF08](srcChannels, destPlanarBuffers, *(_QWORD *)&channelCount, srcStrideBytes, srcWidth, srcHeight, srcRowBytes, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_ChunkyToPlanarF(const void *srcChannels[], const vImage_Buffer *destPlanarBuffers[], unsigned int channelCount, size_t srcStrideBytes, vImagePixelCount srcWidth, vImagePixelCount srcHeight, size_t srcRowBytes, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CF10](srcChannels, destPlanarBuffers, *(_QWORD *)&channelCount, srcStrideBytes, srcWidth, srcHeight, srcRowBytes, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_FTo16U(const vImage_Buffer *src, const vImage_Buffer *dest, float offset, float scale, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CF18](src, dest, *(_QWORD *)&flags, offset, scale);
}

vImage_Error vImageConvert_Planar16FtoPlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CF28](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_Planar16UtoARGB16U(const vImage_Buffer *aSrc, const vImage_Buffer *rSrc, const vImage_Buffer *gSrc, const vImage_Buffer *bSrc, const vImage_Buffer *argbDest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CF38](aSrc, rSrc, gSrc, bSrc, argbDest, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_Planar8toARGB8888(const vImage_Buffer *srcA, const vImage_Buffer *srcR, const vImage_Buffer *srcG, const vImage_Buffer *srcB, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CF80](srcA, srcR, srcG, srcB, dest, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_Planar8toPlanar16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CF88](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_Planar8toPlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, Pixel_F maxFloat, Pixel_F minFloat, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CF90](src, dest, *(_QWORD *)&flags, maxFloat, minFloat);
}

vImage_Error vImageConvert_Planar8toRGB888(const vImage_Buffer *planarRed, const vImage_Buffer *planarGreen, const vImage_Buffer *planarBlue, const vImage_Buffer *rgbDest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CF98](planarRed, planarGreen, planarBlue, rgbDest, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_PlanarFToARGB8888(const vImage_Buffer *alpha, const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, const vImage_Buffer *dest, const Pixel_FFFF maxFloat, const Pixel_FFFF minFloat, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CFA0](alpha, red, green, blue, dest, maxFloat, minFloat, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_PlanarFtoARGBFFFF(const vImage_Buffer *srcA, const vImage_Buffer *srcR, const vImage_Buffer *srcG, const vImage_Buffer *srcB, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CFB0](srcA, srcR, srcG, srcB, dest, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_PlanarFtoPlanar16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CFC0](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_PlanarFtoPlanar8(const vImage_Buffer *src, const vImage_Buffer *dest, Pixel_F maxFloat, Pixel_F minFloat, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CFC8](src, dest, *(_QWORD *)&flags, maxFloat, minFloat);
}

vImage_Error vImageConvert_PlanarFtoRGBFFF(const vImage_Buffer *planarRed, const vImage_Buffer *planarGreen, const vImage_Buffer *planarBlue, const vImage_Buffer *rgbDest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CFD0](planarRed, planarGreen, planarBlue, rgbDest, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_PlanarToChunky8(const vImage_Buffer *srcPlanarBuffers[], void *destChannels[], unsigned int channelCount, size_t destStrideBytes, vImagePixelCount destWidth, vImagePixelCount destHeight, size_t destRowBytes, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CFD8](srcPlanarBuffers, destChannels, *(_QWORD *)&channelCount, destStrideBytes, destWidth, destHeight, destRowBytes, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_PlanarToChunkyF(const vImage_Buffer *srcPlanarBuffers[], void *destChannels[], unsigned int channelCount, size_t destStrideBytes, vImagePixelCount destWidth, vImagePixelCount destHeight, size_t destRowBytes, vImage_Flags flags)
{
  return MEMORY[0x1E0C8CFE0](srcPlanarBuffers, destChannels, *(_QWORD *)&channelCount, destStrideBytes, destWidth, destHeight, destRowBytes, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_RGB888toPlanar8(const vImage_Buffer *rgbSrc, const vImage_Buffer *redDest, const vImage_Buffer *greenDest, const vImage_Buffer *blueDest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D040](rgbSrc, redDest, greenDest, blueDest, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_RGBA8888toRGB888(const vImage_Buffer *a1, const vImage_Buffer *a2, vImage_Flags a3)
{
  return MEMORY[0x1E0C8D070](a1, a2, *(_QWORD *)&a3);
}

vImage_Error vImageConvert_RGBAFFFFtoRGBFFF(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D078](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_RGBFFFtoPlanarF(const vImage_Buffer *rgbSrc, const vImage_Buffer *redDest, const vImage_Buffer *greenDest, const vImage_Buffer *blueDest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D080](rgbSrc, redDest, greenDest, blueDest, *(_QWORD *)&flags);
}

vImage_Error vImageConvert_RGBFFFtoRGB888_dithered(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_F maxFloat[3], const Pixel_F minFloat[3], int dither, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D088](src, dest, maxFloat, minFloat, *(_QWORD *)&dither, *(_QWORD *)&flags);
}

vImageConverterRef vImageConverter_CreateForCGToCVImageFormat(const vImage_CGImageFormat *srcFormat, vImageCVImageFormatRef destFormat, const CGFloat *backgroundColor, vImage_Flags flags, vImage_Error *error)
{
  return (vImageConverterRef)MEMORY[0x1E0C8D108](srcFormat, destFormat, backgroundColor, *(_QWORD *)&flags, error);
}

vImageConverterRef vImageConverter_CreateForCVToCGImageFormat(vImageCVImageFormatRef srcFormat, const vImage_CGImageFormat *destFormat, const CGFloat *backgroundColor, vImage_Flags flags, vImage_Error *error)
{
  return (vImageConverterRef)MEMORY[0x1E0C8D110](srcFormat, destFormat, backgroundColor, *(_QWORD *)&flags, error);
}

vImageConverterRef vImageConverter_CreateWithCGColorConversionInfo(CGColorConversionInfoRef colorConversionInfoRef, const vImage_CGImageFormat *sFormat, const vImage_CGImageFormat *dFormat, const CGFloat *bg, vImage_Flags flags, vImage_Error *error)
{
  return (vImageConverterRef)MEMORY[0x1E0C8D118](colorConversionInfoRef, sFormat, dFormat, bg, *(_QWORD *)&flags, error);
}

vImageConverterRef vImageConverter_CreateWithCGImageFormat(const vImage_CGImageFormat *srcFormat, const vImage_CGImageFormat *destFormat, const CGFloat *backgroundColor, vImage_Flags flags, vImage_Error *error)
{
  return (vImageConverterRef)MEMORY[0x1E0C8D120](srcFormat, destFormat, backgroundColor, *(_QWORD *)&flags, error);
}

vImage_Error vImageConverter_MustOperateOutOfPlace(const vImageConverterRef converter, const vImage_Buffer *srcs, const vImage_Buffer *dests, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D150](converter, srcs, dests, *(_QWORD *)&flags);
}

vImage_Error vImageConvolveFloatKernel_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernelHeight, uint32_t kernelWidth, float bias, const Pixel_8888 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D168](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(_QWORD *)&kernelHeight, *(_QWORD *)&kernelWidth, bias);
}

vImage_Error vImageConvolveMultiKernel_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const int16_t *kernels[4], uint32_t kernel_height, uint32_t kernel_width, const int32_t divisors[4], const int32_t biases[4], const Pixel_8888 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D170](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernels, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width);
}

vImage_Error vImageConvolveWithBias_ARGB16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, float bias, const Pixel_ARGB_16F backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D178](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width, bias);
}

vImage_Error vImageConvolveWithBias_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const int16_t *kernel, uint32_t kernel_height, uint32_t kernel_width, int32_t divisor, int32_t bias, const Pixel_8888 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D180](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width);
}

vImage_Error vImageConvolveWithBias_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, float bias, const Pixel_FFFF backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D188](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width, bias);
}

vImage_Error vImageConvolveWithBias_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, float bias, Pixel_16F backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D190](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width, bias);
}

vImage_Error vImageConvolveWithBias_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const int16_t *kernel, uint32_t kernel_height, uint32_t kernel_width, int32_t divisor, int32_t bias, Pixel_8 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D198](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width);
}

vImage_Error vImageConvolveWithBias_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, float bias, Pixel_F backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D1A0](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width, bias, backgroundColor);
}

vImage_Error vImageConvolve_ARGB16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, const Pixel_ARGB_16F backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D1A8](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width);
}

vImage_Error vImageConvolve_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const int16_t *kernel, uint32_t kernel_height, uint32_t kernel_width, int32_t divisor, const Pixel_8888 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D1B0](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width);
}

vImage_Error vImageConvolve_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, const Pixel_FFFF backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D1B8](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width);
}

vImage_Error vImageConvolve_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, Pixel_16F backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D1C0](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width);
}

vImage_Error vImageConvolve_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const int16_t *kernel, uint32_t kernel_height, uint32_t kernel_width, int32_t divisor, Pixel_8 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D1D0](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width);
}

vImage_Error vImageConvolve_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, Pixel_F backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D1D8](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width, backgroundColor);
}

vImage_Error vImageCopyBuffer(const vImage_Buffer *src, const vImage_Buffer *dest, size_t pixelSize, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D1E0](src, dest, pixelSize, *(_QWORD *)&flags);
}

CGImageRef vImageCreateCGImageFromBuffer(const vImage_Buffer *buf, const vImage_CGImageFormat *format, void (__cdecl *callback)(void *, void *), void *userData, vImage_Flags flags, vImage_Error *error)
{
  return (CGImageRef)MEMORY[0x1E0C8D1E8](buf, format, callback, userData, *(_QWORD *)&flags, error);
}

GammaFunction vImageCreateGammaFunction(float gamma, int gamma_type, vImage_Flags flags)
{
  return (GammaFunction)MEMORY[0x1E0C8D1F0](*(_QWORD *)&gamma_type, *(_QWORD *)&flags, gamma);
}

vImage_Error vImageDilate_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const unsigned __int8 *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D200](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, *(_QWORD *)&flags);
}

vImage_Error vImageDilate_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D208](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, *(_QWORD *)&flags);
}

vImage_Error vImageDilate_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const unsigned __int8 *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D210](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, *(_QWORD *)&flags);
}

vImage_Error vImageDilate_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D218](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, *(_QWORD *)&flags);
}

vImage_Error vImageErode_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const unsigned __int8 *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D260](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, *(_QWORD *)&flags);
}

vImage_Error vImageErode_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D268](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, *(_QWORD *)&flags);
}

vImage_Error vImageErode_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const unsigned __int8 *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D270](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, *(_QWORD *)&flags);
}

vImage_Error vImageErode_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D278](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, *(_QWORD *)&flags);
}

vImage_Error vImageFlatten_ARGB8888ToRGB888(const vImage_Buffer *a1, const vImage_Buffer *a2, const Pixel_8888 a3, BOOL a4, vImage_Flags a5)
{
  return MEMORY[0x1E0C8D298](a1, a2, a3, a4, *(_QWORD *)&a5);
}

vImage_Error vImageFlatten_ARGBFFFFToRGBFFF(const vImage_Buffer *a1, const vImage_Buffer *a2, const Pixel_FFFF a3, BOOL a4, vImage_Flags a5)
{
  return MEMORY[0x1E0C8D2A0](a1, a2, a3, a4, *(_QWORD *)&a5);
}

vImage_Error vImageFlatten_RGBA8888ToRGB888(const vImage_Buffer *a1, const vImage_Buffer *a2, const Pixel_8888 a3, BOOL a4, vImage_Flags a5)
{
  return MEMORY[0x1E0C8D2A8](a1, a2, a3, a4, *(_QWORD *)&a5);
}

vImage_Error vImageFlatten_RGBAFFFFToRGBFFF(const vImage_Buffer *a1, const vImage_Buffer *a2, const Pixel_FFFF a3, BOOL a4, vImage_Flags a5)
{
  return MEMORY[0x1E0C8D2B0](a1, a2, a3, a4, *(_QWORD *)&a5);
}

vImage_Error vImageFloodFill_ARGB16U(const vImage_Buffer *srcDest, void *tempBuffer, vImagePixelCount seedX, vImagePixelCount seedY, Pixel_ARGB_16U newValue, int connectivity, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D2B8](srcDest, tempBuffer, seedX, seedY, newValue, *(_QWORD *)&connectivity, *(_QWORD *)&flags);
}

vImage_Error vImageFloodFill_ARGB8888(const vImage_Buffer *srcDest, void *tempBuffer, vImagePixelCount seedX, vImagePixelCount seedY, Pixel_8888 newValue, int connectivity, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D2C0](srcDest, tempBuffer, seedX, seedY, newValue, *(_QWORD *)&connectivity, *(_QWORD *)&flags);
}

vImage_Error vImageFloodFill_Planar16U(const vImage_Buffer *srcDest, void *tempBuffer, vImagePixelCount seedX, vImagePixelCount seedY, Pixel_16U newValue, int connectivity, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D2C8](srcDest, tempBuffer, seedX, seedY, newValue, *(_QWORD *)&connectivity, *(_QWORD *)&flags);
}

vImage_Error vImageFloodFill_Planar8(const vImage_Buffer *srcDest, void *tempBuffer, vImagePixelCount seedX, vImagePixelCount seedY, Pixel_8 newValue, int connectivity, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D2D0](srcDest, tempBuffer, seedX, seedY, newValue, *(_QWORD *)&connectivity, *(_QWORD *)&flags);
}

vImage_Error vImageGamma_Planar8toPlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, const GammaFunction gamma, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D2D8](src, dest, gamma, *(_QWORD *)&flags);
}

vImage_Error vImageGamma_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, const GammaFunction gamma, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D2E0](src, dest, gamma, *(_QWORD *)&flags);
}

vImage_Error vImageGetPerspectiveWarp(const float srcPoints[4][2], const float destPoints[4][2], vImage_PerpsectiveTransform *transform, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D2E8](srcPoints, destPoints, transform, *(_QWORD *)&flags);
}

vImage_Error vImageHistogramCalculation_ARGB8888(const vImage_Buffer *src, vImagePixelCount *histogram[4], vImage_Flags flags)
{
  return MEMORY[0x1E0C8D2F0](src, histogram, *(_QWORD *)&flags);
}

vImage_Error vImageHistogramCalculation_ARGBFFFF(const vImage_Buffer *src, vImagePixelCount *histogram[4], unsigned int histogram_entries, Pixel_F minVal, Pixel_F maxVal, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D2F8](src, histogram, *(_QWORD *)&histogram_entries, *(_QWORD *)&flags, minVal, maxVal);
}

vImage_Error vImageHistogramCalculation_Planar8(const vImage_Buffer *src, vImagePixelCount *histogram, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D300](src, histogram, *(_QWORD *)&flags);
}

vImage_Error vImageHistogramCalculation_PlanarF(const vImage_Buffer *src, vImagePixelCount *histogram, unsigned int histogram_entries, Pixel_F minVal, Pixel_F maxVal, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D308](src, histogram, *(_QWORD *)&histogram_entries, *(_QWORD *)&flags, minVal, maxVal);
}

vImage_Error vImageHistogramSpecification_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const vImagePixelCount *desired_histogram[4], vImage_Flags flags)
{
  return MEMORY[0x1E0C8D310](src, dest, desired_histogram, *(_QWORD *)&flags);
}

vImage_Error vImageHistogramSpecification_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImagePixelCount *desired_histogram[4], unsigned int histogram_entries, Pixel_F minVal, Pixel_F maxVal, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D318](src, dest, tempBuffer, desired_histogram, *(_QWORD *)&histogram_entries, *(_QWORD *)&flags, minVal, maxVal);
}

vImage_Error vImageHistogramSpecification_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, const vImagePixelCount *desired_histogram, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D320](src, dest, desired_histogram, *(_QWORD *)&flags);
}

vImage_Error vImageHistogramSpecification_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImagePixelCount *desired_histogram, unsigned int histogram_entries, Pixel_F minVal, Pixel_F maxVal, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D328](src, dest, tempBuffer, desired_histogram, *(_QWORD *)&histogram_entries, *(_QWORD *)&flags, minVal, maxVal);
}

vImage_Error vImageHorizontalReflect_ARGB16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D330](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageHorizontalReflect_ARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D338](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageHorizontalReflect_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D340](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageHorizontalReflect_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D348](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageHorizontalReflect_CbCr16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D350](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageHorizontalReflect_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D358](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageHorizontalReflect_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D360](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageHorizontalReflect_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D368](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageHorizontalShearD_ARGB16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_ARGB_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D370](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShearD_ARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_ARGB_16U backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D378](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShearD_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_8888 backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D380](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShearD_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_FFFF backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D388](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShearD_CbCr16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_16F16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D390](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShearD_CbCr16U(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_16U16U backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D398](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShearD_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D3A0](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShearD_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, Pixel_8 backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D3A8](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShearD_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, Pixel_F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D3B0](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, *(_QWORD *)&flags, xTranslate, shearSlope, backColor);
}

vImage_Error vImageHorizontalShear_CbCr8(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float xTranslate, float shearSlope, ResamplingFilter filter, const Pixel_88 backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D3B8](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShear_Planar16U(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float xTranslate, float shearSlope, ResamplingFilter filter, Pixel_16U backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D3C0](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageInterpolatedLookupTable_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_F *table, vImagePixelCount tableEntries, float maxFloat, float minFloat, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D3C8](src, dest, table, tableEntries, *(_QWORD *)&flags, maxFloat, minFloat);
}

vImage_Error vImageLookupTable_Planar16(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_16U table[65536], vImage_Flags flags)
{
  return MEMORY[0x1E0C8D3D0](src, dest, table, *(_QWORD *)&flags);
}

vImage_Error vImageLookupTable_Planar8toPlanar24(const vImage_Buffer *src, const vImage_Buffer *dest, const uint32_t table[256], vImage_Flags flags)
{
  return MEMORY[0x1E0C8D3E0](src, dest, table, *(_QWORD *)&flags);
}

vImage_Error vImageLookupTable_PlanarFtoPlanar8(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_8 table[4096], vImage_Flags flags)
{
  return MEMORY[0x1E0C8D3F8](src, dest, table, *(_QWORD *)&flags);
}

vImage_Error vImageMatrixMultiply_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const int16_t matrix[16], int32_t divisor, const int16_t *pre_bias, const int32_t *post_bias, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D410](src, dest, matrix, *(_QWORD *)&divisor, pre_bias, post_bias, *(_QWORD *)&flags);
}

vImage_Error vImageMatrixMultiply_ARGB8888ToPlanar8(const vImage_Buffer *src, const vImage_Buffer *dest, const int16_t matrix[4], int32_t divisor, const int16_t pre_bias[4], int32_t post_bias, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D418](src, dest, matrix, *(_QWORD *)&divisor, pre_bias, *(_QWORD *)&post_bias, *(_QWORD *)&flags);
}

vImage_Error vImageMatrixMultiply_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, const float matrix[16], const float *pre_bias, const float *post_bias, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D420](src, dest, matrix, pre_bias, post_bias, *(_QWORD *)&flags);
}

vImage_Error vImageMatrixMultiply_ARGBFFFFToPlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, const float matrix[4], const float pre_bias[4], float post_bias, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D428](src, dest, matrix, pre_bias, *(_QWORD *)&flags, post_bias);
}

vImage_Error vImageMatrixMultiply_Planar8(const vImage_Buffer *srcs[], const vImage_Buffer *dests[], uint32_t src_planes, uint32_t dest_planes, const int16_t matrix[], int32_t divisor, const int16_t *pre_bias, const int32_t *post_bias, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D430](srcs, dests, *(_QWORD *)&src_planes, *(_QWORD *)&dest_planes, matrix, *(_QWORD *)&divisor, pre_bias, post_bias);
}

vImage_Error vImageMatrixMultiply_PlanarF(const vImage_Buffer *srcs[], const vImage_Buffer *dests[], uint32_t src_planes, uint32_t dest_planes, const float matrix[], const float *pre_bias, const float *post_bias, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D438](srcs, dests, *(_QWORD *)&src_planes, *(_QWORD *)&dest_planes, matrix, pre_bias, post_bias, *(_QWORD *)&flags);
}

vImage_Error vImageMax_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D440](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel_height, kernel_width, *(_QWORD *)&flags);
}

vImage_Error vImageMax_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D448](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel_height, kernel_width, *(_QWORD *)&flags);
}

vImage_Error vImageMax_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D450](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel_height, kernel_width, *(_QWORD *)&flags);
}

vImage_Error vImageMax_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D458](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel_height, kernel_width, *(_QWORD *)&flags);
}

vImage_Error vImageMin_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D460](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel_height, kernel_width, *(_QWORD *)&flags);
}

vImage_Error vImageMin_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D468](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel_height, kernel_width, *(_QWORD *)&flags);
}

vImage_Error vImageMin_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D470](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel_height, kernel_width, *(_QWORD *)&flags);
}

vImage_Error vImageMin_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D478](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel_height, kernel_width, *(_QWORD *)&flags);
}

vImage_Error vImageMultiDimensionalInterpolatedLookupTable_PlanarF(const vImage_Buffer srcs[], const vImage_Buffer dests[], void *tempBuffer, vImage_MultidimensionalTable table, vImage_InterpolationMethod method, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D480](srcs, dests, tempBuffer, table, *(_QWORD *)&method, *(_QWORD *)&flags);
}

vImage_MultidimensionalTable vImageMultidimensionalTable_Create(const uint16_t *tableData, uint32_t numSrcChannels, uint32_t numDestChannels, const uint8_t table_entries_per_dimension[], vImageMDTableUsageHint hint, vImage_Flags flags, vImage_Error *err)
{
  return (vImage_MultidimensionalTable)MEMORY[0x1E0C8D488](tableData, *(_QWORD *)&numSrcChannels, *(_QWORD *)&numDestChannels, table_entries_per_dimension, *(_QWORD *)&hint, *(_QWORD *)&flags, err);
}

vImage_Error vImageMultidimensionalTable_Release(vImage_MultidimensionalTable table)
{
  return MEMORY[0x1E0C8D490](table);
}

vImage_Error vImageOverwriteChannelsWithPixel_ARGB16U(const Pixel_ARGB_16U the_pixel, const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t copyMask, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D498](the_pixel, src, dest, copyMask, *(_QWORD *)&flags);
}

vImage_Error vImageOverwriteChannelsWithPixel_ARGB8888(const Pixel_8888 the_pixel, const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t copyMask, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D4A0](the_pixel, src, dest, copyMask, *(_QWORD *)&flags);
}

vImage_Error vImageOverwriteChannelsWithPixel_ARGBFFFF(const Pixel_FFFF the_pixel, const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t copyMask, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D4A8](the_pixel, src, dest, copyMask, *(_QWORD *)&flags);
}

vImage_Error vImageOverwriteChannelsWithScalar_ARGB8888(Pixel_8 scalar, const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t copyMask, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D4B0](scalar, src, dest, copyMask, *(_QWORD *)&flags);
}

vImage_Error vImageOverwriteChannelsWithScalar_ARGBFFFF(Pixel_F scalar, const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t copyMask, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D4B8](src, dest, copyMask, *(_QWORD *)&flags, scalar);
}

vImage_Error vImageOverwriteChannelsWithScalar_Planar16F(Pixel_16F scalar, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D4C0](scalar, dest, *(_QWORD *)&flags);
}

vImage_Error vImageOverwriteChannelsWithScalar_Planar8(Pixel_8 scalar, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D4D0](scalar, dest, *(_QWORD *)&flags);
}

vImage_Error vImageOverwriteChannelsWithScalar_PlanarF(Pixel_F scalar, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D4D8](dest, *(_QWORD *)&flags, scalar);
}

vImage_Error vImagePermuteChannels_RGB888(const vImage_Buffer *src, const vImage_Buffer *dest, const uint8_t permuteMap[3], vImage_Flags flags)
{
  return MEMORY[0x1E0C8D538](src, dest, permuteMap, *(_QWORD *)&flags);
}

vImage_Error vImagePerspectiveWarp_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_PerpsectiveTransform *transform, vImage_WarpInterpolation interpolation, Pixel_8888 backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D550](src, dest, tempBuffer, transform, *(_QWORD *)&interpolation, backColor, *(_QWORD *)&flags);
}

vImage_Error vImagePerspectiveWarp_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_PerpsectiveTransform *transform, vImage_WarpInterpolation interpolation, Pixel_8 backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D568](src, dest, tempBuffer, transform, *(_QWORD *)&interpolation, backColor, *(_QWORD *)&flags);
}

vImage_Error vImagePiecewiseGamma_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, const float exponentialCoeffs[3], const float gamma, const float linearCoeffs[2], const Pixel_8 boundary, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D570](src, dest, exponentialCoeffs, linearCoeffs, boundary, *(_QWORD *)&flags, gamma);
}

vImage_Error vImagePiecewiseGamma_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, const float exponentialCoeffs[3], const float gamma, const float linearCoeffs[2], const float boundary, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D578](src, dest, exponentialCoeffs, linearCoeffs, *(_QWORD *)&flags, gamma, boundary);
}

vImage_Error vImagePremultipliedAlphaBlendDarken_RGBA8888(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D598](srcTop, srcBottom, dest, *(_QWORD *)&flags);
}

vImage_Error vImagePremultipliedAlphaBlendLighten_RGBA8888(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D5A0](srcTop, srcBottom, dest, *(_QWORD *)&flags);
}

vImage_Error vImagePremultipliedAlphaBlendMultiply_RGBA8888(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D5A8](srcTop, srcBottom, dest, *(_QWORD *)&flags);
}

vImage_Error vImagePremultipliedAlphaBlendScreen_RGBA8888(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D5B0](srcTop, srcBottom, dest, *(_QWORD *)&flags);
}

vImage_Error vImagePremultipliedAlphaBlend_ARGB8888(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D5C8](srcTop, srcBottom, dest, *(_QWORD *)&flags);
}

vImage_Error vImagePremultipliedAlphaBlend_ARGBFFFF(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D5D0](srcTop, srcBottom, dest, *(_QWORD *)&flags);
}

vImage_Error vImagePremultipliedConstAlphaBlend_ARGB8888(const vImage_Buffer *srcTop, Pixel_8 constAlpha, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D5E8](srcTop, constAlpha, srcBottom, dest, *(_QWORD *)&flags);
}

vImage_Error vImagePremultipliedConstAlphaBlend_ARGBFFFF(const vImage_Buffer *srcTop, Pixel_F constAlpha, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D5F0](srcTop, srcBottom, dest, *(_QWORD *)&flags, constAlpha);
}

vImage_Error vImageRotate90_ARGB16F(const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, const Pixel_ARGB_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D648](src, dest, rotationConstant, backColor, *(_QWORD *)&flags);
}

vImage_Error vImageRotate90_ARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, const Pixel_ARGB_16U backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D650](src, dest, rotationConstant, backColor, *(_QWORD *)&flags);
}

vImage_Error vImageRotate90_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, const Pixel_8888 backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D658](src, dest, rotationConstant, backColor, *(_QWORD *)&flags);
}

vImage_Error vImageRotate90_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, const Pixel_FFFF backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D668](src, dest, rotationConstant, backColor, *(_QWORD *)&flags);
}

vImage_Error vImageRotate90_CbCr16F(const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, const Pixel_16F16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D670](src, dest, rotationConstant, backColor, *(_QWORD *)&flags);
}

vImage_Error vImageRotate90_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, const Pixel_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D678](src, dest, rotationConstant, backColor, *(_QWORD *)&flags);
}

vImage_Error vImageRotate90_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, Pixel_8 backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D688](src, dest, rotationConstant, backColor, *(_QWORD *)&flags);
}

vImage_Error vImageRotate90_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, Pixel_F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D690](src, dest, rotationConstant, *(_QWORD *)&flags, backColor);
}

vImage_Error vImageRotate_ARGB16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, const Pixel_ARGB_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D698](src, dest, tempBuffer, backColor, *(_QWORD *)&flags, angleInRadians);
}

vImage_Error vImageRotate_ARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, const Pixel_ARGB_16U backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D6A0](src, dest, tempBuffer, backColor, *(_QWORD *)&flags, angleInRadians);
}

vImage_Error vImageRotate_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, const Pixel_8888 backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D6A8](src, dest, tempBuffer, backColor, *(_QWORD *)&flags, angleInRadians);
}

vImage_Error vImageRotate_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, const Pixel_FFFF backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D6B0](src, dest, tempBuffer, backColor, *(_QWORD *)&flags, angleInRadians);
}

vImage_Error vImageRotate_CbCr16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, const Pixel_16F16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D6B8](src, dest, tempBuffer, backColor, *(_QWORD *)&flags, angleInRadians);
}

vImage_Error vImageRotate_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, const Pixel_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D6C0](src, dest, tempBuffer, backColor, *(_QWORD *)&flags, angleInRadians);
}

vImage_Error vImageRotate_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, Pixel_8 backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D6C8](src, dest, tempBuffer, backColor, *(_QWORD *)&flags, angleInRadians);
}

vImage_Error vImageRotate_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, Pixel_F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D6D0](src, dest, tempBuffer, *(_QWORD *)&flags, angleInRadians, backColor);
}

vImage_Error vImageSepConvolve_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernelX, uint32_t kernelX_width, const float *kernelY, uint32_t kernelY_width, float bias, const Pixel_8888 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D760](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernelX, *(_QWORD *)&kernelX_width, kernelY, bias);
}

vImage_Error vImageSepConvolve_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernelX, uint32_t kernelX_width, const float *kernelY, uint32_t kernelY_width, float bias, Pixel_16F backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D768](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernelX, *(_QWORD *)&kernelX_width, kernelY, bias);
}

vImage_Error vImageSepConvolve_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernelX, uint32_t kernelX_width, const float *kernelY, uint32_t kernelY_width, float bias, Pixel_16U backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D770](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernelX, *(_QWORD *)&kernelX_width, kernelY, bias);
}

vImage_Error vImageSepConvolve_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernelX, uint32_t kernelX_width, const float *kernelY, uint32_t kernelY_width, float bias, Pixel_F backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D778](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernelX, *(_QWORD *)&kernelX_width, kernelY, bias, backgroundColor);
}

vImage_Error vImageTableLookUp_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_8 alphaTable[256], const Pixel_8 redTable[256], const Pixel_8 greenTable[256], const Pixel_8 blueTable[256], vImage_Flags flags)
{
  return MEMORY[0x1E0C8D780](src, dest, alphaTable, redTable, greenTable, blueTable, *(_QWORD *)&flags);
}

vImage_Error vImageTentConvolve_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, uint32_t kernel_height, uint32_t kernel_width, const Pixel_8888 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D790](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width, backgroundColor);
}

vImage_Error vImageTentConvolve_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, uint32_t kernel_height, uint32_t kernel_width, Pixel_8 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D798](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, *(_QWORD *)&kernel_height, *(_QWORD *)&kernel_width, backgroundColor);
}

vImage_Error vImageVerticalReflect_ARGB16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D7E8](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageVerticalReflect_ARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D7F0](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageVerticalReflect_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D7F8](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageVerticalReflect_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D800](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageVerticalReflect_CbCr16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D808](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageVerticalReflect_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D810](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageVerticalReflect_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D818](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageVerticalReflect_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D820](src, dest, *(_QWORD *)&flags);
}

vImage_Error vImageVerticalShearD_ARGB16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, const Pixel_ARGB_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D828](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, yTranslate, shearSlope);
}

vImage_Error vImageVerticalShearD_ARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, const Pixel_ARGB_16U backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D830](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, yTranslate, shearSlope);
}

vImage_Error vImageVerticalShearD_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, const Pixel_8888 backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D838](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, yTranslate, shearSlope);
}

vImage_Error vImageVerticalShearD_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, const Pixel_FFFF backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D840](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, yTranslate, shearSlope);
}

vImage_Error vImageVerticalShearD_CbCr16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, const Pixel_16F16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D848](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, yTranslate, shearSlope);
}

vImage_Error vImageVerticalShearD_CbCr16U(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, const Pixel_16U16U backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D850](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, yTranslate, shearSlope);
}

vImage_Error vImageVerticalShearD_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, const Pixel_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D858](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, yTranslate, shearSlope);
}

vImage_Error vImageVerticalShearD_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, Pixel_8 backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D860](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, yTranslate, shearSlope);
}

vImage_Error vImageVerticalShearD_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, Pixel_F backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D868](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, *(_QWORD *)&flags, yTranslate, shearSlope, backColor);
}

vImage_Error vImageVerticalShear_CbCr8(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float yTranslate, float shearSlope, ResamplingFilter filter, const Pixel_88 backColor, vImage_Flags flags)
{
  return MEMORY[0x1E0C8D870](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(_QWORD *)&flags, yTranslate, shearSlope);
}

